@article{rayyan-727967003,
  title={Systematic literature reviews in software engineering - A systematic literature review},
  year={2009},
  journal={Information and Software Technology},
  volume={51},
  number={1},
  pages={7-15},
  author={Kitchenham, Barbara and Pearl Brereton, O. and Budgen, David and Turner, Mark and Bailey, John and Linkman, Stephen},
  url={http://dx.doi.org/10.1016/j.infsof.2008.09.009},
  keywords={~guide, systematic review, Cost estimation, Evidence-based software engineering, Tertiary study, Systematic literature review, Systematic review quality, Software},
  abstract={Background: In 2004 the concept of evidence-based software engineering (EBSE) was introduced at the ICSE04 conference. Aims: This study assesses the impact of systematic literature reviews (SLRs) which are the recommended EBSE method for aggregating evidence. Method: We used the standard systematic literature review method employing a manual search of 10 journals and 4 conference proceedings. Results: Of 20 relevant studies, eight addressed research trends rather than technique evaluation. Seven SLRs addressed cost estimation. The quality of SLRs was fair with only three scoring less than 2 out of 4. Conclusions: Currently, the topic areas covered by SLRs are limited. European researchers, particularly those at the Simula Laboratory appear to be the leading exponents of systematic literature reviews. The series of cost estimation SLRs demonstrate the potential value of EBSE for synthesising evidence and making it available to practitioners. © 2008 Elsevier B.V. All rights reserved.}
}

@article{rayyan-727967004,
  title={FAST2: An intelligent assistant for finding relevant papers},
  year={2019},
  journal={Expert Systems with Applications},
  issn={09574174},
  volume={120},
  pages={57-71},
  author={Yu, Zhe and Menzies, Tim},
  url={https://linkinghub.elsevier.com/retrieve/pii/S0957417418307413},
  language={en},
  keywords={~tool, tool, Active learning, Text mining, Literature reviews, Relevance feedback, Selection process, Semi-supervised learning, Intelligence},
  abstract={Literature reviews are essential for any researcher trying to keep up to date with the burgeoning software engineering literature. Finding relevant papers can be hard due to the huge amount of candidates provided by search. FAST2 is a novel tool for assisting the researchers to find the next promising paper to read. This paper describes FAST2 and tests it on four large systematic literature review datasets. We show that FAST2 robustly optimizes the human effort to find most (95%) of the relevant software engineering papers while also compensating for the errors made by humans during the review process. The effectiveness of FAST2 can be attributed to three key innovations: (1) a novel way of applying external domain knowledge (a simple two or three keyword search) to guide the initial selection of papers— which helps to find relevant research papers faster with less variances; (2) an estimator of the number of remaining relevant papers yet to be found—which helps the reviewer decide when to stop the review; (3) a novel human error correction algorithm—which corrects a majority of human misclassifications (labeling relevant papers as non-relevant or vice versa) without imposing too much extra human effort.}
}

@article{rayyan-727967005,
  title={Guidelines for conducting systematic mapping studies in software engineering: An update},
  year={2015},
  journal={Information and Software Technology},
  issn={09505849},
  volume={64},
  pages={1-18},
  author={Petersen, Kai and Vakkalanka, Sairam and Kuzniarz, Ludwik},
  url={https://linkinghub.elsevier.com/retrieve/pii/S0950584915000646},
  language={en},
  keywords={~guide, ~scoping review, Software engineering, Guidelines, Systematic mapping studies, Software},
  abstract={Context: Systematic mapping studies are used to structure a research area, while systematic reviews are focused on gathering and synthesizing evidence. The most recent guidelines for systematic mapping are from 2008. Since that time, many suggestions have been made of how to improve systematic literature reviews (SLRs). There is a need to evaluate how researchers conduct the process of systematic mapping and identify how the guidelines should be updated based on the lessons learned from the existing systematic maps and SLR guidelines.     Objective: To identify how the systematic mapping process is conducted (including search, study selection, analysis and presentation of data, etc.); to identify improvement potentials in conducting the systematic mapping process and updating the guidelines accordingly.     Method: We conducted a systematic mapping study of systematic maps, considering some practices of systematic review guidelines as well (in particular in relation to defining the search and to conduct a quality assessment).     Results: In a large number of studies multiple guidelines are used and combined, which leads to different ways in conducting mapping studies. The reason for combining guidelines was that they differed in the recommendations given.     Conclusion: The most frequently followed guidelines are not sufficient alone. Hence, there was a need to provide an update of how to conduct systematic mapping studies. New guidelines have been proposed consolidating existing findings.}
}

@article{rayyan-727967006,
  title={Guidelines for including grey literature and conducting multivocal literature reviews in software engineering},
  year={2019},
  journal={Information and Software Technology},
  issn={09505849},
  volume={106},
  pages={101-121},
  author={Garousi, Vahid and Felderer, Michael and Mäntylä, Mika V.},
  url={https://linkinghub.elsevier.com/retrieve/pii/S0950584918301939},
  language={en},
  keywords={~guide, Evidence-based software engineering, ~grey literature, Systematic literature review, Systematic mapping study, Guidelines, Multivocal literature review, Grey literature, Literature study, Software},
  abstract={Context: A Multivocal Literature Review (MLR) is a form of a Systematic Literature Review (SLR) which includes the grey literature (e.g., blog posts, videos and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). MLRs are useful for both researchers and practitioners since they provide summaries both the state-of-the art and –practice in a given area. MLRs are popular in other fields and have recently started to appear in software engineering (SE). As more MLR studies are conducted and reported, it is important to have a set of guidelines to ensure high quality of MLR processes and their results.     Objective: There are several guidelines to conduct SLR studies in SE. However, several phases of MLRs differ from those of traditional SLRs, for instance with respect to the search process and source quality assessment. Therefore, SLR guidelines are only partially useful for conducting MLR studies. Our goal in this paper is to present guidelines on how to conduct MLR studies in SE.     Method: To develop the MLR guidelines, we benefit from several inputs: (1) existing SLR guidelines in SE, (2), a literature survey of MLR guidelines and experience papers in other fields, and (3) our own experiences in conducting several MLRs in SE. We took the popular SLR guidelines of Kitchenham and Charters as the baseline and extended/adopted them to conduct MLR studies in SE. All derived guidelines are discussed in the context of an already-published MLR in SE as the running example.     Results: The resulting guidelines cover all phases of conducting and reporting MLRs in SE from the planning phase, over conducting the review to the final reporting of the review. In particular, we believe that incorporating and adopting a vast set of experience-based recommendations from MLR guidelines and experience papers in other fields have enabled us to propose a set of guidelines with solid foundations.     Conclusion: Having been developed on the basis of several types of experience and evidence, the provided MLR guidelines will support researchers to effectively and efficiently conduct new MLRs in any area of SE. The authors recommend the researchers to utilize these guidelines in their MLR studies and then share their lessons learned and experiences.}
}

@article{rayyan-727967007,
  title={The pains and gains of microservices: A Systematic grey literature review},
  year={2018},
  journal={Journal of Systems and Software},
  issn={01641212},
  volume={146},
  pages={215-232},
  author={Soldani, Jacopo and Tamburri, Damian Andrew and Van Den Heuvel, Willem-Jan},
  url={https://linkinghub.elsevier.com/retrieve/pii/S0164121218302139},
  language={en},
  keywords={Systematic literature review, Microservices, Microservices design, Microservices development, Microservices operation, Systematic grey literature review},
  abstract={The design, development, and operation of microservices are picking up more and more momentum in the IT industry. At the same time, academic work on the topic is at an early stage, and still on the way to distilling the actual “Pains & Gains” of microservices as an architectural style. Having witnessed this gap, we set forth to systematically analyze the industrial grey literature on microservices, to identify the technical/operational pains and gains of the microservice-based architectural style. We conclude by discussing research directions stemming out from our analysis.}
}

@article{rayyan-727967008,
  title={Guidelines for the search strategy to update systematic literature reviews in software engineering},
  year={2020},
  journal={Information and Software Technology},
  issn={09505849},
  volume={127},
  pages={106366},
  author={Wohlin, Claes and Mendes, Emilia and Felizardo, Katia Romero and Kalinowski, Marcos},
  url={https://linkinghub.elsevier.com/retrieve/pii/S095058491930223X},
  language={en},
  keywords={~guide, Systematic literature reviews, Software engineering, Searching for evidence, Snowballing, Systematic literature review update, Software},
  abstract={Context: Systematic Literature Reviews (SLRs) have been adopted within Software Engineering (SE) for more than a decade to provide meaningful summaries of evidence on several topics. Many of these SLRs are now potentially not fully up-to-date, and there are no standard proposals on how to update SLRs in SE.     Objective: The objective of this paper is to propose guidelines on how to best search for evidence when updating SLRs in SE, and to evaluate these guidelines using an SLR that was not employed during the formulation of the guidelines.     Method: To propose our guidelines, we compare and discuss outcomes from applying different search strategies to identify primary studies in a published SLR, an SLR update, and two replications in the area of effort estimation. These guidelines are then evaluated using an SLR in the area of software ecosystems, its update and a replication.     Results: The use of a single iteration forward snowballing with Google Scholar, and employing as a seed set the original SLR and its primary studies is the most cost-effective way to search for new evidence when updating SLRs. Furthermore, the importance of having more than one researcher involved in the selection of papers when applying the inclusion and exclusion criteria is highlighted through the results.     Conclusions: Our proposed guidelines formulated based upon an effort estimation SLR, its update and two repli- cations, were supported when using an SLR in the area of software ecosystems, its update and a replication. Therefore, we put forward that our guidelines ought to be adopted for updating SLRs in SE.}
}

@article{rayyan-727967009,
  title={What is DevOps?: A Systematic Mapping Study on Definitions and Practices},
  year={2016},
  journal={XP '16 Workshops: Scientific Workshop Proceedings of XP2016},
  issn={978-1-4503-4134-9},
  pages={1-11},
  author={Jabbari, Ramtin and bin Ali, Nauman and Petersen, Kai and Tanveer, Binish},
  url={https://dl.acm.org/doi/10.1145/2962695.2962707},
  language={en},
  publisher={ACM},
  keywords={DevOps definition, DevOps practice, Software development method},
  abstract={Context: DevOps, the combination of Development and Operations, is a new way of thinking in the software engi- neering domain that recently received much attention. Given that DevOps is a new term and novel concept recently in- troduced, no common understanding of what it entails has been achieved yet. Consequently, definitions of DevOps of- ten only represent a part that is relevant to the concept.     Objective:This study aims to characterize DevOps by ex- ploring central components of DevOps definitions reported in the literature, specifying practices explicitly proposed for DevOps and investigating the similarities and differences be- tween DevOps and other existing methods in software engi- neering.     Method: A systematic mapping study was conducted that used six electronic databases: IEEE, ACM, Inspec, Sco- pus, Wiley Online Library and Web of Science.     Result: 44 studies have been selected that report a defi- nition of DevOps, 15 studies explicitly stating DevOps prac- tices, and 15 studies stating how DevOps is related to other existing methods. Papers in some cases stated a combina- tion of a definition, practices, and relations to other meth- ods, the total number of primary studies was 49.     Conclusion: We proposed a definition for DevOps which may overcome inconsistencies over the various existing defi- nitions of individual research studies. In addition, the prac- tices explicitly proposed for DevOps have been presented as well as the relation to other software development methods.}
}

@article{rayyan-727967010,
  title={A systematic review of systematic review process research in software engineering},
  year={2013},
  journal={Information and Software Technology},
  issn={09505849},
  volume={55},
  number={12},
  pages={2049-2075},
  author={Kitchenham, Barbara and Brereton, Pearl},
  url={https://linkinghub.elsevier.com/retrieve/pii/S0950584913001560},
  language={en},
  keywords={~guide, Systematic literature review, Systematic review, Mapping study, Systematic review methodology, Software},
  abstract={Context: Many researchers adopting systematic reviews (SRs) have also published papers discussing problems with the SR methodology and suggestions for improving it. Since guidelines for SRs in software engineering (SE) were last updated in 2007, we believe it is time to investigate whether the guidelines need to be amended in the light of recent research.     Objective: To identify, evaluate and synthesize research published by software engineering researchers concerning their experiences of performing SRs and their proposals for improving the SR process.     Method: We undertook a systematic review of papers reporting experiences of undertaking SRs and/or discussing techniques that could be used to improve the SR process. Studies were classified with respect to the stage in the SR process they addressed, whether they related to education or problems faced by novices and whether they proposed the use of textual analysis tools.     Results: We identified 68 papers reporting 63 unique studies published in SE conferences and journals between 2005 and mid-2012. The most common criticisms of SRs were that they take a long time, that SE digital libraries are not appropriate for broad literature searches and that assessing the quality of empirical studies of different types is difficult.     Conclusion: We recommend removing advice to use structured questions to construct search strings and including advice to use a quasi-gold standard based on a limited manual search to assist the construction of search stings and evaluation of the search process. Textual analysis tools are likely to be useful for inclusion/exclusion decisions and search string construction but require more stringent evaluation. SE researchers would benefit from tools to manage the SR process but existing tools need independent validation. Quality assessment of studies using a variety of empirical methods remains a major problem.}
}

@article{rayyan-727967011,
  title={On the reliability of mapping studies in software engineering},
  year={2013},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={86},
  number={10},
  pages={2594-2610},
  author={Wohlin, Claes and Runeson, Per and da Mota Silveira Neto, Paulo Anselmo and Engström, Emelie and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
  url={http://www.sciencedirect.com/science/article/pii/S0164121213001234},
  language={en},
  keywords={~scoping review, ~review problems, Review of reviews, Software product lines, Software testing, Systematic literature review, Systematic mapping study, Software},
  abstract={Background     Systematic literature reviews and systematic mapping studies are becoming increasingly common in software engineering, and hence it becomes even more important to better understand the reliability of such studies.     Objective     This paper presents a study of two systematic mapping studies to evaluate the reliability of mapping studies and point out some challenges related to this type of study in software engineering.     Method     The research is based on an in-depth case study of two published mapping studies on software product line testing.     Results     We found that despite the fact that the two studies are addressing the same topic, there are quite a number of differences when it comes to papers included and in terms of classification of the papers included in the two mapping studies.     Conclusions     From this we conclude that although mapping studies are important, their reliability cannot simply be taken for granted. Based on the findings we also provide four conjectures that further research has to address to make secondary studies (systematic mapping studies and systematic literature reviews) even more valuable to both researchers and practitioners.}
}

@article{rayyan-727967012,
  title={A systematic literature review of literature reviews in software testing},
  year={2016},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={80},
  pages={195-216},
  author={Garousi, Vahid and Mäntylä, Mika V.},
  url={http://www.sciencedirect.com/science/article/pii/S0950584916301446},
  language={en},
  keywords={Tertiary study, Software testing, Secondary studies, Surveys, Systematic literature reviews, Systematic mapping, Software},
  abstract={Context     Any newcomer or industrial practitioner is likely to experience difficulties in digesting large volumes of knowledge in software testing. In an ideal world, all knowledge used in industry, education and research should be based on high-quality evidence. Since no decision should be made based on a single study, secondary studies become essential in presenting the evidence. According to our search, over 101 secondary studies have been published in the area of software testing since 1994. With this high number of secondary studies, it is important to conduct a review in this area to provide an overview of the research landscape in this area.     Objective     The goal of this study is to systematically map (classify) the secondary studies in software testing. We propose that tertiary studies can serve as summarizing indexes which facilitate finding the most relevant information from secondary studies and thus supporting evidence-based decision making in any given area of software engineering. Our research questions (RQs) investigate: (1) Software-testing-specific areas, (2) Types of RQs investigated, (3) Numbers and Trends, and (4) Citations of the secondary studies.     Method     To conduct the tertiary study, we use the systematic-mapping approach. Additionally, we contrast the testing topics to the number of Google hits to address a general popularity of a testing topic and study the most popular papers in terms of citations. We furthermore demonstrate the practicality and usefulness of our results by mapping them to ISTQB foundation syllabus and to SWEBOK to provide implications for practitioners, testing educators, and researchers.     Results     After a systematic search and voting process, our study pool included 101 secondary studies in the area of software testing between 1994 and 2015. Among our results are the following: (1) In terms of number of secondary studies, model-based approach is the most popular testing method, web services are the most popular system under test (SUT), while regression testing is the most popular testing phase; (2) The quality of secondary studies, as measured by a criteria set established in the community, is slowly increasing as the years go by; and (3) Analysis of research questions, raised and studied in the pool of secondary studies, showed that there is a lack of ‘causality’ and ‘relationship’ type of research questions, a situation which needs to be improved if we, as a community, want to advance as a scientific field. (4) Among secondary studies, we found that regular surveys receive significantly more citations than SMs (p=0.009) and SLRs (p=0.014).     Conclusion     Despite the large number of secondary studies, we found that many important areas of software testing currently lack secondary studies, e.g., test management, role of product risk in testing, human factors in software testing, beta-testing (A/B-testing), exploratory testing, testability, test stopping criteria, and test-environment development. Having secondary studies in those areas is important for satisfying industrial and educational needs in software testing. On the other hand, education material of ISTQB foundation syllabus and SWEBOK could benefit from the inclusion of the latest research topics, namely search-based testing, use of cloud-computing for testing and symbolic execution.}
}

@article{rayyan-727967013,
  title={Guidelines for snowballing in systematic literature studies and a replication in software engineering},
  year={2014},
  issn={978-1-4503-2476-2},
  pages={1-10},
  author={Wohlin, Claes},
  url={https://doi.org/10.1145/2601248.2601268},
  publisher={Association for Computing Machinery},
  series={EASE '14},
  keywords={~guide, replication, snowball search, snowballing, systematic literature review, systematic mapping studies, Software},
  abstract={Background: Systematic literature studies have become common in software engineering, and hence it is important to understand how to conduct them efficiently and reliably. Objective: This paper presents guidelines for conducting literature reviews using a snowballing approach, and they are illustrated and evaluated by replicating a published systematic literature review. Method: The guidelines are based on the experience from conducting several systematic literature reviews and experimenting with different approaches. Results: The guidelines for using snowballing as a way to search for relevant literature was successfully applied to a systematic literature review. Conclusions: It is concluded that using snowballing, as a first search strategy, may very well be a good alternative to the use of database searches.}
}

@article{rayyan-727967014,
  title={Tools to support systematic reviews in software engineering: a feature analysis},
  year={2014},
  issn={978-1-4503-2476-2},
  pages={1-10},
  author={Marshall, Christopher and Brereton, Pearl and Kitchenham, Barbara},
  url={https://doi.org/10.1145/2601248.2601270},
  publisher={Association for Computing Machinery},
  series={EASE '14},
  keywords={systematic review, systematic literature review, feature analysis, systematic review support tools, Software},
  abstract={Background The labour intensive and error prone nature of the systematic review process has led to the development and use of a range of tools to provide automated support. Aim The aim of this research is to evaluate a set of candidate tools that provide support for the overall systematic review process. Method A feature analysis is performed to compare and evaluate four candidate tools. Results Each of the candidates has some strengths and some weaknesses. SLuRp has the highest overall score and SLRTOOL has the lowest overall score. SLuRp scores well on process management features such as support for multiple users and document management and less well on ease of installation. Conclusions Although the tools do not yet support the whole systematic review process they provide a good basis for further development. We suggest a community effort to establish a set of features that can inform future tool development.}
}

@article{rayyan-727967015,
  title={Automated Selection and Quality Assessment of Primary Studies: A Systematic Literature Review},
  year={2019},
  journal={Journal of Data and Information Quality},
  issn={1936-1955},
  volume={12},
  number={1},
  pages={4:1-4:26},
  author={Shakeel, Yusra and Krüger, Jacob and Nostitz-Wallwitz, Ivonne Von and Saake, Gunter and Leich, Thomas},
  url={https://doi.org/10.1145/3356901},
  keywords={Systematic literature review, primary study assessment, quality assessment, software engineering, tertiary study},
  abstract={Researchers use systematic literature reviews (SLRs) to synthesize existing evidence regarding a research topic. While being an important means to condense knowledge, conducting an SLR requires a large amount of time and effort. Consequently, researchers have proposed semi-automatic techniques to support different stages of the review process. Two of the most time-consuming tasks are (1) to select primary studies and (2) to assess their quality. In this article, we report an SLR in which we identify, discuss, and synthesize existing techniques of the software-engineering domain that aim to semi-automate these two tasks. Instead of solely providing statistics, we discuss these techniques in detail and compare them, aiming to improve our understanding of supported and unsupported activities. To this end, we identified eight primary studies that report unique techniques that have been published between 2007 and 2016. Most of these techniques rely on text mining and can be beneficial for researchers, but an independent validation using real SLRs is missing for most of them. Moreover, the results indicate the necessity of developing more reliable techniques, providing access to their implementations, and extending their scope to further activities to facilitate the selection and quality assessment of primary studies.}
}

@article{rayyan-727967016,
  title={Tools to Support Systematic Literature Reviews in Software Engineering: A Mapping Study},
  year={2013},
  journal={2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
  pages={296-299},
  author={Marshall, C. and Brereton, P.},
  keywords={Software, systematic literature review, software engineering, automated search strategy, automated tool, mapping study, SE domain, SLR process, Software engineering, Software measurement, support systematic literature reviews, Systematics, Text mining, visualisation techniques, Visualization},
  abstract={Background: Systematic literature reviews (SLRs) have become an established methodology in software engineering (SE) research however they can be very time consuming and error prone. Aim: The aims of this study are to identify and classify tools that can help to automate part or all of the SLR process within the SE domain. Method: A mapping study was performed using an automated search strategy plus snowballing to locate relevant papers. A set of known papers was used to validate the search string. Results: 14 papers were accepted into the final set. Eight presented text mining tools and six discussed the use of visualisation techniques. The stage most commonly targeted was study selection. Only two papers reported an independent evaluation of the tool presented. The majority were evaluated through small experiments and examples of their use. Conclusions: A variety of tools are available to support the SLR process although many are in the early stages of development and usage.}
}

@article{rayyan-727967017,
  title={Improvements in the StArt tool to better support the systematic review process},
  year={2016},
  issn={978-1-4503-3691-8},
  pages={1-5},
  author={Fabbri, Sandra and Silva, Cleiton and Hernandes, Elis and Octaviano, Fábio and Di Thommazo, André and Belgamo, Anderson},
  url={https://doi.org/10.1145/2915970.2916013},
  publisher={Association for Computing Machinery},
  series={EASE '16},
  keywords={systematic review, ~tool, tool, systematic literature review, evidence-based software engineering, StArt tool, tool support},
  abstract={Context: Systematic Review (SR) is a methodology used to find and aggregate relevant existing evidence about a specific research topic of interest. It can be very time-consuming depending on the number of gathered studies that need to be analyzed by researchers. One of the relevant tools found in the literature and preliminarily evaluated by researchers of SRs is StArt, which supports the whole SR process. It has been downloaded by users from more than twenty countries. Objective: To present new features available in StArt to support SR activities. Method: Based on users' feedback and the literature, new features were implemented and are available in the tool, like the SCAS strategy, snowballing techniques, the frequency of keywords and a word cloud for search string refining, collaboration among reviewers, and the StArt online community. Results: The new features, according to users' positive feedback, make the tool more robust to support the conduct of SRs. Conclusion: StArt is a tool that has been continuously developed such that new features are often available to improve the support for the SR process. The StArt online community can improve the interaction among users, facilitating the identification of improvements and new useful features.}
}

@article{rayyan-727967018,
  title={Systematic review toolbox: a catalogue of tools to support systematic reviews},
  year={2015},
  issn={978-1-4503-3350-4},
  pages={1-6},
  author={Marshall, Christopher and Brereton, Pearl},
  url={https://doi.org/10.1145/2745802.2745824},
  publisher={Association for Computing Machinery},
  series={EASE '15},
  keywords={documentation},
  abstract={Systematic review is a widely used research method in software engineering, and in other disciplines, for identifying and analysing empirical evidence. The method is data intensive and time consuming, and hence is usually supported by a wide range of software-based tools. However, systematic reviewers have found that finding and selecting tools can be quite challenging. In this paper, we present the Systematic Review Toolbox; a web-based catalogue of tools, to help reviewers find appropriate tools based on their particular needs.}
}

@article{rayyan-727967019,
  title={Identification of SLR tool needs – results of a community workshop},
  year={2016},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={70},
  pages={122-129},
  author={Hassler, Edgar and Carver, Jeffrey C. and Hale, David and Al-Zubidy, Ahmed},
  url={http://www.sciencedirect.com/science/article/pii/S0950584915001779},
  language={en},
  keywords={Systematic literature review, Community workshops, Research infrastructure, Tool features},
  abstract={Context: With the increasing popularity of the Systematic Literature Review (SLR) process, there is also an increasing need for tool support. Objective:The goal of this work was to consult the software engineering researchers who conduct SLRs to identify and prioritize the necessary SLR tool features. Method: To gather information required to address this goal, we invited SLR authors to participate in an interactive 2 h workshop structured around the Nominal Group Technique. Results: The workshop outcomes indicated that Search & Selection and Collaboration are the two highest priority tool features. The results also showed that most of the high-priority features are not well-supported in current tools. Conclusion: These results support and extend the results of prior work. SLR tool authors can use these findings to guide future development efforts.}
}

@article{rayyan-727967020,
  title={Tools to support systematic reviews in software engineering: a cross-domain survey using semi-structured interviews},
  year={2015},
  issn={978-1-4503-3350-4},
  pages={1-6},
  author={Marshall, Christopher and Brereton, Pearl and Kitchenham, Barbara},
  url={https://doi.org/10.1145/2745802.2745827},
  publisher={Association for Computing Machinery},
  series={EASE '15},
  keywords={systematic review, automated tools, survey, Software},
  abstract={Background: A number of software tools are being developed to support systematic reviewers within the software engineering domain. However, at present, we are not sure which aspects of the review process can most usefully be supported by such tools or what characteristics of the tools are most important to reviewers. Aim: The aim of the study is to explore the scope and practice of tool support for systematic reviewers in other disciplines. Method: Researchers with experience of performing systematic reviews in Healthcare and the Social Sciences were surveyed. Qualitative data was collected through semi-structured interviews and data analysis followed an inductive approach. Results: 13 interviews were carried out. 21 software tools categorised into one of seven types were identified. Reference managers were the most commonly mentioned tools. Features considered particularly important by participants were support for multiple users, support for data extraction and support for tool maintenance. The features and importance levels identified by participants were compared with those proposed for tools to support systematic reviews in software engineering. Conclusions: Many problems faced by systematic reviewers in other disciplines are similar to those faced in software engineering. There is general consensus across domains that improved tools are needed.}
}

@article{rayyan-727967021,
  title={SLuRp: a tool to help large complex systematic literature reviews deliver valid and rigorous results},
  year={2012},
  issn={978-1-4503-1509-8},
  pages={33-36},
  author={Bowes, David and Hall, Tracy and Beecham, Sarah},
  url={https://doi.org/10.1145/2372233.2372243},
  publisher={Association for Computing Machinery},
  series={EAST '12},
  keywords={~tool, tool, systematic literature review tool},
  abstract={Background: Systematic literature reviews are increasingly used in software engineering. Most systematic literature reviews require several hundred papers to be examined and assessed. This is not a trivial task and can be time consuming and error-prone. Aim: We present SLuRp - our open source web enabled database that supports the management of systematic literature reviews. Method: We describe the functionality of SLuRp and explain how it supports all phases in a systematic literature review. Results: We show how we used SLuRp in our SLR. We discuss how SLuRp enabled us to generate complex results in which we had confidence. Conclusions: SLuRp supports all phases of an SLR and enables reliable results to be generated. If we are to have confidence in the outcomes of SLRs it is essential that such automated systems are used.}
}

@article{rayyan-727967022,
  title={Repeatability of systematic literature reviews},
  year={2011},
  journal={15th Annual Conference on Evaluation Assessment in Software Engineering (EASE 2011)},
  pages={46-55},
  author={Kitchenham, B. and Brereton, P. and Li, Z. and Budgen, D. and Burn, A.},
  keywords={software engineering, ACM digital libraries, Case Study, digital libraries, IEEE digital libraries, participant-observer multi-case study, repeatability, Repeatability, SLR, software reviews, Systematic Literature Review, systematic literature reviews},
  abstract={Background: One of the anticipated benefits of systematic literature reviews (SLRs) is that they can be conducted in an auditable way to produce repeatable results. Aim: This study aims to identify under what conditions SLRs are likely to be stable, with respect to the primary studies selected, when used in software engineering. The conditions we investigate in this report are when novice researchers undertake searches with a common goal. Method: We undertook a participant-observer multi-case study to investigate the repeatability of systematic literature reviews. The "cases" in this study were the early stages, involving identification of relevant literature, of two SLRs of unit testing methods. The SLRs were performed independently by two novice researchers. The SLRs were restricted to the ACM and IEEE digital libraries for the years 1986-2005 so their results could be compared with a published expert literature review of unit testing papers. Results: The two SLRs selected very different papers with only six papers out of 32 in common, and both differed substantially from a published secondary study of unit testing papers finding only three of 21 papers. Of the 29 additional papers found by the novice researchers, only 10 were considered relevant. The 10 additional relevant papers would have had an impact on the results of the published study by adding three new categories to the framework and adding papers to three, otherwise empty, cells. Conclusions: In the case of novice researchers, having broadly the same research question will not necessarily guarantee repeatability with respect to primary studies. Systematic reviews must be careful to report their search process fully or they will not be repeatable. Missing papers can have a significant impact on the stability of the results of a secondary study.}
}

@article{rayyan-727967023,
  title={Systematic reviews in software engineering: An empirical investigation},
  year={2013},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={55},
  number={7},
  pages={1341-1354},
  author={Zhang, He and Ali Babar, Muhammad},
  url={http://www.sciencedirect.com/science/article/pii/S0950584912002029},
  language={en},
  keywords={Evidence-based software engineering, Tertiary study, Methodology adoption, Mixed-methods research, Research methodology, Systematic (literature) reviews, Software},
  abstract={Background     Systematic Literature Reviews (SLRs) have gained significant popularity among Software Engineering (SE) researchers since 2004. Several researchers have also been working on improving the scientific and methodological infrastructure to support SLRs in SE. We argue that there is also an apparent and essential need for evidence-based body of knowledge about different aspects of the adoption of SLRs in SE.     Objective     The main objective of this research is to empirically investigate the adoption, value, and use of SLRs in SE research from various perspectives.     Method     We used mixed-methods approach (systematically integrating tertiary literature review, semi-structured interviews and questionnaire-based survey) as it is based on a combination of complementary research methods which are expected to compensate each others’ limitations.     Results     A large majority of the participants are convinced of the value of using a rigourous and systematic methodology for literature reviews in SE research. However, there are concerns about the required time and resources for SLRs. One of the most important motivators for performing SLRs is new findings and inception of innovative ideas for further research. The reported SLRs are more influential compared to the traditional literature reviews in terms of number of citations. One of the main challenges of conducting SLRs is drawing a balance between methodological rigour and required effort.     Conclusions     SLR has become a popular research methodology for conducting literature review and evidence aggregation in SE. There is an overall positive perception about this relatively new methodology to SE research. The findings provide interesting insights into different aspects of SLRs. We expect that the findings can provide valuable information to readers about what can be expected from conducting SLRs and the potential impact of such reviews.}
}

@article{rayyan-727967024,
  title={Text-Mining Techniques and Tools for Systematic Literature Reviews: A Systematic Literature Review},
  year={2017},
  journal={2017 24th Asia-Pacific Software Engineering Conference (APSEC)},
  pages={41-50},
  author={Feng, L. and Chiam, Y. K. and Lo, S. K.},
  keywords={~tool, systematic literature review, software engineering, Systematics, Systematic Literature Review, appropriate SLR automation strategies, Bibliographies, data mining, Data mining, Guidelines, mixed search strategy, Search problems, SLR activities, software engineering research, text analysis, Text mining techniques, text-mining techniques, Tool support, Tools},
  abstract={Despite the importance of conducting systematic literature reviews (SLRs) for identifying the research gaps in software engineering (SE) research, SLRs are a complex, multi-stage, and time-consuming process if performed manually. Conducting an SLR in line with the guidelines and practice in the SE domain requires considerable effort and expertise. The objective of this SLR is to identify and classify text-mining techniques and tools that can help facilitate SLR activities. This study also investigates the adoption of text-mining (TM) techniques to support SLR in the SE domain. We performed a mixed search strategy to identify relevant studies published from January 1, 2004, to December 31, 2016. We shortlisted 32 papers into the final set of relevant studies published in the SE, medicine and social science disciplines. The majority of the text-mining techniques attempted to support the study selection stage. Only 12 out of the 14 studies in the SE domain applied text-mining techniques, focusing primarily on facilitating the search and study selection stages. By learning from the experience of applying TM techniques in clinical medicine and social science fields, we believe that SE researchers can adopt appropriate SLR automation strategies for use in the SE field.}
}

@article{rayyan-727967025,
  title={Reducing efforts of software engineering systematic literature reviews updates using text classification},
  year={2020},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={128},
  pages={106395},
  author={Watanabe, Willian Massami and Felizardo, Katia Romero and Candido, Arnaldo and de Souza, Érica Ferreira and Neto, José Ede de Campos and Vijaykumar, Nandamudi Lankalapalli},
  url={http://www.sciencedirect.com/science/article/pii/S0950584920301592},
  language={en},
  keywords={~tool, Systematic literature review, SLR, Automatic selection, Document classification, Review update, Text categorization, Text classification, Software},
  abstract={Context     Systematic Literature Reviews (SLRs) are frequently used to synthesize evidence in Software Engineering (SE), however replicating and keeping SLRs up-to-date is a major challenge. The activity of studies selection in SLR is labor intensive due to the large number of studies that must be analyzed. Different approaches have been investigated to support SLR processes, such as: Visual Text Mining or Text Classification. But acquiring the initial dataset is time-consuming and labor intensive.     Objective     In this work, we proposed and evaluated the use of Text Classification to support the studies selection activity of new evidences to update SLRs in SE.     Method     We applied Text Classification techniques to investigate how effective and how much effort could be spared during the studies selection phase of an SLR update. Considering the SLRs update scenario, the studies analyzed in the primary SLR could be used as a classified dataset to train Supervised Machine Learning algorithms. We conducted an experiment with 8 Software Engineering SLRs. In the experiments, we investigated the use of multiple preprocessing and feature extraction tasks such as tokenization, stop words removal, word lemmatization, TF-IDF (Term-Frequency/Inverse-Document-Frequency) with Decision Tree and Support Vector Machines as classification algorithms. Furthermore, we configured the classifier activation threshold for maximizing Recall, hence reducing the number of Missed selected studies.     Results     The techniques accuracies were measured and the results achieved on average a F-Score of 0.92 and 62% of exclusion rate when varying the activation threshold of the classifiers, with a 4% average number of Missed selected studies. Both the Exclusion rate and number of Missed selected studies were significantly different when compared to classifier which did not use the configuration of the activation threshold.     Conclusion     The results showed the potential of the techniques in reducing the effort required of SLRs updates.}
}

@article{rayyan-727967026,
  title={An SLR-tool: search process in practice: a tool to conduct and manage systematic literature review (SLR)},
  year={2020},
  issn={978-1-4503-7122-3},
  pages={81-84},
  author={Hinderks, Andreas and Mayo, Francisco José Domínguez and Thomaschewski, Jörg and Escalona, María José},
  url={https://doi.org/10.1145/3377812.3382137},
  publisher={Association for Computing Machinery},
  series={ICSE '20},
  keywords={tool, systematic literature review, SLR},
  abstract={Systematic Literature Reviews (SLRs) have established themselves as a method in the field of software engineering. The aim of an SLR is to systematically analyze existing literature in order to answer a research question. In this paper, we present a tool to support an SLR process. The main focus of the SLR tool (https://www.slr-tool.com/) is to create and manage an SLR project, to import search results from search engines, and to manage search results by including or excluding each paper. A demo video of our SLR tool is available at https://youtu.be/Jan8JbwiE4k.}
}

@article{rayyan-727967027,
  title={Research synthesis in software engineering: A tertiary study},
  year={2011},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={53},
  number={5},
  pages={440-455},
  author={Cruzes, Daniela S. and Dybå, Tore},
  url={https://www.sciencedirect.com/science/article/pii/S095058491100005X},
  language={en},
  series={Special Section on Best Papers from XP2010},
  keywords={Evidence-based software engineering, ~review problems, Empirical software engineering, Systematic review, Mixed-methods, Qualitative methods, Software},
  abstract={Context     Comparing and contrasting evidence from multiple studies is necessary to build knowledge and reach conclusions about the empirical support for a phenomenon. Therefore, research synthesis is at the center of the scientific enterprise in the software engineering discipline.     Objective     The objective of this article is to contribute to a better understanding of the challenges in synthesizing software engineering research and their implications for the progress of research and practice.     Method     A tertiary study of journal articles and full proceedings papers from the inception of evidence-based software engineering was performed to assess the types and methods of research synthesis in systematic reviews in software engineering.     Results     As many as half of the 49 reviews included in the study did not contain any synthesis. Of the studies that did contain synthesis, two thirds performed a narrative or a thematic synthesis. Only a few studies adequately demonstrated a robust, academic approach to research synthesis.     Conclusion     We concluded that, despite the focus on systematic reviews, there is limited attention paid to research synthesis in software engineering. This trend needs to change and a repertoire of synthesis methods needs to be an integral part of systematic reviews to increase their significance and utility for research and practice.}
}

@article{rayyan-727967028,
  title={A family of experiments to evaluate the understandability of TRiStar and i* for modeling teleo-reactive systems},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={114},
  pages={82-100},
  author={Morales, José Miguel and Navarro, Elena and Sánchez, Pedro and Alonso, Diego},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216000042},
  keywords={Requirements engineering, Teleo-reactive, TRiStar},
  abstract={The teleo-reactive approach facilitates reactive system development without losing sight of the system goals. Objective To introduce TRiStar as an extension of i* notation to specify teleo-reactive systems. To evaluate whether the notational extension is an improvement in terms of effectiveness and efficiency over the original language when it is used to specify teleo-reactive systems. Method A family of experiments was carried out with final-year engineering students and experienced software development professionals in which the participants were asked to fill in a form designed to evaluate the efficiency and effectiveness of each of the languages. Results Both the statistical results of the experiments, analyzed separately, and the meta-analysis of the experiments as a whole, allow us to conclude that TRiStar notation is more effective and efficient than i* as a requirements specification language for modeling teleo-reactive systems. Conclusion The extensions made on i* have led to TRiStar definition, a more effective and efficient goal-oriented notation than the original i* language.}
}

@article{rayyan-727967029,
  title={Quality of software requirements specification in agile projects: A cross-case analysis of six companies},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={142},
  pages={171-194},
  author={Medeiros, Juliana and Vasconcelos, Alexandre and Silva, Carla and Goulão, Miguel},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218300888},
  keywords={Agile methods, Agile Requirements Engineering, Empirical study, Requirements specification, Software},
  abstract={Agile Software Development (ASD) has several limitations concerning its requirements engineering activities. Improving the quality of Software Requirements Specifications (SRSs) in ASD may help to gain a competitive advantage in the software industry. Based on the findings of a Systematic Mapping study, six industrial case studies in different contexts were conducted to investigate and characterize the requirements specification activity in ASD. Data collected from documents, observations, and interviews with software engineers were triangulated, analyzed, and synthesized using Grounded Theory and Meta-Ethnography. The analysis and cross-synthesis of the six case studies resulted in a model describing the phenomenon. This model defines the simplicity and objectivity as essential quality factors of SRSs in ASD. The main factors that affect the SRSs quality in ASD projects are related to their customer-driven nature that leads to prolix SRSs, hindering its understanding from the developer perspective. The emerged model is supported by explanations and provides a deeper understanding of the requirements specification activity in ASD. This creates opportunities for further studies and improvements in SRSs for ASD in industry.}
}

@article{rayyan-727967030,
  title={Domain-specific language modelling with UML profiles by decoupling abstract and concrete syntaxes},
  year={2010},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={83},
  number={12},
  pages={2591-2606},
  author={Pardillo, Jesús and Cachero, Cristina},
  url={https://www.sciencedirect.com/science/article/pii/S0164121210002153},
  keywords={Diagramming, Modelling, Profiles, Syntax, UML, Visual languages},
  abstract={UML profiling presents some acknowledged deficiencies, among which the lack of expressiveness of the profiled notations, together with the high coupling between abstract and concrete syntaxes outstand. These deficiencies may cause distress among UML-profile modellers, who are often forced to extend from unsuitable metaclasses for mere notational reasons, or even to model domain-specific languages from scratch just to avoid the UML-profiling limitations. In order to palliate this situation, this article presents an extension of the UML profile metamodel to support arbitrarily-complex notational extensions by decoupling the UML abstract and concrete syntax. Instead of defining yet another metamodel for UML-notational profiling, notational extensions are modelled with DI, i.e., the UML notation metamodel for diagram interchange, keeping in this way the extension within the standard. Profiled UML notations are rendered with DI by defining the graphical properties involved, the domain-specific constraints applied to DI, and the rendering routines associated. Decoupling abstract and concrete syntax in UML profiles increases the notation expressiveness while decreasing the abstract-syntax complexity.}
}

@article{rayyan-727967031,
  title={Risk management practices in information security: Exploring the status quo in the DACH region},
  year={2020},
  journal={Computers & Security},
  issn={0167-4048},
  volume={92},
  pages={101776},
  author={Brunner, Michael and Sauerwein, Clemens and Felderer, Michael and Breu, Ruth},
  url={https://www.sciencedirect.com/science/article/pii/S0167404820300614},
  keywords={Collaboration patterns, Exploratory survey, Information security management, Information security risk management, State of practice, Risk Management},
  abstract={Information security management aims at ensuring proper protection of information values and information processing systems (i.e. assets). Information security risk management techniques are incorporated to deal with threats and vulnerabilities that impose risks to information security properties of these assets. This paper investigates the current state of risk management practices being used in information security management in the DACH region (Germany, Austria, Switzerland). We used an anonymous online survey targeting strategic and operative information security and risk managers and collected data from 26 organizations. We analyzed general practices, documentation artifacts, patterns of stakeholder collaboration as well as tool types and data sources used by enterprises to conduct information security management activities. Our findings show that the state of practice of information security risk management is in need of improvement. Current industrial practice heavily relies on manual data collection and complex potentially subjective decision processes with multiple stakeholders involved. Dedicated risk management tools and methods are used selectively and neglected in favor of general-purpose documentation tools and direct communication between stakeholders. In light of our results we propose guidelines for the development of risk management practices that are better aligned with the current operational situation in information security management.}
}

@article{rayyan-727967032,
  title={A review of studies on expert estimation of software development effort},
  year={2004},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={70},
  number={1},
  pages={37-60},
  author={Jørgensen, M},
  url={https://www.sciencedirect.com/science/article/pii/S0164121202001565},
  keywords={Effort estimation, Expert judgment, Project planning, Software development, Software},
  abstract={This paper provides an extensive review of studies related to expert estimation of software development effort. The main goal and contribution of the review is to support the research on expert estimation, e.g., to ease other researcher's search for relevant expert estimation studies. In addition, we provide software practitioners with useful estimation guidelines, based on the research-based knowledge of expert estimation processes. The review results suggest that expert estimation is the most frequently applied estimation strategy for software projects, that there is no substantial evidence in favour of use of estimation models, and that there are situations where we can expect expert estimates to be more accurate than formal estimation models. The following 12 expert estimation “best practice” guidelines are evaluated through the review: (1) evaluate estimation accuracy, but avoid high evaluation pressure; (2) avoid conflicting estimation goals; (3) ask the estimators to justify and criticize their estimates; (4) avoid irrelevant and unreliable estimation information; (5) use documented data from previous development tasks; (6) find estimation experts with relevant domain background and good estimation records; (7) Estimate top-down and bottom-up, independently of each other; (8) use estimation checklists; (9) combine estimates from different experts and estimation strategies; (10) assess the uncertainty of the estimate; (11) provide feedback on estimation accuracy and development task relations; and, (12) provide estimation training opportunities. We found supporting evidence for all 12 estimation principles, and provide suggestions on how to implement them in software organizations.}
}

@article{rayyan-727967033,
  title={Comparing risk identification techniques for safety and security requirements},
  year={2013},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={86},
  number={4},
  pages={1124-1151},
  author={Raspotnig, Christian and Opdahl, Andreas},
  url={https://www.sciencedirect.com/science/article/pii/S0164121212003305},
  keywords={Comparison, Requirement elicitation, Risk identification, Safety, Security, Technique},
  abstract={When developing systems where safety and security are important aspects, these aspects have to be given special attention throughout the development, in particular in the requirements phase. There are many similar techniques within the safety and security fields, but few comparisons about what lessons that could be learnt and benefits to be gained. In this paper different techniques for identifying risk, hazard and threat of computer-supported systems are compared. This is done by assessing the techniques' ability to identify different risks in computer-supported systems in the environment where they operate. The purpose of this paper is therefore to investigate whether and how the techniques can mutually strengthen each other. The result aids practitioners in the selection and combination of techniques and researchers in focusing on gaps between the two fields. Among other things, the findings suggest that many safety techniques enforce a creative and systematic process by applying guide-words and structuring the results in worksheets, while security techniques tend to integrate system models with security models.}
}

@article{rayyan-727967034,
  title={Approaches for resilience and antifragility in collaborative business ecosystems},
  year={2020},
  journal={Technological Forecasting and Social Change},
  issn={0040-1625},
  volume={151},
  pages={119846},
  author={Ramezani, Javaneh and Camarinha-Matos, Luis M},
  url={https://www.sciencedirect.com/science/article/pii/S0040162519304494},
  keywords={Collaboration, Antifragility, Business ecosystem, Disruptions, Resilience},
  abstract={Contemporary business ecosystems are continuously challenged by unexpected disruptive events, which are increasing in their frequency and effects. A critical question is why do some organizations collapse in face of extreme events, while others not? On the other hand, current engineering and socio-technical systems were designed to operate in “mostly stable” situations; sporadic instability and disturbances are at best captured by exception handling mechanisms, focusing on reliability and robustness. Recent and more ambitious design goals, however, aim at building systems that are expected to cope with severe disruptions, and survive or even thrive in a context of volatility and uncertainty. This led to an increasing attention to the concepts of resilience and antifragility. As such, this article introduces the findings of a comprehensive literature survey aimed at shedding light on emerging concepts and approaches to handle disruptions in business ecosystems. Main contributions include a clarification of related concepts, identification and classification of disruption sources and drivers, and extensive lists of strategies and underlying capabilities to cope with disruptions. Related perspectives and approaches developed in multiple knowledge areas are also analysed and synthesized. Finally, a collection of engineered systems implementing promising approaches to increase resilience and antifragility are presented.}
}

@article{rayyan-727967035,
  title={Modeling continuous integration practice differences in industry software development},
  year={2014},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={87},
  pages={48-59},
  author={Ståhl, Daniel and Bosch, Jan},
  url={https://www.sciencedirect.com/science/article/pii/S0164121213002276},
  keywords={Agile software development, Continuous integration, Software},
  abstract={Continuous integration is a software practice where developers integrate frequently, at least daily. While this is an ostensibly simple concept, it does leave ample room for interpretation: what is it the developers integrate with, what happens when they do, and what happens before they do? These are all open questions with regards to the details of how one implements the practice of continuous integration, and it is conceivable that not all such implementations in the industry are alike. In this paper we show through a literature review that there are differences in how the practice of continuous integration is interpreted and implemented from case to case. Based on these findings we propose a descriptive model for documenting and thereby better understanding implementations of the continuous integration practice and their differences. The application of the model to an industry software development project is then described in an illustrative case study.}
}

@article{rayyan-727967036,
  title={Monitoring the service-based system lifecycle with SALMon},
  year={2015},
  journal={Expert Systems with Applications},
  issn={0957-4174},
  volume={42},
  number={19},
  pages={6507-6521},
  author={Oriol, Marc and Franch, Xavier and Marco, Jordi},
  url={https://www.sciencedirect.com/science/article/pii/S0957417415002213},
  keywords={Monitoring, QoS, Quality of Service, Service based system, Web service},
  abstract={Context and motivation Service-Based Systems are highly dynamic software systems composed of several web services. In contrast to other types of systems, Service-Based Systems rely on service providers to ensure that their web services comply with the agreed Quality of Service. Delivering an adequate Quality of Service is a critical and significant challenge that requires monitoring along the different activities in the Service-Based System's lifecycle. Question/problem Current monitoring systems are designed to support specific activities (e.g. service selection, adaptation, etc.), but do not fulfil the requirements of all the activities in the Service-Based System's lifecycle. Principal ideas/results In this paper, we present SALMon, a QoS monitoring framework able to support the whole Service-Based System's lifecycle. SALMon is highly versatile, since it combines different strategies for its configuration (model-based and invocation-based) and for the way it gets the Quality of Service (passive monitoring and online testing). Furthermore, its architecture supports easy extensibility with new quality attributes, independence of the technology of the monitored services and interoperability with other tools. We conducted a performance evaluation over real web services using suitable estimators for response time and evaluated both its overhead and capacity. Contribution SALMon provides infrastructure that can be used in very different scenarios, as exemplified in this paper, both in terms of the lifecycle's phase addressed and the type of system (pure Service-Oriented Architecture, cloud-based systems, etc.). This diversity of situations addressed makes SALMon a significant contribution both for practitioners that may be interested in integrating a working technology in their software solutions, and for researchers who can conduct their investigation on top of a reliable infrastructure.}
}

@article{rayyan-727967037,
  title={Construct specific coupling measurement for C++ software},
  year={2012},
  journal={Computer Languages, Systems & Structures},
  issn={1477-8424},
  volume={38},
  number={4},
  pages={300-319},
  author={English, Michael and Cahill, Tony and Buckley, Jim},
  url={https://www.sciencedirect.com/science/article/pii/S1477842412000243},
  keywords={Software measurement, C++, Coupling, Encapsulation, Friend, Information hiding, Inheritance, Object-oriented software, Software metric, Software},
  abstract={Studies which consider the extent to which the encapsulation of a class is weakened by direct access to its hidden members (such as through the use of the friend construct in C++) are scarce, and those that do exist are based on metric suites where the enabling mechanism of the coupling is ignored. This can lead to conclusions of limited construct validity where incorrect causes of coupling are suggested. In this paper a suite of software metrics which measure the amount of coupling enabled by different C++ programming language constructs (such as friendship and inheritance) are proposed. The metrics presented are based on a formal data model which can be easily adapted for other OO languages. This formal approach removes the scope for ambiguity in the metric definitions. These metrics provide a more accurate reflection of the causative agents of coupling in Object Oriented Systems and their utility is illustrated in an empirical study towards the end of the paper.}
}

@article{rayyan-727967038,
  title={Evolutionary coupling measurement: Making sense of the current chaos},
  year={2017},
  journal={Science of Computer Programming},
  issn={0167-6423},
  volume={135},
  pages={4-19},
  author={Kirbas, Serkan and Hall, Tracy and Sen, Alper},
  url={https://www.sciencedirect.com/science/article/pii/S0167642316301587},
  keywords={Evolutionary coupling, Measurement, Measurement theory},
  abstract={Objective: The aim of this research is to evaluate the measurement of evolutionary coupling (EC) in software artefacts from a measurement theory perspective. Background: Evolutionary coupling (EC) can be defined as the implicit relationship between two or more software artefacts which are frequently changed together. Previous studies on EC show that EC measures which are based on software change history information play an important role in measuring software quality and predicting defects. The many previous EC measures published are disparate and no comprehensive evaluation of the current EC measures exists. Therefore it is hard for researchers and practitioners to compare, choose and use EC measures. Methods: We define 19 evaluation criteria based on the principles of measurement theory and metrology. We evaluate previously published EC measures by applying these criteria. Results: Our evaluation results revealed that current EC measurement has the particular weaknesses around establishing sound empirical relation systems, defining detailed and standardised measurement procedures as well as scale type and mathematical validation. Conclusions: We provide information about the quality of existing EC measures and measurement methods. The results suggest that there is more work to be done to put EC measurement on a firm footing that will enable the reliable measurement of EC and the accurate replication of EC measurement.}
}

@article{rayyan-727967039,
  title={Definition and evaluation of a COSMIC measurement procedure for sizing Web applications in a model-driven development environment},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={104},
  pages={144-161},
  author={Abrahão, Silvia and De Marco, Lucia and Ferrucci, Filomena and Gomez, Jaime and Gravino, Carmine and Sarro, Federica},
  url={https://www.sciencedirect.com/science/article/pii/S095058491730280X},
  keywords={COSMIC, Functional size measurement, Model-driven development, OO-H method, Web applications},
  abstract={Context. Model-driven development approaches facilitate the production of Web applications. Among them, the Object-Oriented Hypermedia method (OO-H) has been successfully used for the development of industrial Web applications. Similarly to other development approaches, it is important also in this context to put measures in place to support project managers in resource allocation, cost and schedule control, and productivity monitoring. Objective. This motivated us to define a measurement procedure, named OO-HCFP, specifically conceived for OO-H Web applications based on COSMIC, a second-generation functional size measurement method. Method. We present mapping and measurement rules devised to automatically derive size measures from OO-H models. We also carry out an empirical study to evaluate whether our proposed measurement procedure, OO-HCFP, is useful for estimating the effort needed to realise industrial Web applications developed with OO-H. Results. The estimates obtained by using OO-HCFP are more accurate than those obtained by using other measurement approaches based on Function Points and design measures. Conclusions. The proposed approach can be profitably exploited to size Web applications developed with OO-H. Based on our experience, we also provide some guidelines to support the formulation of COSMIC measurement procedures for other model-driven approaches.}
}

@article{rayyan-727967040,
  title={To what extent can maintenance problems be predicted by code smell detection? – An empirical study},
  year={2013},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={55},
  number={12},
  pages={2223-2242},
  author={Yamashita, Aiko and Moonen, Leon},
  url={https://www.sciencedirect.com/science/article/pii/S0950584913001614},
  keywords={Empirical study, Code smells, Maintainability, Smell},
  abstract={Context Code smells are indicators of poor coding and design choices that can cause problems during software maintenance and evolution. Objective This study is aimed at a detailed investigation to which extent problems in maintenance projects can be predicted by the detection of currently known code smells. Method A multiple case study was conducted, in which the problems faced by six developers working on four different Java systems were registered on a daily basis, for a period up to four weeks. Where applicable, the files associated to the problems were registered. Code smells were detected in the pre-maintenance version of the systems, using the tools Borland Together and InCode. In-depth examination of quantitative and qualitative data was conducted to determine if the observed problems could be explained by the detected smells. Results From the total set of problems, roughly 30% percent were related to files containing code smells. In addition, interaction effects were observed amongst code smells, and between code smells and other code characteristics, and these effects led to severe problems during maintenance. Code smell interactions were observed between collocated smells (i.e., in the same file), and between coupled smells (i.e., spread over multiple files that were coupled). Conclusions The role of code smells on the overall system maintainability is relatively minor, thus complementary approaches are needed to achieve more comprehensive assessments of maintainability. Moreover, to improve the explanatory power of code smells, interaction effects amongst collocated smells and coupled smells should be taken into account during analysis.}
}

@article{rayyan-727967041,
  title={Calculating completeness of software project scope definition},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={94},
  pages={208-233},
  author={ul Hassan, Isma and Ahmad, Naveed and Zuhaira, Behjat},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917304846},
  keywords={Project planning, Project management, Scope definition, Scope management, Scope statement, Software},
  abstract={Context Software project plan is the basis of the project execution, and its quality depends on completeness of software scope definition. A method is required that should gauge the completeness of different aspects of scope definition, thus, providing guidance to practitioners to reconsider areas that have not been defined well. Objective This paper aims to evaluate completeness of software project scope definition. It identifies a detailed list of different aspects of software project scope definition, and builds a method where numerical score can be assigned to a scope definition. Method A detailed list of different elements of software project scope definition is identified through literature. These elements are then empirically evaluated through an electronic survey, conducted with the industrial experts. Once finalized, these elements are ranked and assigned weights to systematically build a scorecard that is used to calculate scope definition score. Evaluation of the proposed method is done through a series of formal experiments. Results Evaluation results suggest that the proposed method is useful not only in calculating completeness of software projects scope definition, but it also serves as a guide for practitioners to determine specific aspects that require further consideration.}
}

@article{rayyan-727967042,
  title={Energy management for user's thermal and power needs: A survey},
  year={2019},
  journal={Energy Reports},
  issn={2352-4847},
  volume={5},
  pages={1048-1076},
  author={Fiorini, Laura and Aiello, Marco},
  url={https://www.sciencedirect.com/science/article/pii/S2352484719300927},
  keywords={Building, Distributed energy systems, Energy management systems, Heating and power, Microgrid, Optimization, Resource scheduling},
  abstract={The increasing world energy consumption, the diversity in energy sources, and the pressing environmental goals have made the energy supply–demand balance a major challenge. Additionally, as reducing energy costs is a crucial target in the short term, while sustainability is essential in the long term, the challenge is twofold and contains clashing goals. A more sustainable system and end-users' behavior can be promoted by offering economic incentives to manage energy use, while saving on energy bills. In this paper, we survey the state-of-the-art in energy management systems for operation scheduling of distributed energy resources and satisfying end-user's electrical and thermal demands. We address questions such as: how can the energy management problem be formulated? Which are the most common optimization methods and how to deal with forecast uncertainties? Quantitatively, what kind of improvements can be obtained? We provide a novel overview of concepts, models, techniques, and potential economic and emission savings to enhance energy management systems design.}
}

@article{rayyan-727967043,
  title={Object to NoSQL Database Mappers (ONDM): A systematic survey and comparison of frameworks},
  year={2019},
  journal={Information Systems},
  issn={0306-4379},
  volume={85},
  pages={1-20},
  author={Reniers, Vincent and Van Landuyt, Dimitri and Rafique, Ansar and Joosen, Wouter},
  url={https://www.sciencedirect.com/science/article/pii/S0306437918304290},
  keywords={NoSQL abstraction, NoSQL mapping patterns, Object database mappers, Object-NoSQL mapping},
  abstract={Context: Software applications frequently interact with database systems to persist and retrieve objects. Object mapping frameworks address (i) the bi-directional conversion of data between object and target database and (ii) provide a programmatic interface for querying and storing data. The rise of NoSQL databases poses challenges beyond object-relational mapping (ORM) frameworks to abstract from various data models and non-standardized API's, but also take into account the different database capabilities (e.g. unsupported query operators, data ordering). Objective: A systematic survey study of existing Object-NoSQL data mapping (ONDM) frameworks. Specific focus is given to the level of abstraction of data and operations to multiple database technologies, as a means to limit vendor and technology lock-in and an enabler for multi-store and polyglot architectures. Additional attention is paid to mapping strategies that are specific to NoSQL databases (e.g. object embedding, schema flexibility). Method: A systematic search methodology identifies all relevant object mapping frameworks (in total 341 frameworks). Subsequently, a subset of ONDM frameworks is selected and systematically compared in terms of criteria of: database support, interface and query functionality, architecture and software coupling. Secondly, we provide an in-depth comparison of object-oriented mapping strategies for classes, inheritance, relationships, and attribute types to NoSQL data models. Results: ONDM frameworks are most prevalent in Java, Node.JS, Python, and overall 54 frameworks support multiple (NoSQL) databases. Interfaces are frequently standardized and commonly feature a uniform query language and even native DB query mapping. However, database portability may be hindered due to non-uniform abstractions. As for mapping strategies, current frameworks do not fully exploit NoSQL's modeling potential, such as (i) the embedding of relationship data within referring objects' records, (ii) mapping at the individual object-level vs. class-level, and (iii) lacking collection normalization despite being supported for associations or when using relational databases. Conclusion: The study consolidates knowledge on available ONDM frameworks, and applied object-document, object-graph, and object-column mapping patterns. The study can guide practitioners in framework selection, and pinpoints areas of future development and research in this domain, most notably towards improved support for flexible, NoSQL-aware mapping strategies.}
}

@article{rayyan-727967044,
  title={Evaluation of estimation models using the Minimum Interval of Equivalence},
  year={2016},
  journal={Applied Soft Computing},
  issn={1568-4946},
  volume={49},
  pages={956-967},
  author={Dolado, José Javier and Rodriguez, Daniel and Harman, Mark and Langdon, William B and Sarro, Federica},
  url={https://www.sciencedirect.com/science/article/pii/S1568494616301557},
  keywords={Bootstrap, Credible intervals, Equivalence Hypothesis Testing, Soft computing, Software estimations},
  abstract={This article proposes a new measure to compare soft computing methods for software estimation. This new measure is based on the concepts of Equivalence Hypothesis Testing (EHT). Using the ideas of EHT, a dimensionless measure is defined using the Minimum Interval of Equivalence and a random estimation. The dimensionless nature of the metric allows us to compare methods independently of the data samples used. The motivation of the current proposal comes from the biases that other criteria show when applied to the comparison of software estimation methods. In this work, the level of error for comparing the equivalence of methods is set using EHT. Several soft computing methods are compared, including genetic programming, neural networks, regression and model trees, linear regression (ordinary and least mean squares) and instance-based methods. The experimental work has been performed on several publicly available datasets. Given a dataset and an estimation method we compute the upper point of Minimum Interval of Equivalence, MIEu, on the confidence intervals of the errors. Afterwards, the new measure, MIEratio, is calculated as the relative distance of the MIEu to the random estimation. Finally, the data distributions of the MIEratios are analysed by means of probability intervals, showing the viability of this approach. In this experimental work, it can be observed that there is an advantage for the genetic programming and linear regression methods by comparing the values of the intervals.}
}

@article{rayyan-727967045,
  title={Artificial intelligence in service-oriented software design},
  year={2016},
  journal={Engineering Applications of Artificial Intelligence},
  issn={0952-1976},
  volume={53},
  pages={86-104},
  author={Rodríguez, Guillermo and Soria, Álvaro and Campo, Marcelo},
  url={https://www.sciencedirect.com/science/article/pii/S0952197616300677},
  keywords={Artificial Intelligence, Service-oriented design, Web service composition, Web service development, Web services discovery, Software Design, Software, Intelligence},
  abstract={Service-Oriented Architecture (SOA) has gained considerable popularity for the development of distributed enterprise-wide applications within the software industry. The SOA paradigm promotes the reusability and integrability of software in heterogeneous environments by means of open standards. Most software companies capitalize on SOA by discovering and composing services already accessible over the Internet, whereas other organizations need internal control of applications and develop new services with quality-attribute properties tailored to their particular environment. Therefore, based on architectural and business requirements, developers can elaborate different alternatives within a SOA framework to design their software applications. Each of these alternatives will imply trade-offs among quality attributes, such as performance, dependability and availability, among others. In this context, Artificial Intelligence (AI) can assist developers in dealing with service-oriented design with the positive impact on scalability and management of generic quality attributes. In this paper, we offer a detailed, conceptualized and synthesized analysis of AI research works that have aimed at discovering, composing, or developing services. We also identify open research issues and challenges in the aforementioned research areas. The results of the characterization of 69 contemporary approaches and potential research directions for the areas are also shown. It is concluded that AI has aimed at exploiting the semantic resources and achieving quality-attribute properties so as to produce flexible and adaptive-to-change service discovery, composition, and development.}
}

@article{rayyan-727967046,
  title={A systematic study on meta-heuristic approaches for solving the graph coloring problem},
  year={2020},
  journal={Computers & Operations Research},
  issn={0305-0548},
  volume={120},
  pages={104850},
  author={Mostafaie, Taha and Modarres Khiyabani, Farzin and Navimipour, Nima Jafari},
  url={https://www.sciencedirect.com/science/article/pii/S0305054819302928},
  keywords={Directed graph, High runtime, Meta-heuristic, Optimal coloring, Robust GCP, Color},
  abstract={Typically, Graph Coloring Problem (GCP) is one of the key features for graph stamping in graph theory. The general approach is to paint at least edges, vertices, or the surface of the graph with some colors. In the simplest case, a kind of coloring is preferable in which two vertices are not adjacent to the same color. Similarly, the two edges in the same joint should not have the same color. In addition, the same goes for the surface color of the graph. This is one of the NP-hard issues well studied in graph theory. Therefore, many different meta-heuristic techniques are presented to solve the problem and provide high performance. Seemingly, regardless of the importance of the nature-stimulated meta-heuristic methods to solve the GCP, there is not any inclusive report and detailed review about overviewing and investigating the crucial problems of the field. As a result, the present study introduces a wide-ranging reporting of nature- stimulated meta-heuristic methods, which are used throughout the graph coloring. The literature review contains a classification of significant techniques. This study mainly aims at emphasizing the optimization algorithms to handle the GCP problems. Furthermore, the advantages and disadvantages of the meta-heuristic algorithms in solving the GCP and their key issues are examined to offer more advanced meta-heuristic techniques in the future.}
}

@article{rayyan-727967047,
  title={Assessing the effectiveness of goal-oriented modeling languages: A family of experiments},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={116},
  pages={106171},
  author={Abrahão, Silvia and Insfran, Emilio and González-Ladrón-de-Guevara, Fernando and Fernández-Diego, Marta and Cano-Genoves, Carlos and Pereira de Oliveira, Raphael},
  url={https://www.sciencedirect.com/science/article/pii/S0950584919301673},
  keywords={Requirements engineering, Controlled experiments, Goal modeling, GRL, I*},
  abstract={Context Several goal-oriented languages focus on modeling stakeholders' objectives, interests or wishes. However, these languages can be used for various purposes (e.g., exploring system solutions or evaluating alternatives), and there are few guidelines on how to use these models downstream to the software requirements and design artifacts. Moreover, little attention has been paid to the empirical evaluation of this kind of languages. In a previous work, we proposed value@GRL as a specialization of the Goal Requirements Language (GRL) to specify stakeholders' goals when dealing with early requirements in the context of incremental software development. Objective This paper compares the value@GRL language with the i* language, with respect to the quality of goal models, the participants' modeling time and productivity when creating the models, and their perceptions regarding ease of use and usefulness. Method A family of experiments was carried out with 184 students and practitioners in which the participants were asked to specify a goal model using each of the languages. The participants also filled in a questionnaire that allowed us to assess their perceptions. Results The results of the individual experiments and the meta-analysis indicate that the quality of goal models obtained with value@GRL is higher than that of i*, but that the participants required less time to create the goal models when using i*. The results also show that the participants perceived value@GRL to be easier to use and more useful than i* in at least two experiments of the family. Conclusions value@GRL makes it possible to obtain goal models with good quality when compared to i*, which is one of the most frequently used goal-oriented modeling languages. It can, therefore, be considered as a promising emerging approach in this area. Several insights emerged from the study and opportunities for improving both languages are outlined.}
}

@article{rayyan-727967048,
  title={Influencing models and determinants in big data analytics research: A bibliometric analysis},
  year={2020},
  journal={Information Processing & Management},
  issn={0306-4573},
  volume={57},
  number={4},
  pages={102234},
  author={Aboelmaged, Mohamed and Mouakket, Samar},
  url={https://www.sciencedirect.com/science/article/pii/S0306457319313366},
  keywords={Literature review, Adoption frameworks, Bibliometric analysis, Big data analytics, Technology adoption, Theoretical models, Bibliometrics},
  abstract={Incorporating big data analytics into a particular context brings various challenges that rest on the model or framework through which individuals or organisations adopt big data to achieve their objectives. Although these models have recently triggered scholars' attention in various domains, in-depth knowledge of using each of these models in big data research is still blurred. This study enriches our knowledge on emerging models and theories that shape big data analytics adoption (BDAD) research through a bibliometric analysis of 229 studies (143 journal articles and 86 conference papers) published in indexed sources between 2013 and 2019. As a result, twenty models on BDAD have emerged (e.g., “Dynamic Capabilities”, “Resource-Based View”, “Technology Acceptance Model”, “Diffusion of Innovation”, etc.). The analysis reveals that BDAD research to demonstrate attributes suggestive of a topic at an initial stage of development as it is broadly dispersed across different domains employs a wide range of models, some of which overlap. Most of the applied models are generic in nature focusing on variance-based relationships and snapshot prediction with little consensus. There is a conspicuous dearth of process models, firm-level analysis and cultural orientation in contemporary BDAD research. Insights of this bibliometric study could guide rigorous big data research and practice in various contexts. The study concludes with research implications and limitations that offer promising prospects for forthcoming research.}
}

@article{rayyan-727967049,
  title={The impact of Software Testing education on code reliability: An empirical assessment},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={137},
  pages={497-511},
  author={Lazzarini Lemos, Otávio Augusto and Fagundes Silveira, Fábio and Cutigi Ferrari, Fabiano and Garcia, Alessandro},
  url={https://www.sciencedirect.com/science/article/pii/S0164121217300419},
  keywords={Computer science education, Software Testing, Student experiments, Software},
  abstract={Software Testing (ST) is an indispensable part of software development. Proper testing education is thus of paramount importance. Indeed, the mere exposition to ST knowledge might have an impact on programming skills. In particular, it can encourage the production of more correct - and thus reliable - code. Although this is intuitive, to the best of our knowledge, there are no studies about such effects. Concerned with this, we have conducted two investigations related to ST education: (1) a large experiment with students to evaluate the possible impact of ST knowledge on the production of reliable code; and (2) a survey with professors that teach introductory programming courses to evaluate their level of ST knowledge. Our study involved 60 senior-level computer science students, 8 auxiliary functions with 92 test cases, a total of 248 implementations, and 53 professors of diverse subfields that completed our survey. The investigation with students shows that ST knowledge can improve code reliability in terms of correctness in as much as 20%, on average. On the other hand, the survey with professors reveals that, in general, university instructors tend to lack the same knowledge that would help students increase their programming skills toward more reliable code.}
}

@article{rayyan-727967050,
  title={Operationalised product quality models and assessment: The Quamoco approach},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={62},
  pages={101-123},
  author={Wagner, Stefan and Goeb, Andreas and Heinemann, Lars and Kläs, Michael and Lampasona, Constanza and Lochmann, Klaus and Mayr, Alois and Plösch, Reinhold and Seidl, Andreas and Streit, Jonathan and Trendowicz, Adam},
  url={https://www.sciencedirect.com/science/article/pii/S0950584915000452},
  keywords={Product quality, Quality assessment, Quality model},
  abstract={Context Software quality models provide either abstract quality characteristics or concrete quality measurements; there is no seamless integration of these two aspects. Quality assessment approaches are, hence, also very specific or remain abstract. Reasons for this include the complexity of quality and the various quality profiles in different domains which make it difficult to build operationalised quality models. Objective In the project Quamoco, we developed a comprehensive approach aimed at closing this gap. Method The project combined constructive research, which involved a broad range of quality experts from academia and industry in workshops, sprint work and reviews, with empirical studies. All deliverables within the project were peer-reviewed by two project members from a different area. Most deliverables were developed in two or three iterations and underwent an evaluation. Results We contribute a comprehensive quality modelling and assessment approach: (1) A meta quality model defines the structure of operationalised quality models. It includes the concept of a product factor, which bridges the gap between concrete measurements and abstract quality aspects, and allows modularisation to create modules for specific domains. (2) A largely technology-independent base quality model reduces the effort and complexity of building quality models for specific domains. For Java and C# systems, we refined it with about 300 concrete product factors and 500 measures. (3) A concrete and comprehensive quality assessment approach makes use of the concepts in the meta-model. (4) An empirical evaluation of the above results using real-world software systems showed: (a) The assessment results using the base model largely match the expectations of experts for the corresponding systems. (b) The approach and models are well understood by practitioners and considered to be both consistent and well suited for getting an overall view on the quality of a software product. The validity of the base quality model could not be shown conclusively, however. (5) The extensive, open-source tool support is in a mature state. (6) The model for embedded software systems is a proof-of-concept for domain-specific quality models. Conclusion We provide a broad basis for the development and application of quality models in industrial practice as well as a basis for further extension, validation and comparison with other approaches in research.}
}

@article{rayyan-727967051,
  title={Chapter 1 - software development},
  year={1994},
  journal={Software engineer's pocket book},
  issn={978-0-7506-0749-0},
  pages={1-61},
  author={Tooley, Michael and Tooley, Michael},
  url={https://www.sciencedirect.com/science/article/pii/B9780750607490500054},
  publisher={Newnes},
  keywords={Software}
}

@article{rayyan-727967052,
  title={Empirical validation of a usability inspection method for model-driven Web development},
  year={2013},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={86},
  number={1},
  pages={161-186},
  author={Fernandez, Adrian and Abrahão, Silvia and Insfran, Emilio},
  url={https://www.sciencedirect.com/science/article/pii/S016412121200218X},
  keywords={Model-driven development, Web applications, Family of experiments, Usability inspection},
  abstract={Web applications should be usable in order to be accepted by users and to improve their success probability. Despite the fact that this requirement has promoted the emergence of several usability evaluation methods, there is a need for empirically validated methods that provide evidence about their effectiveness and that can be properly integrated into early stages of Web development processes. Model-driven Web development processes have grown in popularity over the last few years, and offer a suitable context in which to perform early usability evaluations due to their intrinsic traceability mechanisms. These issues have motivated us to propose a Web Usability Evaluation Process (WUEP) which can be integrated into model-driven Web development processes. This paper presents a family of experiments that we have carried out to empirically validate WUEP. The family of experiments was carried out by 64 participants, including PhD and Master's computer science students. The objective of the experiments was to evaluate the participants' effectiveness, efficiency, perceived ease of use and perceived satisfaction when using WUEP in comparison to an industrial widely used inspection method: Heuristic Evaluation (HE). The statistical analysis and meta-analysis of the data obtained separately from each experiment indicated that WUEP is more effective and efficient than HE in the detection of usability problems. The evaluators were also more satisfied when applying WUEP, and found it easier to use than HE. Although further experiments must be carried out to strengthen these results, WUEP has proved to be a promising usability inspection method for Web applications which have been developed by using model-driven development processes.}
}

@article{rayyan-727967053,
  title={On using planning poker for estimating user stories},
  year={2012},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={85},
  number={9},
  pages={2086-2095},
  author={Mahnič, Viljan and Hovelja, Tomaž},
  url={https://www.sciencedirect.com/science/article/pii/S0164121212001021},
  keywords={Effort estimation, Expert judgment, Agile software development, Planning poker, Scrum, User story},
  abstract={While most studies in psychology and forecasting stress the possible hazards of group processes when predicting effort and schedule, agile software development methods recommend the use of a group estimation technique called planning poker for estimating the size of user stories and developing release and iteration plans. It is assumed that the group discussion through planning poker helps in identifying activities that individual estimators could overlook, thus providing more accurate estimates and reducing the over-optimism that is typical for expert judgment-based methods. In spite of the widespread use of agile methods, there is little empirical evidence regarding the accuracy of planning poker estimates. In order to fill this gap a study was conducted requiring 13 student teams to develop a Web-based student records information system. All teams were given the same set of user stories which had to be implemented in three Sprints. Each team estimated the stories using planning poker and the estimates provided by each team member during the first round were averaged to obtain the statistical combination for further comparison. In the same way the stories were estimated by a group of experts. The study revealed that students' estimates were over-optimistic and that planning poker additionally increased the over-optimism. On the other hand, the experts' estimates obtained through planning poker were much closer to actual effort spent and tended to be more accurate than the statistical combination of their individual estimates. The results indicate that the optimism bias caused by group discussion diminishes or even disappears as the expertise of the people involved in the group estimation process increases.}
}

@article{rayyan-727967054,
  title={Ontology-based context modeling in service-oriented computing: A systematic mapping},
  year={2017},
  journal={Data & Knowledge Engineering},
  issn={0169-023X},
  volume={110},
  pages={24-53},
  author={Cabrera, Oscar and Franch, Xavier and Marco, Jordi},
  url={https://www.sciencedirect.com/science/article/pii/S0169023X16301380},
  keywords={Systematic mapping, Context modeling, Context-aware computing, Ontology, Service-oriented computing},
  abstract={Context Service-oriented computing and context-aware computing are two consolidated paradigms that are changing the way of providing and consuming software services. Whilst service-oriented computing is based on service-oriented architectures for providing flexible software services, context-aware computing articulates different phases of a context life cycle for changing the behavior of such services. The synergy between both paradigms provides the context to this study. Objective This study analyzes the current state of the art of context models, specifically: (1) which are these proposals and how are they related; (2) what are their structural characteristics; (3) what context information is the most addressed; and (4) what are their most consolidated definitions. Given their dominance on the field, the study focuses on ontology-based approaches. Method We conducted a systematic mapping by establishing a review protocol that integrates automatic and manual searches from different sources. We applied a rigorous method to elicit the keywords from the research questions and selection criteria to retrieve the papers to evaluate. Results Overall, 138 primary studies were selected to answer our research questions. These proposals were studied in depth by analyzing: 1) distribution along time and their relationships; 2) size correlated with the number of classes and levels of the context model, and coverage of the definitions provided as indicator of quality provided; 3) most addressed context information; 4) most consolidated definitions of context information. Conclusions The contribution of this survey is to make available a unified and consolidated body of knowledge on context for service-oriented computing that could be instantiated and used as starting point in a variety of use cases. This sweeping view on the anatomy of context models may help avoiding the postulation of new proposals not aligned with the current research.}
}

@article{rayyan-727967055,
  title={Verification of model transformations: A survey of the state-of-the-art},
  year={2013},
  journal={Electronic Notes in Theoretical Computer Science},
  issn={1571-0661},
  volume={292},
  pages={5-25},
  author={Calegari, Daniel and Szasz, Nora},
  url={https://www.sciencedirect.com/science/article/pii/S1571066113000042},
  keywords={formal verification, model transformations, Model-Driven Engineering},
  abstract={Within the Model-Driven Engineering paradigm, software development is based on the definition of models providing different views of the system to be constructed and model transformations supporting a (semi)automatic development process. The verification of models and model transformations is crucial in order to improve the quality and the reliability of the products developed using this paradigm. In this context, the verification of a model transformation has three main components: the transformation itself, the properties of interest addressed, and the verification techniques used to establish the properties. In this paper we present an exhaustive review of the literature on the verification of model transformations analyzing these three components. We also take a problem-based approach exemplifying those aspects of interest that could be verified on a model transformation and show how this can be done. Finally, we conclude the need of an integrated environment for addressing the heterogeneous verification of model transformations.}
}

@article{rayyan-727967056,
  title={Distributed modeling of use case diagrams with a method based on think-pair-square: Results from two controlled experiments},
  year={2014},
  journal={Journal of Visual Languages & Computing},
  issn={1045-926X},
  volume={25},
  number={4},
  pages={494-517},
  author={Scanniello, Giuseppe and Erra, Ugo},
  url={https://www.sciencedirect.com/science/article/pii/S1045926X14000329},
  keywords={Distributed Software Development, Experiments, Requirements Modeling},
  abstract={Objective: In this paper, we present the results of two controlled experiments conducted to assess a new method based on think-pair-square in the distributed modeling of use case diagrams. Methods: This new method has been implemented within an integrated environment, which allows distributed synchronous modeling and communication among team members. To study the effect of the participants׳ familiarity with the method and the integrated environment, the second experiment is a replication conducted with the same participants as the original experiment. The results show a significant difference in favor of face-to-face (i.e., the chosen baseline) for the time to complete modeling tasks, with no significant impact on the quality of the produced models. Results: The results on participants׳ familiarity indicate a significant effect on the task completion time (i.e., more familiar participants spent less time), with no significant impact on quality. Practice: One of the most interesting practical implications of our study is - in case the time difference is not an issue, but moving people might be a problem, the new method and environment could represent a viable alternative to face-to-face. Another significant result is that also people not perfectly trained on our method and environment may benefit from their use: the training phase could be shortened or skipped. In addition, face-to-face is less prone to consolidate participants׳ working style and to develop a shared working habit of participants. Implications: This work is in the direction of the media-effect theories applied to requirements engineering. The results indicate that the participants in the experiments significantly spent less time when modeling use case diagrams using face-to-face. Conversely, no significant difference was observed on the quality of the artifacts produced by the participants in the these tasks.}
}

@article{rayyan-727967057,
  title={Cloud computing and education: A state-of-the-art survey},
  year={2015},
  journal={Computers & Education},
  issn={0360-1315},
  volume={80},
  pages={132-151},
  author={González-Martínez, José A and Bote-Lorenzo, Miguel L and Gómez-Sánchez, Eduardo and Cano-Parra, Rafael},
  url={https://www.sciencedirect.com/science/article/pii/S0360131514001985},
  keywords={Architectures for educational technology system, Distributed learning environments, Improving classroom teaching, Interactive learning environments, Public spaces and computing},
  abstract={This paper surveys the state of the art on the use and research of cloud computing in education following a systematic methodology. After a comprehensive search of the scientific literature, 112 works were selected for the review. The survey identifies and analyzes the advantages and risks that the use of cloud computing may have for the main stakeholders in education, which can be useful to identify the scenarios in which the use of cloud computing in an educational context may have significant advantages. Furthermore, the survey categorizes and discusses the main technical and domain-specific research challenges, thus facilitating researchers the task of finding relevant issues, in which they can focus their efforts.}
}

@article{rayyan-727967058,
  title={Privacy in smart toys: Risks and proposed solutions},
  year={2020},
  journal={Electronic Commerce Research and Applications},
  issn={1567-4223},
  volume={39},
  pages={100922},
  author={de Paula Albuquerque, Otávio and Fantinato, Marcelo and Kelner, Judith and de Albuquerque, Anna Priscilla},
  url={https://www.sciencedirect.com/science/article/pii/S1567422319300997},
  keywords={Literature review, Children, Connected toys, Data privacy, Internet of Toys, Scoping review, Privacy},
  abstract={Smart toys have become popular as technological solutions offer a better experience for children. However, the technology employed greatly increases the risks to children's privacy, which does not seem to have become a real concern for toy makers. We investigated this issue through a study driven by two major research questions: which are the major smart toys-related children's privacy risks and which are the major mitigation so to such risks. To answer these questions, we conducted a scoping review. As a result, we selected 26 primary studies and elaborated two classifications of risks and proposed solutions – technical and domain-specific. The most mentioned technical risk is data disclosure, while from a domain-specific perspective there is much concern on the children's physical and psychological safety. From a mitigation standpoint, many recommendations and solutions have been proposed, but without a more common type of contribution. As a main conclusion, we observed that toy makers and privacy regulations are not yet ready regarding children's privacy for a more active smart toys market.}
}

@article{rayyan-727967059,
  title={Application layer HTTP-GET flood DDoS attacks: Research landscape and challenges},
  year={2017},
  journal={Computers & Security},
  issn={0167-4048},
  volume={65},
  pages={344-372},
  author={Singh, Karanpreet and Singh, Paramvir and Kumar, Krishan},
  url={https://www.sciencedirect.com/science/article/pii/S0167404816301365},
  keywords={Application layer DDoS attacks, Denial of service, HTTP-GET flood, Sophisticated attacks, Systematic survey},
  abstract={Application layer Distributed Denial of Service (DDoS) attacks have empowered conventional flooding based DDoS with more subtle attacking methods that pose an ever-increasing challenge to the availability of Internet based web services. These attacks hold the potential to cause similar damaging effects as their lower layer counterparts using relatively fewer attacking assets. Being the dominant part of the Internet, HTTP is the prime target of GET flooding attacks, a common practice followed among various application layer DDoS attacks. With the presence of new and improved attack programs, identifying these attacks always seems convoluted. A swift rise in the frequency of these attacks has led to a favorable shift in interest among researchers. Over the recent years, a significant research contribution has been dedicated toward devising new techniques for countering HTTP-GET flood DDoS attacks. In this paper, we conduct a survey of such research contributions following a well-defined systematic process. A total of 63 primary studies published before August 2015 were selected from six different electronic databases following a careful scrutinizing process. We formulated four research questions that capture various aspects of the identified primary studies. These aspects include detection attributes, datasets, software tools, attack strategies, and underlying modeling methods. The field background required to understand the evolution of HTTP-GET flood DDoS attacks is also presented. The aim of this systematic survey is to gain insights into the current research on the detection of these attacks by comprehensively analyzing the selected primary studies to answer a predefined set of research questions. This survey also discusses various challenges that need to be addressed, and acquaints readers with recommendations for possible future research directions.}
}

@article{rayyan-727967060,
  title={Graphical user interface (GUI) testing: Systematic mapping and repository},
  year={2013},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={55},
  number={10},
  pages={1679-1694},
  author={Banerjee, Ishan and Nguyen, Bao and Garousi, Vahid and Memon, Atif},
  url={https://www.sciencedirect.com/science/article/pii/S0950584913000669},
  keywords={Bibliometrics, Systematic mapping, GUI application, Paper repository, Testing},
  abstract={Context GUI testing is system testing of a software that has a graphical-user interface (GUI) front-end. Because system testing entails that the entire software system, including the user interface, be tested as a whole, during GUI testing, test cases—modeled as sequences of user input events—are developed and executed on the software by exercising the GUI's widgets (e.g., text boxes and clickable buttons). More than 230 articles have appeared in the area of GUI testing since 1991. Objective In this paper, we study this existing body of knowledge using a systematic mapping (SM). Method The SM is conducted using the guidelines proposed by Petersen et al. We pose three sets of research questions. We define selection and exclusion criteria. From the initial pool of 230 articles, published in years 1991–2011, our final pool consisted of 136 articles. We systematically develop a classification scheme and map the selected articles to this scheme. Results We present two types of results. First, we report the demographics and bibliometrics trends in this domain, including: top-cited articles, active researchers, top venues, and active countries in this research area. Moreover, we derive the trends, for instance, in terms of types of articles, sources of information to derive test cases, types of evaluations used in articles, etc. Our second major result is a publicly-accessible repository that contains all our mapping data. We plan to update this repository on a regular basis, making it a “live” resource for all researchers. Conclusion Our SM provides an overview of existing GUI testing approaches and helps spot areas in the field that require more attention from the research community. For example, much work is needed to connect academic model-based techniques with commercially available tools. To this end, studies are needed to compare the state-of-the-art in GUI testing in academic techniques and industrial tools.}
}

@article{rayyan-727967061,
  title={Finding faults: A scoping study of fault diagnostics for Industrial Cyber–Physical Systems},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={168},
  pages={110638},
  author={Dowdeswell, Barry and Sinha, Roopak and MacDonell, Stephen G},
  url={https://www.sciencedirect.com/science/article/pii/S0164121220301114},
  keywords={Aerospace, Automotive, Avionics, Faults, Industrial control, Industrial cyber–physical systems},
  abstract={Context: As Industrial Cyber–Physical Systems (ICPS) become more connected and widely-distributed, often operating in safety-critical environments, we require innovative approaches to detect and diagnose the faults that occur in them. Objective: We profile fault identification and diagnosis techniques employed in the aerospace, automotive, and industrial control domains. Each of these sectors has adopted particular methods to meet their differing diagnostic needs. By examining both theoretical presentations as well as case studies from production environments, we present a profile of the current approaches being employed and identify gaps. Methodology: A scoping study was used to identify and compare fault detection and diagnosis methodologies that are presented in the current literature. We created categories for the different diagnostic approaches via a pilot study and present an analysis of the trends that emerged. We then compared the maturity of these approaches by adapting and using the NASA Technology Readiness Level (TRL) scale. Results: Fault identification and analysis studies from 127 papers published from 2004 to 2019 reveal a wide diversity of promising techniques, both emerging and in-use. These range from traditional Physics-based Models to Data-Driven Artificial Intelligence (AI) and Knowledge-Based approaches. Hybrid techniques that blend aspects of these three broad categories were also encountered. Predictive diagnostics or prognostics featured prominently across all sectors, along with discussions of techniques including Fault trees, Petri nets and Markov approaches. We also profile some of the techniques that have reached the highest Technology Readiness Levels, showing how those methods are being applied in real-world environments beyond the laboratory. Conclusions: Our results suggest that the continuing wide use of both Model-Based and Data-Driven AI techniques across all domains, especially when they are used together in hybrid configuration, reflects the complexity of the current ICPS application space. While creating sufficiently-complete models is labor intensive, Model-free AI techniques were evidenced as a viable way of addressing aspects of this challenge, demonstrating the increasing sophistication of current machine learning systems. Connecting ICPS together to share sufficient telemetry to diagnose and manage faults is difficult when the physical environment places demands on ICPS. Despite these challenges, the most mature papers present robust fault diagnosis and analysis techniques which have moved beyond the laboratory and are proving valuable in real-world environments.}
}

@article{rayyan-727967062,
  title={Securing web applications from injection and logic vulnerabilities: Approaches and challenges},
  year={2016},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={74},
  pages={160-180},
  author={Deepa, G and Thilagam, P Santhi},
  url={https://www.sciencedirect.com/science/article/pii/S0950584916300234},
  keywords={Application logic vulnerabilities, Business logic vulnerabilities, Cross-site scripting, Injection flaws, SQL injection, Web application security},
  abstract={Context: Web applications are trusted by billions of users for performing day-to-day activities. Accessibility, availability and omnipresence of web applications have made them a prime target for attackers. A simple implementation flaw in the application could allow an attacker to steal sensitive information and perform adversary actions, and hence it is important to secure web applications from attacks. Defensive mechanisms for securing web applications from the flaws have received attention from both academia and industry. Objective: The objective of this literature review is to summarize the current state of the art for securing web applications from major flaws such as injection and logic flaws. Though different kinds of injection flaws exist, the scope is restricted to SQL Injection (SQLI) and Cross-site scripting (XSS), since they are rated as the top most threats by different security consortiums. Method: The relevant articles recently published are identified from well-known digital libraries, and a total of 86 primary studies are considered. A total of 17 articles related to SQLI, 35 related to XSS and 34 related to logic flaws are discussed. Results: The articles are categorized based on the phase of software development life cycle where the defense mechanism is put into place. Most of the articles focus on detecting the flaws and preventing the attacks against web applications. Conclusion: Even though various approaches are available for securing web applications from SQLI and XSS, they are still prevalent due to their impact and severity. Logic flaws are gaining attention of the researchers since they violate the business specifications of applications. There is no single solution to mitigate all the flaws. More research is needed in the area of fixing flaws in the source code of applications.}
}

@article{rayyan-727967063,
  title={A standard-based framework to integrate software work in small settings},
  year={2017},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={54},
  pages={162-175},
  author={Sanchez-Gordon, Mary-Luz and de Amescua, Antonio and O'Connor, Rory V and Larrucea, Xabier},
  url={https://www.sciencedirect.com/science/article/pii/S0920548916301891},
  keywords={Human factors, Small companies, Small settings, Socio-technical system, Software process improvement, VSE, Software},
  abstract={Small software companies have to work hard in order to survive. They usually find it challenging to spend time and effort on improving their operations and processes. Therefore, it is important to address such needs by the introduction of a proposed framework that specifies ways of getting things done while consciously encourage them to enhance their ability to improve. Although there are many software process improvement approaches, none of them address the human factors of small companies in a comprehensive and holistic way. Samay is a proposed framework to integrate human factors in the daily work as a way to deal with that challenge. This study suggests managing human factors but pointing out the software process life cycle. The purpose is to converge toward a continuous improvement by means of alternative mechanisms that impact on people. This framework was developed based upon reviews of relevant standards (such as ISO/IEC 29110, ISO 10018, OMG Essence and ISO/IEC 33014) and previously published studies in this field. Moreover, an expert review and validation findings supported the view that Samay could support practitioners when small software companies want to start improving their ways of work.}
}

@article{rayyan-727967064,
  title={Cost optimization approaches for scientific workflow scheduling in cloud and grid computing: A review, classifications, and open issues},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={113},
  pages={1-26},
  author={Alkhanak, Ehab Nabiel and Lee, Sai Peck and Rezaei, Reza and Parizi, Reza Meimandi},
  url={https://www.sciencedirect.com/science/article/pii/S0164121215002484},
  keywords={Cloud computing, Scheduling, Scientific workflow},
  abstract={Workflow scheduling in scientific computing systems is one of the most challenging problems that focuses on satisfying user-defined quality of service requirements while minimizing the workflow execution cost. Several cost optimization approaches have been proposed to improve the economic aspect of Scientific Workflow Scheduling (SWFS) in cloud and grid computing. To date, the literature has not yet seen a comprehensive review that focuses on approaches for supporting cost optimization in the context of SWFS in cloud and grid computing. Furthermore, providing valuable guidelines and analysis to understand the cost optimization of SWFS approaches is not well-explored in the current literature. This paper aims to analyze the problem of cost optimization in SWFS by extensively surveying existing SWFS approaches in cloud and grid computing and provide a classification of cost optimization aspects and parameters of SWFS. Moreover, it provides a classification of cost based metrics that are categorized into monetary and temporal cost parameters based on various scheduling stages. We believe that our findings would help researchers and practitioners in selecting the most appropriate cost optimization approach considering identified aspects and parameters. In addition, we highlight potential future research directions in this on-going area of research.}
}

@article{rayyan-727967065,
  title={A decision support framework for metrics selection in goal-based measurement programs: GQM-DSFMS},
  year={2013},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={86},
  number={12},
  pages={3091-3108},
  author={Gencel, Cigdem and Petersen, Kai and Mughal, Aftab Ahmad and Iqbal, Muhammad Imran},
  url={https://www.sciencedirect.com/science/article/pii/S0164121213001726},
  keywords={Optimization, Decision support, Goal based measurement, Goal Question Metric, GQM, Prioritization, Software measurement program, Metronidazole},
  abstract={Software organizations face challenges in managing and sustaining their measurement programs over time. The complexity of measurement programs increase with exploding number of goals and metrics to collect. At the same time, organizations usually have limited budget and resources for metrics collection. It has been recognized for quite a while that there is the need for prioritizing goals, which then ought to drive the selection of metrics. On the other hand, the dynamic nature of the organizations requires measurement programs to adapt to the changes in the stakeholders, their goals, information needs and priorities. Therefore, it is crucial for organizations to use structured approaches that provide transparency, traceability and guidance in choosing an optimum set of metrics that would address the highest priority information needs considering limited resources. This paper proposes a decision support framework for metrics selection (DSFMS) which is built upon the widely used Goal Question Metric (GQM) approach. The core of the framework includes an iterative goal-based metrics selection process incorporating decision making mechanisms in metrics selection, a pre-defined Attributes/Metrics Repository, and a Traceability Model among GQM elements. We also discuss alternative prioritization and optimization techniques for organizations to tailor the framework according to their needs. The evaluation of the GQM-DSFMS framework was done through a case study in a CMMI Level 3 software company.}
}

@article{rayyan-727967066,
  title={Agile methods in embedded system development: Multiple-case study of three industrial cases},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={118},
  pages={134-150},
  author={Könnölä, Kaisa and Suomi, Samuli and Mäkilä, Tuomas and Jokela, Tero and Rantala, Ville and Lehtonen, Teijo},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216300413},
  keywords={Agile, Agile method, Case study, Embedded system},
  abstract={Agile methods are widely utilized in software development but their usage in embedded system development is often limited to software. A case study of three industrial cases was carried out to understand how to tailor agile methods effectively including also hardware development. Agile practices, mostly derived from Scrum, were tailored to fit the needs of each team and the method development was closely followed. Surveys conducted in the beginning and in the end of the cases were compared and complemented with interviews to understand the new working methods and their effects. Case evidence shows that interdependencies between work of each developer were taken into account better, visibility over the whole product increased and need for internal documentation diminished due to improved communication, but dividing hardware tasks into iterations was experienced difficult. With some tailoring, agile practices are beneficial also in the embedded system development. To successfully adopt agile methods into embedded system development, the team must consist of all the project members, the natural cycle lengths of different disciplines and different knowledge between the developers must be accepted and built upon, and the progress of the product must be presented or visualized in the end of each iteration.}
}

@article{rayyan-727967067,
  title={Emergence of DSS efforts in genomics: Past contributions and challenges},
  year={2019},
  journal={Decision Support Systems},
  issn={0167-9236},
  volume={116},
  pages={77-90},
  author={Sen, Arun and Al Kawam, Ahmad and Datta, Aniruddha},
  url={https://www.sciencedirect.com/science/article/pii/S0167923618301684},
  keywords={Clinical decision support, Data integration, Decision support system, Decision support system architecture, Genomics, Knowledge base integration, Genome},
  abstract={Large amounts of data in biomedical research (from clinical data to gene expression data) are being generated. Use of these data sets and their associated knowledge are essential to understand the biological mechanisms behind diseases. While patients' clinical data from EHR can help researchers accurately and appropriately trace the performance of various kinds of medicines on the patients, the microarray data for the same pool of patients can contain valuable information for discovery of disease-associated gene expression patterns and can help classify the patients. However, research in the area of integrating genomic data with clinical data is still in its infancy and is riddled with many challenges. Even though data and knowledge sets are easily available from genome sequences and protein structural data of organisms, they usually are of many different varieties. Integrating them for a better understanding of biological functions at all levels is complicated. If we want to obtain the full benefit of functional genomics, we need to find a seamless way to integrate large amounts of patient datasets with genomic datasets in the field of biomedicine. Few papers in the decision support systems (DSS) literature provide an overview of Genomic Clinical Decision Support (GCDS) challenges that span data, knowledge, input/output, and architecture/implementation. This paper presents a unique effort dedicated to providing a comprehensive listing and a concise description of the DSS methodological challenges that arise from integrating complex and massive-scale genomic data with Clinical Decision Support (CDS) systems.}
}

@article{rayyan-727967068,
  title={A COSMIC function points based test effort estimation model for mobile applications},
  year={2019},
  journal={Journal of King Saud University - Computer and Information Sciences},
  issn={1319-1578},
  author={Kaur, Anureet and Kaur, Kulwant},
  url={https://www.sciencedirect.com/science/article/pii/S131915781831317X},
  keywords={COSMIC, Estimation, Mobile applications, Multiple linear regression, Test effort},
  abstract={With the substantial proliferation in demand for applications running on mobile devices, developers and testers are foreseen to release applications with high caliber, on time and inside budget. Mobile application testing is imperative and perilous activity in the application development lifecycle, which confirms quality and unswervingly impacts the development effort and affluence of the application. The estimation of effort for testing of apps is a key figure that helps test managers in making approximate accurate decisions for coordinating testing resources. In this study, a regression model is presented to predict test effort for mobile applications considering COSMIC Function Size Measurement (FSM), mobile app characteristics/factors and test factors. The major benefit of this model is that it tends to be utilized at the beginning of mobile app testing life cycle, and thus can assist testers to effectively lead early effort estimation. The model presented is further validated and evaluated for their effectiveness by using a k-fold cross-validation method. MRE, MMRE, MdMRE, PRED (0.25) and PRED (0.50) indices are used for measuring the accuracy of the model and findings suggest that the proposed model gives a good prediction and can be exercised in the mobile software industry for predicting test effort.}
}

@article{rayyan-727967069,
  title={Using a multi-method approach to understand Agile software product lines},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={57},
  pages={527-542},
  author={da Silva, Ivonei Freitas and da Mota Silveira Neto, Paulo Anselmo and O'Leary, Pádraig and de Almeida, Eduardo Santana and de Lemos Meira, Silvio Romero},
  url={https://www.sciencedirect.com/science/article/pii/S0950584914001438},
  keywords={Software product lines, Agile, Case study, Expert opinion, Mapping study, Multi-method approach, Software},
  abstract={Context Software product lines (SPLs) and Agile are approaches that share similar objectives. The main difference is the way in which these objectives are met. Typically evidence on what activities of Agile and SPL can be combined and how they can be integrated stems from different research methods performed separately. The generalizability of this evidence is low, as the research topic is still relatively new and previous studies have been conducted using only one research method. Objective This study aims to increase understanding of Agile SPL and improve the generalizability of the identified evidence through the use of a multi-method approach. Method Our multi-method research combines three complementary methods (Mapping Study, Case Study and Expert Opinion) to consolidate the evidence. Results This combination results in 23 findings that provide evidence on how Agile and SPL could be combined. Conclusion Although multi-method research is time consuming and requires a high degree of effort to plan, design, and perform, it helps to increase the understanding on Agile SPL and leads to more generalizable evidence. The findings confirm a synergy between Agile and SPL and serve to improve the body of evidence in Agile SPL. When researchers and practitioners develop new Agile SPL approaches, it will be important to consider these synergies.}
}

@article{rayyan-727967070,
  title={Automatic recall of software lessons learned for software project managers},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={115},
  pages={44-57},
  author={Abdellatif, Tamer Mohamed and Capretz, Luiz Fernando and Ho, Danny},
  url={https://www.sciencedirect.com/science/article/pii/S0950584919301612},
  keywords={Information extraction, Knowledge management, Software lessons learned recall, Software project management, Textual information retrieval models, Topic modeling application, Software},
  abstract={Context Lessons learned (LL) records constitute the software organization memory of successes and failures. LL are recorded within the organization repository for future reference to optimize planning, gain experience, and elevate market competitiveness. However, manually searching this repository is a daunting task, so it is often disregarded. This can lead to the repetition of previous mistakes or even missing potential opportunities. This, in turn, can negatively affect the organization's profitability and competitiveness. Objective We aim to present a novel solution that provides an automatic process to recall relevant LL and to push those LL to project managers. This will dramatically save the time and effort of manually searching the unstructured LL repositories and thus encourage the LL exploitation. Method We exploit existing project artifacts to build the LL search queries on-the-fly in order to bypass the tedious manual searching. An empirical case study is conducted to build the automatic LL recall solution and evaluate its effectiveness. The study employs three of the most popular information retrieval models to construct the solution. Furthermore, a real-world dataset of 212 LL records from 30 different software projects is used for validation. Top-k and MAP well-known accuracy metrics are used as well. Results Our case study results confirm the effectiveness of the automatic LL recall solution. Also, the results prove the success of using existing project artifacts to dynamically build the search query string. This is supported by a discerning accuracy of about 70% achieved in the case of top-k. Conclusion The automatic LL recall solution is valid with high accuracy. It will eliminate the effort needed to manually search the LL repository. Therefore, this will positively encourage project managers to reuse the available LL knowledge – which will avoid old pitfalls and unleash hidden business opportunities.}
}

@article{rayyan-727967071,
  title={Validating a model-driven software architecture evaluation and improvement method: A family of experiments},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={57},
  pages={405-429},
  author={Gonzalez-Huerta, Javier and Insfran, Emilio and Abrahão, Silvia and Scanniello, Giuseppe},
  url={https://www.sciencedirect.com/science/article/pii/S0950584914001359},
  keywords={Meta-analysis, Family of experiments, ATAM, Quality attributes, Software architecture evaluation methods, Software architectures, Software},
  abstract={Context Software architectures should be evaluated during the early stages of software development in order to verify whether the non-functional requirements (NFRs) of the product can be fulfilled. This activity is even more crucial in software product line (SPL) development, since it is also necessary to identify whether the NFRs of a particular product can be achieved by exercising the variation mechanisms provided by the product line architecture or whether additional transformations are required. These issues have motivated us to propose QuaDAI, a method for the derivation, evaluation and improvement of software architectures in model-driven SPL development. Objective We present in this paper the results of a family of four experiments carried out to empirically validate the evaluation and improvement strategy of QuaDAI. Method The family of experiments was carried out by 92 participants: Computer Science Master's and undergraduate students from Spain and Italy. The goal was to compare the effectiveness, efficiency, perceived ease of use, perceived usefulness and intention to use with regard to participants using the evaluation and improvement strategy of QuaDAI as opposed to the Architecture Tradeoff Analysis Method (ATAM). Results The main result was that the participants produced their best results when applying QuaDAI, signifying that the participants obtained architectures with better values for the NFRs faster, and that they found the method easier to use, more useful and more likely to be used. The results of the meta-analysis carried out to aggregate the results obtained in the individual experiments also confirmed these results. Conclusions The results support the hypothesis that QuaDAI would achieve better results than ATAM in the experiments and that QuaDAI can be considered as a promising approach with which to perform architectural evaluations that occur after the product architecture derivation in model-driven SPL development processes when carried out by novice software evaluators.}
}

@article{rayyan-727967072,
  title={Process mining techniques and applications – A systematic mapping study},
  year={2019},
  journal={Expert Systems with Applications},
  issn={0957-4174},
  volume={133},
  pages={260-295},
  author={dos Santos Garcia, Cleiton and Meincheim, Alex and Faria Junior, Elio Ribeiro and Dallagassa, Marcelo Rosano and Sato, Denise Maria Vecino and Carvalho, Deborah Ribeiro and Santos, Eduardo Alves Portela and Scalabrin, Edson Emilio},
  url={https://www.sciencedirect.com/science/article/pii/S0957417419303161},
  keywords={Process mining, Process mining applications, Process mining case studies, Workflow mining},
  abstract={Process mining is a growing and promising study area focused on understanding processes and to help capture the more significant findings during real execution rather than, those methods that, only observed idealized process model. The objective of this article is to map the active research topics of process mining and their main publishers by country, periodicals, and conferences. We also extract the reported application studies and classify these by exploration domains or industry segments that are taking advantage of this technique. The applied research method was systematic mapping, which began with 3713 articles. After applying the exclusion criteria, 1278 articles were selected for review. In this article, an overview regarding process mining is presented, the main research topics are identified, followed by identification of the most applied process mining algorithms, and finally application domains among different business segments are reported on. It is possible to observe that the most active research topics are associated with the process discovery algorithms, followed by conformance checking, and architecture and tools improvements. In application domains, the segments with major case studies are healthcare followed by information and communication technology, manufacturing, education, finance, and logistics.}
}

@article{rayyan-727967073,
  title={Code smells as system-level indicators of maintainability: An empirical study},
  year={2013},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={86},
  number={10},
  pages={2639-2653},
  author={Yamashita, Aiko and Counsell, Steve},
  url={https://www.sciencedirect.com/science/article/pii/S0164121213001258},
  keywords={Empirical study, Code smells, Maintainability, System evaluation, Smell},
  abstract={Context Code smells are manifestations of design flaws that can degrade code maintainability. So far, no research has investigated if these indicators are useful for conducting system-level maintainability evaluations. Aim The research in this paper investigates the potential of code smells to reflect system-level indicators of maintainability. Method We evaluated four medium-sized Java systems using code smells and compared the results against previous evaluations on the same systems based on expert judgment and the Chidamber and Kemerer suite of metrics. The systems were maintained over a period of up to 4 weeks. During maintenance, effort (person-hours) and number of defects were measured to validate the different evaluation approaches. Results Most code smells are strongly influenced by size; consequently code smells are not good indicators for comparing the maintainability of systems differing greatly in size. Also, from the comparison of the different evaluation approaches, expert judgment was found as the most accurate and flexible since it considered effects due to the system's size and complexity and could adapt to different maintenance scenarios. Conclusion Code smell approaches show promise as indicators of the need for maintenance in a way that other purely metric-based approaches lack.}
}

@article{rayyan-727967074,
  title={Architecture enforcement concerns and activities - An expert study},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={145},
  pages={79-97},
  author={Schröder, Sandra and Soliman, Mohamed and Riebisch, Matthias},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218301614},
  keywords={Empirical study, Architecture enforcement, Architecture erosion, Software architecture, Software architecture in industry},
  abstract={Architecture enforcement is concerned with the correct and seamless implementation of architecture design decisions in order to ensure software quality. In a previous study, we conducted an empirical study in order to gain insight into the industrial practice of architecture enforcement. There, we asked 12 software architects from industry about their experience with the architecture enforcement process. As a result, we identified architecture enforcement concerns and activities. In this paper, we extend our contributions of the existing study. Firstly, we conducted five additional interviews with software architects from two different domains, namely the enterprise application and the automotive domain. This adds new architecture concerns and activities to the existing list. Secondly, we conducted a literature review. We compared our findings from the interviews with the results from the literature review and evaluated how architects' enforcement concerns and activities are discussed in literature. We found that several concerns and activities are already known from literature, but are not viewed in the context of architecture enforcement by the software architecture community. Lastly, we connected the discovered architecture concerns and activities with each other. Those relationships determine the reason why a specific architecture enforcement activity is conducted.}
}

@article{rayyan-727967075,
  title={Skill assessment in learning experiences based on serious games: A Systematic Mapping Study},
  year={2017},
  journal={Computers & Education},
  issn={0360-1315},
  volume={113},
  pages={42-60},
  author={Caballero-Hernández, Juan Antonio and Palomo-Duarte, Manuel and Dodero, Juan Manuel},
  url={https://www.sciencedirect.com/science/article/pii/S0360131517301136},
  keywords={Interactive learning environments, Game Based Learning, Serious games, Skill assessment, Systematic Mapping Study},
  abstract={Serious games are games with an educational purpose. In these games, players develop their skills by facing a number of challenges, and students are assessed according to their game playing behaviour. Assessment of serious game-based learning experiences has to take into account diverse features as game genre, pedagogical aim or game context. This paper analyses how skills are usually assessed in learning experiences based on serious games. To reach this objective, a systematic mapping study of more than 400 papers is undertaken. Papers were identified and classified according to a framework based on four categories: assessment aim, implementation, integration and primary assessment type. The reviewed literature mainly deals with contributions on methods and approaches for serious games. Results have revealed that most assessment methods are applied for a formative purpose more than for a certification purpose. Most frequent implementations such as game scoring and integrations like monitoring states were also uncovered. The main primary type of assessment detected was in-process. In addition, several limitations were found in the assessment methods: regarding the aim of assessment, certification of previous or attained skills was usually implemented out of the game; the scope of some implementations was limited because results were predefined earlier; and most of methods analysed present scalability issues because they rely on manual assessments. Such findings are analysed and discussed to clarify the state of the art and provide recommendations for further work in the area of serious games-based learning.}
}

@article{rayyan-727967076,
  title={Knowledge sharing at the construction sector – facilitators and inhibitors},
  year={2017},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={121},
  pages={998-1005},
  author={Leal, Carmem and Cunha, Sandra and Couto, Iolanda},
  url={https://www.sciencedirect.com/science/article/pii/S1877050917323311},
  keywords={Construction Industry, Facilitator Factors, Inhibitor Factors, Knowledge Sharing, Organizational Performance},
  abstract={In modern economic society, organizations' competitiveness relies heavily on their ability to leverage and manage knowledge rather than in physical assets, some authors refer even though the concepts of knowledge management (KM) are being well implemented in the industrial sector, the construction segment, however, appears to have some difficulties in adopting the practices and strategies offered by KM. It is known that the construction sector (CS) is experiencing a time of instability and recession, and one of the causes of this may involve the lack of adaptation to knowledge era. Therefore, this paper, based on literature review, gives an overview of inhibitor and the facilitator factors of knowledge sharing (KS) with the objective to answer two questions: which of the KS inhibitors and facilitators characterize the CS? has the CS its specifics KS inhibitors and facilitators? It's presented a framework in order to verify the existence of exclusive factors of the sector. At this point it is possible to suggest that CS has mainly organizational factors as inhibitors and the facilitator's factors are almost individual. The facilitating factors might be the consequence of an effective and successful KS strategy and can be seen as guidelines to improve the construction industry organizational performance.}
}

@article{rayyan-727967077,
  title={Optimization of deployment architecture in SOA systems},
  year={2020},
  journal={Procedia Manufacturing},
  issn={2351-9789},
  volume={44},
  pages={543-550},
  author={Woźniak, Adrian P},
  url={https://www.sciencedirect.com/science/article/pii/S2351978920308441},
  keywords={Optimization, Business Process, Service-Oriented Architecture, SOA},
  abstract={Service-Oriented Architecture (SOA) has been a popular foundation in corporate architecture management for years. Numerous forms of integration following this approach are developing to such an extent that they automate entire business processes in the form of service call sequences. The level of complexity of integration is growing and the popularity of methods aimed at improving the efficiency of SOA systems is increasing. This article proposes a new method for improving the efficiency of SOA systems. It is organized in the following manner. The first chapter introduces the most important concepts related to SOA. The second presents results of the literature analysis. The third describes the optimization problem and the fourth shows a potential solution. Finally, some studies of the described method and a summary of thereof are presented.}
}

@article{rayyan-727967078,
  title={IFC editorial board},
  year={2009},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={51},
  number={1},
  pages={IFC},
  url={https://www.sciencedirect.com/science/article/pii/S0950584908001432}
}

@article{rayyan-727967079,
  title={A systematic mapping study of clone visualization},
  year={2020},
  journal={Computer Science Review},
  issn={1574-0137},
  volume={37},
  pages={100266},
  author={Hammad, Muhammad and Basit, Hamid Abdul and Jarzabek, Stan and Koschke, Rainer},
  url={https://www.sciencedirect.com/science/article/pii/S1574013719302679},
  keywords={Clone, Feature analysis, Human–computer interaction, Information needs, User goals, Visualization techniques},
  abstract={Knowing code clones (similar code fragments) is helpful in software maintenance and re-engineering. As clone detectors return huge numbers of clones, visualization techniques have been proposed to make cloning information more comprehensible and useful for programmers. We present a mapping study of clone visualization techniques, classifying visualizations in respect to the user goals to be achieved by means of clone visualizations and relevant clone-related information needs. Our mapping study will aid tool users in selecting clone visualization tools suitable for the task at hand, tool vendors in improving capabilities of their tools, and researchers in identifying open problems in clone visualization research.}
}

@article{rayyan-727967080,
  title={Going with the flow: An activity theory analysis of flow techniques in software development},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={133},
  pages={160-173},
  author={Dennehy, Denis and Conboy, Kieran},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216302011},
  keywords={Activity theory, Continuous software development, Flow, Kanban, Lean, Software},
  abstract={Managing flow is fundamental to continuous development, particularly in knowledge intensive work activities such as software development. However, while numerous articles describe flow tools and practice there is little research on their application in context. This is a significant limitation given that software development is a highly complex and socially embedded activity. This research applies activity theory (AT) to examine the adoption of flow techniques by using the multiple-case method in two companies. AT is particularly pertinent in this study as it identifies contradictions, which manifest themselves as problems such as errors or a breakdown of communication in the organisation and congruencies between flow techniques and the development context and indeed contradictions between components of flow techniques themselves. Rather than view contradictions as a threat to flow or as an argument to abandon, a theoretical contribution of this study is that it shows how contradictions and congruencies can be used to reflect, learn, and identify new ways of structuring and enacting the flow activity. It also provides an immediate practical contribution by identifying a set of lessons drawn from the cases studied that may be applicable in future implementations of flow techniques.}
}

@article{rayyan-727967081,
  title={Source code metrics: A systematic mapping study},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={128},
  pages={164-197},
  author={Nuñez-Varela, Alberto S and Pérez-Gonzalez, Héctor G and Martínez-Perez, Francisco E and Soubervielle-Montalvo, Carlos},
  url={https://www.sciencedirect.com/science/article/pii/S0164121217300663},
  keywords={Systematic mapping study, Aspect-oriented metrics, Feature-oriented metrics, Object-oriented metrics, Software metrics, Source code metrics, Metronidazole},
  abstract={Context Source code metrics are essential components in the software measurement process. They are extracted from the source code of the software, and their values allow us to reach conclusions about the quality attributes measured by the metrics. Objectives This paper aims to collect source code metrics related studies, review them, and perform an analysis, while providing an overview on the current state of source code metrics and their current trends. Method A systematic mapping study was conducted. A total of 226 studies, published between the years 2010 and 2015, were selected and analyzed. Results Almost 300 source code metrics were found. Object oriented programming is the most commonly studied paradigm with the Chidamber and Kemerer metrics, lines of code, McCabe's cyclomatic complexity, and number of methods and attributes being the most used metrics. Research on aspect and feature oriented programming is growing, especially for the current interest in programming concerns and software product lines. Conclusions Object oriented metrics have gained much attention, but there is a current need for more studies on aspect and feature oriented metrics. Software fault prediction, complexity and quality assessment are recurrent topics, while concerns, big scale software and software product lines represent current trends.}
}

@article{rayyan-727967082,
  title={An exploration of IoT platform development},
  year={2020},
  journal={Information Systems},
  issn={0306-4379},
  volume={87},
  pages={101409},
  author={Fahmideh, Mahdi and Zowghi, Didar},
  url={https://www.sciencedirect.com/science/article/pii/S0306437919302728},
  keywords={Development process lifecycle, Evaluation framework, IoT platform, Smart city},
  abstract={IoT (Internet of Things) platforms are key enablers for smart city initiatives, targeting the improvement of citizens' quality of life and economic growth. As IoT platforms are dynamic, proactive, and heterogeneous socio-technical artefacts, systematic approaches are required for their development. Limited surveys have exclusively explored how IoT platforms are developed and maintained from the perspective of information system development process lifecycle. In this paper, we present a detailed analysis of 63 approaches. This is accomplished by proposing an evaluation framework as a cornerstone to highlight the characteristics, strengths, and weaknesses of these approaches. The survey results not only provide insights of empirical findings, recommendations, and mechanisms for the development of quality aware IoT platforms, but also identify important issues and gaps that need to be addressed.}
}

@article{rayyan-727967083,
  title={A study of value in agile software development organizations},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={125},
  pages={271-288},
  author={Alahyari, Hiva and Berntsson Svensson, Richard and Gorschek, Tony},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216302539},
  keywords={Agile software development, Empirical, Value, Software},
  abstract={The Agile manifesto focuses on the delivery of valuable software. In Lean, the principles emphasise value, where every activity that does not add value is seen as waste. Despite the strong focus on value, and that the primary critical success factor for software intensive product development lies in the value domain, no empirical study has investigated specifically what value is. This paper presents an empirical study that investigates how value is interpreted and prioritised, and how value is assured and measured. Data was collected through semi-structured interviews with 23 participants from 14 agile software development organisations. The contribution of this study is fourfold. First, it examines how value is perceived amongst agile software development organisations. Second, it compares the perceptions and priorities of the perceived values by domains and roles. Third, it includes an examination of what practices are used to achieve value in industry, and what hinders the achievement of value. Fourth, it characterises what measurements are used to assure, and evaluate value-creation activities.}
}

@article{rayyan-727967084,
  title={Combining service-orientation and software product line engineering: A systematic mapping study},
  year={2013},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={55},
  number={11},
  pages={1845-1859},
  author={Mohabbati, Bardia and Asadi, Mohsen and Gašević, Dragan and Hatala, Marek and Müller, Hausi A},
  url={https://www.sciencedirect.com/science/article/pii/S0950584913001274},
  keywords={Software product lines, Systematic mapping, Service-oriented architecture, Software},
  abstract={Context Service-Orientation (SO) is a rapidly emerging paradigm for the design and development of adaptive and dynamic software systems. Software Product Line Engineering (SPLE) has also gained attention as a promising and successful software reuse development paradigm over the last decade and proven to provide effective solutions to deal with managing the growing complexity of software systems. Objective This study aims at characterizing and identifying the existing research on employing and leveraging SO and SPLE. Method We conducted a systematic mapping study to identify and analyze related literature. We identified 81 primary studies, dated from 2000–2011 and classified them with respect to research focus, types of research and contribution. Result The mapping synthesizes the available evidence about combining the synergy points and integration of SO and SPLE. The analysis shows that the majority of studies focus on service variability modeling and adaptive systems by employing SPLE principles and approaches. In particular, SPLE approaches, especially feature-oriented approaches for variability modeling, have been applied to the design and development of service-oriented systems. While SO is employed in software product line contexts for the realization of product lines to reconcile the flexibility, scalability and dynamism in product derivations thereby creating dynamic software product lines. Conclusion Our study summarizes and characterizes the SO and SPLE topics researchers have investigated over the past decade and identifies promising research directions as due to the synergy generated by integrating methods and techniques from these two areas.}
}

@article{rayyan-727967085,
  title={Literature review of data model quality metrics of data warehouse},
  year={2015},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={48},
  pages={236-243},
  author={Gosain, Anjana and Heena},
  url={https://www.sciencedirect.com/science/article/pii/S1877050915006857},
  keywords={Literature review, Conceptual Data model quality metrics, Data warehouse Data Model Quality Metrics, Understandability, Metronidazole},
  abstract={Quality of data warehouse is very crucial for managerial strategic decisions. Multidimensional data modeling has been accepted as a basis for data warehouse, thus data model quality has a great impact on overall quality of data warehouse. Metrics act as a tool to measure the quality of data warehouse model. Various authors have proposed metrics to assess the quality attributes of conceptual data models for data warehouse such as understandability, maintainability etc. All the related research work inspires us to investigate the metrics proposed to measure data warehouse data model quality, the various quality factors assessed and to provide a ground work for research advancement in this field. A total of 22 studies were selected and analyzed to identify the various validation techniques used to prove usage and practical utility of metrics and the quality factors measured by these metrics. Opportunities for future work lie in the gaps that were found in the validation of the metrics and the lack of quality factors measured.}
}

@article{rayyan-727967086,
  title={Defining multi-tenancy: A systematic mapping study on the academic and the industrial perspective},
  year={2015},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={100},
  pages={139-148},
  author={Kabbedijk, Jaap and Bezemer, Cor-Paul and Jansen, Slinger and Zaidman, Andy},
  url={https://www.sciencedirect.com/science/article/pii/S0164121214002313},
  keywords={Systematic mapping study, Definition, Multi-tenancy},
  abstract={Software as a service is frequently offered in a multi-tenant style, where customers of the application and their end-users share resources such as software and hardware among all users, without necessarily sharing data. It is surprising that, with such a popular paradigm, little agreement exists with regard to the definition, domain, and challenges of multi-tenancy. This absence is detrimental to the research community and the industry, as it hampers progress in the domain of multi-tenancy and enables organizations and academics to wield their own definitions to further their commercial or research agendas. In this article, a systematic mapping study on multi-tenancy is described in which 761 academic papers and 371 industrial blogs are analysed. Both the industrial and academic perspective are assessed, in order to get a complete overview. The definition and topic maps provide a comprehensive overview of the domain, while the research agenda, listing four important research topics, provides a roadmap for future research efforts.}
}

@article{rayyan-727967087,
  title={Synthesizing information systems knowledge: A typology of literature reviews},
  year={2015},
  journal={Information & Management},
  issn={0378-7206},
  volume={52},
  number={2},
  pages={183-199},
  author={Paré, Guy and Trudel, Marie-Claude and Jaana, Mirou and Kitsiou, Spyros},
  url={https://www.sciencedirect.com/science/article/pii/S0378720614001116},
  keywords={Literature review, Evidence-based practice, Research synthesis, Typology, Information Systems},
  abstract={In this article we develop a typology of review types and provide a descriptive insight into the most common reviews found in top IS journals. Our assessment reveals that the number of IS reviews has increased over the years. The majority of the 139 reviews are theoretical in nature, followed by narrative reviews, meta-analyses, descriptive reviews, hybrid reviews, critical reviews, and scoping reviews. Considering the calls for IS research to develop a cumulative tradition, we hope more review articles will be published in the future and encourage researchers who start a review to use our typology to position their contribution.}
}

@article{rayyan-727967088,
  title={PRISE: A process to support iStar extensions},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={168},
  pages={110649},
  author={Gonçalves, Enyo and Araujo, João and Castro, Jaelson},
  url={https://www.sciencedirect.com/science/article/pii/S0164121220301175},
  keywords={Extension, Goal modelling, iStar, Modelling language, Process},
  abstract={iStar is a goal-based requirement modelling language, being used both in industrial and academic projects of different domains. Often the language is extended to incorporate new constructs related to an application domain or to adjust it to practical situations during requirements modelling. These iStar extensions have been proposed in an ad hoc way resulting in many problems of incompleteness, inconsistency and conflicts. Recently, the language was standardised, but it continues being extended. Thus, we consider that this is an adequate moment to study how to support the proposals of the next iStar extensions. In this paper, we define PRISE, a process to support the creation of iStar extensions which is driven by model-based development concepts, reuse of existing iStar extensions and guidelines of experts. This process can be customised. We illustrate the usage of PRISE by recreating five existing iStar extensions. Finally, we evaluated PRISE with interviews and a survey with experts; and, we performed an interview to analyse the opinion about the usage of the PRISE to create a new iStar extension by a novice. The evaluation and validation indicate good results to avoid problems and increase the quality of the proposals and well receptivity by the experts and novice.}
}

@article{rayyan-727967089,
  title={IFC editorial board},
  year={2008},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={50},
  number={7},
  pages={IFC},
  url={https://www.sciencedirect.com/science/article/pii/S0950584908000608}
}

@article{rayyan-727967090,
  title={IFC editorial board},
  year={2012},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={54},
  number={5},
  pages={IFC},
  url={https://www.sciencedirect.com/science/article/pii/S0950584912000213}
}

@article{rayyan-727967091,
  title={Cloud migration process—A survey, evaluation framework, and open challenges},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={120},
  pages={31-69},
  author={Gholami, Mahdi Fahmideh and Daneshgar, Farhad and Low, Graham and Beydoun, Ghassan},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216300966},
  keywords={Cloud computing, Evaluation framework, Cloud migration, Legacy application, Migration methodology, Process model},
  abstract={Moving mission-oriented enterprise software applications to cloud environments is a crucial IT task and requires a systematic approach. The foci of this paper is to provide a detailed review of extant cloud migration approaches from the perspective of the process model. To this aim, an evaluation framework is proposed and used to appraise and compare existing approaches for highlighting their features, similarities, and key differences. The survey distills the status quo and makes a rich inventory of important activities, recommendations, techniques, and concerns that are common in a typical cloud migration process in one place. This enables both academia and practitioners in the cloud computing community to get an overarching view of the process of the legacy application migration to the cloud. Furthermore, the survey identifies a number challenges that have not been yet addressed by existing approaches, developing opportunities for further research endeavours.}
}

@article{rayyan-727967092,
  title={Usability evaluation methods for the web: A systematic mapping study},
  year={2011},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={53},
  number={8},
  pages={789-817},
  author={Fernandez, Adrian and Insfran, Emilio and Abrahão, Silvia},
  url={https://www.sciencedirect.com/science/article/pii/S0950584911000607},
  keywords={Systematic mapping, Usability evaluation methods, Web development},
  abstract={Context In recent years, many usability evaluation methods (UEMs) have been employed to evaluate Web applications. However, many of these applications still do not meet most customers' usability expectations and many companies have folded as a result of not considering Web usability issues. No studies currently exist with regard to either the use of usability evaluation methods for the Web or the benefits they bring. Objective The objective of this paper is to summarize the current knowledge that is available as regards the usability evaluation methods (UEMs) that have been employed to evaluate Web applications over the last 14years. Method A systematic mapping study was performed to assess the UEMs that have been used by researchers to evaluate Web applications and their relation to the Web development process. Systematic mapping studies are useful for categorizing and summarizing the existing information concerning a research question in an unbiased manner. Results The results show that around 39% of the papers reviewed reported the use of evaluation methods that had been specifically crafted for the Web. The results also show that the type of method most widely used was that of User Testing. The results identify several research gaps, such as the fact that around 90% of the studies applied evaluations during the implementation phase of the Web application development, which is the most costly phase in which to perform changes. A list of the UEMs that were found is also provided in order to guide novice usability practitioners. Conclusions From an initial set of 2703 papers, a total of 206 research papers were selected for the mapping study. The results obtained allowed us to reach conclusions concerning the state-of-the-art of UEMs for evaluating Web applications. This allowed us to identify several research gaps, which subsequently provided us with a framework in which new research activities can be more appropriately positioned, and from which useful information for novice usability practitioners can be extracted.}
}

@article{rayyan-727967093,
  title={Bayesian network model for task effort estimation in agile software development},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={127},
  pages={109-119},
  author={Dragicevic, Srdjana and Celar, Stipe and Turic, Mili},
  url={https://www.sciencedirect.com/science/article/pii/S0164121217300171},
  keywords={Agile software development, Bayesian network, Effort prediction, Software},
  abstract={Even though the use of agile methods in software development is increasing, the problem of effort estimation remains quite a challenge, mostly due to the lack of many standard metrics to be used for effort prediction in plan-driven software development. The Bayesian network model presented in this paper is suitable for effort prediction in any agile method. Simple and small, with inputs that can be easily gathered, the suggested model has no practical impact on agility. This model can be used as early as possible, during the planning stage. The structure of the proposed model is defined by the authors, while the parameter estimation is automatically learned from a dataset. The data are elicited from completed agile projects of a single software company. This paper describes various statistics used to assess the precision of the model: mean magnitude of relative error, prediction at level m, accuracy (the percentage of successfully predicted instances over the total number of instances), mean absolute error, root mean squared error, relative absolute error and root relative squared error. The obtained results indicate very good prediction accuracy.}
}

@article{rayyan-727967094,
  title={UML models consistency management: Guidelines for software quality manager},
  year={2016},
  journal={International Journal of Information Management},
  issn={0268-4012},
  volume={36},
  number={6},
  pages={883-899},
  author={Bashir, Raja Sehrab and Lee, Sai Peck and Khan, Saif Ur Rehman and Chang, Victor and Farid, Shahid},
  url={https://www.sciencedirect.com/science/article/pii/S0268401216303425},
  keywords={UML model consistency, UML model transformation, Software},
  abstract={Unified Modeling Language (UML) has become the de-facto standard to design today's large-size object-oriented systems. However, focusing on multiple UML diagrams is a main cause of breaching the consistency problem, which ultimately reduces the overall software model's quality. Consistency management techniques are widely used to ensure the model consistency by correct model-to-model and model-to-code transformation. Consistency management becomes a promising area of research especially for model-driven architecture. In this paper, we extensively review UML consistency management techniques. The proposed techniques have been classified based on the parameters identified from the research literature. Moreover, we performed a qualitative comparison of consistency management techniques in order to identify current research trends, challenges and research gaps in this field of study. Based on the results, we concluded that researchers have not provided more attention on exploring inter-model and semantic consistency problems. Furthermore, state-of-the-art consistency management techniques mostly focus only on three UML diagrams (i.e., class, sequence and state chart) and the remaining UML diagrams have been overlooked. Consequently, due to this incomplete body of knowledge, researchers are unable to take full advantage of overlooked UML diagrams, which may be otherwise useful to handle the consistency management challenge in an efficient manner.}
}

@article{rayyan-727967095,
  title={A critical examination of recent industrial surveys on agile method usage},
  year={2014},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={94},
  pages={87-97},
  author={Stavru, Stavros},
  url={https://www.sciencedirect.com/science/article/pii/S0164121214000764},
  keywords={Agile software development, Research synthesis, Survey research},
  abstract={Context Practitioners and researchers often claim that agile methods have moved into the mainstream for the last few years. To support this claim they refer to recent industrial surveys which tend to report high rates of agile method usage. However many of these industrial surveys are conducted by agile consultants, tool vendors, professional societies and independent technology and market research organizations. This raises some important concerns about the possible conflict of interest and the overall trustworthiness of these studies. Objective In response to the above concerns, a secondary study was carried out. Its objective was to examine industrial surveys published in 2011 and 2012, determine the extent to which we could trust their reported high rates of agile method usage and provide recommendations on how quality of research could be improved in the future. Method Following a rigorous search procedure, nine industrial surveys on agile method usage published in 2011 and 2012 were extracted from both academia and industry. Their thoroughness in reporting and trustworthiness were evaluated using a newly proposed assessment framework based on Guba's four attributes of trustworthiness (truth value, applicability, consistency and neutrality) and a number of methods for assessing survey research in related fields as information, communication and management studies. Results The careful examination of the reviewed surveys shows that most of the studies have insufficient thoroughness in reporting and (subsequently) low trustworthiness. Only one (out of nine) study is considered as a scientific contribution in determining the current 2011/2012 rate of agile method usage. Conclusions The obtained results support our initial considerations about the trustworthiness of recent industrial surveys on agile method usage and suggest a number of recommendations for increasing the quality and value of future survey research in this regard.}
}

@article{rayyan-727967096,
  title={A systematic mapping study of API usability evaluation methods},
  year={2019},
  journal={Computer Science Review},
  issn={1574-0137},
  volume={33},
  pages={49-68},
  author={Rauf, Irum and Troubitsyna, Elena and Porres, Ivan},
  url={https://www.sciencedirect.com/science/article/pii/S1574013718301515},
  keywords={Usability evaluation methods, API developers, API usability, Cognitive dimensions, Usability factors},
  abstract={An Application Programming Interface (API) provides a programmatic interface to a software component that is often offered publicly and may be used by programmers who are not the API's original designers. APIs play a key role in software reuse. By reusing high quality components and services, developers can increase their productivity and avoid costly defects. The usability of an API is a qualitative characteristic that evaluates how easy it is to use an API. Recent years have seen a considerable increase in research efforts aiming at evaluating the usability of APIs. An API usability evaluation can identify problem areas and provide recommendations for improving the API. In this systematic mapping study, we focus on 47 primary studies to identify the aim and the method of the API usability studies. We investigate which API usability factors are evaluated, at which phases of API development is the usability of API evaluated and what are the current limitations and open issues in API usability evaluation. We believe that the results of this literature review would be useful for both researchers and industry practitioners interested in investigating the usability of API and new API usability evaluation methods.}
}

@article{rayyan-727967097,
  title={Group decision-making in software architecture: A study on industrial practices},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={101},
  pages={51-63},
  author={V, Smrithi Rekha and Muccini, Henry},
  url={https://www.sciencedirect.com/science/article/pii/S0950584918300740},
  keywords={Software architecture, Architectural design decisions, Group decision making, Decision Making, Software},
  abstract={Context A Software Architecture results from a comprehensive process in which several stakeholders deliberate upon the key requirements, issues, solutions and make architectural design decisions. Literature shows that most architectural decisions, in practice, are made in groups. Still, there is a limited understanding of industrial group decision-making practices in software architecture and the challenges that software architecture groups face. Objective Our study, by drawing inspiration from group decision-making theories and models, aims at understanding (i) Existing decision-making practices in software architecture groups (ii) the comparison between practice and theory, (iii) the challenges that the groups face, and (iv) the satisfaction of group members with various aspects of Group Decision Making. Method The study has been conducted through a questionnaire-based survey. 35 practitioners participated in this survey and the responses were analyzed qualitative and quantitatively. Results The analysis of individual responses reveal that software architecture groups (composed, on average, of 3–5 co-located or dispersed members) adopt a discussion based approach while evaluating alternatives, thereby lacking a structured way of decision-making. In these groups, despite the involvement of group members in the discussions, the final decision is made by an individual of authority. Not only is structured decision-making less common, the usage of dedicated software tools for decision-making too is rare. These groups face challenges that are indicative of Groupthink and Group Polarization. Group members feel that quantity of alternatives generated during discussions and tool availability are below satisfactory and they have low satisfaction with the tool support available. Conclusion This study has helped us develop an understanding of software architecture groups, their decision-making practices and challenges faced together with the satisfaction of group members. What the industry needs is integration of group decision-making principles into software architecture decision-making and design of decision-making tools that assist the architecture groups.}
}

@article{rayyan-727967098,
  title={A comparison framework for runtime monitoring approaches},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={125},
  pages={309-321},
  author={Rabiser, Rick and Guinea, Sam and Vierhauser, Michael and Baresi, Luciano and Grünbacher, Paul},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216302618},
  keywords={Literature review, Comparison framework, Runtime monitoring},
  abstract={The full behavior of complex software systems often only emerges during operation. They thus need to be monitored at run time to check that they adhere to their requirements. Diverse runtime monitoring approaches have been developed in various domains and for different purposes. Their sheer number and heterogeneity, however, make it hard to find the right approach for a specific application or purpose. The aim of our research therefore was to develop a comparison framework for runtime monitoring approaches. Our framework is based on an analysis of the literature and existing taxonomies for monitoring languages and patterns. We use examples from existing monitoring approaches to explain the framework. We demonstrate its usefulness by applying it to 32 existing approaches and by comparing 3 selected approaches in the light of different monitoring scenarios. We also discuss perspectives for researchers.}
}

@article{rayyan-727967099,
  title={Data mining techniques in social media: A survey},
  year={2016},
  journal={Neurocomputing},
  issn={0925-2312},
  volume={214},
  pages={654-670},
  author={Injadat, MohammadNoor and Salo, Fadi and Nassif, Ali Bou},
  url={https://www.sciencedirect.com/science/article/pii/S092523121630683X},
  keywords={Data mining, Social media, Social media networks analysis, Survey},
  abstract={Today, the use of social networks is growing ceaselessly and rapidly. More alarming is the fact that these networks have become a substantial pool for unstructured data that belong to a host of domains, including business, governments and health. The increasing reliance on social networks calls for data mining techniques that is likely to facilitate reforming the unstructured data and place them within a systematic pattern. The goal of the present survey is to analyze the data mining techniques that were utilized by social media networks between 2003 and 2015. Espousing criterion-based research strategies, 66 articles were identified to constitute the source of the present paper. After a careful review of these articles, we found that 19 data mining techniques have been used with social media data to address 9 different research objectives in 6 different industrial and services domains. However, the data mining applications in the social media are still raw and require more effort by academia and industry to adequately perform the job. We suggest that more research be conducted by both the academia and the industry since the studies done so far are not sufficiently exhaustive of data mining techniques.}
}

@article{rayyan-727967100,
  title={Incorporating stakeholder concerns in Land Information Systems for urban flood management},
  year={2020},
  journal={Array},
  issn={2590-0056},
  volume={8},
  pages={100037},
  author={Pradeep, Rathnayake Mudiyanselage Manjula and Wijesekera, Nallaperuma Thanthirige Sohan},
  url={https://www.sciencedirect.com/science/article/pii/S2590005620300229},
  keywords={GIS Tool, Hydro model automation, Stakeholder satisfaction, Urban flood, Information Systems},
  abstract={Urbanization increases urban flood. This urges hydrologically and economically planned land development decisions. When decision making, incorporation of hydrology model with GIS is a common practice due to the requirements of accuracy and efficiency. Nevertheless, GIS and hydrology incorporated tools (HydroGIS) should facilitate stakeholder concerns for practical implementation. But there were no single tool and developing such becomes a hard undertaking due to the absence of proper guidelines. Therefore, the present work is to identify the stakeholder concerns and incorporate those in a HydroGIS tool (Land Information System) for Urban Flood Management. For the purpose, it identified and verified the stakeholder concerns through a literature survey and stakeholder discussions. Then, incorporated those into the tool and evaluated the achievement. Present work identified and incorporated the stakeholder requirements; such as the requirement of an automated tool, User friendliness using novel GIS-GUI development guidelines, development of the software using a novel approach of development, security through novel security mechanism and, integrating the hydrology-GIS model using a suitable base software. The systematic incorporation of such requirements into the tool shows the growth in user satisfaction from 48% to 92%. The accurate recognition and incorporation of stakeholder requirements lead to the successful HydroGIS tool in urban flood management.}
}

@article{rayyan-727967101,
  title={Toy user interfaces: Systematic and industrial mapping},
  year={2019},
  journal={Journal of Systems Architecture},
  issn={1383-7621},
  volume={97},
  pages={77-106},
  author={de Albuquerque, Anna Priscilla and Kelner, Judith},
  url={https://www.sciencedirect.com/science/article/pii/S138376211830153X},
  keywords={Mixed-reality, Playful interfaces, Smart toys, Tangible interfaces, Toy user interfaces},
  abstract={Toys are play products designed for leisure and social play activities. Today, play products increasingly incorporate hardware and software computation, often connecting to online services and other computing devices, thus being commonly referred to as “smart toys.” Accordingly, such products can also be categorized as user interfaces as they allow human interaction with digital contents using the physical toys' inputs and outputs. Here we propose the idea of Toy-User-Interfaces (ToyUI) and relate them to the domains of human-computer interaction, mixed reality, and the Internet of Things. We surveyed and categorized a multitude of ToyUIs, gathering information from research papers and toy companies by performing both a systematic mapping (covering research from 2008 to 2017) and an industrial mapping (covering releases from 2012 to 2017). The resulting 297 items were then analyzed according to our classification model, being divided into four categories, eight genres, and 22 setups, considering the play features and the interface features of such items. The classification model and the analysis of the results allowed us to identify six trends in design and technology for ToyUI. Ultimately, our research findings may guide the future of ToyUI projects for researchers, designers, educators, therapists, and end-users.}
}

@article{rayyan-727967102,
  title={Striving to be resilient: What concepts, approaches and practices should be incorporated in resilience management guidelines?},
  year={2017},
  journal={Technological Forecasting and Social Change},
  issn={0040-1625},
  volume={121},
  pages={39-49},
  author={Adini, Bruria and Cohen, Odeya and Eide, Aslak Wegner and Nilsson, Susanna and Aharonson-Daniel, Limor and Herrera, Ivonne A},
  url={https://www.sciencedirect.com/science/article/pii/S0040162517301026},
  keywords={Guidelines, Approaches, Concepts, Evaluation, Practices, Resilience management},
  abstract={Resilience management guidelines address disruptions, changes and opportunities, facilitate anticipation, adaptation, flexibility and provide a foundation for an effective crisis response. The objective and novelty of the study were to propose a holistic framework that enables to evaluate and prioritise concepts, approaches and practices that should be incorporated into European guidelines for resilience management. Based on a modified Delphi process, 51 items achieved a consensus of ¿80%. 84% of the items (n=43) were ranked as important; 13.7% (n=7) as essential; one ranked as somewhat important. The identified items encompass eleven categories as follows: 1) collaboration [11 items]; 2) planning [8 items]; 3) procedures [8 items]; 4) training [6 items]; 5) infrastructure [5 items]; 6) communication [3 items]; 7) governance [3 items]; 8) learning lessons [2 items]; 9) situation understanding (awareness) [1 item]; 10) resources [2 items]; and 11) evaluation [2 items]. The identified concepts, approaches and practices seem to be applicable to a wide range of domains and critical infrastructures, such as crisis management, air traffic management and healthcare, due to their generic and abstract characteristics. Important in the Delphi process is the engagement of potential end users in the development of resilience management guidelines to align this development to their needs. Therefore, the Delphi process involved policy and decision-makers, as well as practitioners and other personnel representing different critical infrastructures and academia, in prioritising concepts aimed at achieving resilient organisations, entities or communities.}
}

@article{rayyan-727967103,
  title={Quality attributes and quality models for ambient assisted living software systems: A systematic mapping},
  year={2017},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={82},
  pages={121-138},
  author={Garcés, Lina and Ampatzoglou, Apostolos and Avgeriou, Paris and Nakagawa, Elisa Yumi},
  url={https://www.sciencedirect.com/science/article/pii/S0950584916302932},
  keywords={Systematic mapping, Quality model, Ambient assisted living, ISO/IEC 25010, Quality attribute, Software},
  abstract={Context: Ambient Assisted Living (AAL) has become an essential, multidisciplinary research topic, aiming at providing software systems and services that assist people in their everyday life activities. Considering the critical nature of AAL systems, several initiatives have already contributed to the improvement of their quality, by mainly focusing on their non-functional requirements. Despite the importance of quality assurance in AAL systems, there is a lack of a comprehensive analysis on how quality assurance is performed in such systems. This fact might in turn lead to an absence of standardization with regard to the quality assurance process of these systems. Objective: We provide a broad, detailed panorama about the state of the art on quality models (QMs) and quality attributes (QAs) that are important for the AAL domain. Method: We performed a Systematic Mapping (SM). We used six publication databases to cover all published material pertinent for our SM. We initially obtained 287 studies that were filtered based on a set of well-defined inclusion/exclusion criteria, resulting into a set of 27 studies that were used for exploring QAs for AAL systems. Results: The most common QAs used in the development of AAL systems were identified and defined. We also characterized important critical attributes for software systems in the AAL domain. Additionally, QAs for some AAL sub-domains were defined. Furthermore, we investigated how QM&QA have been defined, evaluated, and used in that domain. Finally, we offered an analysis of the maturity of the studies identified in our SM. Conclusion: It is necessary to develop a complete QM that: (i) defines all common QAs for AAL systems; (ii) considers variability of QAs among AAL sub-domains; (iii) analyses dependences among QAs; (iv) offers indicators or metrics to measure QAs; and (v) offers means to assess and predict quality of AAL systems.}
}

@article{rayyan-727967104,
  title={Modeling and automatic code generation for wireless sensor network applications using model-driven or business process approaches: A systematic mapping study},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={132},
  pages={50-71},
  author={Teixeira, Sergio and Agrizzi, Bruno Alves and Filho, José Gonçalves Pereira and Rossetto, Silvana and de Lima Baldam, Roquemar},
  url={https://www.sciencedirect.com/science/article/pii/S0164121217301255},
  keywords={Systematic mapping study, Model-driven development, Business process, Wireless sensor networks},
  abstract={This systematic mapping study investigates the modeling and automatic code generation initiatives for wireless sensor network applications based on the IEEE 802.15.4 standard, trying to understand the reasons, characteristics and methods used in the approaches available in the scientific literature, identifying research gaps and potential approaches that can be better exploited, indicating new possibilities of research. The focus is on studies that follow the Model-Driven or Business Process approaches.}
}

@article{rayyan-727967105,
  title={A snowballing literature study on test amplification},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={157},
  pages={110398},
  author={Danglot, Benjamin and Vera-Perez, Oscar and Yu, Zhongxing and Zaidman, Andy and Monperrus, Martin and Baudry, Benoit},
  url={https://www.sciencedirect.com/science/article/pii/S0164121219301736},
  keywords={Automatic testing, Test amplification, Test augmentation, Test optimization, Test regeneration},
  abstract={The adoption of agile approaches has put an increased emphasis on testing, resulting in extensive test suites. These suites include a large number of tests, in which developers embed knowledge about meaningful input data and expected properties as oracles. This article surveys works that exploit this knowledge to enhance manually written tests with respect to an engineering goal (e.g., improve coverage or refine fault localization). While these works rely on various techniques and address various goals, we believe they form an emerging and coherent field of research, which we coin “test amplification”. We devised a first set of papers from DBLP, searching for all papers containing “test” and “amplification” in their title. We reviewed the 70 papers in this set and selected the 4 papers that fit the definition of test amplification. We use them as the seeds for our snowballing study, and systematically followed the citation graph. This study is the first that draws a comprehensive picture of the different engineering goals proposed in the literature for test amplification. We believe that this survey will help researchers and practitioners entering this new field to understand more quickly and more deeply the intuitions, concepts and techniques used for test amplification.}
}

@article{rayyan-727967106,
  title={Do UML object diagrams affect design comprehensibility? Results from a family of four controlled experiments},
  year={2017},
  journal={Journal of Visual Languages & Computing},
  issn={1045-926X},
  volume={41},
  pages={10-21},
  author={Torchiano, Marco and Scanniello, Giuseppe and Ricca, Filippo and Reggio, Gianna and Leotta, Maurizio},
  url={https://www.sciencedirect.com/science/article/pii/S1045926X17301234},
  keywords={UML, Family of experiments, Model comprehension, Object diagram},
  abstract={Objective: The main objective of our study is to assess whether the use of UML (Unified Modeling Language) object diagrams improves comprehensibility of software design when this kind of diagrams is added to UML class diagrams. Method: We have conducted a family of four controlled experiments. We involved groups of bachelor and master students. Results: Results suggest that the use of object diagrams does not always introduce significant benefits in terms of design comprehensibility. We found that benefits strongly depend on the experience of participants and their familiarity with UML. More experienced participants achieved better design comprehensibility when provided with both class and object diagrams, while less experienced seemed to be damaged when using class and object diagrams together. Results also showed the absence of substantial variations in the time needed to comprehend UML models, with or without object diagrams. Implications: Our results suggest that it is important to be aware and take into account experience and UML familiarity before using object diagrams in software modeling.}
}

@article{rayyan-727967107,
  title={Volume contents},
  year={2006},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={48},
  number={12},
  pages={1213-1215},
  url={https://www.sciencedirect.com/science/article/pii/S0950584906001546}
}

@article{rayyan-727967108,
  title={A critical view on PLM/ALM convergence in practice and research},
  year={2016},
  journal={Procedia Technology},
  issn={2212-0173},
  volume={26},
  pages={405-412},
  author={Deuter, Andreas and Rizzo, Stefano},
  url={https://www.sciencedirect.com/science/article/pii/S2212017316303991},
  keywords={ALM, OSLC, PLM},
  abstract={The Internet of Things (IoT) is the main driver for industrial smart products produced by an increasing number of manufacturers. The overall functionality of smart products is a combination of mechanical, electrical/electronic functions (hardware functions) and software functions. For hardware and software there are different lifecycle models: Product lifecycle management (PLM) focuses on hardware, application lifecycle management (ALM) focuses on software. Smart products force manufacturers to converge both lifecycle models step by step. Although seemingly important, the research community leaves this innovative area mostly up to the PLM tool vendors and the ALM tool vendors, resulting in them driving the convergence. This paper points out the mismatch between industry and academia regarding the PLM/ALM convergence. We encourage academia to increase research activities and we propose potential research topics.}
}

@article{rayyan-727967109,
  title={A systematic mapping study of web application testing},
  year={2013},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={55},
  number={8},
  pages={1374-1396},
  author={Garousi, Vahid and Mesbah, Ali and Betin-Can, Aysu and Mirshokraie, Shabnam},
  url={https://www.sciencedirect.com/science/article/pii/S0950584913000396},
  keywords={Bibliometrics, Systematic mapping, Paper repository, Testing, Web application},
  abstract={Context The Web has had a significant impact on all aspects of our society. As our society relies more and more on the Web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 147 papers in the area of web application testing, which have appeared between 2000 and 2011. Objective As this research area matures and the number of related papers increases, it is important to systematically identify, analyze, and classify the publications and provide an overview of the trends in this specialized field. Method We review and structure the body of knowledge related to web application testing through a systematic mapping (SM) study. As part of this study, we pose two sets of research questions, define selection and exclusion criteria, and systematically develop and refine a classification schema. In addition, we conduct a bibliometrics analysis of the papers included in our study. Results Our study includes a set of 79 papers (from the 147 retrieved papers) published in the area of web application testing between 2000 and 2011. We present the results of our systematic mapping study. Our mapping data is available through a publicly-accessible repository. We derive the observed trends, for instance, in terms of types of papers, sources of information to derive test cases, and types of evaluations used in papers. We also report the demographics and bibliometrics trends in this domain, including top-cited papers, active countries and researchers, and top venues in this research area. Conclusion We discuss the emerging trends in web application testing, and discuss the implications for researchers and practitioners in this area. The results of our systematic mapping can help researchers to obtain an overview of existing web application testing approaches and indentify areas in the field that require more attention from the research community.}
}

@article{rayyan-727967110,
  title={State of the art of cyber-physical systems security: An automatic control perspective},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={149},
  pages={174-216},
  author={Zacchia Lun, Yuriy and D'Innocenzo, Alessandro and Smarra, Francesco and Malavolta, Ivano and Di Benedetto, Maria Domenica},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218302681},
  keywords={Systematic mapping study, Security, Cyber-physical systems},
  abstract={Cyber-physical systems are integrations of computation, networking, and physical processes. Due to the tight cyber-physical coupling and to the potentially disrupting consequences of failures, security here is one of the primary concerns. Our systematic mapping study sheds light on how security is actually addressed when dealing with cyber-physical systems from an automatic control perspective. The provided map of 138 selected studies is defined empirically and is based on, for instance, application fields, various system components, related algorithms and models, attacks characteristics and defense strategies. It presents a powerful comparison framework for existing and future research on this hot topic, important for both industry and academia.}
}

@article{rayyan-727967111,
  title={A comparative study of two fuzzy logic models for software development effort estimation},
  year={2013},
  journal={Procedia Technology},
  issn={2212-0173},
  volume={7},
  pages={305-314},
  author={Garcia-Diaz, Noel and Lopez-Martin, Cuauhtemoc and Chavoya, Arturo},
  url={https://www.sciencedirect.com/science/article/pii/S221201731300039X},
  keywords={Fuzzy logic, Linear regression, Software effort estimation, Fuzzy Logic, Software},
  abstract={Software development effort estimation (SDEE) has been the focus of research in recent years. No single software development estimation technique is best for all situations and linear regression (LR) has frequently been used for both small and industrial software projects. Fuzzy logic (FL) has been applied as an alternative technique to SDEE using a Mamdani Model. In order to compare the estimation accuracy of the Mamdani and Takagi-Sugeno fuzzy systems with that of an LR model, a sample of small projects was used to generate two FL models and an LR equation. Then the FL models and the LR equation were validated by estimating the effort of projects elaborated by other developers. This latter group of projects was subdivided into projects with Effort¡100 and Effort ≥100 (as it has been demonstrated that the estimation accuracy depends on the effort, which is an amount of time in human-hours). The results showed that the Takagi-Sugeno fuzzy system was more accurate than the Mamdani system and the LR model for SDEE of projects with Effort 100. It can be concluded that a Takagi-Sugeno fuzzy system can be useful for estimating the effort of projects with Effort≥100 when they have been individually developed on a disciplined process.}
}

@article{rayyan-727967112,
  title={Legal ontologies over time: A systematic mapping study},
  year={2019},
  journal={Expert Systems with Applications},
  issn={0957-4174},
  volume={130},
  pages={12-30},
  author={de Oliveira Rodrigues, Cleyton Mário and de Freitas, Frederico Luiz Gonçalves and Barreiros, Emanoel Francisco Spósito and de Azevedo, Ryan Ribeiro and de Almeida Filho, Adauto Trigueiro},
  url={https://www.sciencedirect.com/science/article/pii/S0957417419302398},
  keywords={Systematic mapping study, Legal expert system, Legal ontology, Legal theory, Semantic web},
  abstract={Over the last 30 years, AI & Law has provided breakthroughs in studies involving case-based reasoning, rule-based reasoning, information retrieval and, most recently, conceptual models for knowledge representation and reasoning, known as Legal Ontologies. Ontologies have been widely used by legal practitioners, scholars, and lay people in a variety of situations, such as simulating legal actions, semantic search and indexing, and to keep up-to-date with the continual change of laws and regulations. Given the high number of legal ontologies produced, the need to summarize this research realm through a well-defined methodological procedure is urgent need. This study presents the results of a systematic mapping of the literature, aiming at categorizing legal ontologies along certain dimensions, such as purpose, level of generality, underlying legal theories, among other aspects. The reasons to carry out a systematic mapping are twofold: in addition to explaining the maturation of the area over recent decades, it helps to avoid the old problem of reinventing the wheel. Through organizing and classifying what has already been produced, it is possible to realize that the development of legal ontologies can rise to the level of reusability where prefabricated models might be coupled with new and more complex ontologies for practical law.}
}

@article{rayyan-727967113,
  title={Chapter one - mobile application quality assurance},
  year={2019},
  volume={112},
  pages={1-77},
  author={Holl, Konstantin and Elberzhager, Frank and Memon, Atif M},
  url={https://www.sciencedirect.com/science/article/pii/S0065245817300530},
  publisher={Elsevier},
  series={Advances in computers},
  keywords={Software testing, Mobile applications, Mapping study, Quality assurance, State of the art},
  abstract={Mobile applications have become highly pervasive in recent years. Their quality is essential since application failures can lead to serious consequences, such as damage of corporate reputation or financial loss. The goal of this work is to identify and expose approaches that address the issue of quality assurance for mobile applications. In order to drive our systematic mapping study, we derived eight research questions based on the stated goal. Ultimately, we systematically identified 311 articles based on 4607 captured records. We created clustered views to answer the research questions and used existing surveys to complement our overview of current challenges. The results show an overall upward trend of publications since 2003. Hot topics include automation of GUI tests and assurance of nonfunctional qualities. Aspects of future research could be the integration of review techniques into existing approaches and focusing more strongly on defects addressing the specific characteristics of mobile applications.}
}

@article{rayyan-727967114,
  title={A systematic mapping study of mobile application testing techniques},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={117},
  pages={334-356},
  author={Zein, Samer and Salleh, Norsaremah and Grundy, John},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216300140},
  keywords={Software testing, Systematic mapping, Mobile application testing},
  abstract={The importance of mobile application specific testing techniques and methods has been attracting much attention of software engineers over the past few years. This is due to the fact that mobile applications are different than traditional web and desktop applications, and more and more they are moving to being used in critical domains. Mobile applications require a different approach to application quality and dependability and require an effective testing approach to build high quality and more reliable software. We performed a systematic mapping study to categorize and to structure the research evidence that has been published in the area of mobile application testing techniques and challenges that they have reported. Seventy nine (79) empirical studies are mapped to a classification schema. Several research gaps are identified and specific key testing issues for practitioners are identified: there is a need for eliciting testing requirements early during development process; the need to conduct research in real-world development environments; specific testing techniques targeting application life-cycle conformance and mobile services testing; and comparative studies for security and usability testing.}
}

@article{rayyan-727967115,
  title={A computational literature review of the technology acceptance model},
  year={2016},
  journal={International Journal of Information Management},
  issn={0268-4012},
  volume={36},
  number={6},
  pages={1248-1259},
  author={Mortenson, Michael J and Vidgen, Richard},
  url={https://www.sciencedirect.com/science/article/pii/S0268401216300329},
  keywords={Citation analysis, Literature review, Co-authorship analysis, Computational literature review, Journal ranking, Lda, Social network analysis, Technology acceptance model, Topic models},
  abstract={A literature review is a central part of any research project, allowing the existing research to be mapped and new research questions to be posited. However, due to the limitations of human data processing, the literature review can suffer from an inability to handle large volumes of research articles. The computational literature review (CLR) is proposed here as a complementary part of a wider literature review process. The CLR automates some of the analysis of research articles with analyses of impact (citations), structure (co-authorship networks) and content (topic modeling of abstracts). A contribution of the paper is to demonstrate how the content of abstracts can be analyzed automatically to provide a set of research topics within a literature corpus. The CLR software can be used to support three use cases: (1) analysis of the literature for a research area, (2) analysis and ranking of journals, and (3) analysis and ranking of individual scholars and research teams. The working of the CLR software is illustrated through application to the technology acceptance model (TAM) using a set of 3,386 articles. The CLR is an open source offering, developed in the statistical programming language R, and made freely available to researchers to use and develop further.}
}

@article{rayyan-727967116,
  title={Visual comparison of software cost estimation models by regression error characteristic analysis},
  year={2010},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={83},
  number={4},
  pages={621-637},
  author={Mittas, Nikolaos and Angelis, Lefteris},
  url={https://www.sciencedirect.com/science/article/pii/S016412120900288X},
  keywords={Estimation by analogy, Regression analysis, Regression error characteristic curves, Software cost estimation, Software},
  abstract={The well-balanced management of a software project is a critical task accomplished at the early stages of the development process. Due to this requirement, a wide variety of prediction methods has been introduced in order to identify the best strategy for software cost estimation. The selection of the best technique is usually based on measures of error whereas in more recent studies researchers use formal statistical procedures. The former approach can lead to unstable and erroneous results due to the existence of outlying points whereas the latter cannot be easily presented to non-experts and has to be carried out by an expert with statistical background. In this paper, we introduce the regression error characteristic (REC) analysis, a powerful visualization tool with interesting geometrical properties, in order to validate and compare different prediction models easily, by a simple inspection of a graph. Moreover, we propose a formal framework covering different aspects of the estimation process such as the calibration of the prediction methodology, the identification of factors that affect the error, the investigation of errors on certain ranges of the actual cost and the examination of the distribution of the cost for certain errors. Application of REC analysis to the ISBSG10 dataset for comparing estimation by analogy and linear regression illustrates the benefits and the significant information obtained.}
}

@article{rayyan-727967117,
  title={Empirical findings on team size and productivity in software development},
  year={2012},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={85},
  number={3},
  pages={562-570},
  author={Rodríguez, D and Sicilia, M A and García, E and Harrison, R},
  url={https://www.sciencedirect.com/science/article/pii/S0164121211002366},
  keywords={Effort estimation datasets, ISBSG repository, Productivity, Team size, Software},
  abstract={The size of software project teams has been considered to be a driver of project productivity. Although there is a large literature on this, new publicly available software repositories allow us to empirically perform further research. In this paper we analyse the relationships between productivity, team size and other project variables using the International Software Benchmarking Standards Group (ISBSG) repository. To do so, we apply statistical approaches to a preprocessed subset of the ISBSG repository to facilitate the study. The results show some expected correlations between productivity, effort and time as well as corroborating some other beliefs concerning team size and productivity. In addition, this study concludes that in order to apply statistical or data mining techniques to these type of repositories extensive preprocessing of the data needs to be performed due to ambiguities, wrongly recorded values, missing values, unbalanced datasets, etc. Such preprocessing is a difficult and error prone activity that would need further guidance and information that is not always provided in the repository.}
}

@article{rayyan-727967118,
  title={Distributed virtual machine consolidation: A systematic mapping study},
  year={2018},
  journal={Computer Science Review},
  issn={1574-0137},
  volume={28},
  pages={118-130},
  author={Ashraf, Adnan and Byholm, Benjamin and Porres, Ivan},
  url={https://www.sciencedirect.com/science/article/pii/S1574013717300953},
  keywords={Cloud computing, Consolidation, Data center, Energy-efficiency, Placement, Virtual machine},
  abstract={Background: Virtual Machine (VM) consolidation is an effective technique to improve resource utilization and reduce energy footprint in cloud data centers. It can be implemented in a centralized or a distributed fashion. Distributed VM consolidation approaches are currently gaining popularity because they are often more scalable than their centralized counterparts and they avoid a single point of failure. Objective: To present a comprehensive, unbiased overview of the state-of-the-art on distributed VM consolidation approaches. Method: A Systematic Mapping Study (SMS) of the existing distributed VM consolidation approaches. Results: 19 papers on distributed VM consolidation categorized in a variety of ways. The results show that the existing distributed VM consolidation approaches use four types of algorithms, optimize a number of different objectives, and are often evaluated with experiments involving simulations. Conclusion: There is currently an increasing amount of interest on developing and evaluating novel distributed VM consolidation approaches. A number of research gaps exist where the focus of future research may be directed.}
}

@article{rayyan-727967119,
  title={Developing and using checklists to improve software effort estimation: A multi-case study},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={146},
  pages={286-309},
  author={Usman, Muhammad and Petersen, Kai and Börstler, Jürgen and Santos Neto, Pedro},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218302073},
  keywords={Checklist, Agile software development, Case study, Expert judgment based effort estimation, Software},
  abstract={Expert judgment based effort estimation techniques are widely used for estimating software effort. In the absence of process support, experts may overlook important factors during estimation, leading to inconsistent estimates. This might cause underestimation, which is a common problem in software projects. This multi-case study aims to improve expert estimation of software development effort. Our goal is two-fold: 1) to propose a process to develop and evolve estimation checklists for agile teams, and 2) to evaluate the usefulness of the checklists in improving expert estimation processes. The use of checklists improved the accuracy of the estimates in two case companies. In particular, the underestimation bias was reduced to a large extent. For the third case, we could not perform a similar analysis, due to the unavailability of historical data. However, when checklist was used in two sprints, the estimates were quite accurate (median Balanced Relative Error (BRE) bias of -0.05). The study participants from the case companies observed several benefits of using the checklists during estimation, such as increased confidence in estimates, improved consistency due to help in recalling relevant factors, more objectivity in the process, improved understanding of the tasks being estimated, and reduced chances of missing tasks.}
}

@article{rayyan-727967120,
  title={Requirements for cross-domain knowledge sharing in collaborative product-service system design},
  year={2016},
  journal={Procedia CIRP},
  issn={2212-8271},
  volume={47},
  pages={108-113},
  author={Wiesner, Stefan and Lampathaki, Fenareti and Biliri, Evmorfia and Thoben, Klaus-Dieter},
  url={https://www.sciencedirect.com/science/article/pii/S2212827116300889},
  keywords={Collaborative Networks, Knowledge Management, Product-Service System Design},
  abstract={In the case of Product-Service Systems (PSS), the design phase is characterized by a demand for intensive exchange of knowledge between stakeholders from different domains. Thus, a comprehensive approach for knowledge sharing would support the integrated development of PSS. Existing attempts are however mainly focusing on using explicit service knowledge for product design and service operations only. Knowledge exchange between domains, including tacit knowledge and sentiment, for the integrated design of products and services have received less attention. The objective of this paper is to present the initial results on the requirements for cross-domain knowledge sharing when designing innovative PSS.}
}

@article{rayyan-727967121,
  title={Analyzing and handling local bias for calibrating parametric cost estimation models},
  year={2013},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={55},
  number={8},
  pages={1496-1511},
  author={Yang, Ye and He, Zhimin and Mao, Ke and Li, Qi and Nguyen, Vu and Boehm, Barry and Valerdi, Ricardo},
  url={https://www.sciencedirect.com/science/article/pii/S0950584913000645},
  keywords={Effort estimation, COCOMO II, Local bias, Model maintenance, Parametric model, Weighted sampling, Calibration, Bias (Epidemiology)},
  abstract={Context Parametric cost estimation models need to be continuously calibrated and improved to assure more accurate software estimates and reflect changing software development contexts. Local calibration by tuning a subset of model parameters is a frequent practice when software organizations adopt parametric estimation models to increase model usability and accuracy. However, there is a lack of understanding about the cumulative effects of such local calibration practices on the evolution of general parametric models over time. Objective This study aims at quantitatively analyzing and effectively handling local bias associated with historical cross-company data, thus improves the usability of cross-company datasets for calibrating and maintaining parametric estimation models. Method We design and conduct three empirical studies to measure, analyze and address local bias in cross-company dataset, including: (1) defining a method for measuring the local bias associated with individual organization data subset in the overall dataset; (2) analyzing the impacts of local bias on the performance of an estimation model; (3) proposing a weighted sampling approach to handle local bias. The studies are conducted on the latest COCOMO II calibration dataset. Results Our results show that the local bias largely exists in cross company dataset, and the local bias negatively impacts the performance of parametric model. The local bias based weighted sampling technique helps reduce negative impacts of local bias on model performance. Conclusion Local bias in cross-company data does harm model calibration and adds noisy factors to model maintenance. The proposed local bias measure offers a means to quantify degree of local bias associated with a cross-company dataset, and assess its influence on parametric model performance. The local bias based weighted sampling technique can be applied to trade-off and mitigate potential risk of significant local bias, which limits the usability of cross-company data for general parametric model calibration and maintenance.}
}

@article{rayyan-727967122,
  title={CODE reuse in practice: Benefiting or harming technical debt},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={167},
  pages={110618},
  author={Feitosa, Daniel and Ampatzoglou, Apostolos and Gkortzis, Antonios and Bibi, Stamatia and Chatzigeorgiou, Alexander},
  url={https://www.sciencedirect.com/science/article/pii/S0164121220300960},
  keywords={Case study, Reuse, Technical debt},
  abstract={During the last years the TD community is striving to offer methods and tools for reducing the amount of TD, but also understand the underlying concepts. One popular practice that still has not been investigated in the context of TD, is software reuse. The aim of this paper is to investigate the relation between white-box code reuse and TD principal and interest. In particular, we target at unveiling if the reuse of code can lead to software with better levels of TD. To achieve this goal, we performed a case study on approximately 400 OSS systems, comprised of 897 thousand classes, and compare the levels of TD for reused and natively-written classes. The results of the study suggest that reused code usually has less TD interest; however, the amount of principal in them is higher. A synthesized view of the aforementioned results suggest that software engineers shall opt to reuse code when necessary, since apart from the established reuse benefits (i.e., cost savings, increased productivity, etc.) are also getting benefits in terms of maintenance. Apart from understanding the phenomenon per se, the results of this study provide various implications to research and practice.}
}

@article{rayyan-727967123,
  title={Exploring software security approaches in software development lifecycle: A systematic mapping study},
  year={2017},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={50},
  pages={107-115},
  author={Mohammed, Nabil M and Niazi, Mahmood and Alshayeb, Mohammad and Mahmood, Sajjad},
  url={https://www.sciencedirect.com/science/article/pii/S0920548916301155},
  keywords={Systematic mapping study, Empirical study, Software development life cycle, Software security, Software},
  abstract={There is an increase use of security driven approaches to support software development activities, such as requirements, design and implementation. The objective of this paper is to identify the existing software security approaches used in the software development lifecycle (SDLC). In order to meet our goal, we conducted a systematic mapping study to identify the primary studies on the use of software security techniques in SDLC. In total, we selected and categorized 118 primary studies. After analyzing the selected studies, we identified 52 security approaches and we categorized them in to five main categories, namely, ‘secure requirements modeling', ‘vulnerability identification, adaption and mitigation', ‘software security focused process', ‘extended UML-based secure modeling profiles', ‘non UML-based secure modeling notations'. The results show that the most frequently used approaches are static analysis and dynamic analysis that provide security checks in the coding phase. In addition, our results show that many studies in this review considered security checks around the coding stage of software development. This work will assist software development organizations in better understanding the existing software security approaches used in the software development lifecycle. It can also provide researchers with a firm basis on which to develop new software security approaches.}
}

@article{rayyan-727967124,
  title={Software effort models should be assessed via leave-one-out validation},
  year={2013},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={86},
  number={7},
  pages={1879-1890},
  author={Kocaguneli, Ekrem and Menzies, Tim},
  url={https://www.sciencedirect.com/science/article/pii/S0164121213000538},
  keywords={Bias, Software cost estimation, Prediction system, Variance, Software},
  abstract={Context More than half the literature on software effort estimation (SEE) focuses on model comparisons. Each of those requires a sampling method (SM) to generate the train and test sets. Different authors use different SMs such as leave-one-out (LOO), 3Way and 10Way cross-validation. While LOO is a deterministic algorithm, the N-way methods use random selection to build their train and test sets. This introduces the problem of conclusion instability where different authors rank effort estimators in different ways. Objective To reduce conclusion instability by removing the effects of a sampling method's random test case generation. Method Calculate bias and variance (B&V) values following the assumption that a learner trained on the whole dataset is taken as the true model; then demonstrate that the B&V and runtime values for LOO are similar to N-way by running 90 different algorithms on 20 different SEE datasets. For each algorithm, collect runtimes, B&V values under LOO, 3Way and 10Way. Results We observed that: (1) the majority of the algorithms have statistically indistinguishable B&V values under different SMs and (2) different SMs have similar run times. Conclusion In terms of their generated B&V values and runtimes, there is no reason to prefer N-way over LOO. In terms of reproducibility, LOO removes one cause of conclusion instability (the random selection of train and test sets). Therefore, we depreciate N-way and endorse LOO validation for assessing effort models.}
}

@article{rayyan-727967125,
  title={Views on quality requirements in academia and practice: commonalities, differences, and context-dependent grey areas},
  year={2020},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={121},
  pages={106253},
  author={Vogelsang, Andreas and Eckhardt, Jonas and Mendez, Daniel and Berger, Moritz},
  url={https://www.sciencedirect.com/science/article/pii/S095058491930271X},
  keywords={Requirements engineering, Survey, Context factors, Eempirical study, Non-functional requirements, Quality requirements},
  abstract={Context: Quality requirements (QRs) are a topic of constant discussions both in industry and academia. Debates entwine around the definition of quality requirements, the way how to handle them, or their importance for project success. While many academic endeavors contribute to the body of knowledge about QRs, practitioners may have different views. In fact, we still lack a consistent body of knowledge on QRs since much of the discussion around this topic is still dominated by observations that are strongly context-dependent. This holds for both academic and practitioners' views. Our assumption is that, in consequence, those views may differ. Objective: We report on a study to better understand the extent to which available research statements on quality requirements, as found in exemplary peer-reviewed and frequently cited publications, are reflected in the perception of practitioners. Our goal is to analyze differences, commonalities, and context-dependent grey areas in the views of academics and practitioners to allow a discussion on potential misconceptions (on either sides) and opportunities for future research. Method: We conducted a survey with 109 practitioners to assess whether they agree with research statements about QRs reflected in the literature. Based on a statistical model, we evaluate the impact of a set of context factors to the perception of research statements. Results: Our results show that a majority of the statements is well respected by practitioners; however, not all of them. When examining the different groups and backgrounds of respondents, we noticed interesting deviations of perceptions within different groups that may lead to new research questions. Conclusions:Our results help identifying prevalent context-dependent differences about how academics and practitioners view QRs and pinpointing statements where further research might be useful.}
}

@article{rayyan-727967126,
  title={Model-based testing using UML activity diagrams: A systematic mapping study},
  year={2019},
  journal={Computer Science Review},
  issn={1574-0137},
  volume={33},
  pages={98-112},
  author={Ahmad, Tanwir and Iqbal, Junaid and Ashraf, Adnan and Truscan, Dragos and Porres, Ivan},
  url={https://www.sciencedirect.com/science/article/pii/S1574013718302314},
  keywords={Software testing, Systematic mapping study, Model-based testing, Test generation, UML activity diagram},
  abstract={Context: The Unified Modeling Language (UML) has become the de facto standard for software modeling. UML models are often used to visualize, understand, and communicate the structure and behavior of a system. UML activity diagrams (ADs) are often used to elaborate and visualize individual use cases. Due to their higher level of abstraction and process-oriented perspective, UML ADs are also highly suitable for model-based test generation. In the last two decades, different researchers have used UML ADs for test generation. Despite the growing use of UML ADs for model-based testing, there are currently no comprehensive and unbiased studies on the topic. Objective: To present a comprehensive and unbiased overview of the state-of-the-art on model-based testing using UML ADs. Method: We review and structure the current body of knowledge on model-based testing using UML ADs by performing a systematic mapping study using well-known guidelines. We pose nine research questions, outline our selection criteria, and develop a classification scheme. Results: The results comprise 41 primary studies analyzed against nine research questions. We also highlight the current trends and research gaps in model-based testing using UML ADs and discuss some shortcomings for researchers and practitioners working in this area. The results show that the existing approaches on model-based testing using UML ADs tend to rely on intermediate formats and formalisms for model verification and test generation, employ a multitude of graph-based coverage criteria, and use graph search algorithms. Conclusion: We present a comprehensive overview of the existing approaches on model-based testing using UML ADs. We conclude that (1) UML ADs are not being used for non-functional testing, (2) only a few approaches have been validated against realistic, industrial case studies, (3) most approaches target very restricted application domains, and (4) there is currently a clear lack of holistic approaches for model-based testing using UML ADs.}
}

@article{rayyan-727967127,
  title={Systematic mapping study in small business: The quest for contemporary understanding},
  year={2014},
  journal={Procedia - Social and Behavioral Sciences},
  issn={1877-0428},
  volume={143},
  pages={916-920},
  author={Moraes, Caroline and Philippsen, Luiz and Lirani, Heloisa and Yamanaka, Lie and Rosim, Daniela and Filho, Edmundo Escrivão},
  url={https://www.sciencedirect.com/science/article/pii/S1877042814044553},
  keywords={systematic mapping, A unified theory, mapping},
  abstract={A unified theory of small businesses has been widely proposed. Nevertheless there are very few studies about the main concerns in small business literature. Thus, a systematic mapping study of the available literature was conducted. The main purpose of this article is the classification and thematic analysis of the most relevant articles on small business research. Using the Web of Science® database, over 500 articles were selected with the string “small business” and its plural on the title. The articles were published from 2005 to 2013 and received over 10 citations. Titles and abstracts were analyzed in order to provide a view of the major themes and tendencies of the literature on small business. One major thematic area was established, “organizational environment”, which has grown in the last decade. On the other hand, classic themes such as “specificity”, “creation, failure and success”, “owner”, and “strategy” have declined considerably since the eighties, to be replaced by growing areas such as “management” and “new processes”. “Public policy”, “financing” and “functional areas” have remained fairly popular. Systematic mapping studies can provide an overview of the major contemporary themes and concerns regarding small business theory. Besides the academic contribution, understanding the major concerns will also have an impact on public policy, highlighting specific needs and priority projects. Additional studies should be performed, using more strings, such as “small firm(s)” or “small enterprise(s)”, and other relevant databases to determine possible gaps in the existing research.}
}

@article{rayyan-727967128,
  title={Architecture for embedded open software ecosystems},
  year={2014},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={92},
  pages={128-142},
  author={Eklund, Ulrik and Bosch, Jan},
  url={https://www.sciencedirect.com/science/article/pii/S0164121214000211},
  keywords={Software architecture, Embedded software, Software ecosystem, Software},
  abstract={Software is prevalent in embedded products and may be critical for the success of the products, but manufacturers may view software as a necessary evil rather than as a key strategic opportunity and business differentiator. One of the reasons for this can be extensive supplier and subcontractor relationships and the cost, effort or unpredictability of the deliverables from the subcontractors are experienced as a major problem. The paper proposes open software ecosystem as an alternative approach to develop software for embedded systems, and elaborates on the necessary quality attributes of an embedded platform underlying such an ecosystem. The paper then defines a reference architecture consisting of 17 key decisions together with four architectural patterns, and provides the rationale why they are essential for an open software ecosystem platform for embedded systems in general and automotive systems in particular. The reference architecture is validated through a prototypical platform implementation in an industrial setting, providing a deeper understanding of how the architecture could be realised in the automotive domain. Four potential existing platforms, all targeted at the embedded domain (Android, OKL4, AUTOSAR and Robocop), are evaluated against the identified quality attributes to see how they could serve as a basis for an open software ecosystem platform with the conclusion that while none of them is a perfect fit they all have fundamental mechanisms necessary for an open software ecosystem approach.}
}

@article{rayyan-727967129,
  title={A systematic mapping study on text analysis techniques in software architecture},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={144},
  pages={533-558},
  author={Bi, Tingting and Liang, Peng and Tang, Antony and Yang, Chen},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218301493},
  keywords={Systematic mapping study, Software architecture, Text analysis technique, Software},
  abstract={Context Information from artifacts in each phase of the software development life cycle can potentially be mined to enhance architectural knowledge. Many text analysis techniques have been proposed for mining such artifacts. However, there is no comprehensive understanding of what artifacts these text analysis techniques analyze, what information they are able to extract or how they enhance architecting activities. Objective This systematic mapping study aims to study text analysis techniques for mining architecture-related artifacts and how these techniques have been used, and to identify the benefits and limitations of these techniques and tools with respect to enhancing architecting activities. Method We conducted a systematic mapping study and defined five research questions. We analyzed the results using descriptive statistics and qualitative analysis methods. Results Fifty-five studies were finally selected with the following results: (1) Current text analysis research emphasizes on architectural understanding and recovery. (2) A spectrum of text analysis techniques have been used in textual architecture information analysis. (3) Five categories of benefits and three categories of limitations were identified. Conclusions This study shows a steady interest in textual architecture information analysis. The results give clues for future research directions on improving architecture practice through using these text analysis techniques.}
}

@article{rayyan-727967130,
  title={Electronic building permission system: The case of greece},
  year={2015},
  journal={Procedia Engineering},
  issn={1877-7058},
  volume={123},
  pages={50-58},
  author={Bellos, Christos V and Petroutsatou, Kleopatra and Anthopoulos, Leonidas},
  url={https://www.sciencedirect.com/science/article/pii/S1877705815031501},
  keywords={e-government, e-service, electronic building permission, Greece},
  abstract={Electronic building permission systems have been used in several European countries from the early 70s (i.e. Italy, France and Netherlands), while in USA are being utilized several components and sub-modules of the system from the early 30s. The European Commission defined the electronic application for Building Permission as one of the 20 primary e-Government services, which each European Member State was obliged to deliver online by 2005, according to the e-Europe strategies (e-Europe2003). However, Greece, as well as several other European countries failed to address this challenge and the goal of an electronic building permission system across Europe was extended, initially until 2010 (i2010 strategy) and eventually until 2020 (Horizon 2020 strategy). The current study initially defines the primary and secondary functionalities that such a system should have. Furthermore, the study examines the factors that influence the success or failure of such a project and proceed to a case study investigation in Greece by examining the current situation and the level of absorption of the European i2010 strategy regarding the electronic building permission. The research framework encompasses a literature survey of publications and official reports in order to shape a “clear picture” regarding the different approach of e-service delivery. In addition, in this paper are analyzed the results of structured questionnaires that have been sent and of interviews that have been conducted with public organizations in Greece that are responsible for the implementation, installation and utilization of such a system (i.e. Information Society in Greece, Municipalities). In conclusion, this research investigates the framework of the building permission e-services and its current situation in Greece that leads to: (a) an effective public management administration by simplification of licensing procedures, (b) a reduction of depraved behavior and (c) environmental benefits by reducing bureaucracy.}
}

@article{rayyan-727967131,
  title={A framework for semi-automated co-evolution of security knowledge and system models},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={139},
  pages={142-160},
  author={Bürger, Jens and Strüber, Daniel and Gärtner, Stefan and Ruhroth, Thomas and Jürjens, Jan and Schneider, Kurt},
  url={https://www.sciencedirect.com/science/article/pii/S016412121830027X},
  keywords={Co-evolution, Security impact analysis, Security requirements, Software design, Software evolution},
  abstract={Security is an important and challenging quality aspect of software-intensive systems, becoming even more demanding regarding long-living systems. Novel attacks and changing laws lead to security issues that did not necessarily rise from a flawed initial design, but also when the system fails to keep up with a changing environment. Thus, security requires maintenance throughout the operation phase. Ongoing adaptations in response to changed security knowledge are inevitable. A necessary prerequisite for such adaptations is a good understanding of the security-relevant parts of the system and the security knowledge. We present a model-based framework for supporting the maintenance of security during the long-term evolution of a software system. It uses ontologies to manage the system-specific and the security knowledge. With model queries, graph transformation and differencing techniques, knowledge changes are analyzed and the system model is adapted. We introduce the novel concept of Security Maintenance Rules to couple the evolution of security knowledge with co-evolutions of the system model. As evaluation, community knowledge about vulnerabilities is used (Common Weakness Enumeration database). We show the applicability of the framework to the iTrust system from the medical care domain and hence show the benefits of supporting co-evolution for maintaining secure systems.}
}

@article{rayyan-727967132,
  title={Evaluating REST architectures—Approach, tooling and guidelines},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={112},
  pages={156-180},
  author={Costa, Bruno and Pires, Paulo F and Delicato, Flávia C and Merson, Paulo},
  url={https://www.sciencedirect.com/science/article/pii/S0164121215002150},
  keywords={REST, Scenario-based evaluation guidelines, Software architecture evaluation},
  abstract={Architectural decisions determine the ability of the implemented system to satisfy functional and quality attribute requirements. The Representational State Transfer (REST) architectural style has been extensively used recently for integrating services and applications. Its adoption to build SOA-based distributed systems brings several benefits, but also poses new challenges and risks. Particularly important among those risks are failures to effectively address quality attribute requirements such as security, reliability, and performance. A proved efficient technique to identify and help mitigate those risks is the architecture evaluation. In this paper we propose an approach, tooling, and guidelines to aid architecture evaluation activities in REST-based systems. These guidelines can be systematically used along with evaluation methods to reason about design considerations and tradeoffs. To demonstrate how the guidelines can help architecture evaluators, we present a proof of concept describing how to use the guidelines in an ATAM (Architecture Tradeoff Analysis Method) evaluation. We also present the results of a survey conducted with industry specialists who have performed architecture evaluations in real world REST-based systems in order to gauge the suitability and utility of the proposed guidelines. Finally, the paper describes a Web tool developed to facilitate the use of the evaluation guidelines.}
}

@article{rayyan-727967133,
  title={The diffusion stages of business intelligence & analytics (BI&A): A systematic mapping study},
  year={2014},
  journal={Procedia Technology},
  issn={2212-0173},
  volume={16},
  pages={172-179},
  author={Côrte-Real, Nadine and Ruivo, Pedro and Oliveira, Tiago},
  url={https://www.sciencedirect.com/science/article/pii/S2212017314003077},
  keywords={adoption, benefits, BI&A, business intelligence & analytics, impacts, implementation, systematic mapping study, use, Intelligence},
  abstract={Business intelligence & analytics (BI&A) has evolved to become a foundational cornerstone of enterprise decision support. Since the way BI&A is implemented and assimilated is quite different among organizations is important to approach BI&A literature by four selected diffusion stages (adoption, implementation, use and impacts of use). The diffusion stages assume a crucial importance to track the BI&A evolution in organizations and justify the investment made. The main focus of this paper is to evidence BI&A research on its several diffusion stages. It provides an updated bibliography of BI&A articles published in the IS journal and conferences during the period of 2000 and 2013. A total of 30 articles from 11 journals and 8 conferences are reviewed. This study contributes to the BI&A research in three ways. This is the first systematic mapping study focused on BI&A diffusion stages. It contributes to see how BI&A stages have been analyzed (theories used, data collection methods, analysis methods and publication source). Finally, it observes that little attention has been given to BI&A post-adoption stages and proposes future research line on this area.}
}

@article{rayyan-727967134,
  title={A framework for the development of measurement and quality assurance in software-based medical rehabilitation systems},
  year={2012},
  journal={Procedia Engineering},
  issn={1877-7058},
  volume={41},
  pages={53-60},
  author={Ahamed, Nizam Uddin and Sundaraj, Kenneth and Ahmad, R Badlishah and Rahman, Matiur and Ali, Asraf},
  url={https://www.sciencedirect.com/science/article/pii/S1877705812025283},
  keywords={Software, Rehabilitation, Measurement, Medical, Quality, Robot},
  abstract={The field of computer and robot-assisted rehabilitation system is rooted in the principle that software must be largely errorless, user-friendly, robust, accurate with respect to data, respond in a timely manner, and yet inexpensive, which lead to enhanced patient outcomes. In this digitized age, computerized and robotic rehabilitation systems act as a vital support for disabled individuals. Till today, different types of software for medical rehabilitation systems have been developed and applied to the rehabilitation process successfully, but improvement in quality and measurement of rehabilitation software is continuously in progress. Some ways of the software production have been established but further measurement process has always been a necessity. This paper presents the framework and recommends establishment of software quality measurement in computer- and robot-assisted automated medical rehabilitation system. Also, a brief discussion of rehabilitation technique and their software quality is also included. Lastly, we include its importance in medical technology and quality. To satisfy the end user, vendor satisfaction, software measurement and quality assurance are important components in software-based medical rehabilitation systemsy. © 2012 The Authors. Published by Elsevier Ltd. Selection and/or peer-review under responsibility of the Centre of Humanoid Robots and Bio-Sensor (HuRoBs), Faculty of Mechanical Engineering, Universiti Teknologi MARA.}
}

@article{rayyan-727967135,
  title={Software business models from a distribution perspective: A systematic mapping study},
  year={2015},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={64},
  pages={395-402},
  author={Heredia, Alberto and Colomo-Palacios, Ricardo and de Amescua, Antonio},
  url={https://www.sciencedirect.com/science/article/pii/S1877050915026514},
  keywords={systematic mapping, mobile apps., on-premise software, open-source software, SaaS, software business models, Software},
  abstract={Business models (BMs) describe how a company creates and delivers value to customers, the products or services that it offers and the compensation for them. Software companies need to be able to adopt different BMs to be successful in modern economy. Despite the number of publications on the field, there is still not a clear picture of software BMs. The purpose of this study is to structure and characterize the state of the art on software BMs with focus on sales and distribution models to help discover possible research gaps. The authors of this study conducted a systematic mapping study using relevant keywords to identify primary studies in the existing literature related to software BMs from a business management perspective. The search strategy returned 1871 papers and 51 were selected as primary studies. The analysis of results helps clarify the picture of software BMs and highlights the most relevant sources of papers. Results also reveal the broad interest of researchers on this topic. Most of the primary studies were related to service-based BMs, and to a lesser extent on product-based or open-source-based BMs; there is also an increase in the attention of researchers towards models built around mobile apps. While many authors report experience papers, only some authors validate or evaluate new proposals of sales and distribution models.}
}

@article{rayyan-727967136,
  title={Aspect-oriented model-driven code generation: A systematic mapping study},
  year={2013},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={55},
  number={2},
  pages={395-411},
  author={Mehmood, Abid and Jawawi, Dayang N A},
  url={https://www.sciencedirect.com/science/article/pii/S0950584912001863},
  keywords={Aspect-oriented software development, Code generation, Model-driven engineering, Systematic map},
  abstract={Context Model-driven code generation is being increasingly applied to enhance software development from perspectives of maintainability, extensibility and reusability. However, aspect-oriented code generation from models is an area that is currently underdeveloped. Objective In this study we provide a survey of existing research on aspect-oriented modeling and code generation to discover current work and identify needs for future research. Method A systematic mapping study was performed to find relevant studies. Classification schemes have been defined and the 65 selected primary studies have been classified on the basis of research focus, contribution type and research type. Results The papers of solution proposal research type are in a majority. All together aspect-oriented modeling appears being the most focused area divided into modeling notations and process (36%) and model composition and interaction management (26%). The majority of contributions are methods. Conclusion Aspect-oriented modeling and composition mechanisms have been significantly discussed in existing literature while more research is needed in the area of model-driven code generation. Furthermore, we have observed that previous research has frequently focused on proposing solutions and thus there is need for research that validates and evaluates the existing proposals in order to provide firm foundations for aspect-oriented model-driven code generation.}
}

@article{rayyan-727967137,
  title={A framework to study requirements-driven collaboration among agile teams: Findings from two case studies},
  year={2015},
  journal={Computers in Human Behavior},
  issn={0747-5632},
  volume={51},
  pages={1367-1379},
  author={Inayat, Irum and Salim, Siti Salwah},
  url={https://www.sciencedirect.com/science/article/pii/S0747563214005639},
  keywords={Collaboration, Agile methods, Communication and awareness, Distributed agile teams, Requirements-driven collaboration},
  abstract={Stakeholders' collaboration is required to develop requirements in agile software development. Requirements engineering and agile methods share common grounds as they both focus on stakeholder collaboration. The key issue is finding a way to study collaboration driven by requirements in geographically distributed agile teams. In this paper, we aim to propose a framework to study collaboration driven by requirements among agile teams and determine the impact of their collaboration patterns on the iteration performance. We define collaboration in terms of communication as information exchange among members, and awareness knowledge of others. Two case studies were conducted to examine communication and awareness network patterns among distributed agile teams. Data were collected through questionnaires, semi-structured interviews and onsite observation. The findings revealed that the framework aids in determining the core members, collaboration trends, clustering tendency, communication and awareness reciprocity of the teams, small worldliness and centralisation behaviour of the networks and iteration performance of the agile teams. The framework has implications for the industrial practitioners, i.e. managers to learn about their team's collaboration in order to take measures for performance improvement. At the same time, the researchers can use this framework to study other social aspects in variable settings to produce more empirical results.}
}

@article{rayyan-727967138,
  title={Reducing test effort: A systematic mapping study on existing approaches},
  year={2012},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={54},
  number={10},
  pages={1092-1106},
  author={Elberzhager, Frank and Rosbach, Alla and Münch, Jürgen and Eschbach, Robert},
  url={https://www.sciencedirect.com/science/article/pii/S0950584912000894},
  keywords={Software testing, Mapping study, Quality assurance, Efficiency improvement, Test effort reduction},
  abstract={Context Quality assurance effort, especially testing effort, is often a major cost factor during software development, which sometimes consumes more than 50% of the overall development effort. Consequently, one major goal is often to reduce testing effort. Objective The main goal of the systematic mapping study is the identification of existing approaches that are able to reduce testing effort. Therefore, an overview should be presented both for researchers and practitioners in order to identify, on the one hand, future research directions and, on the other hand, potential for improvements in practical environments. Method Two researchers performed a systematic mapping study, focusing on four databases with an initial result set of 4020 articles. Results In total, we selected and categorized 144 articles. Five different areas were identified that exploit different ways to reduce testing effort: approaches that predict defect-prone parts or defect content, automation, test input reduction approaches, quality assurance techniques applied before testing, and test strategy approaches. Conclusion The results reflect an increased interest in this topic in recent years. A lot of different approaches have been developed, refined, and evaluated in different environments. The highest attention was found with respect to automation and prediction approaches. In addition, some input reduction approaches were found. However, in terms of combining early quality assurance activities with testing to reduce test effort, only a small number of approaches were found. Due to the continuous challenge of reducing test effort, future research in this area is expected.}
}

@article{rayyan-727967139,
  title={APRSuite: A suite of components and use cases based on categorical decomposition of automatic program repair techniques and tools},
  year={2020},
  journal={Journal of Computer Languages},
  issn={2590-1184},
  volume={57},
  pages={100927},
  author={Khalilian, Alireza and Baraani-Dastjerdi, Ahmad and Zamani, Bahman},
  url={https://www.sciencedirect.com/science/article/pii/S2590118419300528},
  keywords={Evaluation, Automatic program repair, Debugging, Fault detection and localization, Patch},
  abstract={During the last decade, we are witnessing the advent of a proliferation of techniques and associated tools for automatic program repair (APR). The current techniques and tools provide rich sources of knowledge that should be taken into consideration for future research. An overview of the current APR techniques and tools can serve the research community as a knowledge accumulator. However, APR techniques and tools differ in many aspects making knowledge accumulation challenging. To overcome this challenge, in this paper, we propose to leverage common components that constitute the APR techniques and tools. To achieve this objective, we surveyed current APR techniques and tools to identify the APR Suite of common constituent components, namely as APRSuite. Repair source and defect class are examples of identified components. We grouped these components into several categories such as patch evaluation and target defects. We have also identified some of the possible use cases per component as well as different lessons learned in studies for each component and for each use case. In addition, we developed a principled way for application of the components. The APRSuite and the principled way to apply it comprise a framework for knowledge accumulation, evaluation, and comparison of APR techniques and tools. The novelty of our work lies in its original viewpoint to the process of literature review in the APR research field. To demonstrate the applicability of the framework, we mapped out several concrete APR techniques, as a first instantiation of the framework. We observed that the framework brings discipline into the evaluation and/or comparison of APR techniques and tools. The framework offers these benefits objectively and systematically. We concluded that knowledge accumulation and characterization through literature reviews can be therefore facilitated through the identified suite of components while at the same time the existing component suite can be modified, augmented, or improved.}
}

@article{rayyan-727967140,
  title={Exploring quality measures for the evaluation of feature models: a case study},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={131},
  pages={366-385},
  author={Bezerra, Carla I M and Andrade, Rossana M C and Monteiro, Jose Maria},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216301340},
  keywords={Feature models, Measures, Quality evaluation, Software product line},
  abstract={Evaluating the quality of a feature model is essential to ensure that errors in the early stages do not spread throughout the Software Product Line (SPL). One way to evaluate the feature model is to use measures that could be associated with the feature model quality characteristics and their quality attributes. In this paper, we aim at investigating how measures can be applied to the quality assessment of SPL feature models. We performed an exploratory case study using the COfFEE maintainability measures catalog and the S.P.L.O.T. feature models repository. In order to support this case study, we built a dataset (denoted by MAcchiATO) containing the values of 32 measures from COfFEE for 218 software feature models, extracted from S.P.L.O.T. This research approach allowed us to explore three different data analysis techniques. First, we applied the Spearman's rank correlation coefficient in order to identify relationships between the measures. This analysis showed that not all 32 measures in COfFEE are necessary to reveal the quality of a feature model and just 15 measures could be used. Next, the 32 measures in COfFEE were grouped by applying the Principal Component Analysis and a set of 9 new grouped measures were defined. Finally, we used the Tolerance Interval technique to define statistical thresholds for these 9 new grouped measures. So, our findings suggest that measures can be effectively used to support the quality evaluation of SPL feature models.}
}

@article{rayyan-727967141,
  title={Key activities for product derivation in software product lines},
  year={2011},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={84},
  number={2},
  pages={285-300},
  author={Rabiser, Rick and O'Leary, Pádraig and Richardson, Ita},
  url={https://www.sciencedirect.com/science/article/pii/S0164121210002700},
  keywords={Software product lines, Process, Product derivation, Software},
  abstract={More and more organizations adopt software product lines to leverage extensive reuse and deliver a multitude of benefits such as increased quality and productivity and a decrease in cost and time-to-market of their software development. When compared to the vast amount of research on developing product lines, relatively little work has been dedicated to the actual use of product lines to derive individual products, i.e., the process of product derivation. Existing approaches to product derivation have been developed independently for different aims and purposes. While the definition of a general approach applicable to every domain may not be possible, it would be interesting for researchers and practitioners to know which activities are common in existing approaches, i.e., what are the key activities in product derivation. In this paper we report on how we compared two product derivation approaches developed by the authors in two different, independent research projects. Both approaches independently sought to identify product derivation activities, one through a process reference model and the other through a tool-supported derivation approach. Both approaches have been developed and validated in research industry collaborations with different companies. Through the comparison of the approaches we identify key product derivation activities. We illustrate the activities' importance with examples from industry collaborations. To further validate the activities, we analyze three existing product derivation approaches for their support for these activities. The validation provides evidence that the identified activities are relevant to product derivation and we thus conclude that they should be considered (e.g., as a checklist) when developing or evaluating a product derivation approach.}
}

@article{rayyan-727967142,
  title={Volume contents},
  year={2005},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={47},
  number={15},
  pages={1047-1048},
  url={https://www.sciencedirect.com/science/article/pii/S095058490500162X}
}

@article{rayyan-727967143,
  title={Investigating Web size metrics for early Web cost estimation},
  year={2005},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={77},
  number={2},
  pages={157-172},
  author={Mendes, Emilia and Mosley, Nile and Counsell, Steve},
  url={https://www.sciencedirect.com/science/article/pii/S0164121204001463},
  keywords={Metronidazole},
  abstract={This paper's aim is to bring light to this issue by identifying size metrics and cost drivers for early Web cost estimation based on current practices of several Web Companies worldwide. This is achieved using two surveys and a case study. The first survey (S1) used a search engine to obtain Web project quote forms employed by Web companies worldwide to provide initial quotes on Web development projects. The 133 Web project quote forms gathered data on size metrics, cost factors, contingency and possibly profit metrics. These metrics were organised into categories and ranked. Results indicated that the two most common size metrics used for Web cost estimation were “total number of Web pages” (70%) and “which features/functionality to be provided by the application” (66%). The results of S1 were then validated by a mature Web company that has more than 12years of experience in Web development and a portfolio of more than 50 Web applications. The analysis was conducted using an interview. Finally, once the case study was finished, a second validation was conducted using a survey (S2) involving local New Zealand Web companies. The results of both validations were used to prepare Web project data entry forms to gather data on Web projects worldwide. After gathering data on 67 real Web projects worldwide, multivariate regression applied to the data confirmed that the number of Web pages and features/functionality provided by the application to be developed were the two most influential effort predictors.}
}

@article{rayyan-727967144,
  title={Interactions between environmental sustainability goals and software product quality: A mapping study},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={95},
  pages={108-129},
  author={García-Mireles, Gabriel Alberto and Moraga, Mª Ángeles and García, Félix and Calero, Coral and Piattini, Mario},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917303142},
  keywords={ISO/IEC 25010, Environmental sustainability, Greenability, Interaction, Software product quality, Software},
  abstract={Context Sustainability is considered as either a quality requirement or a quality characteristic that should be included in software when environmental protection concerns are being taken into account. However, addressing sustainability in software projects might have an impact on the quality of the software product delivered. Conflicting goals between sustainability and particular software product characteristics should be studied when developing application software, since achieving users' requirements can be a hindrance in the quest to meet sustainability goals. Objective This paper aims to provide an overview of the approaches found in the literature for dealing with interactions between software product quality and sustainability in the context of application software. Method A systematic mapping study is conducted to identify practices for managing interactions between software quality characteristics and sustainability. The selected papers are classified according to the quality characteristic considered and their influence on sustainability. Results Most of the 66 selected papers focused on validating current technologies concerning their support for sustainability (46%%). The interaction between performance efficiency and energy efficiency is what is reported most and there is a fairly positive interaction. In addition, reliability and usability point to a positive interaction with energy efficiency, while security shows a conflicting interaction with energy efficiency. Functional suitability and maintainability can present both positive and negative interaction, with different goals derived from environmental sustainability. Conclusions Interactions between software quality and sustainability have been addressed within an explorative approach. There is a need for additional research work to characterize the impact of interaction on both software quality and sustainability. Furthermore, proposals should be validated in industrial settings.}
}

@article{rayyan-727967145,
  title={Revisiting software ecosystems Research: A longitudinal literature study},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={117},
  pages={84-103},
  author={Manikas, Konstantinos},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216000406},
  keywords={Longitudinal literature study, Software ecosystem maturity, Software ecosystems, Software},
  abstract={‘Software ecosystems' is argued to first appear as a concept more than 10 years ago and software ecosystem research started to take off in 2010. We conduct a systematic literature study, based on the most extensive literature review in the field up to date, with two primarily aims: (a) to provide an updated overview of the field and (b) to document evolution in the field. In total, we analyze 231 papers from 2007 until 2014 and provide an overview of the research in software ecosystems. Our analysis reveals a field that is rapidly growing, both in volume and empirical focus, while becoming more mature. We identify signs of field maturity from the increase in: (i) the number of journal articles, (ii) the empirical models within the last two years, and (iii) the number of ecosystems studied. However, we note that the field is far from mature and identify a set of challenges that are preventing the field from evolving. We propose means for future research and the community to address them. Finally, our analysis shapes the view of the field having evolved outside the existing definitions of software ecosystems and thus propose the update of the definition of software ecosystems.}
}

@article{rayyan-727967146,
  title={Cross lifecycle variability analysis: Utilizing requirements and testing artifacts},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={143},
  pages={208-230},
  author={Steinberger, Michal and Reinhartz-Berger, Iris and Tomer, Amir},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218300864},
  keywords={Software product lines, Ontology, Application lifecycle management, Software reuse, Variability analysis},
  abstract={Variability analysis is an essential activity that supports increasing and systemizing reuse across similar software products. Current studies use different types of artifacts for analyzing variability, most notably are architecture or design, requirements, and code. While architecture, design, and code help understand and model the differences in solutions and realizations, requirements enable capturing differences in a higher level of abstraction through the intended use of the software products or their behavior. However, analyzing variability based on requirements may result in inaccurate outcomes, due to the informal and incomplete nature of requirements. To tackle this deficiency, we call for augmenting requirements-based variability analysis with other behavior-related cross-lifecycle artifacts. Particularly, we extend an approach that compares and analyzes software behaviors based on requirements taking into account both ontological and semantic considerations. Using test cases and their relations to requirements, our extension, named SOVA R-TC, extract software behaviors more comprehensively, including their preconditions, post-conditions, and expected results. The outputs of SOVA R-TC are feature diagrams, which group similar behaviors and present variability of software products in a tree structure. Empirically evaluating outcomes of SOVA R-TC, they seem to be perceived as significantly better than outcomes generated based on requirements only.}
}

@article{rayyan-727967147,
  title={Performance evaluation of techniques to detect discontinuity in large-scale-systems},
  year={2016},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={94},
  pages={324-331},
  author={Malik, Haroon and Shakshuki, Elhadi M},
  url={https://www.sciencedirect.com/science/article/pii/S1877050916317987},
  keywords={Data center, Anomaly detection, Performance test},
  abstract={Contemporary data centres rely heavily on forecasts to accurately predict future workload. The accuracy of a forecast greatly depends upon the merit of performance data fed to the underlying algorithms. One of the fundamental problems faced by analysts in preparing data for use in forecasting is the timely identification of data discontinuities. A discontinuity is an abrupt change in a time-series pattern of a performance counter that persists but does not recur. We used a supervised and an unsupervised techniques to automatically identify the important performance counters that are likely indicators of discontinuities within performance data. We compared the performance of our approaches by conducting a case study on the performance data obtained from a large scale cloud provider as well as on open source benchmarks systems. The supervised counter selection approach has superior results in terms of unsupervised approach but bears an overhead of manual labelling of the performance data.}
}

@article{rayyan-727967148,
  title={Open source software ecosystems: A Systematic mapping},
  year={2017},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={91},
  pages={160-185},
  author={Franco-Bedoya, Oscar and Ameller, David and Costal, Dolors and Franch, Xavier},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917304512},
  keywords={Systematic mapping, Literature review, Software ecosystem, Open source software, OSS, OSSECO, SECO, Software},
  abstract={Context: Open source software (OSS) and software ecosystems (SECOs) are two consolidated research areas in software engineering. OSS influences the way organizations develop, acquire, use and commercialize software. SECOs have emerged as a paradigm to understand dynamics and heterogeneity in collaborative software development. For this reason, SECOs appear as a valid instrument to analyze OSS systems. However, there are few studies that blend both topics together. Objective: The purpose of this study is to evaluate the current state of the art in OSS ecosystems (OSSECOs) research, specifically: (a) what the most relevant definitions related to OSSECOs are; (b) what the particularities of this type of SECO are; and (c) how the knowledge about OSSECO is represented. Method: We conducted a systematic mapping following recommended practices. We applied automatic and manual searches on different sources and used a rigorous method to elicit the keywords from the research questions and selection criteria to retrieve the final papers. As a result, 82 papers were selected and evaluated. Threats to validity were identified and mitigated whenever possible. Results: The analysis allowed us to answer the research questions. Most notably, we did the following: (a) identified 64 terms related to the OSSECO and arranged them into a taxonomy; (b) built a genealogical tree to understand the genesis of the OSSECO term from related definitions; (c) analyzed the available definitions of SECO in the context of OSS; and (d) classified the existing modelling and analysis techniques of OSSECOs. Conclusion: As a summary of the systematic mapping, we conclude that existing research on several topics related to OSSECOs is still scarce (e.g., modelling and analysis techniques, quality models, standard definitions, etc.). This situation calls for further investigation efforts on how organizations and OSS communities actually understand OSSECOs.}
}

@article{rayyan-727967149,
  title={Continuous deployment of software intensive products and services: A systematic mapping study},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={123},
  pages={263-291},
  author={Rodríguez, Pilar and Haghighatkhah, Alireza and Lwakatare, Lucy Ellen and Teppola, Susanna and Suomalainen, Tanja and Eskeli, Juho and Karvonen, Teemu and Kuvaja, Pasi and Verner, June M and Oivo, Markku},
  url={https://www.sciencedirect.com/science/article/pii/S0164121215002812},
  keywords={Systematic mapping study, Software development, Continuous deployment, Software},
  abstract={The software intensive industry is moving towards the adoption of a value-driven and adaptive real-time business paradigm. The traditional view of software as an item that evolves through releases every few months is being replaced by the continuous evolution of software functionality. This study aims to classify and analyse the literature related to continuous deployment in the software domain in order to scope the phenomenon, provide an overview of the state-of-the-art, investigate the scientific evidence in the reported results and identify areas suitable for further research. We conducted a systematic mapping study and classified the continuous deployment literature. The benefits and challenges related to continuous deployment were also analysed. RESULTS: The systematic mapping study includes 50 primary studies published between 2001 and 2014. An in-depth analysis of the primary studies revealed ten recurrent themes that characterize continuous deployment and provide researchers with directions for future work. In addition, a set of benefits and challenges of which practitioners may take advantage were identified. CONCLUSION: Overall, although the topic area is very promising, it is still in its infancy, thus offering a plethora of new opportunities for both researchers and software intensive companies.}
}

@article{rayyan-727967150,
  title={On the testing resource allocation problem: Research trends and perspectives},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={161},
  pages={110462},
  author={Pietrantuono, Roberto},
  url={https://www.sciencedirect.com/science/article/pii/S0164121219302365},
  keywords={Literature review, Testing, Survey, Reliability allocation, Resource allocation, Test planning, Resource Allocation},
  abstract={In testing a software application, a primary concern is how to effectively plan the assignment of resources available for testing to the software components so as to achieve a target goal under given constraints. In the literature, this is known as testing resources allocation problem (TRAP). Researchers spent a lot of effort to propose models for supporting test engineers in this task, and a variety of solutions exist to assess the best trade-off between testing time, cost and quality of delivered products. This article presents a systematic mapping study aimed at systematically exploring the TRAP research area in order to provide an overview on the type of research performed and on results currently available. A sample of 68 selected studies has been classified and analyzed according to defined dimensions. Results give an overview of the state of the art, provide guidance to improve practicability and allow outlining a set of directions for future research and applications of TRAP solutions.}
}

@article{rayyan-727967151,
  title={The evolution of agile UXD},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={102},
  pages={1-5},
  author={Da Silva, Tiago Silva and Silveira, Milene Selbach and Maurer, Frank and Silveira, Fábio Fagundes},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917301490},
  abstract={Context Agile User eXperience Design (Agile UXD) is a current theme and a trending topic for the future of software development. The integration of UX Design within Agile development is seen as one of the frontiers for Agile Methods as a balance between upfront design as advocated by UX and the you-ain't-gonna-need-it (YAGNI) principle from the agile community must be found. Objective In this paper, we analyze the evolution and current state of Agile UXD to provide a brief overview of the topic and to point out still unaddressed gaps, challenges, and future trends. Method We systematically analyzed the existing research literature on how this topic evolved over time. We identified three categories with distinctive sets of work and classified them as Early, Middle and Recent years. Results We noticed that the Process and Practice dimension has already crossed the line that separates Agile and UXD, the People and Social dimension is crossing this line right now, and the Technology and Artifact is the dimension that took the longest to be addressed, and it did not cross the line yet. Crossing the line means that there is already a full understanding from the Agile side of UX needs and vice versa. Conclusion Agile UXD is a need for today's software development teams. However, integrated teams still need to understand that UXD is not a role, but discipline and culture for the whole Agile environment.}
}

@article{rayyan-727967152,
  title={Lithuanian case study on evaluating suitability, acceptance and use of IT tools by students – An example of applying Technology Enhanced Learning Research methods in Higher Education},
  year={2020},
  journal={Computers in Human Behavior},
  issn={0747-5632},
  volume={107},
  pages={106274},
  author={Kurilovas, Eugenijus and Kubilinskiene, Svetlana},
  url={https://www.sciencedirect.com/science/article/pii/S0747563220300303},
  keywords={Evaluation, Acceptance and use, Human-computer interaction, IT tools, Suitability, Technology Enhanced Learning Research in Higher Education},
  abstract={The aim of the paper is to present application of the methodology (i.e. model and method) to evaluate suitability, acceptance and use of information technology (IT) tools for particular students as well as a case study of evaluating two IT tools widely used while studying human-computer interaction (HCI) design at Lithuanian Universities. These IT tools are “Axure” and “Balsamiq Mockups”. High-quality IT tools should be optimised to particular students according to their personal needs, e.g. learning styles. In the paper, optimised IT tools should have the highest probabilistic suitability indexes for particular students according to Felder-Silverman learning styles model. Personalised IT tools' evaluation methodology presented in the paper is based on (1) well-known principles of Multiple Criteria Decision Analysis for identifying quality evaluation criteria; (2) Educational Technology Acceptance & Satisfaction Model (ETAS-M) based on well-known Unified Theory on Acceptance and Use of Technology (UTAUT) model, and (3) probabilistic suitability indexes to identify technology's suitability to particular students' needs according to their learning styles. In the paper, there is also a case study of implementing the methodology in Lithuania. This methodology is applicable in real life situations where teachers have to help students to apply IT tools that are most suitable, acceptable and useable for their needs and thus to improve students' motivation, which in its turn creates the conditions for better and more efficient education. The case study aims at analysing the results of evaluating HCI tools using this novel methodology. The methodology is one of the examples of application of Technology Enhanced Learning Research in Higher Education.}
}

@article{rayyan-727967153,
  title={A hybrid method for evaluating enterprise architecture implementation},
  year={2017},
  journal={Evaluation and Program Planning},
  issn={0149-7189},
  volume={60},
  pages={1-16},
  author={Nikpay, Fatemeh and Ahmad, Rodina and Yin Kia, Chiam},
  url={https://www.sciencedirect.com/science/article/pii/S014971891630012X},
  keywords={Decision making, Evaluation, Enterprise architecture, Hybrid method, Information system},
  abstract={Enterprise Architecture (EA) implementation evaluation provides a set of methods and practices for evaluating the EA implementation artefacts within an EA implementation project. There are insufficient practices in existing EA evaluation models in terms of considering all EA functions and processes, using structured methods in developing EA implementation, employing matured practices, and using appropriate metrics to achieve proper evaluation. The aim of this research is to develop a hybrid evaluation method that supports achieving the objectives of EA implementation. To attain this aim, the first step is to identify EA implementation evaluation practices. To this end, a Systematic Literature Review (SLR) was conducted. Second, the proposed hybrid method was developed based on the foundation and information extracted from the SLR, semi-structured interviews with EA practitioners, program theory evaluation and Information Systems (ISs) evaluation. Finally, the proposed method was validated by means of a case study and expert reviews. This research provides a suitable foundation for researchers who wish to extend and continue this research topic with further analysis and exploration, and for practitioners who would like to employ an effective and lightweight evaluation method for EA projects.}
}

@article{rayyan-727967154,
  title={Determining relevant training data for effort estimation using Window-based COCOMO calibration},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={147},
  pages={124-146},
  author={Nguyen, Vu and Boehm, Barry and Huang, LiGuo},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218302310},
  keywords={Project management, COCOMO, Model calibration, Moving windows, Software estimation, Window-based calibration, Calibration},
  abstract={Context A software estimation model is often built using historical project data. As software development practices change over time, however, a model based on past data may not make accurate predictions for a new project. Objectives We investigate the use of moving windows to determine relevant training data for COCOMO calibration. Method We present a windowing calibration approach to calibrating COCOMO and assess performance of effort estimation models calibrated using windows and all data. Results Our results show that calibrating COCOMO using small windows of the most recently completed projects generates superior estimates than using all available historical projects. Large windows tend to produce worse estimates. Conclusions This study provides empirical evidence to support the use of small windows of projects completed so far to calibrate models when COCOMO-like data is available. Additionally, when the change in software development over time is rapid, the use of windows is more justifiable for improving estimation accuracy.}
}

@article{rayyan-727967155,
  title={Research literature clustering using diffusion maps},
  year={2013},
  journal={Journal of Informetrics},
  issn={1751-1577},
  volume={7},
  number={4},
  pages={874-886},
  author={Nieminen, Paavo and Pölönen, Ilkka and Sipola, Tuomo},
  url={https://www.sciencedirect.com/science/article/pii/S1751157713000680},
  keywords={Data mining, Clustering, Diffusion map, Knowledge discovery process, Literature mapping},
  abstract={We apply the knowledge discovery process to the mapping of current topics in a particular field of science. We are interested in how articles form clusters and what are the contents of the found clusters. A framework involving web scraping, keyword extraction, dimensionality reduction and clustering using the diffusion map algorithm is presented. We use publicly available information about articles in high-impact journals. The method should be of use to practitioners or scientists who want to overview recent research in a field of science. As a case study, we map the topics in data mining literature in the year 2011.}
}

@article{rayyan-727967156,
  title={Extracting useful software development information from mobile application reviews: A survey of intelligent mining techniques and tools},
  year={2018},
  journal={Expert Systems with Applications},
  issn={0957-4174},
  volume={113},
  pages={186-199},
  author={Tavakoli, Mohammadali and Zhao, Liping and Heydari, Atefeh and Nenadić, Goran},
  url={https://www.sciencedirect.com/science/article/pii/S0957417418303361},
  keywords={app development, App review, App review mining, Intelligent app review mining techniques, Intelligent app review mining tools, Mobile application review, Software, Intelligence},
  abstract={Mobile application (app) websites such as Google Play and AppStore allow users to review their downloaded apps. Such reviews can be useful for app users, as they may help users make an informed decision; such reviews can also be potentially useful for app developers, if they contain valuable information concerning user needs and requirements. However, in order to unleash the value of app reviews for mobile app development, intelligent mining tools that can help discern relevant reviews from irrelevant ones must be provided. This paper surveys the state of the art in the development of such tools and techniques behind them. To gain insight into the maturity of the current support mining tools, the paper will also find out what app development information these tools have discovered and what challenges they are facing. The results of this survey can inform the development of more effective and intelligent app review mining techniques and tools.}
}

@article{rayyan-727967157,
  title={A systematic mapping study on software product line evolution: From legacy system reengineering to product line refactoring},
  year={2013},
  journal={Science of Computer Programming},
  issn={0167-6423},
  volume={78},
  number={8},
  pages={1010-1034},
  author={Laguna, Miguel A and Crespo, Yania},
  url={https://www.sciencedirect.com/science/article/pii/S0167642312000895},
  keywords={Software product line, Evolution, Legacy system, Reengineering, Refactoring, Software},
  abstract={Software product lines (SPLs) are used in industry to develop families of similar software systems. Legacy systems, either highly configurable or with a story of versions and local variations, are potential candidates for reconfiguration as SPLs using reengineering techniques. Existing SPLs can also be restructured using specific refactorings to improve their internal quality. Although many contributions (including industrial experiences) can be found in the literature, we lack a global vision covering the whole life cycle of an evolving product line. This study aims to survey existing research on the reengineering of legacy systems into SPLs and the refactoring of existing SPLs in order to identify proven approaches and pending challenges for future research in both subfields. We launched a systematic mapping study to find as much literature as possible, covering the diverse terms involved in the search string (restructuring, refactoring, reengineering, etc. always connected with SPLs) and filtering the papers using relevance criteria. The 74 papers selected were classified with respect to several dimensions: main focus, research and contribution type, academic or industrial validation if included, etc. We classified the research approaches and analyzed their feasibility for use in industry. The results of the study indicate that the initial works focused on the adaptation of generic reengineering processes to SPL extraction. Starting from that foundation, several trends have been detected in recent research: the integrated or guided reengineering of (typically object-oriented) legacy code and requirements; specific aspect-oriented or feature-oriented refactoring into SPLs, and more recently, refactoring for the evolution of existing product lines. A majority of papers include academic or industrial case studies, though only a few are based on quantitative data. The degree of maturity of both subfields is different: Industry examples for the reengineering of the legacy system subfield are abundant, although more evaluation research is needed to provide better evidence for adoption in industry. Product line evolution through refactoring is an emerging topic with some pending challenges. Although it has recently received some attention, the theoretical foundation is rather limited in this subfield and should be addressed in the near future. To sum up, the main contributions of this work are the classification of research approaches as well as the analysis of remaining challenges, open issues, and research opportunities.}
}

@article{rayyan-727967158,
  title={A systematic mapping study of software product lines testing},
  year={2011},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={53},
  number={5},
  pages={407-423},
  author={da Mota Silveira Neto, Paulo Anselmo and do Carmo Machado, Ivan and McGregor, John D and de Almeida, Eduardo Santana and de Lemos Meira, Silvio Romero},
  url={https://www.sciencedirect.com/science/article/pii/S0950584910002193},
  keywords={Software product lines, Software testing, Mapping study, Software},
  abstract={Context In software development, Testing is an important mechanism both to identify defects and assure that completed products work as specified. This is a common practice in single-system development, and continues to hold in Software Product Lines (SPL). Even though extensive research has been done in the SPL Testing field, it is necessary to assess the current state of research and practice, in order to provide practitioners with evidence that enable fostering its further development. Objective This paper focuses on Testing in SPL and has the following goals: investigate state-of-the-art testing practices, synthesize available evidence, and identify gaps between required techniques and existing approaches, available in the literature. Method A systematic mapping study was conducted with a set of nine research questions, in which 120 studies, dated from 1993 to 2009, were evaluated. Results Although several aspects regarding testing have been covered by single-system development approaches, many cannot be directly applied in the SPL context due to specific issues. In addition, particular aspects regarding SPL are not covered by the existing SPL approaches, and when the aspects are covered, the literature just gives brief overviews. This scenario indicates that additional investigation, empirical and practical, should be performed. Conclusion The results can help to understand the needs in SPL Testing, by identifying points that still require additional investigation, since important aspects regarding particular points of software product lines have not been addressed yet.}
}

@article{rayyan-727967159,
  title={On the declarative paradigm in hybrid business process representations: A conceptual framework and a systematic literature study},
  year={2020},
  journal={Information Systems},
  issn={0306-4379},
  volume={91},
  pages={101505},
  author={Abbad Andaloussi, Amine and Burattin, Andrea and Slaats, Tijs and Kindler, Ekkart and Weber, Barbara},
  url={https://www.sciencedirect.com/science/article/pii/S0306437920300168},
  keywords={Business process modeling, Declarative process modeling, Hybrid process model, Process flexibility, Understandability of process models},
  abstract={Process modeling plays a central role in the development of today's process-aware information systems both on the management level (e.g., providing input for requirements elicitation and fostering communication) and on the enactment level (providing a blue-print for process execution and enabling simulation). The literature comprises a variety of process modeling approaches proposing different modeling languages (i.e., imperative and declarative languages) and different types of process artifact support (i.e., process models, textual process descriptions, and guided simulations). However, the use of an individual modeling language or a single type of process artifact is usually not enough to provide a clear and concise understanding of the process. To overcome this limitation, a set of so-called “hybrid” approaches combining languages and artifacts have been proposed, but no common grounds have been set to define and categorize them. This work aims at providing a fundamental understanding of these hybrid approaches by defining a unified terminology, providing a conceptual framework and proposing an overarching overview to identify and analyze them. Since no common terminology has been used in the literature, we combined existing concepts and ontologies to define a “Hybrid Business Process Representation” (HBPR). Afterwards, we conducted a Systematic Literature Review (SLR) to identify and investigate the characteristics of HBPRs combining imperative and declarative languages or artifacts. The SLR resulted in 30 articles which were analyzed. The results indicate the presence of two distinct research lines and show common motivations driving the emergence of HBPRs, a limited maturity of existing approaches, and diverse application domains. Moreover, the results are synthesized into a taxonomy classifying different types of representations. Finally, the outcome of the study is used to provide a research agenda delineating the directions for future work.}
}

@article{rayyan-727967160,
  title={Representing software project vision by means of video: A quality model for vision videos},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={162},
  pages={110479},
  author={Karras, Oliver and Schneider, Kurt and Fricker, Samuel A},
  url={https://www.sciencedirect.com/science/article/pii/S0164121219302535},
  keywords={Quality model, Production, Quality characteristic, Video, Vision, Vision video, Software},
  abstract={Establishing a shared software project vision is a key challenge in Requirements Engineering (RE). Several approaches use videos to represent visions. However, these approaches omit how to produce a good video. This missing guidance is one crucial reason why videos are not established in RE. We propose a quality model for videos representing a vision, so-called vision videos. Based on two literature reviews, we elaborate ten quality characteristics of videos and five quality characteristics of visions which together form a quality model for vision videos that includes all 15 quality characteristics. We provide two representations of the quality model: (a) a hierarchical decomposition of vision video quality into the quality characteristics and (b) a mapping of these characteristics to the video production and use process. While the hierarchical decomposition supports the evaluation of vision videos, the mapping provides guidance for video production. In an evaluation with 139 students, we investigated whether the 15 characteristics are related to the overall quality of vision videos perceived by the subjects from a developer's the point of view. Six characteristics (video length, focus, prior knowledge, clarity, pleasure, and stability) correlated significantly with the likelihood that the subjects perceived a vision video as good. These relationships substantiate a fundamental relevance of the proposed quality model. Therefore, we conclude that the quality model is a sound basis for future refinements and extensions.}
}

@article{rayyan-727967161,
  title={A mapping study to investigate component-based software system metrics},
  year={2013},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={86},
  number={3},
  pages={587-603},
  author={Abdellatief, Majdi and Sultan, Abu Bakar Md and Ghani, Abdul Azim Abdul and Jabar, Marzanah A},
  url={https://www.sciencedirect.com/science/article/pii/S0164121212002798},
  keywords={Systematic mapping study, Software metrics, Component-based software system, Software components, Software quality, Metronidazole, Software},
  abstract={A component-based software system (CBSS) is a software system that is developed by integrating components that have been deployed independently. In the last few years, many researchers have proposed metrics to evaluate CBSS attributes. However, the practical use of these metrics can be difficult. For example, some of the metrics have concepts that either overlap or are not well defined, which could hinder their implementation. The aim of this study is to understand, classify and analyze existing research in component-based metrics, focusing on approaches and elements that are used to evaluate the quality of CBSS and its components from a component consumer's point of view. This paper presents a systematic mapping study of several metrics that were proposed to measure the quality of CBSS and its components. We found 17 proposals that could be applied to evaluate CBSSs, while 14 proposals could be applied to evaluate individual components in isolation. Various elements of the software components that were measured are reviewed and discussed. Only a few of the proposed metrics are soundly defined. The quality assessment of the primary studies detected many limitations and suggested guidelines for possibilities for improving and increasing the acceptance of metrics. However, it remains a challenge to characterize and evaluate a CBSS and its components quantitatively. For this reason, much effort must be made to achieve a better evaluation approach in the future.}
}

@article{rayyan-727967162,
  title={How have we evaluated software pattern application? A systematic mapping study of research design practices},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={65},
  pages={14-38},
  author={Riaz, Maria and Breaux, Travis and Williams, Laurie},
  url={https://www.sciencedirect.com/science/article/pii/S0950584915000774},
  keywords={Systematic review, Mapping study, Empirical design, Empirical evaluation, Software pattern, Research Design, Software},
  abstract={Context Software patterns encapsulate expert knowledge for constructing successful solutions to recurring problems. Although a large collection of software patterns is available in literature, empirical evidence on how well various patterns help in problem solving is limited and inconclusive. The context of these empirical findings is also not well understood, limiting applicability and generalizability of the findings. Objective To characterize the research design of empirical studies exploring software pattern application involving human participants. Method We conducted a systematic mapping study to identify and analyze 30 primary empirical studies on software pattern application, including 24 original studies and 6 replications. We characterize the research design in terms of the questions researchers have explored and the context of empirical research efforts. We also classify the studies in terms of measures used for evaluation, and threats to validity considered during study design and execution. Results Use of software patterns in maintenance is the most commonly investigated theme, explored in 16 studies. Object-oriented design patterns are evaluated in 14 studies while 4 studies evaluate architectural patterns. We identified 10 different constructs with 31 associated measures used to evaluate software patterns. Measures for ‘efficiency' and ‘usability' are commonly used to evaluate the problem solving process. While measures for ‘completeness', ‘correctness' and ‘quality' are commonly used to evaluate the final artifact. Overall, ‘time to complete a task' is the most frequently used measure, employed in 15 studies to measure ‘efficiency'. For qualitative measures, studies do not report approaches for minimizing biases 27% of the time. Nine studies do not discuss any threats to validity. Conclusion Subtle differences in study design and execution can limit comparison of findings. Establishing baselines for participants' experience level, providing appropriate training, standardizing problem sets, and employing commonly used measures to evaluate performance can support replication and comparison of results across studies.}
}

@article{rayyan-727967163,
  title={Understanding the order of agile practice introduction: Comparing agile maturity models and practitioners' experience},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={156},
  pages={1-20},
  author={Nurdiani, Indira and Börstler, Jürgen and Fricker, Samuel and Petersen, Kai and Chatzipetrou, Panagiota},
  url={https://www.sciencedirect.com/science/article/pii/S0164121219301207},
  keywords={Agile maturity model, Agile practice, Introduction strategies},
  abstract={Context: Agile maturity models (AMMs) suggest that agile practices are introduced in a certain order. However, whether the order of agile practice introduction as suggested in the AMMs is relevant in industry has not been evaluated in an empirical study. Objectives: In this study, we want to investigate: (1) order of agile practice introduction mentioned in AMMs, (2) order of introducing agile practices in industry, and (3) similarities and differences between (1) and (2). Methods: We conducted a literature survey to identify strategies proposed by the AMMs. We then compared the AMMs' suggestions to the strategies used by practitioners, which we elicited from a survey and a series of interviews from an earlier study. Results: The literature survey revealed 12 AMMs which provide explicit mappings of agile practices to maturity levels. These mappings showed little agreement on when practices should be introduced. Comparison of the AMMs' suggestions and the empirical study revealed that the guidance suggested by AMMs are not aligned with industry practice. Conclusion: Currently, AMMs do not provide sufficient information to guide agile adoption in industry. Our results suggest that there might be no universal strategy for agile adoption that works better than others.}
}

@article{rayyan-727967164,
  title={A method for defining a regional software ecosystem strategy: Colombia as a case study},
  year={2016},
  journal={Technological Forecasting and Social Change},
  issn={0040-1625},
  volume={104},
  pages={247-258},
  author={Larrucea, Xabier and Nanclares, Felix and Santamaria, Izaskun},
  url={https://www.sciencedirect.com/science/article/pii/S0040162516000093},
  keywords={Regional software ecosystems, Strategy, TRM, Colombia, Software},
  abstract={Software ecosystems (SECO) have been related to products or to a community of developers around a product. The SECO concept can also be applied to describe regional software ecosystems in which different software companies collaborate in a specific market based on a set of concrete technologies and using a set of capabilities. This paper details a regional SECO concept and a method based on regional endogenous capabilities and country needs to define a SECO strategy. Traditional strategy definition approaches are top-down, whereas this approach is a blended approach that merges bottom-up based on current regional capabilities and top-down based on market and technology trends. This paper presents a large case study performed in 6 regions of Colombia. We conducted 49 interviews and 16 workshops in which 654 attendees participated, and we developed the Colombian ICT national strategic plan based on this approach.}
}

@article{rayyan-727967165,
  title={How does object-oriented code refactoring influence software quality? Research landscape and challenges},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={157},
  pages={110394},
  author={Kaur, Satnam and Singh, Paramvir},
  url={https://www.sciencedirect.com/science/article/pii/S0164121219301694},
  keywords={Systematic mapping study, Object-oriented software, Software quality, Quality measures, Refactoring activity, Software},
  abstract={Context Software refactoring aims to improve software quality and developer productivity. Numerous empirical studies investigating the impact of refactoring activities on software quality have been conducted over the last two decades. Objective This study aims to perform a comprehensive systematic mapping study of existing empirical studies on evaluation of the effect of object-oriented code refactoring activities on software quality attributes. Method We followed a multi-stage scrutinizing process to select 142 primary studies published till December 2017. The selected primary studies were further classified based on several aspects to answer the research questions defined for this work. In addition, we applied vote-counting approach to combine the empirical results and their analysis reported in primary studies. Results The findings indicate that studies conducted in academic settings found more positive impact of refactoring on software quality than studies performed in industries. In general, refactoring activities caused all quality attributes to improve or degrade except for cohesion, complexity, inheritance, fault-proneness and power consumption attributes. Furthermore, individual refactoring activities have variable effects on most quality attributes explored in primary studies, indicating that refactoring does not always improve all quality attributes. Conclusions This study points out several open issues which require further investigation, e.g., lack of industrial validation, lesser coverage of refactoring activities, limited tool support, etc.}
}

@article{rayyan-727967166,
  title={A survey of open source multiphysics frameworks in engineering},
  year={2015},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={51},
  pages={1088-1097},
  author={Babur, Önder and Smilauer, Vit and Verhoeff, Tom and van den Brand, Mark},
  url={https://www.sciencedirect.com/science/article/pii/S1877050915010819},
  keywords={Domain Analysis, Feature Model, Modelling and Simulation, Multiphysics, Multiscale},
  abstract={This paper presents a systematic survey of open source multiphysics frameworks in the en- gineering domains. These domains share many commonalities despite the diverse application areas. A thorough search for the available frameworks with both academic and industrial ori- gins has revealed numerous candidates. Considering key characteristics such as project size, maturity and visibility, we selected Elmer, OpenFOAM and Salome for a detailed analysis. All the public documentation for these tools has been manually collected and inspected. Based on the analysis, we built a feature model for multiphysics in engineering, which captures the commonalities and variability in the domain. We in turn validated the resulting model via two other tools; Kratos by manual inspection, and OOFEM by means of expert validation by domain experts.}
}

@article{rayyan-727967167,
  title={Bug report severity level prediction in open source software: A survey and research opportunities},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={115},
  pages={58-78},
  author={Gomes, Luiz Alberto Ferreira and da Silva Torres, Ricardo and Côrtes, Mario Lúcio},
  url={https://www.sciencedirect.com/science/article/pii/S0950584919301648},
  keywords={Systematic mapping, Machine learning, Bug reports, Bug tracking systems, Severity level prediction, Software maintenance, Software repositories, Software},
  abstract={Context: The severity level attribute of a bug report is considered one of the most critical variables for planning evolution and maintenance in Free/Libre Open Source Software. This variable measures the impact the bug has on the successful execution of the software system and how soon a bug needs to be addressed by the development team. Both business and academic community have made an extensive investigation towards the proposal methods to automate the bug report severity prediction. Objective: This paper aims to provide a comprehensive mapping study review of recent research efforts on automatically bug report severity prediction. To the best of our knowledge, this is the first review to categorize quantitatively more than ten aspects of the experiments reported in several papers on bug report severity prediction. Method: The mapping study review was performed by searching four electronic databases. Studies published until December 2017 were considered. The initial resulting comprised of 54 papers. From this set, a total of 18 papers were selected. After performing snowballing, more nine papers were selected. Results: From the mapping study, we identified 27 studies addressing bug report severity prediction on Free/Libre Open Source Software. The gathered data confirm the relevance of this topic, reflects the scientific maturity of the research area, as well as, identify gaps, which can motivate new research initiatives. Conclusion: The message drawn from this review is that unstructured text features along with traditional machine learning algorithms and text mining methods have been playing a central role in the most proposed methods in literature to predict bug severity level. This scenario suggests that there is room for improving prediction results using state-of-the-art machine learning and text mining algorithms and techniques.}
}

@article{rayyan-727967168,
  title={Ontologies application in organizational learning: A literature review},
  year={2012},
  journal={Expert Systems with Applications},
  issn={0957-4174},
  volume={39},
  number={8},
  pages={7555-7561},
  author={Valaski, Joselaine and Malucelli, Andreia and Reinehr, Sheila},
  url={https://www.sciencedirect.com/science/article/pii/S0957417412000887},
  keywords={Ontology, Learning organization, Organizational learning, Learning},
  abstract={Although ontologies and organizational learning are issues that have been discussed for many years, there is not an approach on literature that gives an overview about how both issues have been applied together. This literature review has the objective of exploring how ontologies are being applied in the organizational learning process recently; as a consequence, only studies from the year of 2005 onwards have been searched. The identification process produced 353 papers from 11 different databases. After applying the exclusion criteria, the set was reduced to 11 papers, which clearly fitted to the criteria defined for accomplishment of the systematic review, which were then analyzed and classified. The papers have been classified according to the structure and level of the ontologies. Furthermore, the Information Technology (IT) used in conjunction with ontology was identified, as well as the way ontologies and IT can act as a means of facilitating the organizational learning process. It was observed that although ontologies are rather important, a very few number of researches have applied ontologies in the organizational learning processes. In a general way, ontologies and IT encourage the sharing of knowledge and formalization.}
}

@article{rayyan-727967169,
  title={The state of the art in automated requirements elicitation},
  year={2013},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={55},
  number={10},
  pages={1695-1709},
  author={Meth, Hendrik and Brhel, Manuel and Maedche, Alexander},
  url={https://www.sciencedirect.com/science/article/pii/S0950584913000827},
  keywords={Automation, Requirements Elicitation, Requirements Engineering, Requirements Reuse, Systematic Review},
  abstract={Context In large software development projects a huge number of unstructured text documents from various stakeholders becomes available and needs to be analyzed and transformed into structured requirements. This elicitation process is known to be time-consuming and error-prone when performed manually by a requirements engineer. Consequently, substantial research has been done to automate the process through a plethora of tools and technologies. Objective This paper aims to capture the current state of automated requirements elicitation and derive future research directions by identifying gaps in the existing body of knowledge and through relating existing works to each other. More specifically, we are investigating the following research question: What is the state of the art in research covering tool support for automated requirements elicitation from natural language documents? Method A systematic review of the literature in automated requirements elicitation is performed. Identified works are categorized using an analysis framework comprising tool categories, technological concepts and evaluation approaches. Furthermore, the identified papers are related to each other through citation analysis to trace the development of the research field. Results We identified, categorized and related 36 relevant publications. Summarizing the observations we made, we propose future research to (1) investigate alternative elicitation paradigms going beyond a pure automation approach (2) compare the effects of different types of knowledge on elicitation results (3) apply comparative evaluation methods and multi-dimensional evaluation measures and (4) strive for a closer integration of research activities across the sub-fields of automatic requirements elicitation. Conclusion Through the results of our paper, we intend to contribute to the Requirements Engineering body of knowledge by (1) conceptualizing an analysis framework for works in the area of automated requirements elicitation, going beyond former classifications (2) providing an extensive overview and categorization of existing works in this area (3) formulating concise directions for future research.}
}

@article{rayyan-727967170,
  title={Software project management in high maturity: A systematic literature mapping},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={148},
  pages={56-87},
  author={Cerdeiral, Cristina T and Santos, Gleison},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218302218},
  keywords={High maturity project management, Maturity models, Quantitative project management, Software},
  abstract={High maturity in software development involves statistically controlling the performance of critical subprocesses and using the predictability thus gained to manage projects with better planning precision and monitoring control. Maturity models such as CMMI mention statistical and other quantitative methods, techniques, and tools supporting high-maturity project management, but do not provide details about them, their use or their available types. Thus, knowledge is lacking on how to support software process improvement initiatives to select and apply statistical and other quantitative methods, techniques and tools in this context. The goal of this study is to identify various methods, techniques, and tools which can assist in high-maturity software project management. By conducting a systematic literature mapping, we identified 108 papers describing 153 contributions. We describe the contributions identified, classifying them by their type, their software technology maturation phase, the method by which they were evaluated, the development methods and characteristics which they support, and the process/indicator areas to which they were applied. We hope this work can help fill the knowledge gap on the statistical and other quantitative methods, techniques and tools actually being proposed, evaluated, experimented with and adopted by organizations to support quantitative high-maturity software project management.}
}

@article{rayyan-727967171,
  title={Development of service-oriented architectures using model-driven development: A mapping study},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={62},
  pages={42-66},
  author={Ameller, David and Burgués, Xavier and Collell, Oriol and Costal, Dolors and Franch, Xavier and Papazoglou, Mike P},
  url={https://www.sciencedirect.com/science/article/pii/S0950584915000361},
  keywords={Model-driven development, Mapping study, SOA, Service-oriented architecture, State of the art, MDD},
  abstract={Context Model-Driven Development (MDD) and Service-Oriented Architecture (SOA) are two challenging research areas in software engineering. MDD is about improving software development whilst SOA is a service-based conceptual development style, therefore investigating the available proposals in the literature to use MDD when developing SOA may be insightful. However, no studies have been found with this purpose. Objective This work aims at assessing the state of the art in MDD for SOA systems. It mainly focuses on: what are the characteristics of MDD approaches that support SOA; what types of SOA are supported; how do they handle non-functional requirements. Method We conducted a mapping study following a rigorous protocol. We identified the representative set of venues that should be included in the study. We applied a search string over the set of selected venues. As result, 129 papers were selected and analysed (both frequency analysis and correlation analysis) with respect to the defined classification criteria derived from the research questions. Threats to validity were identified and mitigated whenever possible. Results The analysis allows us to answer the research questions. We highlight: (1) predominance of papers from Europe and written by researchers only; (2) predominance of top-down transformation in software development activities; (3) inexistence of consolidated methods; (4) significant percentage of works without tool support; (5) SOA systems and service compositions more targeted than single services and SOA enterprise systems; (6) limited use of metamodels; (7) very limited use of NFRs; and (8) limited application in real cases. Conclusion This mapping study does not just provide the state of the art in the topic, but also identifies several issues that deserve investigation in the future, for instance the need of methods for activities other than software development (e.g., migration) or the need of conducting more real case studies.}
}

@article{rayyan-727967172,
  title={Risk perceptions and approaches in multi-organizations: A research review 2000–2012},
  year={2014},
  journal={International Journal of Project Management},
  issn={0263-7863},
  volume={32},
  number={4},
  pages={640-653},
  author={Lehtiranta, Liisa},
  url={https://www.sciencedirect.com/science/article/pii/S0263786313001245},
  keywords={Multi-organization, Multidisciplinary, Opportunity, Risk management, Risk perception, Uncertainty},
  abstract={Shared risks and opportunities set specific premises for risk management (RM) in temporary multi-organizations (TMOs). However, most project RM research is presented from the perspective of a single-organizational project delivery team or covers limited risk perceptions and RM approaches. This paper aims to address how well the body of knowledge on multi-organizational RM corresponds to a state-of-art understanding on project RM and to identify which gaps need to be addressed in future research. The review involves: 1) the preferred view of risk as threat and/or opportunity, 2) the nature of addressed risks as anticipated or unanticipated risks or unrealistic assumptions, 3) the role of the multi-organization as the source of risks and/or resources for risk management (RM), and 4) the allocation of risk responsibilities. The review covers research papers published between 2000 and 2012 in four journals: International Journal of Project Management (IJPM), Project Management Journal (PMJ), Journal of Construction Engineering and Management (JCEM), and IEEE Transactions on Software Engineering (TSE). 105 eligible research papers were identified. The results and conclusion outline the identified main gaps in multi-organizational RM research compared to the state-of-art RM research and TMO-specific characteristics. The results can be used to inform research agendas on more holistic and dynamic multi-organizational RM concepts.}
}

@article{rayyan-727967173,
  title={Selection of third party software in Off-The-Shelf-based software development—An interview study with industrial practitioners},
  year={2011},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={84},
  number={4},
  pages={620-637},
  author={Ayala, Claudia and Hauge, Øyvind and Conradi, Reidar and Franch, Xavier and Li, Jingyue},
  url={https://www.sciencedirect.com/science/article/pii/S0164121210002864},
  keywords={Software engineering, Empirical study, Software reuse, Component selection, Off-The-Shelf-based software development, Software},
  abstract={The success of software development using third party components highly depends on the ability to select a suitable component for the intended application. The evidence shows that there is limited knowledge about current industrial OTS selection practices. As a result, there is often a gap between theory and practice, and the proposed methods for supporting selection are rarely adopted in the industrial practice. This paper's goal is to investigate the actual industrial practice of component selection in order to provide an initial empirical basis that allows the reconciliation of research and industrial endeavors. The study consisted of semi-structured interviews with 23 employees from 20 different software-intensive companies that mostly develop web information system applications. It provides qualitative information that help to further understand these practices, and emphasize some aspects that have been overlooked by researchers. For instance, although the literature claims that component repositories are important for locating reusable components; these are hardly used in industrial practice. Instead, other resources that have not received considerable attention are used with this aim. Practices and potential market niches for software-intensive companies have been also identified. The results are valuable from both the research and the industrial perspectives as they provide a basis for formulating well-substantiated hypotheses and more effective improvement strategies.}
}

@article{rayyan-727967174,
  title={Programming languages for data-Intensive HPC applications: A systematic mapping study},
  year={2020},
  journal={Parallel Computing},
  issn={0167-8191},
  volume={91},
  pages={102584},
  author={Amaral, Vasco and Norberto, Beatriz and Goulão, Miguel and Aldinucci, Marco and Benkner, Siegfried and Bracciali, Andrea and Carreira, Paulo and Celms, Edgars and Correia, Luís and Grelck, Clemens and Karatza, Helen and Kessler, Christoph and Kilpatrick, Peter and Martiniano, Hugo and Mavridis, Ilias and Pllana, Sabri and Respício, Ana and Simão, José and Veiga, Luís and Visa, Ari},
  url={https://www.sciencedirect.com/science/article/pii/S0167819119301759},
  keywords={Big data, Data-intensive applications, Domain-Specific language (DSL), General-Purpose language (GPL), High performance computing (HPC), Programming languages, Systematic mapping study (SMS)},
  abstract={A major challenge in modelling and simulation is the need to combine expertise in both software technologies and a given scientific domain. When High-Performance Computing (HPC) is required to solve a scientific problem, software development becomes a problematic issue. Considering the complexity of the software for HPC, it is useful to identify programming languages that can be used to alleviate this issue. Because the existing literature on the topic of HPC is very dispersed, we performed a Systematic Mapping Study (SMS) in the context of the European COST Action cHiPSet. This literature study maps characteristics of various programming languages for data-intensive HPC applications, including category, typical user profiles, effectiveness, and type of articles. We organised the SMS in two phases. In the first phase, relevant articles are identified employing an automated keyword-based search in eight digital libraries. This lead to an initial sample of 420 papers, which was then narrowed down in a second phase by human inspection of article abstracts, titles and keywords to 152 relevant articles published in the period 2006–2018. The analysis of these articles enabled us to identify 26 programming languages referred to in 33 of relevant articles. We compared the outcome of the mapping study with results of our questionnaire-based survey that involved 57 HPC experts. The mapping study and the survey revealed that the desired features of programming languages for data-intensive HPC applications are portability, performance and usability. Furthermore, we observed that the majority of the programming languages used in the context of data-intensive HPC applications are text-based general-purpose programming languages. Typically these have a steep learning curve, which makes them difficult to adopt. We believe that the outcome of this study will inspire future research and development in programming languages for data-intensive HPC applications.}
}

@article{rayyan-727967175,
  title={Model-based testing of software product lines: Mapping study and research roadmap},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={167},
  pages={110608},
  author={Petry, Kleber L and OliveiraJr, Edson and Zorzo, Avelino F},
  url={https://www.sciencedirect.com/science/article/pii/S0164121220300868},
  keywords={Systematic mapping study, Software product line, MBT Roadmap, Model-b-ased testing, Reuse of test cases, Variability, Software},
  abstract={Model-Based Testing (MBT) has been successfully applied to Software Product Lines (SPL). This paper provides a panorama of state-of-the-art on MBT of SPLs. We performed a systematic mapping for answering questions related with domains, approaches, solution types, variability, test case automation, artifacts, and evaluation. We built a roadmap from 44 selected studies. Main obtained results are: Software and Automotive domains are most considered; Black-box testing is widely performed; most studies have fully-automated support; variability is considered in most studies; Finite State Machines is the most used model to test SPLs; Behavioral-based and Scenario-based are the most used models; Case Studies and Experiments are used to evaluate MBT solutions and the majority is performed in industrial environments; traceability is not widely explored for MBT solutions. Furthermore, we provide a roadmap synthesizing studies based on used models, more formal artifacts, supporting tools, variability management, (semi-)automation, and traceability. The roadmap contributes to identify related primary studies based on given artifacts, variability management, tools, automation, and traceability techniques and to identify, from a given primary study, which artifacts, tools, variability management, automation and traceability techniques are related. Therefore, the roadmap serves as a guide to researchers and practitioners on how to model-based test SPLs.}
}

@article{rayyan-727967176,
  title={MeSRAM – A method for assessing robustness of measurement programs in large software development organizations and its industrial evaluation},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={113},
  pages={76-100},
  author={Staron, Miroslaw and Meding, Wilhelm},
  url={https://www.sciencedirect.com/science/article/pii/S0164121215002368},
  keywords={Software engineering, Case study, Measurement program, Metrics},
  abstract={Measurement programs in large software development organizations contain a large number of indicators, base and derived measures to monitor products, processes and projects. The diversity and the number of these measures causes the measurement programs to become large, combining multiple needs, measurement tools and organizational goals. For the measurement program to effectively support organization's goals, it should be scalable, automated, standardized and flexible – i.e. robust. In this paper we present a method for assessing the robustness of measurement programs. The method is based on the robustness model which has been developed in collaboration between seven companies and a university. The purpose of the method is to support the companies to optimize the value obtained from the measurement programs and their cost. We evaluated the method at the seven companies and the results from applying the method to each company quantified the robustness of their programs, reflecting the real-world status of the programs and pinpointed strengths and improvements of the programs.}
}

@article{rayyan-727967177,
  title={A systematic mapping study on the combination of static and dynamic quality assurance techniques},
  year={2012},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={54},
  number={1},
  pages={1-15},
  author={Elberzhager, Frank and Münch, Jürgen and Nha, Vi Tran Ngoc},
  url={https://www.sciencedirect.com/science/article/pii/S0950584911001388},
  keywords={Systematic mapping study, Testing, Combination, Dynamic quality assurance, Inspection, Static quality assurance},
  abstract={Context A lot of different quality assurance techniques exist to ensure high quality products. However, most often they are applied in isolation. A systematic combination of different static and dynamic quality assurance techniques promises to exploit synergy effects, such as higher defect detection rates or reduced quality assurance costs. However, a systematic overview of such combinations and reported evidence about achieving synergy effects with such kinds of combinations is missing. Objective The main goal of this article is the classification and thematic analysis of existing approaches that combine different static and dynamic quality assurance technique, including reported effects, characteristics, and constraints. The result is an overview of existing approaches and a suitable basis for identifying future research directions. Method A systematic mapping study was performed by two researchers, focusing on four databases with an initial result set of 2498 articles, covering articles published between 1985 and 2010. Results In total, 51 articles were selected and classified according to multiple criteria. The two main dimensions of a combination are integration (i.e., the output of one quality assurance technique is used for the second one) and compilation (i.e., different quality assurance techniques are applied to ensure a common goal, but in isolation). The combination of static and dynamic analyses is one of the most common approaches and usually conducted in an integrated manner. With respect to the combination of inspection and testing techniques, this is done more often in a compiled way than in an integrated way. Conclusion The results show an increased interest in this topic in recent years, especially with respect to the integration of static and dynamic analyses. Inspection and testing techniques are currently mostly performed in an isolated manner. The integration of inspection and testing techniques is a promising research direction for the exploitation of additional synergy effects.}
}

@article{rayyan-727967178,
  title={IFC editorial board},
  year={2010},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={52},
  number={8},
  pages={IFC},
  url={https://www.sciencedirect.com/science/article/pii/S0950584910000868}
}

@article{rayyan-727967179,
  title={Quality models for web services: A systematic mapping},
  year={2014},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={56},
  number={10},
  pages={1167-1182},
  author={Oriol, Marc and Marco, Jordi and Franch, Xavier},
  url={https://www.sciencedirect.com/science/article/pii/S0950584914000822},
  keywords={Systematic mapping, Web service, Quality model, Quality of service},
  abstract={Context Quality of Service (QoS) is a major issue in various web service related activities. Quality models have been proposed as the engineering artefact to provide a common framework of understanding for QoS, by defining the quality factors that apply to web service usage. Objective The goal of this study is to evaluate the current state of the art of the proposed quality models for web services, specifically: (1) which are these proposals and how are they related; (2) what are their structural characteristics; (3) what quality factors are the most and least addressed; and (4) what are their most consolidated definitions. Method We have conducted a systematic mapping by defining a robust protocol that combines automatic and manual searches from different sources. We used a rigorous method to elicitate the keywords from the research questions and a selection criteria to retrieve the final papers to evaluate. We have adopted the ISO/IEC 25010 standard to articulate our analysis. Results We have evaluated 47 different quality models from 65 papers that fulfilled the selection criteria. By analyzing in depth these quality models, we have: (1) distributed the proposals along the time dimension and identified their relationships; (2) analyzed their size (visualizing the number of nodes and levels) and definition coverage (as indicator of quality of the proposals); (3) quantified the coverage of the different ISO/IEC 25010 quality factors by the proposals; (4) identified the quality factors that appeared in at least 30% of the surveyed proposals and provided the most consolidated definitions for them. Conclusions We believe that this panoramic view on the anatomy of the quality models for web services may be a good reference for prospective researchers and practitioners in the field and especially may help avoiding the definition of new proposals that do not align with current research.}
}

@article{rayyan-727967180,
  title={A systematic mapping study on open information extraction},
  year={2018},
  journal={Expert Systems with Applications},
  issn={0957-4174},
  volume={112},
  pages={372-387},
  author={Glauber, Rafael and Barreiro Claro, Daniela},
  url={https://www.sciencedirect.com/science/article/pii/S0957417418303932},
  keywords={Systematic mapping study, Open information extraction, Open knowledge acquisition, Open relation extraction, Open relation mapping, Information Storage and Retrieval},
  abstract={Open information extraction (Open IE) is a task for extracting relationship triples in plain texts without previously determining these relationships. The Open IE systems are generally applied to solutions on the web-scale such improving question answering systems, ontology constructions, document filtering and clustering. Since 2007, within the first Open IE system TEXTRUNNER, other related works have been proposed in this area. Despite other secondary studies on Open IE, useful information available to initiate new research in the area is limited. Thus, we propose a review of the literature in Open IE by a systematic mapping study. We have retrieved 2484 articles about Open IE in Science Direct, IEEE Xplore, ACM Digital Library, Scopus and Google Scholar databases. Among them, 2411 were filtered by exclusion criteria proposed in our systematic mapping protocol. The remaining 73 papers represent the state-of-the-art from the past seven years. Different researchers have proposed important contributions and have pointed out some open problems for Open IE. As a result, we summarized these contributions and identified significant gaps that could be envisioned as future works.}
}

@article{rayyan-727967181,
  title={Empirical research for software architecture decision making: An analysis},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={149},
  pages={360-381},
  author={Razavian, Maryam and Paech, Barbara and Tang, Antony},
  url={https://www.sciencedirect.com/science/article/pii/S016412121830267X},
  keywords={Decision making, Software architecture, Empirical research, Human aspects, Decision Making, Software},
  abstract={Context Despite past empirical research in software architecture decision making, we have not yet systematically studied how to perform such empirical research. Software architecture decision making involves humans, their behavioral issues and practice. As such, research on decision making needs to involve not only engineering but also social science research methods. Objective This paper studies empirical research on software architecture decision making. We want to understand what research methods have been used to study human decision making in software architecture. Further, we want to provide guidance for future studies. Method We analyzed research papers on software architecture decision making. We classified the papers according to different sub-dimensions of empirical research design like research logic, research purpose, research methodology and process. We introduce the study focus matrix and the research cycle to capture the focus and the goals of a software architecture decision making study. We identify gaps in current software architecture decision making research according to the classification and discuss open research issues inspired by social science research. Conclusion We show the variety of research designs and identify gaps with respect to focus and goals. Few papers study decision making behavior in software architecture design. Also these researchers study mostly the process and much less the outcome and the factors influencing decision making. Furthermore, there is a lack of improvements for software architecture decision making and in particular insights into behavior have not led to new practices. The study focus matrix and the research cycle are two new instruments for researchers to position their research clearly. This paper provides a retrospective for the community and an entry point for new researchers to design empirical studies that embrace the human role in software architecture decision making.}
}

@article{rayyan-727967182,
  title={Cost, benefits and quality of software development documentation: A systematic mapping},
  year={2015},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={99},
  pages={175-198},
  author={Zhi, Junji and Garousi-Yusifoğlu, Vahid and Sun, Bo and Garousi, Golara and Shahnewaz, Shawn and Ruhe, Guenther},
  url={https://www.sciencedirect.com/science/article/pii/S0164121214002131},
  keywords={Systematic mapping, Documentation benefit, Software documentation, Software},
  abstract={Context Software documentation is an integral part of any software development process. Researchers and practitioners have expressed concerns about costs, benefits and quality of software documentation in practice. On the one hand, there is a lack of a comprehensive model to evaluate the quality of documentation. On the other hand, researchers and practitioners need to assess whether documentation cost outweighs its benefit. Objectives In this study, we aim to summarize the existing literature and provide an overview of the field of software documentation cost, benefit and quality. Method We use the systematic-mapping methodology to map the existing body of knowledge related to software documentation cost, benefit and quality. To achieve our objectives, 11 Research Questions (RQ) are raised. The primary papers are carefully selected. After applying the inclusion and exclusion criteria, our study pool included a set of 69 papers from 1971 to 2011. A systematic map is developed and refined iteratively. Results We present the results of a systematic mapping covering different research aspects related to software documentation cost, benefit and quality (RQ 1–11). Key findings include: (1) validation research papers are dominating (27 papers), followed by solution proposals (21 papers). (2) Most papers (61 out of 69) do not mention the development life-cycle model explicitly. Agile development is only mentioned in 6 papers. (3) Most papers include only one “System under Study” (SUS) which is mostly academic prototype. The average number of participants in survey-based papers is 106, the highest one having approximately 1000 participants. (4) In terms of focus of papers, 50 papers focused on documentation quality, followed by 37 papers on benefit, and 12 papers on documentation cost. (5) The quality attributes of documentation that appear in most papers are, in order: completeness, consistency and accessibility. Additionally, improved meta-models for documentation cost, benefit and quality are also presented. Furthermore, we have created an online paper repository of the primary papers analyzed and mapped during this study. Conclusion Our study results show that this research area is emerging but far from mature. Firstly, documentation cost aspect seems to have been neglected in the existing literature and there are no systematic methods or models to measure cost. Also, despite a substantial number of solutions proposed during the last 40 years, more and stronger empirical evidences are still needed to enhance our understanding of this area. In particular, what we expect includes (1) more validation or evaluation studies; (2) studies involving large-scale development projects, or from large number of study participants of various organizations; (3) more industry-academia collaborations; (4) more estimation models or methods to assess documentation quality, benefit and, especially, cost.}
}

@article{rayyan-727967183,
  title={Editorial: Voice of the editorial board},
  year={2012},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={54},
  number={8},
  pages={803},
  author={Wohlin, Claes},
  url={https://www.sciencedirect.com/science/article/pii/S0950584912000559}
}

@article{rayyan-727967184,
  title={Architecting with microservices: A systematic mapping study},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={150},
  pages={77-97},
  author={Di Francesco, Paolo and Lago, Patricia and Malavolta, Ivano},
  url={https://www.sciencedirect.com/science/article/pii/S0164121219300019},
  keywords={Systematic mapping study, Software architecture, Microservices},
  abstract={Context A microservice architecture is composed of a set of small services, each running in its own process and communicating with lightweight mechanisms. Many aspects on architecting with microservices are still unexplored and existing research is still far from being crispy clear. Objective We aim at identifying, classifying, and evaluating the state of the art on architecting with microservices from the following perspectives: publication trends, focus of research, and potential for industrial adoption. Method We apply the systematic mapping methodology. We rigorously selected 103 primary studies and we defined and applied a classification framework to them for extracting key information for subsequent analysis. We synthesized the obtained data and produced a clear overview of the state of the art. Results This work contributes with (i) a classification framework for research studies on architecting with microservices, (ii) a systematic map of current research of the field, (iii) an evaluation of the potential for industrial adoption of research results, and (iv) a discussion of emerging findings and implications for future research. Conclusion This study provides a solid, rigorous, and replicable picture of the state of the art on architecting with microservices. Its results can benefit both researchers and practitioners of the field.}
}

@article{rayyan-727967185,
  title={Function-as-a-Service performance evaluation: A multivocal literature review},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={170},
  pages={110708},
  author={Scheuner, Joel and Leitner, Philipp},
  url={https://www.sciencedirect.com/science/article/pii/S0164121220301527},
  keywords={Cloud computing, Multivocal literature review, Benchmarking, Function-as-a-Service, Performance, Serverless},
  abstract={Function-as-a-Service (FaaS) is one form of the serverless cloud computing paradigm and is defined through FaaS platforms (e.g., AWS Lambda) executing event-triggered code snippets (i.e., functions). Many studies that empirically evaluate the performance of such FaaS platforms have started to appear but we are currently lacking a comprehensive understanding of the overall domain. To address this gap, we conducted a multivocal literature review (MLR) covering 112 studies from academic (51) and grey (61) literature. We find that existing work mainly studies the AWS Lambda platform and focuses on micro-benchmarks using simple functions to measure CPU speed and FaaS platform overhead (i.e., container cold starts). Further, we discover a mismatch between academic and industrial sources on tested platform configurations, find that function triggers remain insufficiently studied, and identify HTTP API gateways and cloud storages as the most used external service integrations. Following existing guidelines on experimentation in cloud systems, we discover many flaws threatening the reproducibility of experiments presented in the surveyed studies. We conclude with a discussion of gaps in literature and highlight methodological suggestions that may serve to improve future FaaS performance evaluation studies.}
}

@article{rayyan-727967186,
  title={Requirement-driven evolution in software product lines: A systematic mapping study},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={122},
  pages={110-143},
  author={Montalvillo, Leticia and Díaz, Oscar},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216301510},
  keywords={Software product lines, Systematic mapping study, Evolution, Software},
  abstract={CONTEXT. Software Product Lines (SPLs) aim to support the development of a whole family of software products through systematic reuse of shared assets. As SPLs exhibit a long life-span, evolution is an even greater concern than for single-systems. For the purpose of this work, evolution refers to the adaptation of the SPL as a result of changing requirements. Hence, evolution is triggered by requirement changes, and not by bug fixing or refactoring. OBJECTIVE. Research on SPL evolution has not been previously mapped. This work provides a mapping study along Petersen's and Kichenham's guidelines, to identify strong areas of knowledge, trends and gaps. RESULTS. We identified 107 relevant contributions. They were classified according to four facets: evolution activity (e.g., identify, analyze and plan, implement), product-derivation approach (e.g., annotation-based, composition-based), research type (e.g., solution, experience, evaluation), and asset type (i.e., variability model, SPL architecture, code assets and products). CONCLUSION. Analyses of the results indicate that “Solution proposals” are the most common type of contribution (31%). Regarding the evolution activity, “Implement change” (43%) and “Analyze and plan change” (37%) are the most covered ones. A finer-grained analysis uncovered some tasks as being underexposed. A detailed description of the 107 papers is also included.}
}

@article{rayyan-727967187,
  title={Empirical research methods for technology validation: Scaling up to practice},
  year={2014},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={95},
  pages={19-31},
  author={Wieringa, Roel},
  url={https://www.sciencedirect.com/science/article/pii/S0164121213002793},
  keywords={Empirical research methodology, Scaling up to practice, Technology validation},
  abstract={Before technology is transferred to the market, it must be validated empirically by simulating future practical use of the technology. Technology prototypes are first investigated in simplified contexts, and these simulations are scaled up to conditions of practice step by step as more becomes known about the technology. This paper discusses empirical research methods for scaling up new requirements engineering (RE) technology. When scaling up to practice, researchers want to generalize from validation studies to future practice. An analysis of scaling up technology in drug research reveals two ways to generalize, namely inductive generalization using statistical inference from samples, and analogic generalization using similarity between cases. Both are supported by abductive inference using mechanistic explanations of phenomena observed in the simulations. Illustrations of these inferences both in drug research and empirical RE research are given. Next, four kinds of methods for empirical RE technology validation are given, namely expert opinion, single-case mechanism experiments, technical action research and statistical difference-making experiments. A series of examples from empirical RE will illustrate the use of these methods, and the role of inductive generalization, analogic generalization, and abductive inference in them. Finally, the four kinds of empirical validation methods are compared with lists of validation methods known from empirical software engineering. The lists are combined to give an overview of some of the methods, instruments and data analysis techniques that may be used in empirical RE.}
}

@article{rayyan-727967188,
  title={The day 1 c-its application green light optimal speed Advisory—A mapping study},
  year={2020},
  journal={Transportation Research Procedia},
  issn={2352-1465},
  volume={49},
  pages={170-182},
  author={Mellegård, Niklas and Reichenberg, Frida},
  url={https://www.sciencedirect.com/science/article/pii/S2352146520307365},
  keywords={Mapping study, C-ITS, Green Light Optimal Speed Advisory (GLOSA), Traffic efficiency},
  abstract={This article reports on a mapping study to investigate the C-ROADS Day 1 C-ITS application Green Light Optimal Speed Advisory (GLOSA). In the study, 64 publications between 2006 and 2019 where reviewed and classified according to a schema developed using thematic analysis of the selected publications. Among the findings were that the typical publication evaluates through simulation benefits for the equipped vehicle, leaving considerable gaps in investigations of societal effects. Additionally, there is a lack of investigation on driver behaviour, both for the equipped vehicle and for fellow road users—the ability to accurately model such behaviour is necessary for reliable simulation results.}
}

@article{rayyan-727967189,
  title={A systematic mapping study on microservices architecture in DevOps},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={170},
  pages={110798},
  author={Waseem, Muhammad and Liang, Peng and Shahin, Mojtaba},
  url={https://www.sciencedirect.com/science/article/pii/S0164121220302053},
  keywords={Systematic Mapping Study, DevOps, Microservices Architecture},
  abstract={Context: Applying Microservices Architecture (MSA) in DevOps has received significant attention in recent years. However, there exists no comprehensive review of the state of research on this topic. Objective: This work aims to systematically identify, analyze, and classify the literature on MSA in DevOps. Methods: A Systematic Mapping Study (SMS) has been conducted on the literature published between January 2009 and July 2018. Results: Forty-seven studies were finally selected and the key results are: (1) Three themes on the research on MSA in DevOps are “microservices development and operations in DevOps”, “approaches and tool support for MSA based systems in DevOps”, and “MSA migration experiences in DevOps”. (2) 24 problems with their solutions regarding implementing MSA in DevOps are identified. (3) MSA is mainly described by using boxes and lines. (4) Most of the quality attributes are positively affected when employing MSA in DevOps. (5) 50 tools that support building MSA based systems in DevOps are collected. (6) The combination of MSA and DevOps has been applied in a wide range of application domains. Conclusion: The results and findings will benefit researchers and practitioners to conduct further research and bring more dedicated solutions for the issues of MSA in DevOps.}
}

@article{rayyan-727967190,
  title={Consensus in a fuzzy environment: A bibliometric study},
  year={2015},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={55},
  pages={660-667},
  author={Cabrerizo, F J and Mart́ınez, M A and Herrera, M and Herrera-Viedma, E},
  url={https://www.sciencedirect.com/science/article/pii/S1877050915015409},
  keywords={bibliometric study, Consensus, fuzzy environment., Bibliometrics},
  abstract={In today's organizations, group decision making has become a part of everyday organizational life. It involves multiple indi- viduals interacting to reach a decision. An important question here is the level of agreement or consensus achieved among the individuals before making the decision. Traditionally, consensus has been meant to be a full and unanimous agreement. How- ever, it is often not reachable in practice. A more reasonable approach is the use of softer consensus measures, which assess the consensus in a more flexible way, reflecting the large spectrum of possible partial agreements and guiding the discussion process until widespread agreement is achieved. As soft consensus measures are more human-consistent in the sense that they better reflect a real human perception of the essence of consensus, consensus models based on these kind of measures have been widely proposed. The aim of this contribution is to present a bibliometric study performed on the consensus approaches that have been proposed in a fuzzy environment. It gives an overview about the research products gathered in this research field. To do so, several points have been studied, among others: countries, journals, top contributing authors, most cited keywords, papers and authors. This allows us to show a quick shot of the state of the art in this research area.}
}

@article{rayyan-727967191,
  title={A Special Issue on Digital Transformation: a new challenge for education and training},
  year={2019},
  journal={Telematics and Informatics},
  issn={0736-5853},
  volume={38},
  pages={59-61},
  author={Moreira, Fernando and Rocha, Álvaro},
  url={https://www.sciencedirect.com/science/article/pii/S0736585319301972}
}

@article{rayyan-727967192,
  title={Developing theory-driven design research},
  year={2018},
  journal={Design Studies},
  issn={0142-694X},
  volume={56},
  pages={84-119},
  author={Cash, Philip J},
  url={https://www.sciencedirect.com/science/article/pii/S0142694X18300140},
  keywords={design research, design science, research methods, research theory, Research Design},
  abstract={Design research is increasingly weak in comparison with other fields; without action to increase scientific, theoretical, and methodological rigour there is a real possibility of the field being superseded and becoming obsolete through lack of impact. The aim of this paper is to show how design research could become more rigorous, relevant and have greater impact. I conduct a two-part review that combines systematic and critical components. Part one characterises the major scientific challenges facing design research, and part two examines how such challenges have been addressed in related fields. I identify key learning indicating future directions for theory-driven design research. I conclude by providing some concrete recommendations for the field of design research and individual design researchers.}
}

@article{rayyan-727967193,
  title={Software product lines traceability: A systematic mapping study},
  year={2017},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={84},
  pages={1-18},
  author={Vale, Tassio and de Almeida, Eduardo Santana and Alves, Vander and Kulesza, Uirá and Niu, Nan and de Lima, Ricardo},
  url={https://www.sciencedirect.com/science/article/pii/S0950584916304463},
  keywords={Software product lines, Systematic mapping study, Software reuse, Software and systems traceability, Software},
  abstract={Context: Traceability in Software Product Lines (SPL) is the ability to interrelate software engineering artifacts through required links to answer specific questions related to the families of products and underlying development processes. Despite the existence of studies to map out available evidence on traceability for single systems development, there is a lack of understanding on common strategies, activities, artifacts, and research gaps for SPL traceability. Objective: This paper analyzes 62 studies dating from 2001 to 2015 and discusses seven aspects of SPL traceability: main goals, strategies, application domains, research intensity, research challenges, rigor, and industrial relevance. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas calling for further research. Method: To gather evidence, we defined a mapping study process adapted from existing guidelines. Driven by a set of research questions, this process comprises three major phases: planning, conducting, and documenting the review. Results: This work provides a structured understanding of SPL traceability, indicating areas for further research. The lack of evidence regarding the application of research methods indicates the need for more rigorous SPL traceability studies with better description of context, study design, and limitations. For practitioners, although most identified studies have low industrial relevance, a few of them have high relevance and thus could provide some decision making support for application of SPL traceability in practice. Conclusions: This work concludes that SPL traceability is maturing and pinpoints areas where further investigation should be performed. As future work, we intend to improve the comparison between traceability proposals for SPL and single-system development.}
}

@article{rayyan-727967194,
  title={A systematic map of medical data preprocessing in knowledge discovery},
  year={2018},
  journal={Computer Methods and Programs in Biomedicine},
  issn={0169-2607},
  volume={162},
  pages={69-85},
  author={Idri, A and Benhar, H and Fernández-Alemán, J L and Kadi, I},
  url={https://www.sciencedirect.com/science/article/pii/S0169260717313706},
  keywords={Mapping study, Clinical data, Data preprocessing, Electronic heath records, Medical datamining},
  abstract={Background and objective Datamining (DM) has, over the last decade, received increased attention in the medical domain and has been widely used to analyze medical datasets in order to extract useful knowledge and previously unknown patterns. However, historical medical data can often comprise inconsistent, noisy, imbalanced, missing and high dimensional data. These challenges lead to a serious bias in predictive modeling and reduce the performance of DM techniques. Data preprocessing is, therefore, an essential step in knowledge discovery as regards improving the quality of data and making it appropriate and suitable for DM techniques. The objective of this paper is to review the use of preprocessing techniques in clinical datasets. Methods We performed a systematic map of studies regarding the application of data preprocessing to healthcare and published between January 2000 and December 2017. A search string was determined on the basis of the mapping questions and the PICO categories. The search string was then applied in digital databases covering the fields of computer science and medical informatics in order to identify relevant studies. The studies were initially selected by reading their titles, abstracts and keywords. Those that were selected at that stage were then reviewed using a set of inclusion and exclusion criteria in order to eliminate any that were not relevant. This process resulted in 126 primary studies. Results Selected studies were analyzed and classified according to their publication years and channels, research type, empirical type and contribution type. The findings of this mapping study revealed that researchers have paid a considerable amount of attention to preprocessing in medical DM in last decade. A significant number of the selected studies used data reduction and cleaning preprocessing tasks. Moreover, the disciplines in which preprocessing have received most attention are: cardiology, endocrinology and oncology. Conclusions Researchers should develop and implement standards for an effective integration of multiple medical data types. Moreover, we identified the need to perform literature reviews.}
}

@article{rayyan-727967195,
  title={Requirements for adopting software process lines},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={164},
  pages={110546},
  author={Agh, Halimeh and Garcia, Félix and Piattini, Mario and Ramsin, Raman},
  url={https://www.sciencedirect.com/science/article/pii/S0164121220300285},
  keywords={Systematic Mapping Study, Empirical Study, Method Tailoring, Software Process Lines, Software},
  abstract={A Software Process Line (SPrL) is potentially suitable for constructing software development methodologies by reusing core assets. However, adopting this approach without prior assessment of its suitability can lead to failure. The aim of this paper is to identify a set of requirements that can be used for deciding whether to adopt the SPrL approach in an organization. Identification of the requirements was accomplished in two stages: the characteristics important in method tailoring were first identified via a Systematic Mapping Study (SMS) that focused on analyzing 43 primary studies; the degree of importance of the identified characteristics was then determined using a questionnaire survey in which 31 experts participated. By analyzing the results of the SMS and the survey, we have identified 12 product-related, 22 project-related, and 10 organization-related requirements. In addition to these requirements, we have also identified two relevant requirements by studying previous research on Software Product Lines (SPL) and Business Process Lines (BPL). The requirements thus identified can help organizations decide on whether to adopt the SPrL approach: the more an organization satisfies the requirements, the more frequently method tailoring occurs in that organization, and hence, the more justified it is to adopt the SPrL approach.}
}

@article{rayyan-727967196,
  title={An empirical evaluation of ISO/IEC 15504-5 capability measures: Reflective or formative},
  year={2017},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={53},
  pages={123-130},
  author={Jung, Ho-Won and Ting, Kwok-Fai},
  url={https://www.sciencedirect.com/science/article/pii/S0920548916301027},
  keywords={Construct specification, Formative, ISO/IEC 15504-5, Measurement model, Process capability, Reflective, Vanishing tetrad test},
  abstract={The development of composite indicators such as capability level (CL) depends on the relationship between a construct (e.g., capability) and its measures [e.g., process attributes (PA) in ISO/IEC 15504-5]. This relationship can be represented either by a reflective model (i.e., a set of measures reflecting the capability of a process) or a formative model (i.e., a set of measures that collectively determines the overall capability of a process). The aim of this study is to provide illustrations of relationship testing procedures with an example: statistically testing whether PAs as capability measures in ISO/IEC 15504-5 are reflective or formative. This test is a requirement of process measurement frameworks in ISO/IEC 33003. Our statistical tests show that PAs are formative measures that are consistent with the aggregation method of PA ratings. Capability and maturity measures in software engineering studies and other disciplines can also utilize our testing procedures to get the confidence of CLs and maturity levels assigned in assessments.}
}

@article{rayyan-727967197,
  title={Reviewing ensemble classification methods in breast cancer},
  year={2019},
  journal={Computer Methods and Programs in Biomedicine},
  issn={0169-2607},
  volume={177},
  pages={89-112},
  author={Hosni, Mohamed and Abnane, Ibtissam and Idri, Ali and Carrillo de Gea, Juan M and Fernández Alemán, José Luis},
  url={https://www.sciencedirect.com/science/article/pii/S0169260719301907},
  keywords={Data mining, Machine learning, Breast cancer, Classification, Ensemble methods, Breast Neoplasms},
  abstract={Context Ensemble methods consist of combining more than one single technique to solve the same task. This approach was designed to overcome the weaknesses of single techniques and consolidate their strengths. Ensemble methods are now widely used to carry out prediction tasks (e.g. classification and regression) in several fields, including that of bioinformatics. Researchers have particularly begun to employ ensemble techniques to improve research into breast cancer, as this is the most frequent type of cancer and accounts for most of the deaths among women. Objective and method The goal of this study is to analyse the state of the art in ensemble classification methods when applied to breast cancer as regards 9 aspects: publication venues, medical tasks tackled, empirical and research types adopted, types of ensembles proposed, single techniques used to construct the ensembles, validation framework adopted to evaluate the proposed ensembles, tools used to build the ensembles, and optimization methods used for the single techniques. This paper was undertaken as a systematic mapping study. Results A total of 193 papers that were published from the year 2000 onwards, were selected from four online databases: IEEE Xplore, ACM digital library, Scopus and PubMed. This study found that of the six medical tasks that exist, the diagnosis medical task was that most frequently researched, and that the experiment-based empirical type and evaluation-based research type were the most dominant approaches adopted in the selected studies. The homogeneous type was that most widely used to perform the classification task. With regard to single techniques, this mapping study found that decision trees, support vector machines and artificial neural networks were those most frequently adopted to build ensemble classifiers. In the case of the evaluation framework, the Wisconsin Breast Cancer dataset was the most frequently used by researchers to perform their experiments, while the most noticeable validation method was k-fold cross-validation. Several tools are available to perform experiments related to ensemble classification methods, such as Weka and R Software. Few researchers took into account the optimisation of the single technique of which their proposed ensemble was composed, while the grid search method was that most frequently adopted to tune the parameter settings of a single classifier. Conclusion This paper reports an in-depth study of the application of ensemble methods as regards breast cancer. Our results show that there are several gaps and issues and we, therefore, provide researchers in the field of breast cancer research with recommendations. Moreover, after analysing the papers found in this systematic mapping study, we discovered that the majority report positive results concerning the accuracy of ensemble classifiers when compared to the single classifiers. In order to aggregate the evidence reported in literature, it will, therefore, be necessary to perform a systematic literature review and meta-analysis in which an in-depth analysis could be conducted so as to confirm the superiority of ensemble classifiers over the classical techniques.}
}

@article{rayyan-727967198,
  title={Search-based fault localisation: A systematic mapping study},
  year={2020},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={123},
  pages={106295},
  author={Leitao-Junior, Plinio S and Freitas, Diogo M and Vergilio, Silvia R and Camilo-Junior, Celso G and Harrison, Rachel},
  url={https://www.sciencedirect.com/science/article/pii/S0950584920300458},
  keywords={Systematic mapping, Meta-heuristic algorithms, Search-based fault localisation},
  abstract={Context Software Fault Localisation (FL) refers to finding faulty software elements related to failures produced as a result of test case execution. This is a laborious and time consuming task. To allow FL automation search-based algorithms have been successfully applied in the field of Search-Based Fault Localisation (SBFL). However, there is no study mapping the SBFL field to the best of our knowledge and we believe that such a map is important to promote new advances in this field. Objective To present the results of a mapping study on SBFL, by characterising the proposed methods, identifying sources of used information, adopted evaluation functions, applied algorithms and elements regarding reported experiments. Method Our mapping followed a defined process and a search protocol. The conducted analysis considers different dimensions and categories related to the main characteristics of SBFL methods. Results All methods are grounded on the coverage spectra category. Overall the methods search for solutions related to suspiciousness formulae to identify possible faulty code elements. Most studies use evolutionary algorithms, mainly Genetic Programming, by using a single-objective function. There is little investigation of real-and-multiple-fault scenarios, and the subjects are mostly written in C and Java. No consensus was observed on how to apply the evaluation metrics. Conclusions Search-based fault localisation has seen a rise in interest in the past few years and the number of studies has been growing. We identified some research opportunities such as exploring new sources of fault data, exploring multi-objective algorithms, analysing benchmarks according to some classes of faults, as well as, the use of a unique definition for evaluation measures.}
}

@article{rayyan-727967199,
  title={Understanding cloud-native applications after 10 years of cloud computing - A systematic mapping study},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={126},
  pages={1-16},
  author={Kratzke, Nane and Quint, Peter-Christian},
  url={https://www.sciencedirect.com/science/article/pii/S0164121217300018},
  keywords={Systematic mapping study, Cloud-native application, CNA, Elastic platform, Microservice, Pattern, Self service, Softwareization},
  abstract={It is common sense that cloud-native applications (CNA) are intentionally designed for the cloud. Although this understanding can be broadly used it does not guide and explain what a cloud-native application exactly is. The term “cloud-native” was used quite frequently in birthday times of cloud computing (2006) which seems somehow obvious nowadays. But the term disappeared almost completely. Suddenly and in the last years the term is used again more and more frequently and shows increasing momentum. This paper summarizes the outcomes of a systematic mapping study analyzing research papers covering “cloud-native” topics, research questions and engineering methodologies. We summarize research focuses and trends dealing with cloud-native application engineering approaches. Furthermore, we provide a definition for the term “cloud-native application” which takes all findings, insights of analyzed publications and already existing and well-defined terminology into account.}
}

@article{rayyan-727967200,
  title={Relevance, benefits, and problems of software modelling and model driven techniques—A survey in the Italian industry},
  year={2013},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={86},
  number={8},
  pages={2110-2126},
  author={Torchiano, Marco and Tomassetti, Federico and Ricca, Filippo and Tiso, Alessandro and Reggio, Gianna},
  url={https://www.sciencedirect.com/science/article/pii/S0164121213000824},
  keywords={Model driven techniques, Personal opinion survey, Software modelling, Software},
  abstract={Context Claimed benefits of software modelling and model driven techniques are improvements in productivity, portability, maintainability and interoperability. However, little effort has been devoted at collecting evidence to evaluate their actual relevance, benefits and usage complications. Goal The main goals of this paper are: (1) assess the diffusion and relevance of software modelling and MD techniques in the Italian industry, (2) understand the expected and achieved benefits, and (3) identify which problems limit/prevent their diffusion. Method We conducted an exploratory personal opinion survey with a sample of 155 Italian software professionals by means of a Web-based questionnaire on-line from February to April 2011. Results Software modelling and MD techniques are very relevant in the Italian industry. The adoption of simple modelling brings common benefits (better design support, documentation improvement, better maintenance, and higher software quality), while MD techniques make it easier to achieve: improved standardization, higher productivity, and platform independence. We identified problems, some hindering adoption (too much effort required and limited usefulness) others preventing it (lack of competencies and supporting tools). Conclusions The relevance represents an important objective motivation for researchers in this area. The relationship between techniques and attainable benefits represents an instrument for practitioners planning the adoption of such techniques. In addition the findings may provide hints for companies and universities.}
}

@article{rayyan-727967201,
  title={Quality of service approaches in IoT: A systematic mapping},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={132},
  pages={186-203},
  author={White, Gary and Nallur, Vivek and Clarke, Siobhán},
  url={https://www.sciencedirect.com/science/article/pii/S016412121730105X},
  keywords={Systematic mapping, Monitoring, Quality model, Internet of things (IoT), Quality of service (QoS)},
  abstract={In an Internet of Things (IoT) environment, the existence of a huge number of heterogeneous devices, which are potentially resource-constrained and/or mobile has led to quality of service (QoS) concerns. Quality approaches have been proposed at various layers of the IoT architecture and take into consideration a number of different QoS factors. This paper evaluates the current state of the art of proposed QoS approaches in the IoT, specifically: (1) What layers of the IoT architecture have had the most research on QoS? (2) What quality factors do the quality approaches take into account when measuring performance? (3) What types of research have been conducted in this area? We have conducted a systematic mapping using a number of automated searches from the most relevant academic databases to address these questions. This mapping has identified a number of state of the art approaches which provides a good reference for researchers. The paper also identifies a number of gaps in the research literature at specific layers of the IoT architecture. It identifies which quality factors, research and contribution facets have been underutilised in the state of the art.}
}

@article{rayyan-727967202,
  title={5W+1H pattern: A perspective of systematic mapping studies and a case study on cloud software testing},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={116},
  pages={206-219},
  author={Jia, Changjiang and Cai, Yan and Yu, Yuen Tak and Tse, T H},
  url={https://www.sciencedirect.com/science/article/pii/S0164121215000370},
  keywords={Systematic mapping study, 5W+1H pattern, Cloud software testing, Software},
  abstract={A common type of study used by researchers to map out the landscape of a research topic is known as mapping study. Such a study typically begins with an exploratory search on the possible ideas of the research topic, which is often done in an unsystematic manner. Hence, the activity of formulating research questions in mapping studies is ill-defined, rendering it difficult for researchers who are new to the topic. There is a need to guide them kicking off a mapping study of an unfamiliar domain. This paper proposes a 5W+1H pattern to help investigators systematically examine a generic set of dimensions in a mapping study toward the formulation of research questions before identifying, reading, and analyzing sufficient articles of the topic. We have validated the feasibility of our proposal by conducting a case study of a mapping study on cloud software testing, that is, software testing for and on cloud computing platforms. The case study reveals that the 5W+1H pattern can lead investigators to define a set of systematic, generic, and complementary research questions, enabling them to kick off and expedite the mapping study process in a well-defined manner. We also share our experiences and lessons learned from our case study on the use of the 5W+1H pattern in mapping studies.}
}

@article{rayyan-727967203,
  title={Enable more frequent integration of software in industry projects},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={142},
  pages={223-236},
  author={Mårtensson, Torvald and Ståhl, Daniel and Bosch, Jan},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218300906},
  keywords={Continuous integration, Continuous delivery, Embedded systems, Large-scale, Software integration, Software},
  abstract={Based on interviews with 20 developers from two case study companies that develop large-scale software-intensive embedded systems, this paper presents twelve factors that affect how often developers commit software to the mainline. The twelve factors are grouped into four themes: “Activity planning and execution”, “System thinking”, “Speed” and “Confidence through test activities”. Based on the interview results and a literature study we present the EMFIS model, which allows companies to explicate a representation of the organization's current situation regarding continuous integration impediments, and visualizes what the organization must focus on in order to enable more frequent integration of software. The model is used to perform an assessment of the twelve factors, where the ratings from participants representing the developers are summarized separately from ratings from participants representing the enablers (responsible for processes, development tools, test environments etc.). The EMFIS model has been validated in workshops and interviews, which in total included 46 individuals in five case study companies. The model was well received during the validation, and was appreciated for its simplicity and its ability to show differences in rating between developers and enablers.}
}

@article{rayyan-727967204,
  title={From factory of the future to future of the factory: Integration approaches},
  year={2017},
  journal={IFAC-PapersOnLine},
  issn={2405-8963},
  volume={50},
  number={1},
  pages={11695-11700},
  author={de Asis Marti Nieto, Flor and Goepp., Virginie and Caillaud, Emmanuel},
  url={https://www.sciencedirect.com/science/article/pii/S2405896317322966},
  keywords={Factory of the Future, Information Systems, Integration, Manufacturing Systems, Production Systems},
  abstract={Nowadays, manufacturing systems transform themselves to become Factories of the Future (FoF) that is to say highly flexible, rapidly adaptable to external changes and aiming for a high degree of sustainability. This trend has generated several research streams that we analyse in this paper through a deep bibliographical review. This review leads us to conceptualize 3 main approaches: Computer Integrated Manufacturing, role of human work force and other integration approaches. A potential issue identified within the research is the interplay between computer-based and organization based approaches to manage the dynamic interactions among product, processes and production systems. The conclusions and orientations of future works are thus outlined to support the development of this co-evolution perspective for the successful transformation of Factories.}
}

@article{rayyan-727967205,
  title={A current study on the limitations of agile methods in industry using secure google forms},
  year={2016},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={78},
  pages={291-297},
  author={Agrawal, Ashish and Atiq, Mohd. Aurangzeb and Maurya, L S},
  url={https://www.sciencedirect.com/science/article/pii/S1877050916000582},
  keywords={Scrum, Agile, Survey, SDLC, Software Development},
  abstract={The Agile methods favours more communication, continuous integration, rapid delivery of software modules, iterative and incremental approach, but at the same time Agile software development has limitations like lack of upfront planning, lack of sufficient documentation, lack of predictability, etc.. Sometimes these limitations and so many methods make Agile software development more stressful. This work is about finding the current limitations and advantages of Agile software development. For finding the actual limitations beyond the literature, an online survey was conducted with the specified sample size of Agile experienced professionals, then the ANOVA test is applied to satisfy the hypothesis.}
}

@article{rayyan-727967206,
  title={What happened to my application? Helping end users comprehend evolution through variation management},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={103},
  pages={55-74},
  author={Kuttal, Sandeep Kaur and Sarma, Anita and Rothermel, Gregg and Wang, Zhendong},
  url={https://www.sciencedirect.com/science/article/pii/S0950584918301198},
  keywords={App inventor, End-user programming, End-user software engineering, Variation management},
  abstract={Context: Millions of end users are creating software applications. These end users typically do not have clear requirements in mind; instead, they debug their programs into existence and reuse their own or other persons' code. These behaviors often result in the creation of numerous variants of programs. Current end-user programming environments do not provide support for managing such variants. Objective: We wish to understand the variant creation behavior of end user programmers. Based on this understanding we wish to develop an automated system to help end user programmers efficiently manage variants. Method: We conducted an on-line survey to understand when and how end-user programmers create program variants and how they manage them. Our 124 survey respondents were recruited via email from among non-computer science majors who had taken at least one course in the computer science department at our university; the respondents were involved in the Engineering, Sciences, Arts, and Management fields. Based on the results of this survey we identified a set of design requirements for providing variation management support for end users. We implemented variation management support in App Inventor – a drag and drop programming environment for creating mobile applications. Our support, AppInventorHelper, is meant to help end-user programmers visualize the provenance of and relationships among variants. We conducted a think-aloud study with 10 participants to evaluate the usability of AppInventorHelper. The participants were selected on a first-come, first-served basis from those who responded to our recruitment email sent to list-servers. They were all end users majoring in electrical engineering, mechanical engineering, or physics. None had formal training in software engineering methods, but all had some experience with visual programming languages. Results: Our (user study) results indicate that AppInventorHelper can help end users navigate through variants and find variants that could be utilized cost-effectively as examples or actual code upon which to build new applications. For example, in one of our empirical studies end users explored variants of a paint application in order to find a variant that could easily be extended to incorporate a new feature. Conclusions: Our survey results show that end users do indeed reuse program variants and suggest that understanding the differences between variants is important. Further, end users prefer running code and looking at outputs, accessing source code and meta information such as filenames, referring to the creation and update dates of programs, and having information on the authors of code. When selecting variants users prefer to look at their major features such as correctness, similarity and authorship information. End users rely primarily on memory to track changes. They seldom make use of online or configuration management tools. Hence, integrated domain-specific variation management tools like AppInventorHelper can significantly help improve users' interactions with the system. A key contribution of our work is a set of design requirements for end-user programming environments that facilitate the management and understanding of the provenance of program variants.}
}

@article{rayyan-727967207,
  title={Integrating IT service management requirements into the organizational management system},
  year={2015},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={37},
  pages={80-91},
  author={Mesquida, Antoni-Lluís and Mas, Antonia},
  url={https://www.sciencedirect.com/science/article/pii/S0920548914000786},
  keywords={Integrated Management System (IMS), ISO 9001, ISO/IEC 20000, IT Service Management System (ITSMS), Quality Management System (QMS)},
  abstract={IT service provider organizations that have implemented a Quality Management System (QMS) according to ISO 9001 can take advantage of all the efforts made when implementing an IT Service Management System (ITSMS). In order to facilitate the integration of these two management systems, we analyze the existing relations between the requirements of the QMS and the ITSMS. Based on these results, we provide a new Integrated Management System (IMS) which widens the scope of the ISO 9001 QMS with the specific IT service management requirements of ISO/IEC 20000-1, and present a guide to support organizations in implementing this IMS.}
}

@article{rayyan-727967208,
  title={Chapter 11 - managing trade-offs in self-adaptive software architectures: A systematic mapping study},
  year={2017},
  journal={Managing trade-offs in adaptable software architectures},
  issn={978-0-12-802855-1},
  pages={249-297},
  author={Salama, M and Bahsoon, R and Bencomo, N and Mistrik, Ivan and Ali, Nour and Kazman, Rick and Grundy, John and Schmerl, Bradley},
  url={https://www.sciencedirect.com/science/article/pii/B9780128028551000113},
  publisher={Morgan Kaufmann},
  address={Boston},
  keywords={Systematic mapping study, Software architecture, Long-living software, Self-adaptation, Self-adaptive architecture, Self-awareness, Trade-offs management, Software},
  abstract={Self-adaptation has been driven by the need to achieve and maintain quality attributes in the face of the continuously changing requirements, as well as the uncertain demand during run-time. Designing architectures that exhibit a good trade-off between multiple quality attributes is challenging, especially in the case of self-adaptive software systems, due to the complexity, heterogeneity, and ultra-large scale of modern software systems. This challenge increases with the dynamic, open, and uncertain operating environment, as well as the need for complying to environmental, regulatory, and sustainability requirements; such as energy consumption regulations. This study aims at analyzing the research landscape that have explicitly addressed trade-offs management for self-adaptive software architectures, to obtain a comprehensive overview on the current state of research on this specialized area. A systematic mapping study was conducted to identify and analyze research works related to analyzing and managing trade-offs to support decision-making for self-adaptive software architectures. Twenty primary studies were evidently selected and analyzed to classify software paradigms, quality attributes considered, and the self-* properties that drive trade-offs management. The results show constant interest in finding solutions for trade-offs management at design-time and run-time, as well as the success of research initiatives even when new research challenges are found. The findings call for foundational framework to analyze and manage trade-offs for self-adaptive software architectures that can explicitly consider specific multiple quality attributes, the run-time dynamics, the uncertainty of the environment and the complex challenges of modern, ultra-large scale systems in particular given software paradigms.}
}

@article{rayyan-727967209,
  title={Adoption of OSS components: A goal-oriented approach},
  year={2015},
  journal={Data & Knowledge Engineering},
  issn={0169-023X},
  volume={99},
  pages={17-38},
  author={López, Lidia and Costal, Dolors and Ayala, Claudia P and Franch, Xavier and Annosi, Maria Carmela and Glott, Ruediger and Haaland, Kirsten},
  url={https://www.sciencedirect.com/science/article/pii/S0169023X15000403},
  keywords={Conceptual modelling, i-star, Ontologies, Open Source Software, OSS adoption},
  abstract={Open Source Software (OSS) has become a strategic asset for a number of reasons, such as short time-to-market software delivery, reduced development and maintenance costs, and its customization capabilities. Therefore, organizations are increasingly becoming OSS adopters, either as a result of a strategic decision or because it is almost unavoidable nowadays, given the fact that most commercial software also relies at some extent in OSS infrastructure. The way in which organizations adopt OSS affects and shapes their businesses. Therefore, knowing the impact of different OSS adoption strategies in the context of an organization may help improving the processes undertaken inside this organization and ultimately pave the road to strategic moves. In this paper, we propose to model OSS adoption strategies using a goal-oriented notation, in which different actors state their objectives and dependencies on each other. These models describe the consequences of adopting one such strategy or another: which are the strategic and operational goals that are supported, which are the resources that emerge, etc. The models rely on an OSS ontology, built upon a systematic literature review, which comprises the activities and resources that characterize these strategies. Different OSS adoption strategy models arrange these ontology elements in diverse ways. In order to assess which is the OSS adoption strategy that better fits the organization needs, the notion of model coverage is introduced, which allows to measure the degree of concordance among every strategy with the model of the organization by comparing the respective models. The approach is illustrated with an example of application in a big telecommunications company.}
}

@article{rayyan-727967210,
  title={IFC editorial board},
  year={2011},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={53},
  number={7},
  pages={IFC},
  url={https://www.sciencedirect.com/science/article/pii/S0950584911000899}
}

@article{rayyan-727967211,
  title={The smart circular economy: A digital-enabled circular strategies framework for manufacturing companies},
  year={2020},
  journal={Journal of Business Research},
  issn={0148-2963},
  volume={120},
  pages={241-261},
  author={Kristoffersen, Eivind and Blomsma, Fenna and Mikalef, Patrick and Li, Jingyue},
  url={https://www.sciencedirect.com/science/article/pii/S0148296320304987},
  keywords={Big data analytics, Sustainability, Circular economy, Digital circular economy, Digitalization, Industry 4.0},
  abstract={Digital technologies (DTs), such as the Internet of Things (IoT), big data, and data analytics, are considered essential enablers of the circular economy (CE). However, as both CE and DTs are emerging fields, there exists little systematic guidance on how DTs can be applied to capture the full potential of circular strategies for improving resource efficiency and productivity. Furthermore, there is little insight into the supporting business analytics (BA) capabilities required to accomplish this. To address this gap, this paper conducts a theory- and practice-based review, resulting in the Smart CE framework that supports translating the circular strategies central to the goals of manufacturing companies in contributing the United Nation's (UN) 12th Sustainable Development Goal, that is, “sustainable consumption and production,” into the BA requirements of DTs. Both scholars and practitioners may find the framework useful to (1) create a common language for aligning activities across the boundaries of disciplines such as information systems and the CE body of knowledge, and (2) identify the gap between the current and entailed BA requirements and identify the strategic initiatives needed to close it. Additionally, the framework is used to organize a database of case examples to identify some best practices related to specific smart circular strategies.}
}

@article{rayyan-727967212,
  title={A systematic mapping study on technical debt and its management},
  year={2015},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={101},
  pages={193-220},
  author={Li, Zengyang and Avgeriou, Paris and Liang, Peng},
  url={https://www.sciencedirect.com/science/article/pii/S0164121214002854},
  keywords={Systematic mapping study, Technical debt, Technical debt management},
  abstract={Context Technical debt (TD) is a metaphor reflecting technical compromises that can yield short-term benefit but may hurt the long-term health of a software system. Objective This work aims at collecting studies on TD and TD management (TDM), and making a classification and thematic analysis on these studies, to obtain a comprehensive understanding on the TD concept and an overview on the current state of research on TDM. Method A systematic mapping study was performed to identify and analyze research on TD and its management, covering publications between 1992 and 2013. Results Ninety-four studies were finally selected. TD was classified into 10 types, 8 TDM activities were identified, and 29 tools for TDM were collected. Conclusions The term “debt” has been used in different ways by different people, which leads to ambiguous interpretation of the term. Code-related TD and its management have gained the most attention. There is a need for more empirical studies with high-quality evidence on the whole TDM process and on the application of specific TDM approaches in industrial settings. Moreover, dedicated TDM tools are needed for managing various types of TD in the whole TDM process.}
}

@article{rayyan-727967213,
  title={A literature review on obsolescence management in COTS-centric cyber physical systems},
  year={2019},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={153},
  pages={135-145},
  author={Alelyani, Turki and Michel, Ronald and Yang, Ye and Wade, Jon and Verma, Dinesh and Törngren, Martin},
  url={https://www.sciencedirect.com/science/article/pii/S1877050919307239},
  keywords={commercial off-the-shelf (COTS), cyber physical system, electronics, hardware, obsolescence, software, system},
  abstract={Commercial off-the-shelf (COTS)-centric cyber physical systems often contain software and hardware elements with life-spans shorter than the systems' intended life-span. Various studies have examined hardware obsolescence, although in most systems, software costs contribute as much, or more, to the total life cycle costs than hardware. The aim of our research effort is to explore, synthesize, and compile past research efforts on obsolescence in the context of COTS-based systems, and propose new ways to overcome related issues. This research effort suggests the need for systematic perspectives to streamline potentially overbearing acquisition processes while focusing on core critical aspects affecting systems sustainment and cost. Significant life cycle costs associated with obsolescence mitigation approaches, therefore, programmatic strategic planning should be adapted to include the context of obsolescence with the objective to improve the efficiency of new COTS-intensive CPS systems with enduring perspectives. The study reveals opportunities and challenges for obsolescence in COTS-based CPSs.}
}

@article{rayyan-727967214,
  title={Empirical studies concerning the maintenance of UML diagrams and their use in the maintenance of code: A systematic mapping study},
  year={2013},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={55},
  number={7},
  pages={1119-1142},
  author={Fernández-Sáez, Ana M and Genero, Marcela and Chaudron, Michel R V},
  url={https://www.sciencedirect.com/science/article/pii/S0950584912002418},
  keywords={Systematic literature review, Systematic mapping study, UML, Software maintenance, Empirical studies},
  abstract={Context The Unified Modelling Language (UML) has, after ten years, become established as the de facto standard for the modelling of object-oriented software systems. It is therefore relevant to investigate whether its use is important as regards the costs involved in its implantation in industry being worthwhile. Method We have carried out a systematic mapping study to collect the empirical studies published in order to discover “What is the current existing empirical evidence with regard to the use of UML diagrams in source code maintenance and the maintenance of the UML diagrams themselves? Results We found 38 papers, which contained 63 experiments and 3 case studies. Conclusion Although there is common belief that the use of UML is beneficial for source code maintenance, since the quality of the modifications is greater when UML diagrams are available, only 3 papers concerning this issue have been published. Most research (60 empirical studies) concerns the maintainability and comprehensibility of the UML diagrams themselves which form part of the system's documentation, since it is assumed that they may influence source code maintainability, although this has not been empirically validated. Moreover, the generalizability of the majority of the experiments is questionable given the material, tasks and subjects used. There is thus a need for more experiments and case studies to be performed in industrial contexts, i.e., with real systems and using maintenance tasks conducted by practitioners under real conditions that truly show the utility of UML diagrams in maintaining code, and that the fact that a diagram is more comprehensible or modifiable influences the maintainability of the code itself. This utility should also be studied from the viewpoint of cost and productivity, and the consistent and simultaneous maintenance of diagrams and code must also be considered in future empirical studies.}
}

@article{rayyan-727967215,
  title={DevOps and software quality: A systematic mapping},
  year={2020},
  journal={Computer Science Review},
  issn={1574-0137},
  volume={38},
  pages={100308},
  author={Mishra, Alok and Otaiwi, Ziadoon},
  url={https://www.sciencedirect.com/science/article/pii/S1574013720304081},
  keywords={Software, Systematic mapping, Automation, Measurement, Software quality, DevOps, Development, Operations},
  abstract={Quality pressure is one of the factors affecting processes for software development in its various stages. DevOps is one of the proposed solutions to such pressure. The primary focus of DevOps is to increase the deployment speed, frequency and quality. DevOps is a mixture of different developments and operations to its multitudinous ramifications in software development industries, DevOps have attracted the interest of many researchers. There are considerable literature surveys on this critical innovation in software development, yet, little attention has been given to DevOps impact on software quality. This research is aimed at analyzing the implications of DevOps features on software quality. DevOps can also be referred to a change in organization cultures aimed at removal of gaps between the development and operations of an organization. The adoption of DevOps in an organization provides many benefits including quality but also brings challenges to an organization. This study presents systematic mapping of the impact of DevOps on software quality. The results of this study provide a better understanding of DevOps on software quality for both professionals and researchers working in this area. The study shows research was mainly focused in automation, culture, continuous delivery, fast feedback of DevOps. There is need of further research in many areas of DevOps (for instance: measurement, development of metrics of different stages to assess its performance, culture, practices toward ensuring quality assurance, and quality factors such as usability, efficiency, software maintainability and portability).}
}

@article{rayyan-727967216,
  title={The impacts of agile and lean practices on project constraints: A tertiary study},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={119},
  pages={162-183},
  author={Nurdiani, Indira and Börstler, Jürgen and Fricker, Samuel A},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216300863},
  keywords={Tertiary study, Agile software development, Lean software development, Project constraints},
  abstract={The growing interest in Agile and Lean software development is reflected in the increasing number of secondary studies on the benefits and limitations of Agile and Lean processes and practices. The aim of this tertiary study is to consolidate empirical evidence regarding Agile and Lean practices and their respective impacts on project constraints as defined in the Project Management Body of Knowledge (PMBOK): scope, quality, schedule, budget, resources, communication, and risk. In this tertiary study, 13 secondary studies were included for detailed analysis. Given the heterogeneity of the data, we were unable to perform a rigorous synthesis. Instead, we mapped the identified Agile and Lean practices, and their impacts on the project constraints described in PMBOK. From 13 secondary studies, we identified 13 Agile and Lean practices. Test-Driven Development (TDD) is studied in ten secondary studies, meanwhile other practices are studied in only one or two secondary studies. This tertiary study provides a consolidated view of the impacts of Agile and Lean practices. The result of this tertiary study indicates that TDD has a positive impact on external quality. However, due to insufficient data or contradictory results, we were unable to make inferences on other Agile and Lean practices. Implications for research and practice are further discussed in the paper.}
}

@article{rayyan-727967217,
  title={Software product line testing – A systematic mapping study},
  year={2011},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={53},
  number={1},
  pages={2-13},
  author={Engström, Emelie and Runeson, Per},
  url={https://www.sciencedirect.com/science/article/pii/S0950584910001709},
  keywords={Systematic literature review, Systematic mapping, Testing, Software product line testing, Software},
  abstract={Context Software product lines (SPL) are used in industry to achieve more efficient software development. However, the testing side of SPL is underdeveloped. Objective This study aims at surveying existing research on SPL testing in order to identify useful approaches and needs for future research. Method A systematic mapping study is launched to find as much literature as possible, and the 64 papers found are classified with respect to focus, research type and contribution type. Results A majority of the papers are of proposal research types (64%). System testing is the largest group with respect to research focus (40%), followed by management (23%). Method contributions are in majority. Conclusions More validation and evaluation research is needed to provide a better foundation for SPL testing.}
}

@article{rayyan-727967218,
  title={Software test maturity assessment and test process improvement: A multivocal literature review},
  year={2017},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={85},
  pages={16-42},
  author={Garousi, Vahid and Felderer, Michael and Hacaloğlu, Tuna},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917300162},
  keywords={Software testing, Systematic literature review, Multivocal literature review, Test management, Test maturity, Test process, Test process assessment, Test process improvement, Software},
  abstract={Context Software testing practices and processes in many companies are far from being mature and are usually conducted in ad-hoc fashions. Such immature practices lead to various negative outcomes, e.g., ineffectiveness of testing practices in detecting all the defects, and cost and schedule overruns of testing activities. To conduct test maturity assessment (TMA) and test process improvement (TPI) in a systematic manner, various TMA/TPI models and approaches have been proposed. Objective It is important to identify the state-of-the-art and the –practice in this area to consolidate the list of all various test maturity models proposed by practitioners and researchers, the drivers of TMA/TPI, the associated challenges and the benefits and results of TMA/TPI. Our article aims to benefit the readers (both practitioners and researchers) by providing the most comprehensive survey of the area, to this date, in assessing and improving the maturity of test processes. Method To achieve the above objective, we have performed a Multivocal Literature Review (MLR) study to find out what we know about TMA/TPI. A MLR is a form of a Systematic Literature Review (SLR) which includes the grey literature (e.g., blog posts and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). We searched the academic literature using the Google Scholar and the grey literature using the regular Google search engine. Results Our MLR and its results are based on 181 sources, 51 (29%) of which were grey literature and 130 (71%) were formally published sources. By summarizing what we know about TMA/TPI, our review identified 58 different test maturity models and a large number of sources with varying degrees of empirical evidence on this topic. We also conducted qualitative analysis (coding) to synthesize the drivers, challenges and benefits of TMA/TPI from the primary sources. Conclusion We show that current maturity models and techniques in TMA/TPI provides reasonable advice for industry and the research community. We suggest directions for follow-up work, e.g., using the findings of this MLR in industry-academia collaborative projects and empirical evaluation of models and techniques in the area of TMA/TPI as reported in this article.}
}

@article{rayyan-727967219,
  title={Reverse engineering database queries from examples: State-of-the-art, challenges, and research opportunities},
  year={2019},
  journal={Information Systems},
  issn={0306-4379},
  volume={83},
  pages={89-100},
  author={Martins, Denis Mayr Lima},
  url={https://www.sciencedirect.com/science/article/pii/S0306437918300978},
  keywords={Databases, Query discovery, Query learning, Query synthesis, Reverse engineering database queries},
  abstract={With the popularization of data access and usage, an increasing number of users without expert knowledge of databases is required to perform data interactions. Often, these users face the challenges of writing and reformulating database queries, which consume a considerable amount of time and frequently yield unsatisfactory results. To facilitate this human–database interaction, researchers have investigated the Query By Example (QBE) paradigm in which database queries are (semi) automatically discovered from data examples given by users. This paradigm allows non-database experts to formulate queries without relying on complex query languages. In this context, this work aims to present a systematic review of the recent developments, open challenges, and research opportunities of the QBE reported in the literature. This work also describes strategies employed to leverage efficient example acquisition and query reverse engineering. The obtained results show that recent research developments have focused on enhancing the expressiveness of produced queries, minimizing user interaction, and enabling efficient query learning in the context of data retrieval, exploration, integration, and analytics. Our findings indicate that future research should concentrate efforts to provide innovative solutions to the challenges of improving controllability and transparency, considering diverse user preferences in the processes of learning personalized queries, ensuring data quality, and improving the support of additional SQL features and operators.}
}

@article{rayyan-727967220,
  title={On dynamic consensus processes in group decision making problems},
  year={2018},
  journal={Information Sciences},
  issn={0020-0255},
  volume={459},
  pages={20-35},
  author={Pérez, I J and Cabrerizo, F J and Alonso, S and Dong, Y C and Chiclana, F and Herrera-Viedma, E},
  url={https://www.sciencedirect.com/science/article/pii/S0020025518303724},
  keywords={Group decision making, Adaptive consensus models, Consensus process, Dynamic decision support systems, Multi period decision making, Decision Making, Group Processes},
  abstract={Consensus in group decision making requires discussion and deliberation between the group members with the aim to reach a decision that reflects the opinions of every group member in order for it to be acceptable by everyone. Traditionally, the consensus reaching problem is theoretically modelled as a multi stage negotiation process, i.e. an iterative process with a number of negotiation rounds, which ends when the consensus level achieved reaches a minimum required threshold value. In real world decision situations, both the consensus process environment and specific parameters of the theoretical model can change during the negotiation period. Consequently, there is a need for developing dynamic consensus process models to represent effectively and realistically the dynamic nature of the group decision making problem. Indeed, over the past few years, static consensus models have given way to new dynamic approaches in order to manage parameter variability or to adapt to environment changes. This paper presents a systematic literature review on the recent evolution of consensus reaching models under dynamic environments and critically analyse their advantages and limitations.}
}

@article{rayyan-727967221,
  title={Factors affecting the results of food preference tests in cats},
  year={2020},
  journal={Research in Veterinary Science},
  issn={0034-5288},
  volume={130},
  pages={247-254},
  author={Pires, Kássia Amariz and Miltenburg, Tânia Zóia and Miranda, Pamela Dieckow and Abade, Cristiane Caroline and Janeiro, Vanderly and Menolli, André Luis Andrade and Mizubuti, Ivone Yurika and Ribeiro, Leonir Bueno and Vasconcellos, Ricardo Souza},
  url={https://www.sciencedirect.com/science/article/pii/S0034528819311178},
  keywords={Domestic felines, Palatability test, Pet food, Taste preference, Two-bowl test, Cats},
  abstract={The aim of this study was to (i) gain an overview of the protocols of food preference tests in cats through a systematic review, (ii) assess the effects of test duration, time of day, and sex, and (iii) propose a statistical approach based on power analysis to determine sample size and analyze the results. The manuscripts included in this review had marked variations in the number of days (2–56), sample size (9–60 cats), feeding times (2.5–1440 min), and number of meals per day (1–2) during the test. Additionally to the literature review, three palatability tests (lasting 10 days each) were conducted with 40 cats (22 males and 18 females, 1.8 ± 0.16 years, 3.73 ± 0.90 kg) to assess the effects of test duration, time of day, and gender on the results. From the second day of the test, the sensitivity of the results was higher, because on the first day the results in one of the tests differed from the others (p = .0058). There was no difference (p ¿ .05) between times of day (morning vs afternoon) or gender (males vs females) on the results of the feed intake ratio. For a SD of 0.20, p ¡ .05, and delta of 0.10, the minimum number of cats for two-bowl assays is 23 (test power higher than 0.75).The sample size and test duration are critical factors in the decision making by the investigators about the design of food preference tests in cats. The use of a power test is recommended upon planning a food preference test protocol in cats.}
}

@article{rayyan-727967222,
  title={A framework to support selection of cloud providers based on security and privacy requirements},
  year={2013},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={86},
  number={9},
  pages={2276-2293},
  author={Mouratidis, Haralambos and Islam, Shareeful and Kalloniatis, Christos and Gritzalis, Stefanos},
  url={https://www.sciencedirect.com/science/article/pii/S0164121213000575},
  keywords={Cloud computing, Privacy, Secure software engineering},
  abstract={Cloud computing is an evolving paradigm that is radically changing the way humans store, share and access their digital files. Despite the many benefits, such as the introduction of a rapid elastic resource pool, and on-demand service, the paradigm also creates challenges for both users and providers. In particular, there are issues related to security and privacy, such as unauthorised access, loss of privacy, data replication and regulatory violation that require adequate attention. Nevertheless, and despite the recent research interest in developing software engineering techniques to support systems based on the cloud, the literature fails to provide a systematic and structured approach that enables software engineers to identify security and privacy requirements and select a suitable cloud service provider based on such requirements. This paper presents a novel framework that fills this gap. Our framework incorporates a modelling language and it provides a structured process that supports elicitation of security and privacy requirements and the selection of a cloud provider based on the satisfiability of the service provider to the relevant security and privacy requirements. To illustrate our work, we present results from a real case study.}
}

@article{rayyan-727967223,
  title={A review of machine learning algorithms for identification and classification of non-functional requirements},
  year={2019},
  journal={Expert Systems with Applications: X},
  issn={2590-1885},
  volume={1},
  pages={100001},
  author={Binkhonain, Manal and Zhao, Liping},
  url={https://www.sciencedirect.com/science/article/pii/S2590188519300010},
  keywords={Machine learning, Requirements engineering, Non-functional requirements, Requirements documents, Requirements identification Requirements classification, Algorithms},
  abstract={Context Recent developments in requirements engineering (RE) methods have seen a surge in using machine-learning (ML) algorithms to solve some difficult RE problems. One such problem is identification and classification of non-functional requirements (NFRs) in requirements documents. ML-based approaches to this problem have shown to produce promising results, better than those produced by traditional natural language processing (NLP) approaches. Yet, a systematic understanding of these ML approaches is still lacking. Method This article reports on a systematic review of 24 ML-based approaches for identifying and classifying NFRs. Directed by three research questions, this article aims to understand what ML algorithms are used in these approaches, how these algorithms work and how they are evaluated. Results (1) 16 different ML algorithms are found in these approaches; of which supervised learning algorithms are most popular. (2) All 24 approaches have followed a standard process in identifying and classifying NFRs. (3) Precision and recall are the most used matrices to measure the performance of these approaches. Finding The review finds that while ML-based approaches have the potential in the classification and identification of NFRs, they face some open challenges that will affect their performance and practical application. Impact The review calls for the close collaboration between RE and ML researchers, to address open challenges facing the development of real-world ML systems. Significance The use of ML in RE opens up exciting opportunities to develop novel expert and intelligent systems to support RE tasks and processes. This implies that RE is being transformed into an application of modern expert systems.}
}

@article{rayyan-727967224,
  title={A systematic examination of knowledge loss in open source software projects},
  year={2019},
  journal={International Journal of Information Management},
  issn={0268-4012},
  volume={46},
  pages={104-123},
  author={Rashid, Mehvish and Clarke, Paul M and O'Connor, Rory V},
  url={https://www.sciencedirect.com/science/article/pii/S0268401217310095},
  keywords={Knowledge Management, Open Source Software, Contributor turnover, Knowledge loss, Knowledge loss impact, Knowledge retention, Software},
  abstract={Context Open Source Software (OSS) development is a knowledge focused activity which relies heavily on contributors who can be volunteers or paid workers and are geographically distributed. While working on OSS projects contributors acquire project related individualistic knowledge and gain experience and skills, which often remains unshared with others and is usually lost once contributors leave a project. All software development organisations face the problem of knowledge loss as employees leave, but this situation is exasperated in OSS projects where most contributors are volunteers with largely unpredictable engagement durations. Contributor turnover is inevitable due to the transient nature of OSS project workforces causing knowledge loss, which threatens the overall sustainability of OSS projects and impacts negatively on software quality and contributor productivity. Objective The objective of this work is to deeply and systematically investigate the phenomenon of knowledge loss due to contributor turnover in OSS projects as presented in the state-of-the-art literature and to synthesise the information presented on the topic. Furthermore, based on the learning arising from our investigation it is our intention to identify mechanisms to reduce the overall effects of knowledge loss in OSS projects. Methodology We use the snowballing methodology to identify the relevant literature on knowledge loss due to contributor turnover in OSS projects. This robust methodology for a literature review includes research question, search strategy, inclusion, exclusion, quality criteria, and data synthesis. The search strategy, and inclusion, exclusions and quality criteria are applied as a part of snowballing procedure. Snowballing is considered an efficient and reliable way to conduct a systematic literature review, providing a robust alternative to mechanically searching individual databases for given topics. Result Knowledge sharing in OSS projects is abundant but there is no evidence of a formal strategy or practice to manage knowledge. Due to the dynamic and diverse nature of OSS projects, knowledge management is considered a challenging task and there is a need for a proactive mechanism to share knowledge in the OSS community for knowledge to be reused in the future by the OSS project contributors. From the collection of papers found using snowballing, we consolidated various themes on knowledge loss due to contributor turnover in OSS projects and identified 11 impacts due to knowledge loss in OSS projects, and 10 mitigations to manage with knowledge loss in OSS projects. Conclusion In this paper, we propose future research directions to investigate integration of proactive knowledge retention practices with the existing OSS practices to reduce the current knowledge loss problem. We suggest that there is insufficient attention paid to KM in general in OSS, in particular there would appear to an absence of proactive measures to reduce the potential impact of knowledge loss. We also propose the need for a KM evaluation metric in OSS projects, similar to the ones that evaluate health of online communities, which should help to inform potential consumers of the OSS of the KM status on a project, something that is not existent today.}
}

@article{rayyan-727967225,
  title={Approaches to promote product quality within software process improvement initiatives: A mapping study},
  year={2015},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={103},
  pages={150-166},
  author={García-Mireles, Gabriel Alberto and Moraga, M Ángeles and García, Félix and Piattini, Mario},
  url={https://www.sciencedirect.com/science/article/pii/S0164121215000369},
  keywords={Systematic mapping study, Software process improvement, Software product quality, Software},
  abstract={Enhancing product quality might be a main goal of a software process improvement initiative (SPI). Quality is, however, a complex concept, and experts recommend identifying relevant product quality characteristics to satisfy users/customers' needs. There is thus a need to understand how SPI initiatives contribute to the improvement of software product quality characteristics. This paper aims to provide an overview of an up-to-date state-of-the-art regarding initiatives that focus on promoting product quality improvement by applying SPI approaches. This goal was achieved by conducting a systematic mapping study, as a result of which we identified 74 primary papers including both theoretical (75.7%) and empirical (24.3%) papers. The main product quality characteristics addressed are security, usability and reliability. Security-related process models, on the other hand, are those most cited (53%). The empirical papers suggest that traditional process reference models, such as CMM, CMMI or ISO 9001, moderately increase product quality characteristics, these principally being maintainability and reliability. However, there is a need for more empirical research to evaluate the impact of SPI initiatives on software product quality by considering contextual factors. SPI initiatives should be more driven by performance goals related to product quality characteristics.}
}

@article{rayyan-727967226,
  title={Software quality trade-offs: A systematic map},
  year={2012},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={54},
  number={7},
  pages={651-662},
  author={Barney, Sebastian and Petersen, Kai and Svahnberg, Mikael and Aurum, Aybüke and Barney, Hamish},
  url={https://www.sciencedirect.com/science/article/pii/S0950584912000195},
  keywords={Software engineering, Software quality, Trade-off approaches, Software},
  abstract={Background Software quality is complex with over investment, under investment and the interplay between aspects often being overlooked as many researchers aim to advance individual aspects of software quality. Aim This paper aims to provide a consolidated overview the literature that addresses trade-offs between aspects of software product quality. Method A systematic literature map is employed to provide an overview of software quality trade-off literature in general. Specific analysis is also done of empirical literature addressing the topic. Results The results show a wide range of solution proposals being considered. However, there is insufficient empirical evidence to adequately evaluate and compare these proposals. Further a very large vocabulary has been found to describe software quality. Conclusion Greater empirical research is required to sufficiently evaluate and compare the wide range of solution proposals. This will allow researchers to focus on the proposals showing greater signs of success and better support industrial practitioners.}
}

@article{rayyan-727967227,
  title={Quality of service approaches in cloud computing: A systematic mapping study},
  year={2015},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={101},
  pages={159-179},
  author={Abdelmaboud, Abdelzahir and Jawawi, Dayang N A and Ghani, Imran and Elsafi, Abubakar and Kitchenham, Barbara},
  url={https://www.sciencedirect.com/science/article/pii/S0164121214002830},
  keywords={Systematic mapping study, Quality of service, Cloud services},
  abstract={Context: Cloud computing is a new computing technology that provides services to consumers and businesses. Due to the increasing use of these services, the quality of service (QoS) of cloud computing has become an important and essential issue since there are many open challenges which need to be addressed related to trust in cloud services. Many research issues have been proposed in QoS approaches in the cloud computing area. Objective: The aim of this study is to survey current research on QoS approaches in cloud computing in order to identify where more emphasis should be placed in both current and future research directions. Method: A systematic mapping study was performed to find the related literature, and 67 articles were selected as primary studies that are classified in relation to the focus, research type and contribution type. Result: The majority of the articles are of the validation research type (64%). Infrastructure as a service (48%) was the largest research focus area, followed by software as a service (36%). The majority of contributions concerned methods (48%), followed by models (32%). Conclusion: The results of this study confirm that QoS approaches in cloud computing have become an important topic in the cloud computing area in recent years and there remain open challenges and gaps which require future research exploration. In particular, tools, metrics and evaluation research are needed in order to provide useful and trustworthy cloud computing services that deliver appropriate QoS.}
}

@article{rayyan-727967228,
  title={Evaluating and selecting software packages: A review},
  year={2009},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={51},
  number={3},
  pages={555-563},
  author={Jadhav, Anil S and Sonar, Rajendra M},
  url={https://www.sciencedirect.com/science/article/pii/S0950584908001262},
  keywords={Evaluation criteria, Software evaluation, Software selection, Software selection tools, Software},
  abstract={Evaluating and selecting software packages that meet an organization's requirements is a difficult software engineering process. Selection of a wrong software package can turn out to be costly and adversely affect business processes. The aim of this paper is to provide a basis to improve the process of evaluation and selection of the software packages. This paper reports a systematic review of papers published in journals and conference proceedings. The review investigates methodologies for selecting software packages, software evaluation techniques, software evaluation criteria, and systems that support decision makers in evaluating software packages. The key findings of the review are: (1) analytic hierarchy process has been widely used for evaluation of the software packages, (2) there is lack of a common list of generic software evaluation criteria and its meaning, and (3) there is need to develop a framework comprising of software selection methodology, evaluation technique, evaluation criteria, and system to assist decision makers in software selection.}
}

@article{rayyan-727967229,
  title={Knowledge management support for enterprise resource planning implementation},
  year={2015},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={72},
  pages={613-621},
  author={Saide and Mahendrawathi, E R},
  url={https://www.sciencedirect.com/science/article/pii/S1877050915036315},
  keywords={Enterprise Resource Planning (ERP), Knowledge Management (KM), Knowledge Transfer, Knowledge Worker, SECI Model},
  abstract={This study addresses the issues of Enterprise Resource Planning (ERP), Knowledge Management (KM) and SECI model (socialization, externalization, combination, internalization). Various research have highlighted the importance of knowledge of ERP users for successful ERP implementation, however a major obstacle from the perspective of integration or knowledge transfer cycle still exists. The main problem in ERP implementation is the difficult integration of tacit (embedded) and explicit knowledge cause most of this knowledge are embedded in ERP external parties (such as consultants, vendors, suppliers, supervisors, experts, and other working partners). The focus of this study is to propose process for transfer knowledge from external organizations into organizations based on the model of SECI. To note that this paper is not to modify the basic model of SECI, but SECI model to making as a function of mediator between the external and internal ERP system implementation in company. The authors used a systematic literature review approach, starts with literature review, problems identification, selection process, assess, synthesize and write down the ideas proposed, and then make conclusions. Finally, the output of this research is a new model (schematic and technical) of the process and transfer knowledge order to maintain and re-use assets from external knowledge obtained during the pre to post ERP implementation to be used jointly by the company.}
}

@article{rayyan-727967230,
  title={Knowledge management initiatives in software testing: A mapping study},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={57},
  pages={378-391},
  author={de Souza, Érica Ferreira and de Almeida Falbo, Ricardo and Vijaykumar, Nandamudi L},
  url={https://www.sciencedirect.com/science/article/pii/S0950584914001335},
  keywords={Software testing, Mapping study, Knowledge management, Software},
  abstract={Context Software testing is a knowledge intensive process, and, thus, Knowledge Management (KM) principles and techniques should be applied to manage software testing knowledge. Objective This study conducts a survey on existing research on KM initiatives in software testing, in order to identify the state of the art in the area as well as the future research. Aspects such as purposes, types of knowledge, technologies and research type are investigated. Method The mapping study was performed by searching seven electronic databases. We considered studies published until December 2013. The initial resulting set was comprised of 562 studies. From this set, a total of 13 studies were selected. For these 13, we performed snowballing and direct search to publications of researchers and research groups that accomplished these studies. Results From the mapping study, we identified 15 studies addressing KM initiatives in software testing that have been reviewed in order to extract relevant information on a set of research questions. Conclusions Although only a few studies were found that addressed KM initiatives in software testing, the mapping shows an increasing interest in the topic in the recent years. Reuse of test cases is the perspective that has received more attention. From the KM point of view, most of the studies discuss aspects related to providing automated support for managing testing knowledge by means of a KM system. Moreover, as a main conclusion, the results show that KM is pointed out as an important strategy for increasing test effectiveness, as well as for improving the selection and application of suited techniques, methods and test cases. On the other hand, inadequacy of existing KM systems appears as the most cited problem related to applying KM in software testing.}
}

@article{rayyan-727967231,
  title={Security patterns: A systematic mapping study},
  year={2020},
  journal={Journal of Computer Languages},
  issn={2590-1184},
  volume={56},
  pages={100938},
  author={Jafari, Abbas Javan and Rasoolzadegan, Abbas},
  url={https://www.sciencedirect.com/science/article/pii/S2590118419300632},
  keywords={Systematic review, Mapping study, Secure software development, Security patterns},
  abstract={Security patterns are a well-established means to encapsulate and communicate proven security solutions and introduce security into the development process. Our objective is to explore the research efforts on security patterns and discuss the current state of the art, which will serve as a guideline for interested researchers, practitioners, and teachers. We have conducted a systematic mapping study of relevant literature from 1997 until the end of 2017 and identified 403 relevant papers, 274 of which were selected for analysis based on quality criteria. This study derives a customized research strategy from established systematic approaches in the literature. The first 3 research questions address the demographics of security pattern research such as topic classification, trends, and distribution between academia and industry, along with prominent researchers and venues. The next 9 research questions focus on more in-depth analyses such as pattern presentation notations and classification criteria, pattern evaluation techniques, and pattern usage environments. We observe that security pattern research is an active and growing field and the patterns are increasingly being used to improve software development approaches. Pattern evaluation is currently the least explored topic by researchers and there is a lack of empirical studies in the field.}
}

@article{rayyan-727967232,
  title={The longitudinal, chronological case study research strategy: A definition, and an example from IBM Hursley Park},
  year={2011},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={53},
  number={7},
  pages={730-746},
  author={Rainer, Austen},
  url={https://www.sciencedirect.com/science/article/pii/S0950584911000048},
  keywords={Chronology, Deadline effect, Longitudinal case study, Qualitative data, Software project, Theory development},
  abstract={Context There is surprisingly little empirical software engineering research (ESER) that has analysed and reported the rich, fine-grained behaviour of phenomena over time using qualitative and quantitative data. The ESER community also increasingly recognises the need to develop theories of software engineering phenomena e.g. theories of the actual behaviour of software projects at the level of the project and over time. Objective To examine the use of the longitudinal, chronological case study (LCCS) as a research strategy for investigating the rich, fine-grained behaviour of phenomena over time using qualitative and quantitative data. Method Review the methodological literature on longitudinal case study. Define the LCCS and demonstrate the development and application of the LCCS research strategy to the investigation of Project C, a software development project at IBM Hursley Park. Use the study to consider prospects for LCCSs, and to make progress on a theory of software project behaviour. Results LCCSs appear to provide insights that are hard to achieve using existing research strategies, such as the survey study. The LCCS strategy has basic requirements that data is time-indexed, relatively fine-grained and collected contemporaneous to the events to which the data refer. Preliminary progress is made on a theory of software project behaviour. Conclusion LCCS appears well suited to analysing and reporting rich, fine-grained behaviour of phenomena over time.}
}

@article{rayyan-727967233,
  title={A mapping study on design-time quality attributes and metrics},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={127},
  pages={52-77},
  author={Arvanitou, Elvira Maria and Ampatzoglou, Apostolos and Chatzigeorgiou, Alexander and Galster, Matthias and Avgeriou, Paris},
  url={https://www.sciencedirect.com/science/article/pii/S016412121730016X},
  keywords={Measurement, Mapping study, Software quality, Design-time quality attributes, Metronidazole},
  abstract={Developing a plan for monitoring software quality is a non-trivial task, in the sense that it requires: (a) the selection of relevant quality attributes, based on application domain and development phase, and (b) the selection of appropriate metrics to quantify quality attributes. The metrics selection process is further complicated due to the availability of various metrics for each quality attribute, and the constraints that impact metric selection (e.g., development phase, metric validity, and available tools). In this paper, we shed light on the state-of-research of design-time quality attributes by conducting a mapping study. We have identified 154 papers that have been included as primary studies. The study led to the following outcomes: (a) low-level quality attributes (e.g., cohesion, coupling, etc.) are more frequently studied than high-level ones (e.g., maintainability, reusability, etc.), (b) maintainability is the most frequently examined high-level quality attribute, regardless of the application domain or the development phase, (c) assessment of quality attributes is usually performed by a single metric, rather than a combination of multiple metrics, and (d) metrics are mostly validated in an empirical setting. These outcomes are interpreted and discussed based on related work, offering useful implications to both researchers and practitioners.}
}

@article{rayyan-727967234,
  title={Identification and management of technical debt: A systematic mapping study},
  year={2016},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={70},
  pages={100-121},
  author={Alves, Nicolli S R and Mendes, Thiago S and de Mendonça, Manoel G and Spínola, Rodrigo O and Shull, Forrest and Seaman, Carolyn},
  url={https://www.sciencedirect.com/science/article/pii/S0950584915001743},
  keywords={Systematic mapping, Software engineering, Technical debt, Software maintenance},
  abstract={Context The technical debt metaphor describes the effect of immature artifacts on software maintenance that bring a short-term benefit to the project in terms of increased productivity and lower cost, but that may have to be paid off with interest later. Much research has been performed to propose mechanisms to identify debt and decide the most appropriate moment to pay it off. It is important to investigate the current state of the art in order to provide both researchers and practitioners with information that enables further research activities as well as technical debt management in practice. Objective This paper has the following goals: to characterize the types of technical debt, identify indicators that can be used to find technical debt, identify management strategies, understand the maturity level of each proposal, and identify what visualization techniques have been proposed to support technical debt identification and management activities. Method A systematic mapping study was performed based on a set of three research questions. In total, 100 studies, dated from 2010 to 2014, were evaluated. Results We proposed an initial taxonomy of technical debt types, created a list of indicators that have been proposed to identify technical debt, identified the existing management strategies, and analyzed the current state of art on technical debt, identifying topics where new research efforts can be invested. Conclusion The results of this mapping study can help to identify points that still require further investigation in technical debt research.}
}

@article{rayyan-727967235,
  title={Reusing empirical knowledge during cloud computing adoption},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={138},
  pages={124-157},
  author={Fahmideh, Mahdi and Beydoun, Ghassan},
  url={https://www.sciencedirect.com/science/article/pii/S0164121217303047},
  keywords={Evidence-based software engineering, Cloud computing adoption, Goal-oriented requirement engineering, Legacy system reengineering, Legacy systems},
  abstract={Moving existing legacy systems to cloud platforms is an ever popular option. But, such endeavour may not be hazard-free and demands a proper understanding of requirements and risks involved prior to taking any action. The time is indeed ripe to undertake a realistic view of what migrating legacy systems to cloud may offer, an understanding of exceptional situations causing system quality goal failure in such a transition, and insights on countermeasures. The cloud migration body of knowledge, although is useful, is dispersed over the current literature. It is hard for busy practitioners to digest, synthesize, and harness this body of knowledge into practice when integrating legacy systems with cloud services. We address this issue by creating an innovative synergy between the approaches evidence-based software engineering and goal-oriented modelling. We develop an evidential repository of commonly occurred obstacles and platform agnostic resolution tactics related to cloud enablement of legacy systems. The repository is further utilized during systematic goal-obstacle elaboration of given cloud migration scenarios. The applicability of our proposed framework is also demonstrated.}
}

@article{rayyan-727967236,
  title={Analysis of research in programming teaching tools: An initial review},
  year={2013},
  journal={Procedia - Social and Behavioral Sciences},
  issn={1877-0428},
  volume={103},
  pages={127-135},
  author={Salleh, Syahanim Mohd and Shukur, Zarina and Judi, Hairulliza Mohamad},
  url={https://www.sciencedirect.com/science/article/pii/S1877042813037622},
  keywords={systematic review, Programming, Teaching tools},
  abstract={This paper describes preliminary results of research related to programming teaching tools. This study focuses on the key issues being highlighted in this research. Among the research questions of the study are: What are the important issues in programming teaching and learning research? What are the methods of the research? What kind of tools involved in programming teaching and learning? What is the level of programming involved? The study applies systematic review approach to 45 research papers derived from the ACM digital database, published between 2005 and 2011. The study found six issues related to programming teaching tools, and most of the issues concern on the techniques and methods used in teaching, learning and assessment. Regarding the level of programming involved, majority of the research focuses on introductory stage.}
}

@article{rayyan-727967237,
  title={Leveraging organizational climate theory for understanding industry-academia collaboration},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={98},
  pages={148-160},
  author={Sherman, Sofia and Hadar, Irit and Luria, Gil},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917304020},
  keywords={Software engineering, Empirical research, Industry-academia collaboration, Management commitment, Organizational climate, Stakeholder involvement},
  abstract={Context Industry-academia collaboration (IAC) in the field of software engineering is widely discussed in the literature, highlighting its importance and benefits. However, along with the benefits, academic researchers face challenges while performing empirical studies in industry, risking their success. Awareness of these challenges and the importance of addressing them has recently grown, and became the center of discussion in several publication venues. Objective In this paper, we aim to address one of the key challenges affecting the success of IAC: stakeholder involvement. To this end, we propose a vision for leveraging organizational climate theory toward an effective management of IAC in software engineering research. Organizational climate is defined as the organization's priorities as perceived by its employees and was found to be an effective means of predicting employee behavior. Method To provide a basis and motivation for our vision, we conducted a literature review, focused on the workshop series of CESI, Conducting Empirical Studies in Industry, in order to elicit the relevant reported challenges of IAC, and to analyze them through the lens of the organizational climate theory. Results Emergent categories of the elicited challenges of IAC are related to the two basic components that determine the emergence of organizational climate: management commitment and communication. This result demonstrates that analyzing stakeholder involvement-related challenges of IAC through the lens of organizational climate theory provides an indication of the climate components that should be enhanced in order to address these challenges. Conclusion The above analysis lays the foundation for our vision that organizational climate may serve as an effective means of addressing the discussed challenges. We propose that developing measures of organizational research collaboration climate and deploying respective interventions for improvement would be instrumental for enhancing stakeholder involvement in IAC. We further propose a research outline toward fulfilling these potential contributions.}
}

@article{rayyan-727967238,
  title={O que revela a literatura internacional sobre os vínculos entre aprendizagem, competências e inovação?},
  year={2015},
  journal={RAI Revista de Administração e Inovação},
  issn={1809-2039},
  volume={12},
  number={2},
  pages={7-37},
  author={Araújo, Guilherme Diniz and da Silva, Anielson Barbosa and Brandão, Jammilly Mikaela Fagundes},
  url={https://www.sciencedirect.com/science/article/pii/S1809203916300699},
  keywords={Systematic Literature Review, Aprendizagem, Competence, Competências, Innovation, Inovação, Learning, Revisão Sistemática da Literatura, Vínculos},
  abstract={RESUMO Este estudo é um esforço inicial para analisar os vínculos entre Aprendizagem, Competências e Inovação. A partir da realização de uma Revisão Sistemática da Literatura internacional disponível no portal de periódicos CAPES entre os anos de 2003 e 2012, obteve-se um panorama geral dos artigos publicados (quantidade, ranking dos periódicos e autores), além da caracterização da abordagem e dos procedimentos metodológicos utilizados nos artigos, as correntes teóricas envolvendo a Aprendizagem, os níveis de análise dos estudos ligados a Competências, as dimensões de Inovação encontradas nos artigos e a categorização e ranking de palavras-chave dos trabalhos publicados, que subsidiaram a proposição de uma estrutura de referência que possibilitou a identificação de seis categorias que servem de base para analisar os vínculos entre os três construtos. São elas: a) conhecimento; b) tecnologia; c) aprendizagem; d) estratégia, capacidade e competências; e) inovação e competitividade; e f) liderança. A partir da análise dos vínculos, também foi possível delimitar sete proposições teóricas que podem servir de base para futuros estudos relacionando os temas. Considera-se que tais proposições reforçam a relevância e a contribuição do estudo para pesquisadores interessados em compreender a natureza dinâmica e complexa da articulação proposta entre os três temas. ABSTRACT This study is an initial effort to examine the links between Learning, Competence and Innovation. From conducting a systematic international literature review available on the CAPES journals repository between the years 2003 and 2012, we obtained an overview of published articles (quantity, ranking of journals and authors), and characterizing the approach and methodological procedures used in the articles, the theoretical approaches involving learning, levels of analysis of studies related to competence, the dimensions of innovation found in the published papers and the categorization and ranking of keywords of the articles, which supported the proposition of a structure reference that enabled the identification of six categories that serve as the basis to analyze the links between the three constructs. They are: a) knowledge, b)technology; c) learning; d) strategy, capacity and competencies; e) innovation and competitiveness and; f) leadership. From the analysis of links, also it was possible to delimit seven theoretical propositions that can serve as a basis for future studies relating the themes. It is considered that such proposals reinforce the relevance and contribution of study for researchers interested in understanding the dynamic and complex nature of the joint proposal between the three themes.}
}

@article{rayyan-727967239,
  title={A context model for IDE-based recommendation systems},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={128},
  pages={200-219},
  author={Gasparic, Marko and Murphy, Gail C and Ricci, Francesco},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216301807},
  keywords={Software development, Commands, Context, Integrated development environment, Recommendation systems for software engineering, Usage},
  abstract={Context, as modeled through variables called contextual factors, can improve human-computer interaction. To date, in applications supporting software development, such as integrated development environments (IDEs) and recommendation systems for software engineering (RSSEs), contextual factors have generally been constrained to project artifacts, such as source code. In this paper, we present a context model that includes thirteen contextual factors, which capture various situations in which developers interact with an IDE. This context model can be used to support and enhance user interaction with an IDE or to improve the accuracy and timing of recommendations produced by RSSEs. To assess whether the proposed contextual factors are informative for a context model, we statistically evaluated the correlations between IDE command usage and different situations, as they are described by the factors. If a contextual factor correlates with the usage of a command this means that the user is using the command differently when the values of the contextual factor change. We discovered that different factors correlate with different commands and that all the factors correlate with some commands, hence, when a context change is detected, we can also expect a change in the interaction with an IDE.}
}

@article{rayyan-727967240,
  title={Blockchain smart contracts formalization: Approaches and challenges to address vulnerabilities},
  year={2020},
  journal={Computers & Security},
  issn={0167-4048},
  volume={88},
  pages={101654},
  author={Singh, Amritraj and Parizi, Reza M and Zhang, Qi and Choo, Kim-Kwang Raymond and Dehghantanha, Ali},
  url={https://www.sciencedirect.com/science/article/pii/S0167404818310927},
  keywords={Systematic review, Blockchain, Formal methods, Smart contracts, Verification},
  abstract={Blockchain as a distributed computing platform enables users to deploy pieces of software (known as smart contracts) for a wealth of next-generation decentralized applications without involving a trusted third-party. The advantages of smart contracts do, however, come at a price. As with most technologies, there are potential security threats, vulnerabilities and various other issues associated with smart contracts. Writing secure and safe smart contracts can be extremely difficult due to various business logics, as well as platform vulnerabilities and limitations. Formal methods have recently been advocated to mitigate these vulnerabilities. This paper aims to provide a first-time study on current formalization research on all smart contract-related platforms on blockchains, which is scarce in the literature. To this end, a timely and rigorous systematic review to examine the state-of-the-art research and achievements published between 2015 and July 2019 is provided. This study is based on a comprehensive review of a set of 35 research papers that have been extracted from eight major online digital databases. The results indicate that the most common formalization technique is theorem proving, which is most often used to verify security properties relating to smart contracts, while other techniques such as symbolic execution and model checking were also frequently used. These techniques were most commonly used to verify the functional correctness of smart contracts. From the language and automation point of views, there were 12 languages (domain specific/specification/general purpose) proposed or used for the formalization of smart contracts on blockchains, while 15 formal method-specific automated tools/frameworks were identified for mitigating various vulnerabilities of smart contracts. From the results of this work, we further highlight three open issues for future research in this emerging domain including: formal testing, automated verification of smart contracts, and domain specific languages (DSLs) for Ethereum. These issues suggest the need for additional, in-depth research. Our study also provides possible future research directions.}
}

@article{rayyan-727967241,
  title={Special issue in honor of professor claes wohlin},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={107},
  pages={1-2},
  author={Ruhe, Guenther},
  url={https://www.sciencedirect.com/science/article/pii/S0950584918302520}
}

@article{rayyan-727967242,
  title={Empirical evaluation in Computer Science research published by ACM},
  year={2009},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={51},
  number={6},
  pages={1081-1085},
  author={Wainer, Jacques and Novoa Barsottini, Claudia G and Lacerda, Danilo and Magalhães de Marco, Leandro Rodrigues},
  url={https://www.sciencedirect.com/science/article/pii/S0950584909000093},
  keywords={Systematic review, Empirical evaluation, Experimentation, Research evaluation},
  abstract={This paper repeats part of the analysis performed in the 1995 paper “Experimental evaluation in Computer Science: a quantitative study” by Tichy and collaborators, for 147 papers randomly selected from the ACM, published in the year 2005. The papers published in 2005 are classified in the following way: 4% theory, 17% empirical, 4.7% hypothesis testing, 3.4% other, and 70% design and modeling (using the 1995 paper categories). Within the design and modeling class, 33% of the papers have no evaluation. The numbers of the 2005 sample are very similar to the original figures for the 1995 sample, which shows that Computer Science research has not increased significantly its empirical or experimental component.}
}

@article{rayyan-727967243,
  title={Research state of the art on GoF design patterns: A mapping study},
  year={2013},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={86},
  number={7},
  pages={1945-1964},
  author={Ampatzoglou, Apostolos and Charalampidou, Sofia and Stamelos, Ioannis},
  url={https://www.sciencedirect.com/science/article/pii/S0164121213000757},
  keywords={Mapping study, Design patterns, Software quality attributes},
  abstract={Design patterns are used in software development to provide reusable and documented solutions to common design problems. Although many studies have explored various aspects of design patterns, no research summarizing the state of research related to design patterns existed up to now. This paper presents the results of a mapping study of about 120 primary studies, to provide an overview of the research efforts on Gang of Four (GoF) design patterns. The research questions of this study deal with (a) if design pattern research can be further categorized in research subtopics, (b) which of the above subtopics are the most active ones and (c) what is the reported effect of GoF patterns on software quality attributes. The results suggest that design pattern research can be further categorized to research on GoF patterns formalization, detection and application and on the effect of GoF patterns on software quality attributes. Concerning the intensity of research activity of the abovementioned subtopics, research on pattern detection and on the effect of GoF patterns on software quality attributes appear to be the most active ones. Finally, the reported research to date on the effect of GoF patterns on software quality attributes are controversial; because some studies identify one pattern's effect as beneficial whereas others report the same pattern's effect as harmful.}
}

@article{rayyan-727967244,
  title={Software development in startup companies: A systematic mapping study},
  year={2014},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={56},
  number={10},
  pages={1200-1218},
  author={Paternoster, Nicolò and Giardino, Carmine and Unterkalmsteiner, Michael and Gorschek, Tony and Abrahamsson, Pekka},
  url={https://www.sciencedirect.com/science/article/pii/S0950584914000950},
  keywords={Systematic mapping study, Software development, Startups, Software},
  abstract={Context Software startups are newly created companies with no operating history and fast in producing cutting-edge technologies. These companies develop software under highly uncertain conditions, tackling fast-growing markets under severe lack of resources. Therefore, software startups present a unique combination of characteristics which pose several challenges to software development activities. Objective This study aims to structure and analyze the literature on software development in startup companies, determining thereby the potential for technology transfer and identifying software development work practices reported by practitioners and researchers. Method We conducted a systematic mapping study, developing a classification schema, ranking the selected primary studies according their rigor and relevance, and analyzing reported software development work practices in startups. Results A total of 43 primary studies were identified and mapped, synthesizing the available evidence on software development in startups. Only 16 studies are entirely dedicated to software development in startups, of which 10 result in a weak contribution (advice and implications (6); lesson learned (3); tool (1)). Nineteen studies focus on managerial and organizational factors. Moreover, only 9 studies exhibit high scientific rigor and relevance. From the reviewed primary studies, 213 software engineering work practices were extracted, categorized and analyzed. Conclusion This mapping study provides the first systematic exploration of the state-of-art on software startup research. The existing body of knowledge is limited to a few high quality studies. Furthermore, the results indicate that software engineering work practices are chosen opportunistically, adapted and configured to provide value under the constrains imposed by the startup context.}
}

@article{rayyan-727967245,
  title={A literature review of Business/IT alignment strategies},
  year={2012},
  journal={Procedia Technology},
  issn={2212-0173},
  volume={5},
  pages={462-474},
  author={Aversano, Lerina and Grasso, Carmine and Tortorella, Maria},
  url={https://www.sciencedirect.com/science/article/pii/S2212017312004823},
  keywords={alignment, Enterprise evolution, Evaluation and analysis, Measurement framework, Modeling},
  abstract={In the last years, the alignment issue was addressed in several researches and numerous methods, techniques and tools were proposed. Indeed, the business and IT performance are tightly coupled, and enterprises cannot be competitive if their business and IT strategies are not aligned. This paper proposes a literature review useful for evaluating different alignment approaches, with the aim of discovering similarity, maturity, capability to measure, model, asses and evolve the alignment level existing among business and technological assets of an enterprise. The proposed framework is applied to analyse the alignment research published in the Information & Management journal and the Journal of Strategic Information Systems, that are the ones that more published on this topic. The achieved evaluation results are presented.}
}

@article{rayyan-727967246,
  title={A systematic identification of consistency rules for UML diagrams},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={144},
  pages={121-142},
  author={Torre, Damiano and Labiche, Yvan and Genero, Marcela and Elaasar, Maged},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218301249},
  keywords={Systematic Mapping Study, Model checking and verification, UML consistency rules},
  abstract={UML diagrams describe different views of one software. These diagrams strongly depend on each other and must therefore be consistent with one another, since inconsistencies between diagrams may be a source of faults during software development activities that rely on these diagrams. It is therefore paramount that consistency rules be defined and that inconsistencies be detected, analyzed and fixed. The relevant literature shows that authors typically define their own consistency rules, sometimes defining the same rules and sometimes defining rules that are already in the UML standard. The reason might be that no consolidated set of rules that are relevant by authors can be found to date. The aim of our research is to provide an up to date, consolidated set of UML consistency rules and obtain a detailed overview of the current research in this area. We therefore followed a systematic procedure in order to collect from literature up to March 2017 and analyze UML consistency rules. We then consolidated a set of 119 UML consistency rules (avoiding redundant definitions or definitions already in the UML standard), which can be used as an important reference for UML-based software development activities, for teaching UML-based software development, and for further research.}
}

@article{rayyan-727967247,
  title={Software test-code engineering: A systematic mapping},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={58},
  pages={123-147},
  author={Garousi Yusifoğlu, Vahid and Amannejad, Yasaman and Betin Can, Aysu},
  url={https://www.sciencedirect.com/science/article/pii/S0950584914001487},
  keywords={Systematic mapping, Survey, Development of test code, Quality assessment of test code, Software test-code engineering, Study repository, Software},
  abstract={Context As a result of automated software testing, large amounts of software test code (script) are usually developed by software teams. Automated test scripts provide many benefits, such as repeatable, predictable, and efficient test executions. However, just like any software development activity, development of test scripts is tedious and error prone. We refer, in this study, to all activities that should be conducted during the entire lifecycle of test-code as Software Test-Code Engineering (STCE). Objective As the STCE research area has matured and the number of related studies has increased, it is important to systematically categorize the current state-of-the-art and to provide an overview of the trends in this field. Such summarized and categorized results provide many benefits to the broader community. For example, they are valuable resources for new researchers (e.g., PhD students) aiming to conduct additional secondary studies. Method In this work, we systematically classify the body of knowledge related to STCE through a systematic mapping (SM) study. As part of this study, we pose a set of research questions, define selection and exclusion criteria, and systematically develop and refine a systematic map. Results Our study pool includes a set of 60 studies published in the area of STCE between 1999 and 2012. Our mapping data is available through an online publicly-accessible repository. We derive the trends for various aspects of STCE. Among our results are the following: (1) There is an acceptable mix of papers with respect to different contribution facets in the field of STCE and the top two leading facets are tool (68%) and method (65%). The studies that presented new processes, however, had a low rate (3%), which denotes the need for more process-related studies in this area. (2) Results of investigation about research facet of studies and comparing our result to other SM studies shows that, similar to other fields in software engineering, STCE is moving towards more rigorous validation approaches. (3) A good mixture of STCE activities has been presented in the primary studies. Among them, the two leading activities are quality assessment and co-maintenance of test-code with production code. The highest growth rate for co-maintenance activities in recent years shows the importance and challenges involved in this activity. (4) There are two main categories of quality assessment activity: detection of test smells and oracle assertion adequacy. (5) JUnit is the leading test framework which has been used in about 50% of the studies. (6) There is a good mixture of SUT types used in the studies: academic experimental systems (or simple code examples), real open-source and commercial systems. (7) Among 41 tools that are proposed for STCE, less than half of the tools (45%) were available for download. It is good to have this percentile of tools to be available, although not perfect, since the availability of tools can lead to higher impact on research community and industry. Conclusion We discuss the emerging trends in STCE, and discuss the implications for researchers and practitioners in this area. The results of our systematic mapping can help researchers to obtain an overview of existing STCE approaches and spot areas in the field that require more attention from the research community.}
}

@article{rayyan-727967248,
  title={A systematic mapping study on soft computing techniques to cloud environment},
  year={2017},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={120},
  pages={31-38},
  author={Ejimogu, Obinna H and Başaran, Seren},
  url={https://www.sciencedirect.com/science/article/pii/S1877050917324171},
  keywords={systematic mapping, Cloud computing, power optimiztion, soft computing, task optimization},
  abstract={Cloud computing plays an essential role in storage and transfer of big capacity data due to a rapid increase in size and the number of organizational activities. There exist numerous studies in which diverse soft computing techniques are applied to the cloud environment. The relevant extant literature that were clustered into five main categories with respect to precedence are; task optimization, power optimization, security, service selection and cost optimization. Yet, it was discovered that there is a dearth of systematic review/mapping studies particularly on soft computing techniques in cloud environment so as to obtain exclusive insight, to identify existing gaps and future research directions. Therefore the aim of this paper is to conduct a systematic mapping study of recent literature on soft computing techniques in cloud environment. For this purpose, 163 articles were chosen as primary sources that were published within the last decade, which were classified based on study focus area, type of research, contribution facet and particularly the type of soft computing technique used. Findings revealed that task optimization takes part as the highly preferred research focus area. Secondly, most of the articles found are of validation studies. The contributions of most of the studies are concerned about methods and finally the top three soft computing techniques were detected as particle swarm optimization (PSO), genetic algorithm (GA) and hybrid systems. The results of this study confirm that applying soft computing techniques in cloud computing has gained more and more significant attention recently but there still remain challenges and gaps which calls for further investigation especially in the area of cost optimization and also artificial bee colony.}
}

@article{rayyan-727967249,
  title={A validated ontology for global software development},
  year={2016},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={46},
  pages={66-78},
  author={Vizcaíno, Aurora and García, Felix and Piattini, Mario and Beecham, Sarah},
  url={https://www.sciencedirect.com/science/article/pii/S0920548916300046},
  keywords={Global software development (GSD), Ontology, Distributed software development (DSD), Global software engineering (GSE), Meta-model, Software},
  abstract={The global software development (GSD) paradigm has, over the last 15fifteen years, shifted from being novel and ground breaking to being widely adopted and mainstream. This wide adoption is partly owing to the many benefits provided by GSD, such as reduced labour costs, proximity to new markets and access to a diverse and experienced skills pool. Yet taking advantage of these benefits is far from straightforward, and research literature now includes a proliferation of guidelines, reviews and models to support the GSD industry. Although this active area of study is firmly established as a research area in its own right, the boundaries between general software engineering and GSD are somewhat confused and poorly defined. In an effort to consolidate our understanding of GSD, we have developed an ontology in order to capture the most relevant terms, concepts and relationships related to the goals, barriers and features of GSD projects. The study we present here builds on research conducted in a collaboration project between industry and academia, in which we developed an ontology in order to provide practitioners with a “common language and conceptualisation”. Its successful outcome encouraged us to create a broader ontology that captures the current trends in GSD literature. The key ontology, along with its three subontologies, are the result of a review of the relevant literature, together with several expert evaluations. This ontology can serve as a useful introduction to GSD for researchers who are new to the paradigm. Moreover, practitioners can take advantage of it in order to contextualise their projects and predict and detect possible barriers. What is more, using a common language will help both researchers and practitioners to avoid ambiguities and misunderstanding.}
}

@article{rayyan-727967250,
  title={The usage of ISBSG data fields in software effort estimation: A systematic mapping study},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={113},
  pages={188-215},
  author={González-Ladrón-de-Guevara, Fernando and Fernández-Diego, Marta and Lokan, Chris},
  url={https://www.sciencedirect.com/science/article/pii/S0164121215002642},
  keywords={Systematic mapping study, Software effort estimation, ISBSG data field, Software},
  abstract={The International Software Benchmarking Standards Group (ISBSG) maintains a repository of data about completed software projects. A common use of the ISBSG dataset is to investigate models to estimate a software project's size, effort, duration, and cost. The aim of this paper is to determine which and to what extent variables in the ISBSG dataset have been used in software engineering to build effort estimation models. For that purpose a systematic mapping study was applied to 107 research papers, obtained after a filtering process, that were published from 2000 until the end of 2013, and which listed the independent variables used in the effort estimation models. The usage of ISBSG variables for filtering, as dependent variables, and as independent variables is described. The 20 variables (out of 71) mostly used as independent variables for effort estimation are identified and analysed in detail, with reference to the papers and types of estimation methods that used them. We propose guidelines that can help researchers make informed decisions about which ISBSG variables to select for their effort estimation models.}
}

@article{rayyan-727967251,
  title={A systematic mapping study on the combination of software architecture and agile development},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={111},
  pages={157-184},
  author={Yang, Chen and Liang, Peng and Avgeriou, Paris},
  url={https://www.sciencedirect.com/science/article/pii/S0164121215002125},
  keywords={Software architecture, Agile development, Architecting approach, Software},
  abstract={Context Combining software architecture and agile development has received significant attention in recent years. However, there exists no comprehensive overview of the state of research on the architecture-agility combination. Objective This work aims to analyze the combination of architecture and agile methods for the purpose of exploration and analysis with respect to architecting activities and approaches, agile methods and practices, costs, benefits, challenges, factors, tools, and lessons learned concerning the combination. Method A systematic mapping study (SMS) was conducted, covering the literature on the architecture-agility combination published between February 2001 and January 2014. Results Fifty-four studies were finally included in this SMS. Some of the highlights: (1) a significant difference exists in the proportion of various architecting activities, agile methods, and agile practices employed in the combination. (2) none of the architecting approaches has been widely used in the combination. (3) there is a lack of description and analysis regarding the costs and failure stories of the combination. (4) twenty challenges, twenty-nine factors, and twenty-five lessons learned were identified. Conclusions The results of this SMS help the software engineering community to reflect on the past thirteen years of research and practice on the architecture-agility combination with a number of implications.}
}

@article{rayyan-727967252,
  title={FM-CF: A framework for classifying feature model building approaches},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={154},
  pages={1-21},
  author={Gacitúa, Ricardo and Sepúlveda, Samuel and Mazo, Raúl},
  url={https://www.sciencedirect.com/science/article/pii/S0164121219300767},
  keywords={Software product lines, Classification, Feature model, Framework, Models},
  abstract={Software product line engineering has emerged as a prominent software engineering paradigm, as it comprises a set of core assets sharing functionality and quality attributes. Feature modelling is one of the most frequently used techniques for modelling the variability within a software product line. There are several proposals for building Feature Models which rely on semi-automated or fully automated means. Unfortunately, automatic feature model construction has been addressed from different viewpoints, so it is not easy to know which is the best approach for automating the building of variability models. In fact, there is no clarity regarding common elements, and the main differences that characterise such approaches. Additionally, the wide variety of terms used to refer to the process of building a Feature Model (e.g. synthesis, location, re-engineering, and weaving) means that approaches are varied and very heterogeneous, making them complex to understand and classify. This paper introduces FM-CF, which is a Conceptual experience-based Framework for classifying approaches for the automatic building of Feature Models. The framework considers a set of categories mainly focused on characterising some aspects, such as input sources, methods and techniques, results, and types of evaluation. A literature review of (semi-) automated Feature Model construction was performed to identify approaches for building Feature Models by (semi-)automatic means, and the main terms used by those approaches. Then the completeness of the framework was evaluated by mapping the set of dimensions and their items, and the terms extracted from the literature. The conceptual framework provides guidance to researchers for choosing the appropriate aspects with which to build Feature Models, and helps in the understanding and clarification of the proposed approaches.}
}

@article{rayyan-727967253,
  title={Testing embedded software: A survey of the literature},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={104},
  pages={14-45},
  author={Garousi, Vahid and Felderer, Michael and Karapıçak, Çağrı Murat and Yılmaz, Uğur},
  url={https://www.sciencedirect.com/science/article/pii/S0950584918301265},
  keywords={Software testing, Systematic literature review, Systematic mapping, Embedded software, Embedded systems, Systematic literature mapping, Software},
  abstract={Context Embedded systems have overwhelming penetration around the world. Innovations are increasingly triggered by software embedded in automotive, transportation, medical-equipment, communication, energy, and many other types of systems. To test embedded software in an effective and efficient manner, a large number of test techniques, approaches, tools and frameworks have been proposed by both practitioners and researchers in the last several decades. Objective However, reviewing and getting an overview of the entire state-of-the-art and the –practice in this area is challenging for a practitioner or a (new) researcher. Also unfortunately, as a result, we often see that many companies reinvent the wheel (by designing a test approach new to them, but existing in the domain) due to not having an adequate overview of what already exists in this area. Method To address the above need, we conducted and report in this paper a systematic literature review (SLR) in the form of a systematic literature mapping (SLM) in this area. After compiling an initial pool of 588 papers, a systematic voting about inclusion/exclusion of the papers was conducted among the authors, and our final pool included 312 technical papers. Results Among the various aspects that we aim at covering, our review covers the types of testing topics studied, types of testing activity, types of test artifacts generated (e.g., test inputs or test code), and the types of industries in which studies have focused on, e.g., automotive and home appliances. Furthermore, we assess the benefits of this review by asking several active test engineers in the Turkish embedded software industry to review its findings and provide feedbacks as to how this review has benefitted them. Conclusion The results of this review paper have already benefitted several of our industry partners in choosing the right test techniques / approaches for their embedded software testing challenges. We believe that it will also be useful for the large world-wide community of software engineers and testers in the embedded software industry, by serving as an “index” to the vast body of knowledge in this important area. Our results will also benefit researchers in observing the latest trends in this area and for identifying the topics which need further investigations.}
}

@article{rayyan-727967254,
  title={Empirical studies on the use of social software in global software development – A systematic mapping study},
  year={2013},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={55},
  number={7},
  pages={1143-1164},
  author={Giuffrida, Rosalba and Dittrich, Yvonne},
  url={https://www.sciencedirect.com/science/article/pii/S0950584913000153},
  keywords={Systematic mapping study, Social media, Computer-supported cooperative work, Distributed teams, Global software development, Social software, Software},
  abstract={Background In Global Software Development (GSD), informal communication and knowledge sharing play an important role. Social Software (SoSo) has the potential to support and foster this key responsibility. Research on the use of SoSo in GSD is still at an early stage: although a number of empirical studies on the usage of SoSo are available in related fields, there exists no comprehensive overview of what has been investigated to date across them. Objective The aim of this review is to map empirical studies on the usage of SoSo in Software Engineering projects and in distributed teams, and to highlight the findings of research works which could prove to be beneficial for GSD researchers and practitioners. Method A Systematic Mapping Study is conducted using a broad search string that allows identifying a variety of studies which can be beneficial for GSD. Papers have been retrieved through a combination of automatic search and snowballing, hence a wide quantitative map of the research area is provided. Additionally, text extracts from the studies are qualitatively synthesised to investigate benefits and challenges of the use of SoSo. Results SoSo is reported as being chiefly used as a support for collaborative work, fostering awareness, knowledge management and coordination among team members. Contrary to the evident high importance of the social aspects offered by SoSo, socialisation is not the most important usage reported. Conclusions This review reports how SoSo is used in GSD and how it is capable of supporting GSD teams. Four emerging themes in global software engineering were identified: the appropriation and development of usage structures; understanding how an ecology of communication channels and tools are used by teams; the role played by SoSo either as a subtext or as an explicit goal; and finally, the surprising low percentage of observational studies.}
}

@article{rayyan-727967255,
  title={The state of the art on design patterns: A systematic mapping of the literature},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={125},
  pages={93-118},
  author={Bafandeh Mayvan, B and Rasoolzadegan, A and Ghavidel Yazdi, Z},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216302321},
  keywords={Systematic mapping study, Systematic review, Design patterns},
  abstract={Design patterns are widely used by software developers to build complex systems. Hence, they have been investigated by many researchers in recent decades. This leads to the emergence of various topics in the design patterns field. The objective of this paper is to present an overview of the research efforts on design patterns for those researchers who seek to enter this area. The main contributions are as follows: (a) identifying research topics in design patterns, (b) quantifying the research emphasis on each topic, and (c) describing the demographics of design patterns research. The last secondary study with similar goals in the design patterns field considers the Gang of Four design patterns only. However, the scope of the current study is all of the design patterns. Moreover, our review covers about six additional years and a larger number of publications and venues. In this systematic mapping study, a total of 2775 papers were identified as relevant, and 637 of them were included. According to the results, design patterns can be classified into six different research topics. As a consequence, it is concluded that Pattern Development, Pattern Mining, and Pattern Usage are the most active topics in the field of design patterns.}
}

@article{rayyan-727967256,
  title={Spot pricing in the Cloud ecosystem: A comparative investigation},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={114},
  pages={1-19},
  author={Li, Zheng and Zhang, He and O'Brien, Liam and Jiang, Shu and Zhou, You and Kihl, Maria and Ranjan, Rajiv},
  url={https://www.sciencedirect.com/science/article/pii/S0164121215002332},
  keywords={Systematic literature review, Cloud computing, Cloud spot pricing},
  abstract={Background: Spot pricing is considered as a significant supplement for building a full-fledged market economy for the Cloud ecosystem. However, it seems that both providers and consumers are still hesitating to enter the Cloud spot market. The relevant academic community also has conflicting opinions about Cloud spot pricing in terms of revenue generation. Aim: This work aims to systematically identify, assess, synthesize and report the published evidence in favor of or against spot-price scheme compared with fixed-price scheme of Cloud computing, so as to help relieve the aforementioned conflict. Method: We employed the systematic literature review (SLR) method to collect and investigate the empirical studies of Cloud spot pricing indexed by major electronic libraries. Results: This SLR identified 61 primary studies that either delivered discussions or conducted experiments to perform comparison between spot pricing and fixed pricing in the Cloud domain. The reported benefits and limitations were summarized to facilitate cost-benefit analysis of being a Cloud spot pricing player, while four types of theories were distinguished to help both researchers and practitioners better understand the Cloud spot market. Conclusions: This SLR shows that the academic community strongly advocates the emerging Cloud spot market. Although there is still a lack of practical and easily deployable market-driven mechanisms, the overall findings of our work indicate that spot pricing plays a promising role in the sustainability of Cloud resource exploitation.}
}

@article{rayyan-727967257,
  title={Research patterns and trends in software effort estimation},
  year={2017},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={91},
  pages={1-21},
  author={Sehra, Sumeet Kaur and Brar, Yadwinder Singh and Kaur, Navdeep and Sehra, Sukhjit Singh},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917304317},
  keywords={Software effort estimation, Latent Dirichlet allocation, Research trends, Software},
  abstract={Context Software effort estimation (SEE) is most crucial activity in the field of software engineering. Vast research has been conducted in SEE resulting into a tremendous increase in literature. Thus it is of utmost importance to identify the core research areas and trends in SEE which may lead the researchers to understand and discern the research patterns in large literature dataset. Objective To identify unobserved research patterns through natural language processing from a large set of research articles on SEE published during the period 1996 to 2016. Method A generative statistical method, called Latent Dirichlet Allocation (LDA), applied on a literature dataset of 1178 articles published on SEE. Results As many as twelve core research areas and sixty research trends have been revealed; and the identified research trends have been semantically mapped to associate core research areas. Conclusions This study summarises the research trends in SEE based upon a corpus of 1178 articles. The patterns and trends identified through this research can help in finding the potential research areas.}
}

@article{rayyan-727967258,
  title={Evaluating prediction systems in software project estimation},
  year={2012},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={54},
  number={8},
  pages={820-827},
  author={Shepperd, Martin and MacDonell, Steve},
  url={https://www.sciencedirect.com/science/article/pii/S095058491200002X},
  keywords={Software engineering, Prediction system, Empirical validation, Randomisation techniques, Software},
  abstract={Context Software engineering has a problem in that when we empirically evaluate competing prediction systems we obtain conflicting results. Objective To reduce the inconsistency amongst validation study results and provide a more formal foundation to interpret results with a particular focus on continuous prediction systems. Method A new framework is proposed for evaluating competing prediction systems based upon (1) an unbiased statistic, Standardised Accuracy, (2) testing the result likelihood relative to the baseline technique of random ‘predictions', that is guessing, and (3) calculation of effect sizes. Results Previously published empirical evaluations of prediction systems are re-examined and the original conclusions shown to be unsafe. Additionally, even the strongest results are shown to have no more than a medium effect size relative to random guessing. Conclusions Biased accuracy statistics such as MMRE are deprecated. By contrast this new empirical validation framework leads to meaningful results. Such steps will assist in performing future meta-analyses and in providing more robust and usable recommendations to practitioners.}
}

@article{rayyan-727967259,
  title={When and what to automate in software testing? A multi-vocal literature review},
  year={2016},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={76},
  pages={92-117},
  author={Garousi, Vahid and Mäntylä, Mika V},
  url={https://www.sciencedirect.com/science/article/pii/S0950584916300702},
  keywords={Systematic literature review, Decision support, Multivocal literature review, Software test automation, Systematic Mapping study, What to automate, When to automate, Software},
  abstract={Context Many organizations see software test automation as a solution to decrease testing costs and to reduce cycle time in software development. However, establishment of automated testing may fail if test automation is not applied in the right time, right context and with the appropriate approach. Objective The decisions on when and what to automate is important since wrong decisions can lead to disappointments and major wrong expenditures (resources and efforts). To support decision making on when and what to automate, researchers and practitioners have proposed various guidelines, heuristics and factors since the early days of test automation technologies. As the number of such sources has increased, it is important to systematically categorize the current state-of-the-art and -practice, and to provide a synthesized overview. Method To achieve the above objective, we have performed a Multivocal Literature Review (MLR) study on when and what to automate in software testing. A MLR is a form of a Systematic Literature Review (SLR) which includes the grey literature (e.g., blog posts and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). We searched the academic literature using the Google Scholar and the grey literature using the regular Google search engine. Results Our MLR and its results are based on 78 sources, 52 of which were grey literature and 26 were formally published sources. We used the qualitative analysis (coding) to classify the factors affecting the when- and what-to-automate questions to five groups: (1) Software Under Test (SUT)-related factors, (2) test-related factors, (3) test-tool-related factors, (4) human and organizational factors, and (5) cross-cutting and other factors. The most frequent individual factors were: need for regression testing (44 sources), economic factors (43), and maturity of SUT (39). Conclusion We show that current decision-support in software test automation provides reasonable advice for industry, and as a practical outcome of this research we have summarized it as a checklist that can be used by practitioners. However, we recommend developing systematic empirically-validated decision-support approaches as the existing advice is often unsystematic and based on weak empirical evidence.}
}

@article{rayyan-727967260,
  title={Ontology-based systems engineering: A state-of-the-art review},
  year={2019},
  journal={Computers in Industry},
  issn={0166-3615},
  volume={111},
  pages={148-171},
  author={Yang, Lan and Cormican, Kathryn and Yu, Ming},
  url={https://www.sciencedirect.com/science/article/pii/S0166361518307887},
  keywords={systematic literature review, ontology, state of the art, systems engineering, Review Literature as Topic},
  abstract={In recent years ontology-based systems engineering has grown significantly. Its raison d'etre is to use ontologies to improve the systems engineering body of knowledge. Specifically, ontologies act as an enabler of good knowledge management as they focus on establishing well-defined domain concepts in terms of terminologies, definitions, and relationships. In addition, the use of formal semantics is essential for explicit, sharable, and reusable knowledge representation. However, little research exists that evaluates the impact and real benefits of ontologies for systems engineering. A thorough review of the state of the art of ontology-based systems engineering will contribute to the future development of the discipline. Therefore, the primary objective of this paper is to draw a clear roadmap of how ontologies support systems engineering and to determine what extent they have been applied in this domain. This review contributes to a holistic examination of the primary studies relevant to the topic of ontology-based systems engineering, spanning nearly two decades. The findings provide an integrated and comprehensive understanding of and shed new light on (1) the systems engineering knowledge areas supported by ontologies; (2) the contribution that ontologies make to systems engineering problems; (3) the existing ontologies that are created to support systems engineering; and (4) the techniques adopted from an ontology engineering perspective. It assesses the influence of ontologies in systems engineering knowledge areas, expounding and highlighting the effects of ontologies. All in all, it presents a comprehensive summary of the state of the art of ontology-based systems engineering, as well as illuminating a roadmap for future directions.}
}

@article{rayyan-727967261,
  title={Smells in software test code: A survey of knowledge in industry and academia},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={138},
  pages={52-81},
  author={Garousi, Vahid and Küçük, Barış},
  url={https://www.sciencedirect.com/science/article/pii/S0164121217303060},
  keywords={Software testing, Systematic mapping, Survey, Automated testing, Multivocal literature mapping, Test anti-patterns, Test automation, Test scripts, Test smells, Smell, Software},
  abstract={As a type of anti-pattern, test smells are defined as poorly designed tests and their presence may negatively affect the quality of test suites and production code. Test smells are the subject of active discussions among practitioners and researchers, and various guidelines to handle smells are constantly offered for smell prevention, smell detection, and smell correction. Since there is a vast grey literature as well as a large body of research studies in this domain, it is not practical for practitioners and researchers to locate and synthesize such a large literature. Motivated by the above need and to find out what we, as the community, know about smells in test code, we conducted a ‘multivocal' literature mapping (classification) on both the scientific literature and also practitioners' grey literature. By surveying all the sources on test smells in both industry (120 sources) and academia (46 sources), 166 sources in total, our review presents the largest catalogue of test smells, along with the summary of guidelines/techniques and the tools to deal with those smells. This article aims to benefit the readers (both practitioners and researchers) by serving as an “index” to the vast body of knowledge in this important area, and by helping them develop high-quality test scripts, and minimize occurrences of test smells and their negative consequences in large test automation projects.}
}

@article{rayyan-727967262,
  title={Analysis and classification of barriers and critical success factors for implementing a cloud data governance strategy},
  year={2017},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={113},
  pages={223-232},
  author={Al-Ruithe, Majid and Benkhelifa, Elhadj},
  url={https://www.sciencedirect.com/science/article/pii/S1877050917317623},
  keywords={Cloud computing, barriers, cloud data governance, critical success factors (CSFs), data governance, systematic literature review (SLR)},
  abstract={The general consensus in literature seems to suggest that data governance refers to the entirety of decision rights and responsibilities concerning the management of data assets in organisations. These definitions do not however provide equal prominence for the data governance within the cloud computing technology context. As such, this deficit calls for in-depth understanding of data governance and cloud Computing. This trend contributes to changes in data governance strategy in the organisation, such as the organisation's structure and regulations, people, technology, process, roles and responsibilities. This is one of the great challenges facing organizations today when they move their data to Cloud Computing environments, particularly how Cloud technology affects data governance. The authors' general observation reveals that the area of data governance in general is under researched and not widely practiced by organisations, let alone when it is concerned with cloud computing, where research is really in its infancy and far from reaching maturity. This paper attempts to identify the possible common barriers and critical success factors for implementing cloud data governance in the hope that it helps the reader to be aware of these barriers and consider them in future developments in the field.}
}

@article{rayyan-727967263,
  title={Analyzing impact of experience curve on ROI in the software product line adoption process},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={59},
  pages={136-148},
  author={Tüzün, Eray and Tekinerdogan, Bedir},
  url={https://www.sciencedirect.com/science/article/pii/S0950584914002079},
  keywords={Productivity, Software reuse, Cost models, Experience curve, Learning curve, Software product line engineering, Software},
  abstract={Context Experience curve is a well-known concept in management and education science, which explains the phenomenon of increased worker efficiency with repetitive production of a good or service. Objective We aim to analyze the impact of the experience curve effect on the Return on Investment (ROI) in the software product line engineering (SPLE) process. Method We first present the results of a systematic literature review (SLR) to explicitly depict the studies that have considered the impact of experience curve effect on software development in general. Subsequently, based on the results of the SLR, the experience curve effect models in the literature, and the SPLE cost models, we define an approach for extending the cost models with the experience curve effect. Finally, we discuss the application of the refined cost models in a real industrial context. Results The SLR resulted in 15 primary studies which confirm the impact of experience curve effect on software development in general but the experience curve effect in the adoption of SPLE got less attention. The analytical discussion of the cost models and the application of the refined SPLE cost models in the industrial context showed a clear impact of the experience curve effect on the time-to-market, cost of development and ROI in the SPLE adoption process. Conclusions The proposed analysis with the newly defined cost models for SPLE adoption provides a more precise analysis tool for the management, and as such helps to support a better decision making.}
}

@article{rayyan-727967264,
  title={Testing in Service Oriented Architectures with dynamic binding: A mapping study},
  year={2011},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={53},
  number={3},
  pages={171-189},
  author={Palacios, Marcos and García-Fanjul, José and Tuya, Javier},
  url={https://www.sciencedirect.com/science/article/pii/S0950584910002168},
  keywords={Software testing, Systematic literature review, Mapping study, SOA, Dynamic binding, Service Oriented Architectures},
  abstract={Context Service Oriented Architectures (SOA) have emerged as a new paradigm to develop interoperable and highly dynamic applications. Objective This paper aims to identify the state of the art in the research on testing in Service Oriented Architectures with dynamic binding. Method A mapping study has been performed employing both manual and automatic search in journals, conference/workshop proceedings and electronic databases. Results A total of 33 studies have been reviewed in order to extract relevant information regarding a previously defined set of research questions. The detection of faults and the decision making based on the information gathered from the tests have been identified as the main objectives of these studies. To achieve these goals, monitoring and test case generation are the most proposed techniques testing both functional and non-functional properties. Furthermore, different stakeholders have been identified as participants in the tests, which are performed in specific points in time during the life cycle of the services. Finally, it has been observed that a relevant group of studies have not validated their approach yet. Conclusions Although we have only found 33 studies that address the testing of SOA where the discovery and binding of the services are performed at runtime, this number can be considered significant due to the specific nature of the reviewed topic. The results of this study have contributed to provide a body of knowledge that allows identifying current gaps in improving the quality of the dynamic binding in SOA using testing approaches.}
}

@article{rayyan-727967265,
  title={A systematic mapping study of infrastructure as code research},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={108},
  pages={65-77},
  author={Rahman, Akond and Mahdavi-Hezaveh, Rezvan and Williams, Laurie},
  url={https://www.sciencedirect.com/science/article/pii/S0950584918302507},
  keywords={Systematic mapping study, Software engineering, Continuous deployment, Configuration as code, Configuration script, Devops, Infrastructure as code},
  abstract={Context: Infrastructure as code (IaC) is the practice to automatically configure system dependencies and to provision local and remote instances. Practitioners consider IaC as a fundamental pillar to implement DevOps practices, which helps them to rapidly deliver software and services to end-users. Information technology (IT) organizations, such as GitHub, Mozilla, Facebook, Google and Netflix have adopted IaC. A systematic mapping study on existing IaC research can help researchers to identify potential research areas related to IaC, for example defects and security flaws that may occur in IaC scripts. Objective: The objective of this paper is to help researchers identify research areas related to infrastructure as code (IaC) by conducting a systematic mapping study of IaC-related research. Method: We conduct our research study by searching five scholar databases. We collect a set of 31,498 publications by using seven search strings. By systematically applying inclusion and exclusion criteria, which includes removing duplicates and removing non-English and non peer-reviewed publications, we identify 32 publications related to IaC. We identify topics addressed in these publications by applying qualitative analysis. Results: We identify four topics studied in IaC-related publications: (i) framework/tool for infrastructure as code; (ii) adoption of infrastructure as code; (iii) empirical study related to infrastructure as code; and (iv) testing in infrastructure as code. According to our analysis, 50.0% of the studied 32 publications propose a framework or tool to implement the practice of IaC or extend the functionality of an existing IaC tool. Conclusion: Our findings suggest that framework or tools is a well-studied topic in IaC research. As defects and security flaws can have serious consequences for the deployment and development environments in DevOps, we observe the need for research studies that will study defects and security flaws for IaC.}
}

@article{rayyan-727967266,
  title={VIVACE: A framework for the systematic evaluation of variability support in process-aware information systems},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={57},
  pages={248-276},
  author={Ayora, Clara and Torres, Victoria and Weber, Barbara and Reichert, Manfred and Pelechano, Vicente},
  url={https://www.sciencedirect.com/science/article/pii/S0950584914001268},
  keywords={Systematic literature review, Business process, Business process variability, Process family, Process-aware information systems, Information Systems},
  abstract={abstract Context The increasing adoption of process-aware information systems (PAISs) such as workflow management systems, enterprise resource planning systems, or case management systems, together with the high variability in business processes (e.g., sales processes may vary depending on the respective products and countries), has resulted in large industrial process model repositories. To cope with this business process variability, the proper management of process variants along the entire process lifecycle becomes crucial. Objective The goal of this paper is to develop a fundamental understanding of business process variability. In particular, the paper will provide a framework for assessing and comparing process variability approaches and the support they provide for the different phases of the business process lifecycle (i.e., process analysis and design, configuration, enactment, diagnosis, and evolution). Method We conducted a systematic literature review (SLR) in order to discover how process variability is supported by existing approaches. Results The SLR resulted in 63 primary studies which were deeply analyzed. Based on this analysis, we derived the VIVACE framework. VIVACE allows assessing the expressiveness of a process modeling language regarding the explicit specification of process variability. Furthermore, the support provided by a process-aware information system to properly deal with process model variants can be assessed with VIVACE as well. Conclusions VIVACE provides an empirically-grounded framework for process engineers that enables them to evaluate existing process variability approaches as well as to select that variability approach meeting their requirements best. Finally, it helps process engineers in implementing PAISs supporting process variability along the entire process lifecycle.}
}

@article{rayyan-727967267,
  title={Empirical evaluation of a decision support model for adopting software product line engineering},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={60},
  pages={77-101},
  author={Tüzün, Eray and Tekinerdogan, Bedir and Kalender, Mert Emin and Bilgen, Semih},
  url={https://www.sciencedirect.com/science/article/pii/S0950584915000026},
  keywords={Systematic literature review, Decision support system, Software product line engineering, Case study design, Software product line engineering feasibility analysis, Software product line transition strategies, Software, Decision Support Techniques},
  abstract={Context The software product line engineering (SPLE) community has provided several different approaches for assessing the feasibility of SPLE adoption and selecting transition strategies. These approaches usually include many rules and guidelines which are very often implicit or scattered over different publications. Hence, for the practitioners it is not always easy to select and use these rules to support the decision making process. Even in case the rules are known, the lack of automated support for storing and executing the rules seriously impedes the decision making process. Objective We aim to evaluate the impact of a decision support system (DSS) on decision-making in SPLE adoption. In alignment with this goal, we provide a decision support model (DSM) and the corresponding DSS. Method First, we apply a systematic literature review (SLR) on the existing primary studies that discuss and present approaches for analyzing the feasibility of SPLE adoption and transition strategies. Second, based on the data extraction and synthesis activities of the SLR, the required questions and rules are derived and implemented in the DSS. Third, for validation of the approach we conduct multiple case studies. Results In the course of the SLR, 31 primary studies were identified from which we could construct 25 aspects, 39 questions and 312 rules. We have developed the DSS tool Transit-PL that embodies these elements. Conclusions The multiple case study validation showed that the adoption of the developed DSS tool is justified to support the decision making process in SPLE adoption.}
}

@article{rayyan-727967268,
  title={Formal verification approaches of self-adaptive systems: A survey},
  year={2019},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={159},
  pages={1853-1862},
  author={Hachicha, Marwa and Halima, Riadh Ben and Kacem, Ahmed Hadj},
  url={https://www.sciencedirect.com/science/article/pii/S1877050919315583},
  keywords={Self-adaptive systems, Challenges, Formal verification, Review, Taxonomy},
  abstract={Today, developing self-adaptive systems is very challenging due to their increasing complexity and dynamism. Consequently, ensuring the correctness of their behavior is a difficult task. In this paper, we present a survey of the different existing approaches proposing the formal verification of self-adaptive systems. To that aim, we discuss several related works in the field. Then, we present a taxonomy of these researches based on some criteria. After that, we present a rich discussion and a comparison between the different existing works. Finally, we identify some perspectives and challenges which can enhance the formal verification of self-adaptive systems.}
}

@article{rayyan-727967269,
  title={Transformed k-nearest neighborhood output distance minimization for predicting the defect density of software projects},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={167},
  pages={110592},
  author={López-Martín, Cuauhtémoc and Villuendas-Rey, Yenny and Azzeh, Mohammad and Bou Nassif, Ali and Banitaan, Shadi},
  url={https://www.sciencedirect.com/science/article/pii/S0164121220300728},
  keywords={Case-based reasoning, ISBSG, Neural networks, Software defect density prediction, Support vector regression, Transformed k-nearest neighborhood output distance minimization, Software},
  abstract={Background Software defect prediction is one of the most important research topics in software engineering. An important product measure to determine the effectiveness of software processes is the defect density (DD). Cased-based reasoning (CBR) has been the prediction technique most widely applied in the software prediction field. The CBR involves k-nearest neighborhood for finding the number (k) of similar software projects selected to be involved in the prediction process. Objective To propose the application of a transformed k-nearest neighborhood output distance minimization (TkDM) algorithm to predict the DD of software projects to compare its prediction accuracy with those obtained from statistical regression, support vector regression, and neural networks. Method Data sets were obtained from the ISBSG release 2018. A leave-one-out cross validation method was performed. Absolute residual was used as the prediction accuracy criterion for models. Results Statistical significance tests among models showed that the TkDM had the best prediction accuracy than those ones from statistical regression, support vector regression, and neural networks. Conclusions A TkDM can be used for predicting the DD of new and enhanced software projects developed and coded in specific platforms and programming languages types.}
}

@article{rayyan-727967270,
  title={A review of cross organizational healthcare data sharing},
  year={2015},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={63},
  pages={425-432},
  author={Azarm-Daigle, Mana and Kuziemsky, Craig and Peyton, Liam},
  url={https://www.sciencedirect.com/science/article/pii/S1877050915024989},
  keywords={Cross-organizational healthcare data sharing, interoperability, literature review, patient privacy, quality of care, technology adoption, Information Dissemination},
  abstract={Increasingly, healthcare is provided by a team of care providers from different organizations. Cross-organizational healthcare data sharing is a major issue in interoperable healthcare organizations. Studies have shown that quality of care can be put at risk when patients are transferred from one organization to another, while the need for protecting patient privacy is sometimes an inhibitor to providing information computing technology (ICT) solutions. This paper presents a systematic literature review of cross-organizational healthcare data sharing. The review includes research related to laws and regulations as well as proposed methodological and ICT solutions. Our methodology for querying, filtering and selecting relevant papers from scientific, academic and general repositories is explained and the selected papers are categorized and compared in terms of scope, contributions, and future directions. Based on this analysis, we outline a possible research direction for developing ICT solutions that healthcare providers and regulators would be willing to adopt. Based on our review, we concluded that inspite of the liberal regulations around data sharing among authorized healthcare providers, these organizations are utterly reluctant to collaborate on patient information. Fear of a breech of personal health information, and the shortage of technological facilitators that are compatible with the existing health information systems, are the main causes of the cross-organizational interoperability problems in the healthcare sector. The existing collaborative technologies require considerable initial investments that the current healthcare system is not willing to spend funds on.}
}

@article{rayyan-727967271,
  title={Entity reconciliation in big data sources: A systematic mapping study},
  year={2017},
  journal={Expert Systems with Applications},
  issn={0957-4174},
  volume={80},
  pages={14-27},
  author={Enríquez, J G and Domínguez-Mayo, F J and Escalona, M J and Ross, M and Staples, G},
  url={https://www.sciencedirect.com/science/article/pii/S0957417417301550},
  keywords={Systematic mapping study, Big data, Entity reconciliation, Heterogeneous databases},
  abstract={The entity reconciliation (ER) problem aroused much interest as a research topic in today's Big Data era, full of big and open heterogeneous data sources. This problem poses when relevant information on a topic needs to be obtained using methods based on: (i) identifying records that represent the same real world entity, and (ii) identifying those records that are similar but do not correspond to the same real-world entity. ER is an operational intelligence process, whereby organizations can unify different and heterogeneous data sources in order to relate possible matches of non-obvious entities. Besides, the complexity that the heterogeneity of data sources involves, the large number of records and differences among languages, for instance, must be added. This paper describes a Systematic Mapping Study (SMS) of journal articles, conferences and workshops published from 2010 to 2017 to solve the problem described before, first trying to understand the state-of-the-art, and then identifying any gaps in current research. Eleven digital libraries were analyzed following a systematic, semiautomatic and rigorous process that has resulted in 61 primary studies. They represent a great variety of intelligent proposals that aim to solve ER. The conclusion obtained is that most of the research is based on the operational phase as opposed to the design phase, and most studies have been tested on real-world data sources, where a lot of them are heterogeneous, but just a few apply to industry. There is a clear trend in research techniques based on clustering/blocking and graphs, although the level of automation of the proposals is hardly ever mentioned in the research work.}
}

@article{rayyan-727967272,
  title={Extracting reusable design decisions for UML-based domain-specific languages: A multi-method study},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={113},
  pages={140-172},
  author={Sobernig, Stefan and Hoisl, Bernhard and Strembeck, Mark},
  url={https://www.sciencedirect.com/science/article/pii/S0164121215002617},
  keywords={Model-driven development, Domain-specific language, Design decision, Design rationale, Domain-specific modeling, Unified modeling language},
  abstract={When developing domain-specific modeling languages (DSMLs), software engineers have to make a number of important design decisions on the DSML itself, or on the software-development process that is applied to develop the DSML. Thus, making well-informed design decisions is a critical factor in developing DSMLs. To support this decision-making process, the model-driven development community has started to collect established design practices in terms of patterns, guidelines, story-telling, and procedural models. However, most of these documentation practices do not capture the details necessary to reuse the rationale behind these decisions in other DSML projects. In this paper, we report on a three-year research effort to compile and to empirically validate a catalog of structured decision descriptions (decision records) for UML-based DSMLs. This catalog is based on design decisions extracted from 90 DSML projects. These projects were identified—among others—via an extensive systematic literature review (SLR) for the years 2005–2012. Based on more than 8,000 candidate publications, we finally selected 84 publications for extracting design-decision data. The extracted data were evaluated quantitatively using a frequent-item-set analysis to obtain characteristic combinations of design decisions and qualitatively to document recurring documentation issues for UML-based DSMLs. We revised the collected decision records based on this evidence and made the decision-record catalog for developing UML-based DSMLs publicly available. Furthermore, our study offers insights into UML usage (e.g. diagram types) and into the adoption of UML extension techniques (e.g. metamodel extensions, profiles).}
}

@article{rayyan-727967273,
  title={A survey of experienced user perceptions about software design patterns},
  year={2013},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={55},
  number={5},
  pages={822-835},
  author={Zhang, Cheng and Budgen, David},
  url={https://www.sciencedirect.com/science/article/pii/S0950584912002297},
  keywords={Survey, Software design, Software design patterns, Software Design, Software},
  abstract={Context Although the concept of the software design pattern is well-established, there is relatively little empirical knowledge about the patterns that experienced users consider to be most valuable. Aim To identify which patterns from the set catalogued by the ‘Gang of Four' are considered to be useful by experienced users, which ones are considered as not being useful, and why this is so. Method We undertook a web-based survey of experienced pattern users, seeking information about their experiences as software developers and maintainers. Our sampling frame consisted of the authors of all of the pattern papers that we had identified in a preceding systematic review of studies of patterns. Results We received 206 usable responses, corresponding to a response rate of 19% from the original sampling frame. Most respondents were involved with software development rather than maintenance. Conclusion While patterns can provide a means of sharing ‘knowledge schemas' between designers, only three patterns were widely regarded as valuable. Around one quarter of the patterns gained very low approval or worse. These observations need to be considered when using patterns; teaching students about the pattern concept; and planning empirical studies about patterns.}
}

@article{rayyan-727967274,
  title={Evaluating and empirically improving the visual syntax of use case diagrams},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={156},
  pages={136-163},
  author={El-Attar, Mohamed},
  url={https://www.sciencedirect.com/science/article/pii/S0164121219301402},
  keywords={UML, Cognitive effectiveness, Use case notation, Visual syntax},
  abstract={Use case modeling is a forefront technique to specify functional requirements of a system. Many research works related to use case modeling have been devoted to improving various aspects of use case modeling and its utilization in software development processes. One key aspect of use case models that has thus far been overlooked by the research community is the visual perception of use case diagrams by its readers. Any model is used transfer a mental idea by a modeler to a model reader. Even if a use case diagram is constructed flawlessly, if it is misread or misinterpreted by its reader then the intrinsic purpose of modeling has failed. This paper provides a two-fold contribution. Firstly, this paper presents an evaluation of the cognitive effectiveness of use case diagrams notation. The evaluation is based on theory principles and empirical evidence mainly from the cognitive science field. Secondly, it provides empirically validated improvements to the use case diagram notation that enhances its cognitive effectiveness. Empirical validation of the improvements is drawn by conducting an industrial survey using business analyst professionals. Empirical validation is also drawn by conducting an experiment using software engineering professionals as subjects.}
}

@article{rayyan-727967275,
  title={Assumptions and their management in software development: A systematic mapping study},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={94},
  pages={82-110},
  author={Yang, Chen and Liang, Peng and Avgeriou, Paris},
  url={https://www.sciencedirect.com/science/article/pii/S0950584916304189},
  keywords={Systematic mapping study, Software development, Assumption, Assumptions management, Software},
  abstract={Context Assumptions are constantly made by stakeholders or generated automatically in software development. However, there is a lack of systematic analysis and comprehensive understanding of the research and practice regarding assumptions and their management. Objective This work aims to explore and analyze the state of the art on assumptions and their management in software development. Method A systematic mapping study that covers the literature from January 2001 to December 2015 on assumptions and their management in software development. Results 134 studies were included: (1) The studies were published in 94 venues, which indicates that assumptions and their management has been a broad topic in software engineering. (2) Only 21 studies defined the assumption concept. (3) Most assumptions are made for or related to the artifacts in requirements engineering and software design, which demonstrates that assumptions should be managed from the early phases of software development. (4) Much effort has been put on Assumptions Making, Description, and Evaluation. Assumptions Maintenance received moderate attention. More than half of the tools identified aim to support assume-guarantee reasoning. For the other tools, most of them can be used to support Assumptions Description. (5) All the identified types of stakeholders are involved in Assumptions Making, followed by Evaluation and Description. Stakeholders involved in requirements engineering, software design, and software construction play a central role in assumptions management. (6) The main challenge is the difficulty of performing assumptions management activities in software development. (7) The identified assumptions management approaches, tools, benefits, and lessons learned are limited to their specific contexts (e.g., context of use). (8) Most of the negative consequences are caused by invalid or implicit assumptions. Conclusions This work provides researchers and practitioners with a reflection of the past fifteen years of research and practice on assumptions and their management in software development.}
}

@article{rayyan-727967276,
  title={Review of IoT applications in agro-industrial and environmental fields},
  year={2017},
  journal={Computers and Electronics in Agriculture},
  issn={0168-1699},
  volume={142},
  pages={283-297},
  author={Talavera, Jesús Martín and Tobón, Luis Eduardo and Gómez, Jairo Alejandro and Culman, María Alejandra and Aranda, Juan Manuel and Parra, Diana Teresa and Quiroz, Luis Alfredo and Hoyos, Adolfo and Garreta, Luis Ernesto},
  url={https://www.sciencedirect.com/science/article/pii/S0168169917304155},
  keywords={Systematic literature review, Agro-industry, Environmental monitoring, Internet of things, IoT},
  abstract={This paper reviews agro-industrial and environmental applications that are using Internet of Things (IoT). It is motivated by the need to identify application areas, trends, architectures and open challenges in these two fields. The underlying survey was developed following a systematic literature review using academic documents written in English and published in peer-reviewed venues from 2006 to 2016. Selected references were clustered into four application domains corresponding to: monitoring, control, logistics, and prediction. Implementation-specific details from each selected reference were compiled to create usage distributions of sensors, actuators, power sources, edge computing modules, communication technologies, storage solutions, and visualization strategies. Finally, the results from the review were compiled into an IoT architecture that represents a wide range of current solutions in agro-industrial and environmental fields.}
}

@article{rayyan-727967277,
  title={The lean gap: A review of lean approaches to large-scale software systems development},
  year={2013},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={86},
  number={11},
  pages={2797-2821},
  author={Pernstål, J and Feldt, R and Gorschek, T},
  url={https://www.sciencedirect.com/science/article/pii/S0164121213001477},
  keywords={Systematic mapping study, Software engineering, Agile software development, Lean software development, Automotive software development, Lean product development, Software},
  abstract={Lean approaches to product development (LPD) have had a strong influence on many industries and in recent years there have been many proponents for lean in software development as it can support the increasing industry need of scaling agile software development. With it's roots in industrial manufacturing and, later, industrial product development, it would seem natural that LPD would adapt well to large-scale development projects of increasingly software-intensive products, such as in the automotive industry. However, it is not clear what kind of experience and results have been reported on the actual use of lean principles and practices in software development for such large-scale industrial contexts. This was the motivation for this study as the context was an ongoing industry process improvement project at Volvo Car Corporation and Volvo Truck Corporation. The objectives of this study are to identify and classify state of the art in large-scale software development influenced by LPD approaches and use this established knowledge to support industrial partners in decisions on a software process improvement (SPI) project, and to reveal research gaps and proposed extensions to LPD in relation to its well-known principles and practices. For locating relevant state of the art we conducted a systematic mapping study, and the industrial applicability and relevance of results and said extensions to LPD were further analyzed in the context of an actual, industrial case. A total of 10,230 papers were found in database searches, of which 38 papers were found relevant. Of these, only 42 percent clearly addressed large-scale development. Furthermore, a majority of papers (76 percent) were non-empirical and many lacked information about study design, context and/or limitations. Most of the identified results focused on eliminating waste and creating flow in the software development process, but there was a lack of results for other LPD principles and practices. Overall, it can be concluded that research in the much hyped field of lean software development is in its nascent state when it comes to large scale development. There is very little support available for practitioners who want to apply lean approaches for improving large-scale software development, especially when it comes to inter-departmental interactions during development. This paper explicitly maps the area, qualifies available research, and identifies gaps, as well as suggests extensions to lean principles relevant for large scale development of software intensive systems.}
}

@article{rayyan-727967278,
  title={Investigating the use of random forest in software effort estimation},
  year={2019},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={148},
  pages={343-352},
  author={Abdelali, Zakrani and Mustapha, Hain and Abdelwahed, Namir},
  url={https://www.sciencedirect.com/science/article/pii/S1877050919300420},
  keywords={Software effort estimation, accuracy evaluation, random forest, regression trees, Software, Trees},
  abstract={Over the last two decades, there has been an important increase in studies dealing with the software development effort estimation (SDEE) using machine learning (ML) techniques that aimed to improve the accuracy of the estimates and to understand the process used to generate these estimates. Among these ML techniques, decision tree-based models have received a considerable scholarly attention thanks to their generalization ability and understandability. However, very few studies have investigated the use of random forest (RF) in software effort estimation. In this paper, a RF model is designed and optimized empirically by varying the values of its key parameters. The performance of the RF is compared with that of classical regression tree (RT). The evaluation was performed through the 30% hold-out validation method using three datasets: ISBSG R8, Tukutuku and COCOMO. To identify the most accurate techniques, we used three widely known accuracy measures: Pred(0.25), MMRE and MdMRE. The results show that the optimized random forest outperforms the regression trees model on all evaluation criteria.}
}

@article{rayyan-727967279,
  title={Measuring and predicting software productivity: A systematic map and review},
  year={2011},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={53},
  number={4},
  pages={317-343},
  author={Petersen, Kai},
  url={https://www.sciencedirect.com/science/article/pii/S0950584910002156},
  keywords={Software development, Measurement, Performance, Efficiency, Prediction, Software productivity, Software},
  abstract={Context Software productivity measurement is essential in order to control and improve the performance of software development. For example, by identifying role models (e.g. projects, individuals, tasks) when comparing productivity data. The prediction is of relevance to determine whether corrective actions are needed, and to discover which alternative improvement action would yield the best results. Objective In this study we identify studies for software productivity prediction and measurement. Based on the identified studies we first create a classification scheme and map the studies into the scheme (systematic map). Thereafter, a detailed analysis and synthesis of the studies is conducted. Method As a research method for systematically identifying and aggregating the evidence of productivity measurement and prediction approaches systematic mapping and systematic review have been used. Results In total 38 studies have been identified, resulting in a classification scheme for empirical research on software productivity. The mapping allowed to identify the rigor of the evidence with respect to the different productivity approaches. In the detailed analysis the results were tabulated and synthesized to provide recommendations to practitioners. Conclusion Risks with simple ratio-based measurement approaches were shown. In response to the problems data envelopment analysis seems to be a strong approach to capture multivariate productivity measures, and allows to identify reference projects to which inefficient projects should be compared. Regarding simulation no general prediction model can be identified. Simulation and statistical process control are promising methods for software productivity prediction. Overall, further evidence is needed to make stronger claims and recommendations. In particular, the discussion of validity threats should become standard, and models need to be compared with each other.}
}

@article{rayyan-727967280,
  title={A literature review on attitudes of health professionals towards health information systems: From e-Health to m-Health},
  year={2014},
  journal={Procedia Technology},
  issn={2212-0173},
  volume={16},
  pages={1317-1326},
  author={Sezgin, Emre and Yıldırım, Sevgi Özkan},
  url={https://www.sciencedirect.com/science/article/pii/S2212017314003752},
  keywords={e-Health, Health Information Systems, Literature Review, Mobile Health, Technology Acceptance, Attitude to Health, Information Systems},
  abstract={This paper presented a literature review research about health professionals' acceptance of HIS and m-Health based on a systematic review procedure. The results were derived from 31 scholar studies of which consisted of 27 HIS studies and 4 m-Health studies. It was aimed to provide insight about acceptance theories and constructs being employed to assess current health information systems and their implementation on mobile platform. Results presented the relevance and contradictions in theories in comparison to HIS and m-Health domains. It is believed that this study can bring new measures in determining influencing factors of m-Health technology users, which can lead the researchers to develop effective models, and so, to assist the developers to improve applications of HIS in the mobile platform.}
}

@article{rayyan-727967281,
  title={Software outsourcing partnership model: An evaluation framework for vendor organizations},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={117},
  pages={402-425},
  author={Ali, Sikandar and Khan, Siffat Ullah},
  url={https://www.sciencedirect.com/science/article/pii/S016412121630019X},
  keywords={Systematic literature review, Critical success factors, Software outsourcing partnership, Software},
  abstract={Software Outsourcing Partnership (SOP) is a new software development paradigm for developing high quality software products. A SOP is different to ordinary software development outsourcing (SDO) relationship. SOP is the enhanced form of conventional outsourcing relationship. The objective of this research paper is to develop a software outsourcing partnership model (SOPM) to identify and analyze factors that are important for vendors in conversion of their existing outsourcing relationship to partnership. We have performed a systematic literature review (SLR) process for the identification of critical success factors (CSFs) from a sample of 111 articles. Further we have categorized the identified CSFs into five partnership levels based on Capability Maturity Model Integration (CMMI) and Software Outsourcing Vendors' Readiness Model (SOVRM). To validate the SLR findings and to find practices for the identified CSFs a questionnaire survey was conducted in the outsourcing industry in which 35 experts, from 8 different countries participated. Two case studies were conducted for evaluation of the SOPM. In this paper our newly developed model, SOPM, has been presented in detail. SOPM has been built with the intent to assist SDO vendor organizations in measuring their capabilities for successful conversion of their contractual outsourcing relationship to outsourcing partnership.}
}

@article{rayyan-727967282,
  title={Architectural design space for modelling and simulation as a service: A review},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={170},
  pages={110752},
  author={Shahin, Mojtaba and Babar, M Ali and Chauhan, Muhammad Aufeef},
  url={https://www.sciencedirect.com/science/article/pii/S0164121220301746},
  keywords={Systematic review, Architecture, Modelling and Simulation as a Service, MSaaS},
  abstract={Modelling and Simulation as a Service (MSaaS) is a promising approach to deploy and execute Modelling and Simulation (M&S) applications quickly and on-demand. An appropriate software architecture is essential to deliver quality M&S applications following the MSaaS concept to a wide range of users. This study aims to characterize the state-of-the-art MSaaS architectures by conducting a systematic review of 31 papers published from 2010 to 2018. Our findings reveal that MSaaS applications are mainly designed using layered architecture style, followed by service-oriented architecture, component-based architecture, and pluggable component-based architecture. We also found that interoperability and deployability have the greatest importance in the architecture of MSaaS applications. In addition, our study indicates that the current MSaaS architectures do not meet the critical user requirements of modern M&S applications appropriately. Based on our results, we recommend that there is a need for more effort and research to (1) design the user interfaces that enable users to build and configure simulation models with minimum effort and limited domain knowledge, (2) provide mechanisms to improve the deployability of M&S applications, and (3) gain a deep insight into how M&S applications should be architected to respond to the emerging user requirements in the military domain.}
}

@article{rayyan-727967283,
  title={Definitions and approaches to model quality in model-based software development – A review of literature},
  year={2009},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={51},
  number={12},
  pages={1646-1669},
  author={Mohagheghi, Parastoo and Dehlen, Vegard and Neple, Tor},
  url={https://www.sciencedirect.com/science/article/pii/S0950584909000457},
  keywords={Systematic review, Modelling, UML, Model-driven development, Model quality, Software},
  abstract={More attention is paid to the quality of models along with the growing importance of modelling in software development. We performed a systematic review of studies discussing model quality published since 2000 to identify what model quality means and how it can be improved. From forty studies covered in the review, six model quality goals were identified; i.e., correctness, completeness, consistency, comprehensibility, confinement and changeability. We further present six practices proposed for developing high-quality models together with examples of empirical evidence. The contributions of the article are identifying and classifying definitions of model quality and identifying gaps for future research.}
}

@article{rayyan-727967284,
  title={Management of quality requirements in agile and rapid software development: A systematic mapping study},
  year={2020},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={123},
  pages={106225},
  author={Behutiye, Woubshet and Karhapää, Pertti and López, Lidia and Burgués, Xavier and Martínez-Fernández, Silverio and Vollmer, Anna Maria and Rodríguez, Pilar and Franch, Xavier and Oivo, Markku},
  url={https://www.sciencedirect.com/science/article/pii/S095058491930240X},
  keywords={Systematic mapping study, Systematic literature reviews, Agile software development, Non-functional requirements, Quality requirements, Rapid software development, Software},
  abstract={Context Quality requirements (QRs) describe the desired quality of software, and they play an important role in the success of software projects. In agile software development (ASD), QRs are often ill-defined and not well addressed due to the focus on quickly delivering functionality. Rapid software development (RSD) approaches (e.g., continuous delivery and continuous deployment), which shorten delivery times, are more prone to neglect QRs. Despite the significance of QRs in both ASD and RSD, there is limited synthesized knowledge on their management in those approaches. Objective This study aims to synthesize state-of-the-art knowledge about QR management in ASD and RSD, focusing on three aspects: bibliometric, strategies, and challenges. Research method Using a systematic mapping study with a snowballing search strategy, we identified and structured the literature on QR management in ASD and RSD. Results We found 156 primary studies: 106 are empirical studies, 16 are experience reports, and 34 are theoretical studies. Security and performance were the most commonly reported QR types. We identified various QR management strategies: 74 practices, 43 methods, 13 models, 12 frameworks, 11 advices, 10 tools, and 7 guidelines. Additionally, we identified 18 categories and 4 non-recurring challenges of managing QRs. The limited ability of ASD to handle QRs, time constraints due to short iteration cycles, limitations regarding the testing of QRs and neglect of QRs were the top categories of challenges. Conclusion Management of QRs is significant in ASD and is becoming important in RSD. This study identified research gaps, such as the need for more tools and guidelines, lightweight QR management strategies that fit short iteration cycles, investigations of the link between QRs challenges and technical debt, and extension of empirical validation of existing strategies to a wider context. It also synthesizes QR management strategies and challenges, which may be useful for practitioners.}
}

@article{rayyan-727967285,
  title={Towards a data governance framework for third generation platforms},
  year={2019},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={151},
  pages={614-621},
  author={Yebenes, Juan and Zorrilla, Marta},
  url={https://www.sciencedirect.com/science/article/pii/S1877050919305447},
  keywords={Industry 4.0, IoT, big data, cloud computing, data bus architecture, Data governance},
  abstract={The fourth industrial revolution considers data as a business asset and therefore this is placed as a central element of the software architecture (data as a service) that will support the horizontal and vertical digitalization of industrial processes. The large volume of data that the environment generates, its heterogeneity and complexity, as well as its reuse for later processes (e.g. analytics, IA) requires the adoption of policies, directives and standards for its right governance. Furthermore, the issues related to the use of resources in the cloud computing must be taken into account with the aim of meeting the requirements of performance and security of the different processes. This article, in the absence of frameworks adapted to this new architecture, proposes an initial schema for developing an effective data governance programme for third generation platforms, that means, a conceptual tool which guides organizations to define, design, develop and deploy services aligned with its vision and business goals in I4.0 era.}
}

@article{rayyan-727967286,
  title={A tertiary study on technical debt: Types, management strategies, research trends, and base information for practitioners},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={102},
  pages={117-145},
  author={Rios, Nicolli and de Mendonça Neto, Manoel Gomes and Spínola, Rodrigo Oliveira},
  url={https://www.sciencedirect.com/science/article/pii/S0950584918300946},
  keywords={Tertiary study, Technical debt, Management strategies, Technical debt types},
  abstract={Context The concept of technical debt (TD) contextualizes problems faced during software evolution considering the tasks that are not carried out adequately during its development. Currently, it is common to associate any impediment related to the software product and its development process to the definition of TD. This can bring confusion and ambiguity in the use of the term. Besides, due to the increasing amount of work in the area, it is difficult to have a comprehensive view of the plethora of proposals on TD management. Objective This paper intends to investigate the current state of research on TD by identifying what research topics have been considered, organizing research directions and practical knowledge that has already been defined, identifying the known types of TD, and organizing what activities, strategies and tools have been proposed to support the management of TD. Method A tertiary study was performed based on a set of five research questions. In total, 13 secondary studies, dated from 2012 to March 2018, were evaluated. Results The results of this tertiary study are beneficial for both practitioners and researchers. We evolved a taxonomy of TD types, identified a list of situations in which debt items can be found in software projects, and organized a map representing the state of the art of activities, strategies and tools to support TD management. Besides, we also summarized some research directions and practical knowledge, and identified the research topics that have been more considered in secondary studies. Conclusion This tertiary study revisited the TD landscape. Its results can help to identify points that still require further investigation in TD research.}
}

@article{rayyan-727967287,
  title={Understanding community participation and engagement in open source software Projects: A systematic mapping study},
  year={2020},
  journal={Journal of King Saud University - Computer and Information Sciences},
  issn={1319-1578},
  author={Kaur, Rajdeep and Kaur Chahal, Kuljit and Saini, Munish},
  url={https://www.sciencedirect.com/science/article/pii/S1319157820305139},
  keywords={Systematic mapping study, Open source software, Community dynamics, Community engagement, Community participation, Software},
  abstract={In the Open Source Software (OSS) paradigm, developers along with users form a community for an OSS project as they share an interest in using/developing the project. Active community engagement is essential for an OSS project to succeed. OSS communities should strive for greater community participation and engagement through the use of tools, practices, and processes. The primary goal of this paper is to presents a review of studies on community participation and engagement in OSS projects based on systematic mapping study and snowballing technique. This study also provides an understanding about the research topics and gaps in the area, utilized research methods and publication venues. We have analyzed 67 research papers related to the study topic. The findings revealed that most of the studies used a combination of survey and questionnaire as a research methodology. We found that community participation and engagement research focuses on 5 main research topics joining process, contribution barriers, motivation, retention, and abandonment. The investigated studies provide more evidence on motivation and contribution barriers but less on the joining process and abandonment. The results presented in this paper will be helpful for researchers to understand the latest trends in this area and identifying the corresponding research gaps.}
}

@article{rayyan-727967288,
  title={Trends in software reuse research: A tertiary study},
  year={2019},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={66},
  pages={103352},
  author={Barros-Justo, José L and Benitti, Fabiane B V and Matalonga, Santiago},
  url={https://www.sciencedirect.com/science/article/pii/S092054891830463X},
  keywords={Tertiary study, Systematic literature review, Software reuse, Trends in software reuse, Software},
  abstract={Context The reuse of software has been a research topic for more than 50 years. Throughout that time, many approaches, tools and proposed techniques have reached maturity. However, it is not yet a widespread practice and some issues need to be further investigated. The latest study on software reuse trends dates back to 2005 and we think that it should be updated. Objective To identify the current trends in software reuse research. Method A tertiary study based on systematic secondary studies published up to July 2018. Results We identified 4,423 works related to software reuse, from which 3,102 were filtered by selection criteria and quality assessment to produce a final set of 56 relevant studies. We identified 30 current research topics and 127 proposals for future work, grouped into three broad categories: Software Product Lines, Other reuse approaches and General reuse topics. Conclusions Frequently reported topics include: Requirements and Testing in the category of Lifecycle phases for Software Product Lines, and Systematic reuse for decision making in the category of General Reuse. The most mentioned future work proposals were Requirements, and Evolution and Variability management for Software Product Lines, and Systematic reuse for decision making. The identified trends, based on future work proposals, demonstrate that software reuse is still an interesting area for research. Researchers can use these trends as a guide to lead their future projects.}
}

@article{rayyan-727967289,
  title={What's up with software metrics? – A preliminary mapping study},
  year={2010},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={83},
  number={1},
  pages={37-51},
  author={Kitchenham, Barbara},
  url={https://www.sciencedirect.com/science/article/pii/S0164121209001599},
  keywords={Mapping study, Software metrics, Secondary study, Literature survey, Empirical evaluation problems, Influential papers, Metronidazole, Software},
  abstract={Background Many papers are published on the topic of software metrics but it is difficult to assess the current status of metrics research. Aim This paper aims to identify trends in influential software metrics papers and assess the possibility of using secondary studies to integrate research results. Method Search facilities in the SCOPUS tool were used to identify the most cited papers in the years 2000–2005 inclusive. Less cited papers were also selected from 2005. The selected papers were classified according factors such as to main topic, goal and type (empirical or theoretical or mixed). Papers classified as “Evaluation studies” were assessed to investigate the extent to which results could be synthesized. Results Compared with less cited papers, the most cited papers were more frequently journal papers, and empirical validation or data analysis studies. However, there were problems with some empirical validation studies. For example, they sometimes attempted to evaluate theoretically invalid metrics and fail to appreciate the importance of the context in which data are collected. Conclusions This paper, together with other similar papers, confirms that there is a large body of research related to software metrics. However, software metrics researchers may need to refine their empirical methodology before they can answer useful empirical questions.}
}

@article{rayyan-727967290,
  title={The maturity of maturity model research: A systematic mapping study},
  year={2012},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={54},
  number={12},
  pages={1317-1339},
  author={Wendler, Roy},
  url={https://www.sciencedirect.com/science/article/pii/S0950584912001334},
  keywords={Systematic mapping study, Maturity models, Design-oriented research, Software management},
  abstract={Context Maturity models offer organizations a simple but effective possibility to measure the quality of their processes. Emerged out of software engineering, the application fields have widened and maturity model research is becoming more important. During the last two decades the publication amount steadily rose as well. Until today, no studies have been available summarizing the activities and results of the field of maturity model research. Objective The objective of this paper is to structure and analyze the available literature of the field of maturity model research to identify the state-of-the-art research as well as research gaps. Method A systematic mapping study was conducted. It included relevant publications of journals and IS conferences. Mapping studies are a suitable method for structuring a broad research field concerning research questions about contents, methods, and trends in the available publications. Results The mapping of 237 articles showed that current maturity model research is applicable to more than 20 domains, heavily dominated by software development and software engineering. The study revealed that most publications deal with the development of maturity models and empirical studies. Theoretical reflective publications are scarce. Furthermore, the relation between conceptual and design-oriented maturity model development was analyzed, indicating that there is still a gap in evaluating and validating developed maturity models. Finally, a comprehensive research framework was derived from the study results and implications for further research are given. Conclusion The mapping study delivers the first systematic summary of maturity model research. The categorization of available publications helps researchers gain an overview of the state-of-the-art research and current research gaps. The proposed research framework supports researchers categorizing their own projects. In addition, practitioners planning to use a maturity model may use the study as starting point to identify which maturity models are suitable for their domain and where limitations exist.}
}

@article{rayyan-727967291,
  title={Creation of Web 2.0 tools ontology to improve learning},
  year={2015},
  journal={Computers in Human Behavior},
  issn={0747-5632},
  volume={51},
  pages={1380-1386},
  author={Kurilovas, Eugenijus and Juskeviciene, Anita},
  url={https://www.sciencedirect.com/science/article/pii/S0747563214005494},
  keywords={Ontology, Collaborative learning, Learning activities, Learning styles, Semantic search, Web 2.0, Learning},
  abstract={The aim of the paper is to present systematic review results on ontology development tools, to establish interconnections between learning styles, preferred learning activities and related Web 2.0 tools, and also to create Web 2.0 tools ontology to interconnect learning activities with relevant Web 2.0 tools. This ontology is necessary for learners to semantically search for suitable Web 2.0 tools while learning in virtual learning environments (VLEs). Suitability of Web 2.0 tools depends on preferred types of learning activities which in its turn depend on preferred learning styles. The research results include: (1) systematic review results on ontology development tools and ontology representation language/formats; (2) established interconnections between learning styles, preferred learning activities, and relevant Web 2.0 tools using sets portrait method, and (3) creating Web 2.0 tools ontology to interconnect preferred learning activities with relevant Web 2.0 tools in VLE. The research results will be implemented in iTEC – pan-European research and development project focused on the design of the future classroom funded by EU 7FP. The research results presented are absolutely novel in scientific literature, and this makes the current study distinct from all other works in the area.}
}

@article{rayyan-727967292,
  title={Chapter 13 - cross-company learning: Handling the data drought},
  year={2015},
  journal={Sharing data and models in software engineering},
  issn={978-0-12-417295-1},
  pages={101-124},
  author={Menzies, Tim and Kocagüneli, Ekrem and Minku, Leandro and Peters, Fayola and Turhan, Burak and Menzies, Tim and Kocagüneli, Ekrem and Minku, Leandro and Peters, Fayola and Turhan, Burak},
  url={https://www.sciencedirect.com/science/article/pii/B9780124172951000138},
  publisher={Morgan Kaufmann},
  address={Boston},
  abstract={In this part of the book Data Science for Software Engineering: Sharing Data and Models, we show that sharing all data is less useful that sharing just the relevant data. There are several useful methods for finding those relevant data regions including simple nearest neighbor, or kNN, algorithms; clustering (to optimize subsequent kNN); and pruning away “bad” regions. Also, we show that with clustering, it is possible to repair missing data in project records.}
}

@article{rayyan-727967293,
  title={Toward successful project management in global software development},
  year={2016},
  journal={International Journal of Project Management},
  issn={0263-7863},
  volume={34},
  number={8},
  pages={1553-1567},
  author={Niazi, Mahmood and Mahmood, Sajjad and Alshayeb, Mohammad and Qureshi, Abdul Majid and Faisal, Kanaan and Cerpa, Narciso},
  url={https://www.sciencedirect.com/science/article/pii/S0263786316300837},
  keywords={Systematic literature review, Empirical study, Project management, Global software development, Knowledge areas, Software},
  abstract={Project management in the context of global software development (GSD) is challenging due to a number of issues. This paper has a two-fold objective: (1) to identify the factors from the literature related to the successful project management in GSD and to validate the identified factors in the real-world practice; (2) to map the identified factors to 10 project management knowledge areas of PMBOK. Our results show a positive correlation between the ranks obtained from the literature and the survey. The results of t-test (i.e., t=1.979, p=0.061¿0.05) show that there is no significant difference between the findings of the literature and survey. Our mapping shows that most of the success factors are related to human resource knowledge area. It is anticipated that the identified success factors can be helpful to practitioners for developing strategic implementation of project management activities in GSD environment.}
}

@article{rayyan-727967294,
  title={Adaptive monitoring: A systematic mapping},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={105},
  pages={161-189},
  author={Zavala, Edith and Franch, Xavier and Marco, Jordi},
  url={https://www.sciencedirect.com/science/article/pii/S0950584918301861},
  keywords={Systematic mapping study, Literature review, State of the art, Adaptive monitoring, Monitor customization, Monitoring reconfiguration},
  abstract={Context Adaptive monitoring is a method used in a variety of domains for responding to changing conditions. It has been applied in different ways, from monitoring systems' customization to re-composition, in different application domains. However, to the best of our knowledge, there are no studies analyzing how adaptive monitoring differs or resembles among the existing approaches. Objective To characterize the current state of the art on adaptive monitoring, specifically to: (a) identify the main concepts in the adaptive monitoring topic; (b) determine the demographic characteristics of the studies published in this topic; (c) identify how adaptive monitoring is conducted and evaluated by the different approaches; (d) identify patterns in the approaches supporting adaptive monitoring. Method We have conducted a systematic mapping study of adaptive monitoring approaches following recommended practices. We have applied automatic search and snowballing sampling on different sources and used rigorous selection criteria to retrieve the final set of papers. Moreover, we have used an existing qualitative analysis method for extracting relevant data from studies. Finally, we have applied data mining techniques for identifying patterns in the solutions. Results We have evaluated 110 studies organized in 81 approaches that support adaptive monitoring. By analyzing them, we have: (1) surveyed related terms and definitions of adaptive monitoring and proposed a generic one; (2) visualized studies' demographic data and arranged the studies into approaches; (3) characterized the main approaches' contributions; (4) determined how approaches conduct the adaptation process and evaluate their solutions. Conclusions This cross-domain overview of the current state of the art on adaptive monitoring may be a solid and comprehensive baseline for researchers and practitioners in the field. Especially, it may help in identifying opportunities of research; for instance, the need of proposing generic and flexible software engineering solutions for supporting adaptive monitoring in a variety of systems.}
}

@article{rayyan-727967295,
  title={Software effort estimation terminology: The tower of Babel},
  year={2006},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={48},
  number={4},
  pages={302-310},
  author={Grimstad, Stein and Jørgensen, Magne and Moløkken-Østvold, Kjetil},
  url={https://www.sciencedirect.com/science/article/pii/S0950584905000674},
  keywords={Software effort estimation, Software estimation guidelines, Structured review, Terminology, Software},
  abstract={It is well documented that the software industry suffers from frequent cost overruns. A contributing factor is, we believe, the imprecise estimation terminology in use. A lack of clarity and precision in the use of estimation terms reduces the interpretability of estimation accuracy results, makes the communication of estimates difficult, and lowers the learning possibilities. This paper reports on a structured review of typical software effort estimation terminology in software engineering textbooks and software estimation research papers. The review provides evidence that the term ‘effort estimate' is frequently used without sufficient clarification of its meaning, and that estimation accuracy is often evaluated without ensuring that the estimated and the actual effort are comparable. Guidelines are suggested on how to reduce this lack of clarity and precision in terminology.}
}

@article{rayyan-727967296,
  title={Architectural tactics for big data cybersecurity analytics systems: A review},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={151},
  pages={81-118},
  author={Ullah, Faheem and Ali Babar, Muhammad},
  url={https://www.sciencedirect.com/science/article/pii/S0164121219300172},
  keywords={Quality attribute, Big data, Architectural tactic, Cybersecurity},
  abstract={Context Big Data Cybersecurity Analytics (BDCA) systems leverage big data technologies for analyzing security events data to protect organizational networks, computers, and data from cyber attacks. Objective We aimed at identifying the most frequently reported quality attributes and architectural tactics for BDCA systems. Method We used Systematic Literature Review (SLR) method for reviewing 74 papers. Result Our findings are twofold: (i) identification of 12 most frequently reported quality attributes for BDCA systems; and (ii) identification and codification of 17 architectural tactics for addressing the identified quality attributes. The identified tactics include six performance tactics, four accuracy tactics, two scalability tactics, three reliability tactics, and one security and usability tactic each. Conclusion Our study reveals that in the context of BDCA (a) performance, accuracy and scalability are the most important quality concerns (b) data analytics is the most critical architectural component (c) despite the significance of interoperability, modifiability, adaptability, generality, stealthiness, and privacy assurance, these quality attributes lack explicit architectural support (d) empirical investigation is required to evaluate the impact of the codified tactics and explore the quality trade-offs and dependencies among the tactics and (e) the reported tactics need to be modelled using a standardized modelling language such as UML.}
}

@article{rayyan-727967297,
  title={Systematic review of organizational motivations for adopting CMM-based SPI},
  year={2008},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={50},
  number={7},
  pages={605-620},
  author={Staples, Mark and Niazi, Mahmood},
  url={https://www.sciencedirect.com/science/article/pii/S0950584907000778},
  keywords={Capability Maturity Model, CMM, CMMI, Software Process Improvement},
  abstract={Background: Software Process Improvement (SPI) is intended to improve software engineering, but can only be effective if used. To improve SPI's uptake, we should understand why organizations adopt SPI. CMM-based SPI approaches are widely known and studied. Objective: We investigated why organizations adopt CMM-based SPI approaches, and how these motivations relate to organizations' size. Method: We performed a systematic review, examining reasons reported in more than forty primary studies. Results: Reasons usually related to product quality and project performance, and less commonly, to process. Organizations reported customer reasons infrequently and employee reasons very rarely. We could not show that reasons related to size. Conclusion: Despite its origins in helping to address customer-related issues for the USAF, CMM-based SPI has mostly been adopted to help organizations improve project performance and product quality issues. This reinforces a view that the goal of SPI is not to improve process per se, but instead to provide business benefits.}
}

@article{rayyan-727967298,
  title={Software-testing education: A systematic literature mapping},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={165},
  pages={110570},
  author={Garousi, Vahid and Rainer, Austen and Lauvås, Per and Arcuri, Andrea},
  url={https://www.sciencedirect.com/science/article/pii/S0164121220300510},
  keywords={Software testing, Systematic literature review, Systematic literature mapping, Education research, Software-engineering education, Software-testing education, Software},
  abstract={Context With the rising complexity and scale of software systems, there is an ever-increasing demand for sophisticated and cost-effective software testing. To meet such a demand, there is a need for a highly-skilled software testing work-force (test engineers) in the industry. To address that need, many university educators worldwide have included software-testing education in their software engineering (SE) or computer science (CS) programs. Many papers have been published in the last three decades (as early as 1992) to share experience from such undertakings. Objective Our objective in this paper is to summarize the body of experience and knowledge in the area of software-testing education to benefit the readers (both educators and researchers) in designing and delivering software testing courses in university settings, and to also conduct further education research in this area. Method To address the above need, we conducted a systematic literature mapping (SLM) to synthesize what the community of educators have published on this topic. After compiling a candidate pool of 307 papers, and applying a set of inclusion/exclusion criteria, our final pool included 204 papers published between 1992 and 2019. Results The topic of software-testing education is becoming more active, as we can see by the increasing number of papers. Many pedagogical approaches (how to best teach testing), course-ware, and specific tools for testing education have been proposed. Many challenges in testing education and insights on how to overcome those challenges have been proposed. Conclusion This paper provides educators and researchers with a classification of existing studies within software-testing education. We further synthesize challenges and insights reported when teaching software testing. The paper also provides a reference (“index”) to the vast body of knowledge and experience on teaching software testing. Our mapping study aims to help educators and researchers to identify the best practices in this area to effectively plan and deliver their software testing courses, or to conduct further education-research in this important area.}
}

@article{rayyan-727967299,
  title={Architecting systems of systems: A tertiary study},
  year={2020},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={118},
  pages={106202},
  author={Cadavid, Héctor and Andrikopoulos, Vasilios and Avgeriou, Paris},
  url={https://www.sciencedirect.com/science/article/pii/S0950584919302083},
  keywords={Tertiary study, Systematic literature review, SoS Architecting, Systems of Systems},
  abstract={Context: The term System of Systems (SoS) has increasingly been used in a wide variety of domains to describe those systems composed of independent constituent systems that collaborate towards a mission that they could not accomplish on their own. There is a significant volume of research by the software architecture community that aims to overcome the challenges involved in architecting SoS, as evidenced by the number of secondary studies in the field published so far. However, the boundaries of such research do not seem to be well defined, at least partially, due to the emergence of SoS-adjacent areas of interest like the Internet of Things.Objective: This paper aims to investigate the current state of research on SoS architecting by synthesizing the demographic data, assessing the quality and the coverage of architecting activities and software quality attributes by the research, and distilling a concept map that reflects a community-wide understanding of the concept of SoS. Method: We conduct what is, to the best of our understanding, the first tertiary study on SoS architecting. Such tertiary study was based on five research questions, and was performed by following the guidelines of Kitchenham et al. In all, 19 secondary studies were evaluated, which is comparable to other tertiary studies. Results: The study illustrates a state of disconnection in the research community, with research gaps in the coverage of particular phases and quality attributes. Furthermore, a more effective approach in classifying systems as SoS is required, as the means of resolving conceptual and terminological overlaps with the related domains. Conclusions: Despite the amount of research in the area of SoS architecting, more coordinated and systematic targeted efforts are required in order to address the identified issues with the current state of research.}
}

@article{rayyan-727967300,
  title={A multivocal literature review on serious games for software process standards education},
  year={2018},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={57},
  pages={36-48},
  author={Calderón, Alejandro and Ruiz, Mercedes and O'Connor, Rory V},
  url={https://www.sciencedirect.com/science/article/pii/S092054891730332X},
  keywords={Systematic literature review, Multivocal literature review, Education, Serious game, Software process standard, Software},
  abstract={Context: The interest in the use of serious games as learning resources for software process standards education and training has increased significantly in recent years. Objective: The main purpose of this work is to record, analyze and characterize the state of the art related to serious games for software process standards education with the goal of identifying the current serious games in terms of the scope, their main features and the perceived benefits of integrating them in software process education, as well as, identifying new research opportunities. Method: The study was conducted as a multivocal literature review that follows a predefined procedure in which studies from the scientific and grey literature are analyzed. Results: A new selection process within the search strategy was defined to conduct this review. 190 papers were retrieved from the literature and 7 papers were selected as primary studies. Our multivocal literature review identified six different serious games for software process education, at the same time analyzed the main methods used to assess them as well as their main outcomes as learning resources. Conclusion: The results of this review reveal that serious games have potential as supporting tools for software process standards education, but that more research and experimental outcomes are needed in order to observe the full potential of serious games as learning resources.}
}

@article{rayyan-727967301,
  title={A systematic review of domain analysis tools},
  year={2010},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={52},
  number={1},
  pages={1-13},
  author={Lisboa, Liana Barachisio and Garcia, Vinicius Cardoso and Lucrédio, Daniel and de Almeida, Eduardo Santana and de Lemos Meira, Silvio Romero and de Mattos Fortes, Renata Pontin},
  url={https://www.sciencedirect.com/science/article/pii/S0950584909000834},
  keywords={Tools, Systematic review, Domain analysis},
  abstract={The domain analysis process is used to identify and document common and variable characteristics of systems in a specific domain. In order to achieve an effective result, it is necessary to collect, organize and analyze several sources of information about different applications in this domain. Consequently, this process involves distinct phases and activities and also needs to identify which artifacts, arising from these activities, have to be traceable and consistent. In this context, performing a domain analysis process without tool support increases the risks of failure, but the used tool should support the complete process and not just a part of it. This article presents a systematic review of domain analysis tools that aims at finding out how the available tools offer support to the process. As a result, the review identified that these tools are usually focused on supporting only one process and there are still gaps in the complete process support. Furthermore, the results can provide insights for new research in the domain engineering area for investigating and defining new tools, and the study also aids in the identification of companies' needs for a domain analysis tool.}
}

@article{rayyan-727967302,
  title={Reconciling software development models: A quasi-systematic review},
  year={2012},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={85},
  number={2},
  pages={351-369},
  author={Magdaleno, Andréa Magalhães and Werner, Cláudia Maria Lima and de Araujo, Renata Mendes},
  url={https://www.sciencedirect.com/science/article/pii/S0164121211002287},
  keywords={Systematic review, Agile, Free/open source software, Plan-driven, Reconciliation among development models, Software process, Software},
  abstract={Purpose The purpose of this paper is to characterize reconciliation among the plan-driven, agile, and free/open source software models of software development. Design/methodology/approach An automated quasi-systematic review identified 42 papers, which were then analyzed. Findings The main findings are: there exist distinct – organization, group and process – levels of reconciliation; few studies deal with reconciliation among the three models of development; a significant amount of work addresses reconciliation between plan-driven and agile development; several large organizations (such as Microsoft, Motorola, and Philips) are interested in trying to combine these models; and reconciliation among software development models is still an open issue, since it is an emerging area and research on most proposals is at an early stage. Research limitations Automated searches may not capture relevant papers in publications that are not indexed. Other data sources not amenable to execution of the protocol were not used. Data extraction was performed by only one researcher, which may increase the risk of threats to internal validity. Implications This characterization is important for practitioners wanting to be current with the state of research. This review will also assist the scientific community working with software development processes to build a common understanding of the challenges that must be faced, and to identify areas where research is lacking. Finally, the results will be useful to software industry that is calling for solutions in this area. Originality/value There is no other systematic review on this subject, and reconciliation among software development models is an emerging area. This study helps to identify and consolidate the work done so far and to guide future research. The conclusions are an important step towards expanding the body of knowledge in the field.}
}

@article{rayyan-727967303,
  title={How to design gamification? A method for engineering gamified software},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={95},
  pages={219-237},
  author={Morschheuser, Benedikt and Hassan, Lobna and Werder, Karl and Hamari, Juho},
  url={https://www.sciencedirect.com/science/article/pii/S095058491730349X},
  keywords={Software engineering, Design science research, Game design, Gameful design, Gamification, Persuasive technology, Playfulness, Software},
  abstract={Context Since its inception around 2010, gamification has become one of the top technology and software trends. However, gamification has also been regarded as one of the most challenging areas of software engineering. Beyond traditional software design requirements, designing gamification requires the command of disciplines such as (motivational/behavioral) psychology, game design, and narratology, making the development of gamified software a challenge for traditional software developers. Gamification software inhabits a finely tuned niche of software engineering that seeks for both high functionality and engagement; beyond technical flawlessness, gamification has to motivate and affect users. Consequently, it has also been projected that most gamified software is doomed to fail. Objective This paper seeks to advance the understanding of designing gamification and to provide a comprehensive method for developing gamified software. Method We approach the research problem via a design science research approach; firstly, by synthesizing the current body of literature on gamification design methods and by interviewing 25 gamification experts, producing a comprehensive list of design principles for developing gamified software. Secondly, and more importantly, we develop a detailed method for engineering of gamified software based on the gathered knowledge and design principles. Finally, we conduct an evaluation of the artifacts via interviews of ten gamification experts and implementation of the engineering method in a gamification project. Results As results of the study, we present the method and key design principles for engineering gamified software. Based on the empirical and expert evaluation, the developed method was deemed as comprehensive, implementable, complete, and useful. We deliver a comprehensive overview of gamification guidelines and shed novel insights into the nature of gamification development and design discourse. Conclusion This paper takes first steps towards a comprehensive method for gamified software engineering.}
}

@article{rayyan-727967304,
  title={All-Learning: The state of the art of the models and the methodologies educational with ICT},
  year={2018},
  journal={Telematics and Informatics},
  issn={0736-5853},
  volume={35},
  number={4},
  pages={944-953},
  author={Ramirez, Gabriel M and Collazos, Cesar A and Moreira, Fernando},
  url={https://www.sciencedirect.com/science/article/pii/S073658531730271X},
  keywords={Integration, Education, ICT, Methodology, Model},
  abstract={This paper presents a systematic review of models and methodologies that integrate information and communication technologies (ICT) and education. The systematic review was based on the methodology of Kitchenham. The steps used and developed correspond to the steps proposed in the methodology. The starting point of the review are the research questions, then keywords, selection of the databases, definition of the inclusion and exclusion criteria, the definition of the search chains, search process and selection of papers, the analyzes of the paper and the results of the systematic review to answer the questions posed. In the systematic review, 919 papers were found in 6 academic databases and 129 relevant papers were selected. The work developed intends to know the different models and methodologies that integrate the ICT and the education. Develop an analysis and characterize to find common elements among models and methodologies. The idea is to find limitations, disadvantages and spaces that allow to propose a new model. This systematic review is the first step in the development of a doctoral research in which the development of a U-Learning model based on Connective Learning and Experience Learning is proposed.}
}

@article{rayyan-727967305,
  title={Service composition approaches in IoT: A systematic review},
  year={2018},
  journal={Journal of Network and Computer Applications},
  issn={1084-8045},
  volume={120},
  pages={61-77},
  author={Asghari, Parvaneh and Rahmani, Amir Masoud and Javadi, Hamid Haj Seyyed},
  url={https://www.sciencedirect.com/science/article/pii/S1084804518302376},
  keywords={Systematic literature review, QoS, Internet of things, Service composition, Smart objects},
  abstract={The Internet of Things (IoT) signifies to an overall system of interconnected physical Things utilizing existing correspondence conventions. One critical inquiry remains in what manner can make and communicate the management of provided services for smart devices by an assortment of protest things that substituted and joined capably. Service composition process permits the interaction between user requirements and smart objects of IoT environment. Leveraging on the service discovery procedure can be influenced on finding the desired services. Consequently, choosing suitable services is the main challenge that covers functionality and required quality to combine several services as the integrated composite service in the IoT. The service composition process has been broadly considered with regards to web suppliers and business processes in the IoT. Currently, the IoT environment identifies the dynamic relationship topics on physical processes that are combined as the enhanced web services heterogeneously. This paper focuses on several service composition approaches that are applied in the IoT environment based on the Systematic Literature Review (SLR) method. The aim of this study is to analytically and statistically categorize and analyze the current research techniques on the service composition in the IoT (published between 2012 and 2017). A technical taxonomy is presented for the service composition approaches according to content of the existing studies that are selected with SLR method in this review with respect to functional and non-functional aspects in service composition approaches. The functional aspect emphasizes on verifying the behavior of service composition approach and the non-functional aspect considers the Quality of Service (QoS) in IoT environment. The approaches are compared with each other according to some technical aspects such as system correctness factors in functional properties approaches, and (QoS) factors, presented algorithms, and existing platforms in non-functional approaches. The advantages and disadvantages of each selected approach discussed as well as providing some hints for solving their weaknesses. A brief contribution to this literature is as follows: (1) Presenting a SLR method for the service composition approaches in IoT, (2) Addressing a discussion of the main challenges, (3) Providing the future research directions and open perspectives.}
}

@article{rayyan-727967306,
  title={Toward semantic IoT load inference attention management for facilitating healthcare and public health collaboration: A survey},
  year={2020},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={177},
  pages={371-378},
  author={Lim, Sachiko and Rahmani, Rahim},
  url={https://www.sciencedirect.com/science/article/pii/S1877050920323188},
  keywords={Crisis management, Federated edge-cloud computing, Healthcare, Public Health, semantic interoperability, Semantic IoT, Semantics},
  abstract={The health of individuals and populations requires concerted and collaborative efforts by healthcare, public health, social care, and personal health management. The inter-sectoral collaborations are more crucial than ever, especially when facing public health crises, including the ongoing pandemic of coronavirus disease-2019 (COVID-19). Although the capabilities of healthcare and public health systems have increased with a dramatic boost in the use of the Internet of Things (IoT), such IoT-enabled systems are often operating in silos. A pressing need, thus, is the seamless integration of those currently incompatible systems. A promising solution is to leverage semantic technologies to increase interoperability among such systems. Therefore, this article aims to: conduct a systematic review on the current state-of-the-art semantic IoT solutions used in health domain; identify the associated challenges; propose a federated edge-cloud semantic IoT architecture to facilitate the healthcare and public health (HC-PH) collaborations for the health and well-being of the individuals and populations.}
}

@article{rayyan-727967307,
  title={A systematic review of shared visualisation to achieve common ground},
  year={2015},
  journal={Journal of Visual Languages & Computing},
  issn={1045-926X},
  volume={28},
  pages={83-99},
  author={Yusoff, Nor'ain Mohd and Salim, Siti Salwah},
  url={https://www.sciencedirect.com/science/article/pii/S1045926X1400158X},
  keywords={Human–computer interaction, Collaborative design, Shared visualisation, Teamwork},
  abstract={This paper reports a systematic review of shared visualisation based on fifteen papers from 2000 to 2013. The findings identified five shared visualisation strategies that represent the ways implemented to process data sharing and knowledge to arrive at the desired level of understanding. Four visualisation techniques were also identified to show how shared cognition is made possible in designing tools for mediating data or knowledge among the users involved. These findings provide research opportunities in integrating rich interactive data visualisation for mobile-based technologies as an effective mean in supporting collaborative work. Finally, social, task and cognitive elements which can be significantly supported by shared visualisation and a guideline for future researchers seeking to design shared visualisation-based systems are presented.}
}

@article{rayyan-727967308,
  title={A systematic review of search-based testing for non-functional system properties},
  year={2009},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={51},
  number={6},
  pages={957-976},
  author={Afzal, Wasif and Torkar, Richard and Feldt, Robert},
  url={https://www.sciencedirect.com/science/article/pii/S0950584908001833},
  keywords={Systematic review, Non-functional system properties, Search-based software testing},
  abstract={Search-based software testing is the application of metaheuristic search techniques to generate software tests. The test adequacy criterion is transformed into a fitness function and a set of solutions in the search space are evaluated with respect to the fitness function using a metaheuristic search technique. The application of metaheuristic search techniques for testing is promising due to the fact that exhaustive testing is infeasible considering the size and complexity of software under test. Search-based software testing has been applied across the spectrum of test case design methods; this includes white-box (structural), black-box (functional) and grey-box (combination of structural and functional) testing. In addition, metaheuristic search techniques have also been applied to test non-functional properties. The overall objective of undertaking this systematic review is to examine existing work into non-functional search-based software testing (NFSBST). We are interested in types of non-functional testing targeted using metaheuristic search techniques, different fitness functions used in different types of search-based non-functional testing and challenges in the application of these techniques. The systematic review is based on a comprehensive set of 35 articles obtained after a multi-stage selection process and have been published in the time span 1996–2007. The results of the review show that metaheuristic search techniques have been applied for non-functional testing of execution time, quality of service, security, usability and safety. A variety of metaheuristic search techniques are found to be applicable for non-functional testing including simulated annealing, tabu search, genetic algorithms, ant colony methods, grammatical evolution, genetic programming (and its variants including linear genetic programming) and swarm intelligence methods. The review reports on different fitness functions used to guide the search for each of the categories of execution time, safety, usability, quality of service and security; along with a discussion of possible challenges in the application of metaheuristic search techniques.}
}

@article{rayyan-727967309,
  title={Towards innovation measurement in the software industry},
  year={2013},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={86},
  number={5},
  pages={1390-1407},
  author={Edison, Henry and bin Ali, Nauman and Torkar, Richard},
  url={https://www.sciencedirect.com/science/article/pii/S0164121213000058},
  keywords={Systematic literature review, Empirical study, Measurement, Metrics, Innovation, Software},
  abstract={In today's highly competitive business environments with shortened product and technology life cycle, it is critical for software industry to continuously innovate. This goal can be achieved by developing a better understanding and control of the activities and determinants of innovation. Innovation measurement initiatives assess innovation capability, output and performance to help develop such an understanding. This study explores various aspects relevant to innovation measurement ranging from definitions, measurement frameworks and metrics that have been proposed in literature and used in practice. A systematic literature review followed by an online questionnaire and interviews with practitioners and academics were employed to identify a comprehensive definition of innovation that can be used in software industry. The metrics for the evaluation of determinants, inputs, outputs and performance were also aggregated and categorised. Based on these findings, a conceptual model of the key measurable elements of innovation was constructed from the findings of the systematic review. The model was further refined after feedback from academia and industry through interviews.}
}

@article{rayyan-727967310,
  title={Considering rigor and relevance when evaluating test driven development: A systematic review},
  year={2014},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={56},
  number={4},
  pages={375-394},
  author={Munir, Hussan and Moayyed, Misagh and Petersen, Kai},
  url={https://www.sciencedirect.com/science/article/pii/S0950584914000135},
  keywords={Productivity, External code quality, Internal code quality, Test-driven development (TDD), Test-last development (TLD)},
  abstract={Context Test driven development (TDD) has been extensively researched and compared to traditional approaches (test last development, TLD). Existing literature reviews show varying results for TDD. Objective This study investigates how the conclusions of existing literature reviews change when taking two study quality dimension into account, namely rigor and relevance. Method In this study a systematic literature review has been conducted and the results of the identified primary studies have been analyzed with respect to rigor and relevance scores using the assessment rubric proposed by Ivarsson and Gorschek 2011. Rigor and relevance are rated on a scale, which is explained in this paper. Four categories of studies were defined based on high/low rigor and relevance. Results We found that studies in the four categories come to different conclusions. In particular, studies with a high rigor and relevance scores show clear results for improvement in external quality, which seem to come with a loss of productivity. At the same time high rigor and relevance studies only investigate a small set of variables. Other categories contain many studies showing no difference, hence biasing the results negatively for the overall set of primary studies. Given the classification differences to previous literature reviews could be highlighted. Conclusion Strong indications are obtained that external quality is positively influenced, which has to be further substantiated by industry experiments and longitudinal case studies. Future studies in the high rigor and relevance category would contribute largely by focusing on a wider set of outcome variables (e.g. internal code quality). We also conclude that considering rigor and relevance in TDD evaluation is important given the differences in results between categories and in comparison to previous reviews.}
}

@article{rayyan-727967311,
  title={A systematic review of security requirements engineering},
  year={2010},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={32},
  number={4},
  pages={153-165},
  author={Mellado, Daniel and Blanco, Carlos and Sánchez, Luis E and Fernández-Medina, Eduardo},
  url={https://www.sciencedirect.com/science/article/pii/S0920548910000255},
  keywords={Systematic review, Requirements engineering, Security, Security requirements, Secure development, Security engineering, Security requirements engineering},
  abstract={One of the most important aspects in the achievement of secure software systems in the software development process is what is known as Security Requirements Engineering. However, very few reviews focus on this theme in a systematic, thorough and unbiased manner, that is, none of them perform a systematic review of security requirements engineering, and there is not, therefore, a sufficiently good context in which to operate. In this paper we carry out a systematic review of the existing literature concerning security requirements engineering in order to summarize the evidence regarding this issue and to provide a framework/background in which to appropriately position new research activities.}
}

@article{rayyan-727967312,
  title={Systematic mapping study: On the coverage of aspect-oriented methodologies for the early phases of the software development life cycle},
  year={2020},
  journal={Journal of King Saud University - Computer and Information Sciences},
  issn={1319-1578},
  author={Pinciroli, Fernando and Barros Justo, Jose Luis and Forradellas, Raymundo},
  url={https://www.sciencedirect.com/science/article/pii/S1319157820305231},
  keywords={Evidence-based software engineering, Systematic mapping study, Systematic review, Aspect-oriented paradigm benefits, Aspect-oriented paradigm challenges, Aspect-oriented system development, Aspect-oriented tools, notations and techniques, Software},
  abstract={Although the number of aspect-oriented software development techniques and tools proposed by the scientific literature have been increasing since the late 80′s, the evidence about the benefits that the aspect-oriented paradigm have reached in real-world settings is scarce. Our objective is to identify and classify the aspect-oriented software development methodologies used to reduce the effort and costs of moving from traditional approaches to the aspect-oriented approach in real-world settings. We conducted a systematic mapping study (SMS). Our search strategies retrieved a set of 3212 papers out of which 115 were selected as relevant studies. We defined eight categories to classify these studies: aspect-oriented methodologies proposed for early aspects; development early phases covered; notations; modeling techniques; supporting tools; aspect-oriented methodologies used in real-world settings; benefits reported and unsolved issues. As a result, 39 named methodologies were reported; they cover the business modeling (14 papers), requirements (93 papers), test cases (1 paper) and design (41 papers) phases of the software development life cycle (SDLC); we found 36 different notations, with UML as the most mentioned (66 papers); 22 modeling techniques were found, where the use cases were the ones that appeared the most, on 43 occasions; 43 support tools of which none was repeated in more than 3 articles; 15 applications in real-world settings; 17 benefits, modularization being the most mentioned with 4 occurrences; and 15 pending improvement opportunities. Finally, we have obtained conclusions: the literature analyzed demonstrates that there are no prevailing standards on methodologies and notations, beyond that the most employed are those belonging to the OMG; we have not found any methodology that includes all phases of the SDLC; the diagrams corresponding to OMG's standards account for 78% of the results in the facet of modeling techniques; there are no tools that have the greatest predilection and the most used is mentioned only three times; in addition, 41% of the articles do not mention any tool; the evidence of aspect-oriented methodologies application in real-world settings reached just the 10%, although the declared benefits are coincident with those promised in the aspect-orientated literature. The mentioned pending issues can guide new studies (RQ8).}
}

@article{rayyan-727967313,
  title={Publication trends in gamification: A systematic mapping study},
  year={2018},
  journal={Computer Science Review},
  issn={1574-0137},
  volume={27},
  pages={33-44},
  author={Kasurinen, Jussi and Knutas, Antti},
  url={https://www.sciencedirect.com/science/article/pii/S1574013716301769},
  keywords={Systematic literature review, Crowdsourcing, Serious games, Gamification, Games for health, MOOCs, Proof-of-concept studies},
  abstract={The term gamification and gamified systems are a trending area of research. However, gamification can indicate several different things, such as applying the game-like elements into the design of the user interface of a software, but not all gamification is necessarily associated with software products. Overall, it is unclear what different aspects are studied under the umbrella of ‘gamification', and what is the current state of the art in the gamification research. In this paper, 1164 gamification studies are analyzed and classified based on their focus areas and the research topics to establish what the research trends in gamification are. Based on the results, e-learning and proof-of-concept studies in the ecological lifestyle and sustainability, assisting computer science studies and improving motivation are the trendiest areas of gamification research. Currently, the most common types of research are the proof-of-concept studies, and theoretical works on the different concepts and elements of gamification.}
}

@article{rayyan-727967314,
  title={Cross-company vs. single-company web effort models using the Tukutuku database: An extended study},
  year={2008},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={81},
  number={5},
  pages={673-690},
  author={Mendes, Emilia and Di Martino, Sergio and Ferrucci, Filomena and Gravino, Carmine},
  url={https://www.sciencedirect.com/science/article/pii/S0164121207002385},
  abstract={In 2004 [Kitchenham, B.A., Mendes, E., 2004a. Software productivity measurement using multiple size measures. IEEE Transactions on Software Engineering 30 (12), 1023–1035, Kitchenham, B.A., Mendes, E., 2004b. A comparison of cross-company and single-company effort estimation models for web applications. In: Proceedings Evaluation and Assessment in Software Engineering (EASE' 04), pp. 47–55] (S1) investigated, using data on 63 Web projects, to what extent a cross-company cost model could be successfully employed to estimate development effort for single-company Web projects. Their effort models were built using Forward Stepwise Regression (SWR) and they found that cross-company predictions were significantly worse than single-company predictions. This study S1 was extended by Mendes and Kitchenham [Mendes, E., Kitchenham, B.A., 2004. Further comparison of cross-company and within company effort estimation models for web applications. In: Proceedings International Software Metrics Symposium (METRICS'04), Chicago, Illinois, September 11–17th, 2004. IEEE Computer Society, pp. 348–357] (S2), who used SWR and Case-based reasoning (CBR), and data on 67 Web projects from the Tukutuku database. They built two cross-company and one single-company models and found that both SWR cross-company models and CBR cross-company data provided predictions significantly worse than single-company predictions. Since 2004 another 83 projects were volunteered to the Tukutuku database, and recently used by Mendes et al. [Mendes, E., Di Martino, S., Ferrucci, F., Gravino, C., in press. Effort estimation: How valuable is it for a web company to use a cross-company data set, compared to using its own single-company data set? In: Proceedings of International World Wide Web Conference (WWW'07), Banff, Canada, 8–12 May] (S3), who partially replicated Mendes and Kitchenham's study (S2), using SWR and CBR. They corroborated some of S2's findings (SWR cross-company model and the CBR cross-company data provided predictions significantly worse than single-company predictions) however they replicated only part of S2. The objective of this paper (S4) is therefore to extend Mendes et al.'s work and fully replicate S2. We used the same dataset used in S3, and our results corroborated most of those obtained in S2. The main difference between S2 and our study was that one of our SWR cross-company models showed significantly similar predictions to the single-company model, which contradicts the findings from S2.}
}

@article{rayyan-727967315,
  title={A systematic review of foresight in project management literature},
  year={2015},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={64},
  pages={792-799},
  author={Silva, Marisa},
  url={https://www.sciencedirect.com/science/article/pii/S1877050915027659},
  keywords={Foresight, Project Management, systematic review.},
  abstract={Projects, like companies, do not fail overnight. When a company fails, this is often not much related to operational aspects but to an inability to think holistically about the driving forces that may change its business landscape in a disruptive way. Based on this, organizations are nowadays seeking different approaches to cope with uncertainty, and as a result, Foresight as a supporting tool to long range planning is gaining popularity at corporate and governmental level. Given that projects share with Foresight the same orientation towards the future and both lead with uncertainty, it is thus relevant to ask whether Foresight can be used to improve Project Management practice. In order to research into these questions, this paper conducted a systematic review on the topic of Foresight in leading Project Management literature. The review revealed that an explicit relationship between Foresight and Project Management exists, and although with limitations, evidence suggests that there is value in adopting Foresight. This study makes a contribution to the body of empirical works in this field and is intended to be primarily used by Project Management practitioners and practically-oriented academics who are interested in developing fresh insights into new approaches for better management of projects.}
}

@article{rayyan-727967316,
  title={Modelling and simulation considerations for an end-to-end supply chain system},
  year={2020},
  journal={Computers & Industrial Engineering},
  issn={0360-8352},
  volume={150},
  pages={106870},
  author={Chilmon, Barbara and Tipi, Nicoleta S},
  url={https://www.sciencedirect.com/science/article/pii/S0360835220305659},
  keywords={Systematic literature review, End-to-end supply chain, Simulation},
  abstract={The efforts of this review paper are twofold: to provide an insightful examination of various contributions to knowledge surrounding simulation methods within an end-to-end supply chain and to guide research agenda by indicating generic elements required to model such systems using simulation. The authors examined 255 publications from 21 peer-reviewed journals in the field of an end-to-end supply chain and simulation using a systematic literature review approach. Each publication was thoroughly reviewed to capture best practices and key characteristics relative to simulation modelling techniques used in the context of complex end-to-end supply chain systems. This allowed for identification of generic elements required to model such systems, which were grouped into Structural, Computational and System Organization pillars. This research contributes to the body of knowledge by defining generic aspects of simulation modelling techniques used to study properties and attributes of complex end-to-end supply chains. The paper advances the theoretical understanding of the simulation methods used and applicability of simulation methodology in modelling end-to-end supply chain systems. The research presents the key findings from the use of simulation in modelling end-to-end supply chains and the main ways in which this modelling technique has informed research and practise.}
}

@article{rayyan-727967317,
  title={Game theory applications in systems-of-systems engineering: A literature review and synthesis},
  year={2019},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={153},
  pages={154-165},
  author={Axelsson, Jakob},
  url={https://www.sciencedirect.com/science/article/pii/S1877050919307252},
  keywords={literature review, game theory, System-of-systems},
  abstract={Systems-of-systems (SoS) are becoming increasingly common in more and more domains, spreading from the initial focus on government-controlled areas such as defense to open market industries. This implies that collaborative SoS are becoming more important, where the constituents need to be given incentives to join and remain within the SoS. Game theory has been proposed as a framework to model and analyze such SoS mechanisms. It aims at providing incentives to the independently operated and managed constituents. This paper presents a systematic literature review on the applications of game theory to SoS engineering, together with a synthesis aiming at capturing the best practices for doing such an analysis. The main conclusions are that game theory can be applied to SoS in a wide range of application areas, and deal with problems related to acquisition, design, and operations. In particular, the operational formation of SoS are well suited for this kind of analysis, and it often requires the use of simulation techniques. However, most results in the field lack a validation in practice.}
}

@article{rayyan-727967318,
  title={A survey on business processes management suites},
  year={2017},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={51},
  pages={71-86},
  author={Meidan, A and García-García, J A and Escalona, M J and Ramos, I},
  url={https://www.sciencedirect.com/science/article/pii/S092054891630040X},
  keywords={Systematic literature review, Quality model, Survey, Business Process Management Suite, Open source},
  abstract={Over the last decade, processes have become an important asset for daily life in organizations because an adequate Business Processes Management (BPM) of an organization (e.g. software development companies) can help achieve organizational objectives. Especially, it is important to efficiently manage these processes vital for the organizational performance in order to continually improve, therefore increasing productivity and competitiveness within the organization (e.g. software processes in software companies). This management is associated with the process lifecycle and, at present, there are many tools (Business Process Management Suites, BPMS) for managing this lifecycle. However, all BPMSs do not provide full support for this lifecycle what makes it more difficult to choose the right BPMS (according to the needs of the organization). This paper presents a survey on BPMS highlighting each phase of the process lifecycle what enables organizations to compare specific BPMS according to their own organizational objectives. This survey has been carried out using a methodology that combines systematic literature review with quality models. This methodology has been used successfully in other contexts. Finally, this paper also describes how this survey has been instantiated on specific open source BPMSs.}
}

@article{rayyan-727967319,
  title={Early software defect prediction: A systematic map and review},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={144},
  pages={216-239},
  author={Özakıncı, Rana and Tarhan, Ayça},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218301213},
  keywords={Systematic literature review, Systematic mapping, Software quality, Early defect prediction, Prediction model, Software defect, Software},
  abstract={Context Software defect prediction is a trending research topic, and a wide variety of the published papers focus on coding phase or after. A limited number of papers, however, includes the prior (early) phases of the software development lifecycle (SDLC). Objective The goal of this study is to obtain a general view of the characteristics and usefulness of Early Software Defect Prediction (ESDP) models reported in scientific literature. Method A systematic mapping and systematic literature review study has been conducted. We searched for the studies reported between 2000 and 2016. We reviewed 52 studies and analyzed the trend and demographics, maturity of state-of-research, in-depth characteristics, success and benefits of ESDP models. Results We found that categorical models that rely on requirement and design phase metrics, and few continuous models including metrics from requirements phase are very successful. We also found that most studies reported qualitative benefits of using ESDP models. Conclusion We have highlighted the most preferred prediction methods, metrics, datasets and performance evaluation methods, as well as the addressed SDLC phases. We expect the results will be useful for software teams by guiding them to use early predictors effectively in practice, and for researchers in directing their future efforts.}
}

@article{rayyan-727967320,
  title={Empirical research methodologies and studies in Requirements Engineering: How far did we come?},
  year={2014},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={95},
  pages={1-9},
  author={Daneva, Maya and Damian, Daniela and Marchetto, Alessandro and Pastor, Oscar},
  url={https://www.sciencedirect.com/science/article/pii/S0164121214001460},
  keywords={Research Design},
  abstract={Since the inception of the RE conference series (1992), both researchers and practitioners in the RE community have acknowledged the significance of empirical evaluation as an instrument to gain knowledge about various aspects of RE phenomena and the validity of our research results. A significant number of empirical studies have been conducted in the search for knowledge about RE problems as well as evidence of successful and less successful application of proposed solutions. This editorial presents the progress empirical RE research has made since 1992. Based on a search in the Scopus digital library, we report from an analysis of peer-reviewed systematic literature reviews and mapping studies to showcase major areas of RE research that use methods from the Empirical Software Engineering paradigm. We summarize prior empirical research in RE and introduce the contributors to this special issue on empirical research methodologies and studies in RE.}
}

@article{rayyan-727967321,
  title={An overview of assessing the quality of peer review reports of scientific articles},
  year={2019},
  journal={International Journal of Information Management},
  issn={0268-4012},
  volume={46},
  pages={286-293},
  author={Sizo, Amanda and Lino, Adriano and Reis, Luis Paulo and Rocha, Álvaro},
  url={https://www.sciencedirect.com/science/article/pii/S0268401218304857},
  keywords={Systematic literature review, Peer review, Reviewers' report assessment, Reviewers' report quality},
  abstract={Assuring the quality control of publications in the scientific literature is one of the main challenges of the peer review process. Consequently, there has been an increasing demand for computing solutions that will help to maintain the quality of this process. Recently, the use of Artificial Intelligence techniques has been highlighted, applied in the detection of plagiarism, bias, among other functions. The assessment of the reviewer's review has also been considered as important in the process, but, little is known about it, for instance, which techniques have been applied in this assessment or which criteria have been assessed. Therefore, this systematic literature review aims to find evidence regarding the computational approaches that have been used to evaluate reviewers' reports. In order to achieve this, five online databases were selected, from which 72 articles were identified that met the inclusion criteria of this review, all of which have been published since 2000. The result returned 10 relevant studies meeting the evaluation requirements of scientific article reviews. The review revealed that mechanisms to rank review reports according to a score, as well as the word analysis, are the most common tools, and that there is no consensus on quality criteria. The systematic literature review has shown that reviewers' report assessment is a valid tool for maintaining quality throughout the process. However, it still needs to be further developed if it is to be used as a resource which surpass a single conference or journal, making the peer review process more rigorous and less based on random choice.}
}

@article{rayyan-727967322,
  title={Test case design for context-aware applications: Are we there yet?},
  year={2017},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={88},
  pages={1-16},
  author={de Sousa Santos, Ismayle and de Castro Andrade, Rossana Maria and Rocha, Lincoln Souza and Matalonga, Santiago and de Oliveira, Káthia Marçal and Travassos, Guilherme Horta},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917302513},
  keywords={Software testing, Systematic review, Context aware application, Awareness},
  abstract={Context Current software systems have increasingly implemented context-aware adaptations to handle the diversity of conditions of their surrounding environment. Therefore, people are becoming used to a variety of context-aware software systems (CASS). This context-awareness brings challenges to the software construction and testing because the context is unpredictable and may change at any time. Therefore, software engineers need to consider the dynamic context changes while testing CASS. Different test case design techniques (TCDT) have been proposed to support the testing of CASS. However, to the best of our knowledge, there is no analysis of these proposals on the advantages, limitations and their effective support to context variation during testing. Objective To gather empirical evidence on TCDT concerned with CASS by identifying, evaluating and synthesizing knowledge available in the literature. Method To undertake a secondary study (quasi-Systematic Literature Review) on TCDT for CASS regarding their assessed quality characteristics, used coverage criteria, test type, and test technique. Results From 833 primary studies published between 2004 and 2014, just 17 studies regard the design of test cases for CASS. Most of them focus on functional suitability. Furthermore, some of them take into account the changes in the context by providing specific test cases for each context configuration (static perspective) during the test execution. These 17 studies revealed five challenges affecting the design of test cases and 20 challenges regarding the testing of CASS. Besides, seven TCDT are not empirically evaluated. Conclusion A few TCDT partially support the testing of CASS. However, it has not been observed evidence on any TCDT supporting the truly context-aware testing, which that can adapt the expected output based on the context variation (dynamic perspective) during the test execution. It is an open issue deserving greater attention from researchers to increase the testing coverage and ensure users confidence in CASS.}
}

@article{rayyan-727967323,
  title={The use of machine learning algorithms in recommender systems: A systematic review},
  year={2018},
  journal={Expert Systems with Applications},
  issn={0957-4174},
  volume={97},
  pages={205-227},
  author={Portugal, Ivens and Alencar, Paulo and Cowan, Donald},
  url={https://www.sciencedirect.com/science/article/pii/S0957417417308333},
  keywords={Machine learning, Application domains, Machine learning algorithms, Performance metrics, Recommender systems, Systematic review of the literature, Algorithms, Learning},
  abstract={Recommender systems use algorithms to provide users with product or service recommendations. Recently, these systems have been using machine learning algorithms from the field of artificial intelligence. However, choosing a suitable machine learning algorithm for a recommender system is difficult because of the number of algorithms described in the literature. Researchers and practitioners developing recommender systems are left with little information about the current approaches in algorithm usage. Moreover, the development of recommender systems using machine learning algorithms often faces problems and raises questions that must be resolved. This paper presents a systematic review of the literature that analyzes the use of machine learning algorithms in recommender systems and identifies new research opportunities. The goals of this study are to (i) identify trends in the use or research of machine learning algorithms in recommender systems; (ii) identify open questions in the use or research of machine learning algorithms; and (iii) assist new researchers to position new research activity in this domain appropriately. The results of this study identify existing classes of recommender systems, characterize adopted machine learning approaches, discuss the use of big data technologies, identify types of machine learning algorithms and their application domains, and analyzes both main and alternative performance metrics.}
}

@article{rayyan-727967324,
  title={Continuous experimentation and the cyber–physical systems challenge: An overview of the literature and the industrial perspective},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={170},
  pages={110781},
  author={Giaimo, Federico and Andrade, Hugo and Berger, Christian},
  url={https://www.sciencedirect.com/science/article/pii/S016412122030193X},
  keywords={Software engineering, Cyber–physical systems, Continuous Experimentation},
  abstract={Context: New software development patterns are emerging aiming at accelerating the process of delivering value. One is Continuous Experimentation, which allows to systematically deploy and run instrumented software variants during development phase in order to collect data from the field of application. While currently this practice is used on a daily basis on web-based systems, technical difficulties challenge its adoption in fields where computational resources are constrained, e.g., cyber–physical systems and the automotive industry. Objective: This paper aims at providing an overview of the engagement on the Continuous Experimentation practice in the context of cyber–physical systems. Method: A systematic literature review has been conducted to investigate the link between the practice and the field of application. Additionally, an industrial multiple case study is reported. Results: The study presents the current state-of-the-art regarding Continuous Experimentation in the field of cyber–physical systems. The current perspective of Continuous Experimentation in industry is also reported. Conclusions: The field has not reached maturity yet. More conceptual analyses are found than solution proposals and the state-of-practice is yet to be achieved. However it is expected that in time an increasing number of solutions will be proposed and validated.}
}

@article{rayyan-727967325,
  title={Agile requirements prioritization in practice: Results of an industrial survey},
  year={2020},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={176},
  pages={3446-3455},
  author={Jarzȩbowicz, Aleksander and Sitko, Natalia},
  url={https://www.sciencedirect.com/science/article/pii/S1877050920319475},
  keywords={Agile Requirements Engineering, Survey, Business Value, Requirements Prioritization, Value-Based Software Engineering},
  abstract={Agile software development stresses the importance of providing the customer with a product of a maximized business value. To achieve that, requirements prioritization is used. Agile development methods like Scrum define guidelines for prioritization, however practitioners do not necessarily have to follow them. Our goal was to investigate the industry practice related to requirements prioritization process, including its timing, participants, criteria used and prioritization techniques applied. We designed an on-line questionnaire (based on literature review) and conducted a survey involving practitioners from Polish IT industry. We received 69 valid responses indicating requirements prioritization practices in industrial Agile projects. We found out that despite the fact that business value is the most common criterion used to prioritize requirements, other criteria like complexity, stability and mutual interdependencies are considered as well. Other findings indicate that consideration of such multiple criteria requires different viewpoints, thus making requirements prioritization a process that has to involve many participants of different roles and competencies.}
}

@article{rayyan-727967326,
  title={Narrowing impact factors for innovative software project management},
  year={2015},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={64},
  pages={957-963},
  author={Maranhão, Robson and Marinho, Marcelo and de Moura, Hermano},
  url={https://www.sciencedirect.com/science/article/pii/S1877050915027489},
  keywords={Systematic literature review, Innovation in Projects Management, Innovative Software Projects., Software Project Management, Software},
  abstract={A large number of project management approaches do not consider the impact that innovations have on projects. Innovation is one of the keys to success in organization, however, the threats identified by innovation in a project day-to-day are real and expectations in a project are often high. Innovative Software Project has a high level of uncertainty and complexity, leading us to suggest that we need a specific approach to manage these threats. The use of management innovation in project can be a determining factor in project success. This paper discusses main impact factors related to Innovative Software Project Management (ISPM) from the findings of systematic literature review about ISPM, aiming to understand how these factors can affect ISPM and contribute to the improvement and success of software projects.}
}

@article{rayyan-727967327,
  title={A systematic review to merge discourses: Interoperability, integration and cyber-physical systems},
  year={2018},
  journal={Journal of Industrial Information Integration},
  issn={2452-414X},
  volume={9},
  pages={14-23},
  author={Gürdür, Didem and Asplund, Fredrik},
  url={https://www.sciencedirect.com/science/article/pii/S2452414X17300687},
  keywords={Maturity models, Data visualization, Interoperability assessment, Interoperability measurement, Tool integration, Tool interoperability},
  abstract={Cyber-physical systems (CPS) are developed through the cooperation of several engineering disciplines. Powerful software tools are utilized by each individual discipline, but it remains challenging to connect these into tool chains for increased efficiency. To support this endeavour, the literature on interoperability assessment was surveyed to identify concepts valuable to transfer from the interoperability to the tool integration research field. Implementation options, types of interoperability and domains described in interoperability assessment models were concepts identified as directly transferable. To avoid the problems with uptake that plague the models identified, visual analytics is suggested as a vehicle for the transfer. Furthermore, based on the use of non-functional properties as an underlying motivation for these models, cost, performance and sustainability are suggested as a common base for future research in both discourses.}
}

@article{rayyan-727967328,
  title={A model for evaluation of enterprise architecture quality},
  year={2020},
  journal={Evaluation and Program Planning},
  issn={0149-7189},
  volume={83},
  pages={101853},
  author={Mirsalari, Seyedeh Reyhaneh and Ranjbarfard, Mina},
  url={https://www.sciencedirect.com/science/article/pii/S0149718920301579},
  keywords={Enterprise architecture, Enterprise architecture evaluation, Enterprise architecture measurement, Enterprise architecture quality attributes},
  abstract={Today, most organizations use an enterprise architecture (EA) approach as a tool to increase the power of management on the organization's information technology. Enterprise architecture is a set of processes that helps an organization to translate its vision into an effective change in the organization's scope by providing a clear understanding of its current state. The purpose of this research is to identify EA quality attributes and its evaluation indicators in the organization. This study was conducted by using mixed method, including qualitative and quantitative parts. In the qualitative section, a variety of EA evaluation indicators were identified by a systematic literature review (SLR) approach, then in the quantitative section the survey data were collected by a questionnaire prepared based on the qualitative part and then exploratory factor analysis (EFA) and confirmatory factor analysis (CFA) were performed. This research presents an EA evaluation model that has seven main quality attributes including alignment and integrity, quality of EA products and services, security, maintainability and portability, reliability, reusability and scalability, and 30 indicators that address all aspects of enterprise architecture. Through this model, organizations can evaluate the quality of implemented EA or AS-IS status of EA and take steps to improve it.}
}

@article{rayyan-727967329,
  title={Capturing software architecture knowledge for pattern-driven design},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={169},
  pages={110714},
  author={Farshidi, Siamak and Jansen, Slinger and van der Werf, Jan Martijn},
  url={https://www.sciencedirect.com/science/article/pii/S0164121220301552},
  keywords={Quality attributes, Architectural patterns, Architectural styles, Design decisions, Knowledge acquisition, Software},
  abstract={Context: Software architecture is a knowledge-intensive field. One mechanism for storing architecture knowledge is the recognition and description of architectural patterns. Selecting architectural patterns is a challenging task for software architects, as knowledge about these patterns is scattered among a wide range of literature. Method: We report on a systematic literature review, intending to build a decision model for the architectural pattern selection problem. Moreover, twelve experienced practitioners at software-producing organizations evaluated the usability and usefulness of the extracted knowledge. Results: An overview is provided of 29 patterns and their effects on 40 quality attributes. Furthermore, we report in which systems the 29 patterns are applied and in which combinations. The practitioners confirmed that architectural knowledge supports software architects with their decision-making process to select a set of patterns for a new problem. We investigate the potential trends among architects to select patterns. Conclusion: With the knowledge available, architects can more rapidly select and eliminate combinations of patterns to design solutions. Having this knowledge readily available supports software architects in making more efficient and effective design decisions that meet their quality concerns.}
}

@article{rayyan-727967330,
  title={Requirements monitoring frameworks: A systematic review},
  year={2016},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={80},
  pages={89-109},
  author={Vierhauser, Michael and Rabiser, Rick and Grünbacher, Paul},
  url={https://www.sciencedirect.com/science/article/pii/S0950584916301288},
  keywords={Systematic literature review, Requirements monitoring, Systems of systems},
  abstract={Context Software systems today often interoperate with each other, thus forming a system of systems (SoS). Due to the scale, complexity, and heterogeneity of SoS, determining compliance with their requirements is challenging, despite the range of existing monitoring approaches. The fragmented research landscape and the diversity of existing approaches, however, make it hard to understand and analyze existing research regarding its suitability for SoS. Objective The aims of this paper are thus to systematically identify, describe, and classify existing approaches for requirements-based monitoring of software systems at runtime. Specifically, we (i) analyze the characteristics and application areas of monitoring approaches proposed in different domains, we (ii) systematically identify frameworks supporting requirements monitoring, and finally (iii) analyze their support for requirements monitoring in SoS. Method We performed a systematic literature review (SLR) to identify existing monitoring approaches and to classify their key characteristics and application areas. Based on this analysis we selected requirements monitoring frameworks, following a definition by Robinson, and analyzed them regarding their support for requirements monitoring in SoS. Results We identified 330 publications, which we used to produce a comprehensive overview of the landscape of requirements monitoring approaches. We analyzed these publications regarding their support for Robinson's requirements monitoring layers, resulting in 37 identified frameworks. We investigated how well these frameworks support requirements monitoring in SoS. Conclusions We conclude that most existing approaches are restricted to certain kinds of checks, particular types of events and data, and mostly also limited to one particular architectural style and technology. This lack of flexibility makes their application in an SoS context difficult. Also, systematic and automated variability management is still missing. Regarding their evaluation, many existing frameworks focus on measuring the performance overhead, while only few frameworks have been assessed in cases studies with real-world systems.}
}

@article{rayyan-727967331,
  title={Exploring principles of user-centered agile software development: A literature review},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={61},
  pages={163-181},
  author={Brhel, Manuel and Meth, Hendrik and Maedche, Alexander and Werder, Karl},
  url={https://www.sciencedirect.com/science/article/pii/S0950584915000129},
  keywords={Systematic literature review, Agile software development, User-centered design, Software},
  abstract={Context In the last decade, software development has been characterized by two major approaches: agile software development, which aims to achieve increased velocity and flexibility during the development process, and user-centered design, which places the goals and needs of the system's end-users at the center of software development in order to deliver software with appropriate usability. Hybrid development models, referred to as user-centered agile software development (UCASD) in this article, propose to combine the merits of both approaches in order to design software that is both useful and usable. Objective This paper aims to capture the current state of the art in UCASD approaches and to derive generic principles from these approaches. More specifically, we investigate the following research question: Which principles constitute a user-centered agile software development approach? Method We conduct a systematic review of the literature on UCASD. Identified works are analyzed using a coding scheme that differentiates four levels of UCASD: the process, practices, people/social and technology dimensions. Through subsequent synthesis, we derive generic principles of UCASD. Results We identified and analyzed 83 relevant publications. The analysis resulted in a comprehensive coding system and five principles for UCASD: (1) separate product discovery and product creation, (2) iterative and incremental design and development, (3) parallel interwoven creation tracks, (4) continuous stakeholder involvement, and (5) artifact-mediated communication. Conclusion Our paper contributes to the software development body of knowledge by (1) providing a broad overview of existing works in the area of UCASD, (2) deriving an analysis framework (in form a coding system) for works in this area, going beyond former classifications, and (3) identifying generic principles of UCASD and associating them with specific practices and processes.}
}

@article{rayyan-727967332,
  title={A systematic review of domain analysis solutions for product lines},
  year={2009},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={82},
  number={12},
  pages={1982-2003},
  author={Khurum, Mahvish and Gorschek, Tony},
  url={https://www.sciencedirect.com/science/article/pii/S016412120900154X},
  keywords={Systematic review, Domain analysis, Domain modeling, Domain scoping, Empirical evidence, Usability, Usefulness},
  abstract={Domain analysis is crucial and central to software product line engineering (SPLE) as it is one of the main instruments to decide what to include in a product and how it should fit in to the overall software product line. For this reason many domain analysis solutions have been proposed both by researchers and industry practitioners. Domain analysis comprises various modeling and scoping activities. This paper presents a systematic review of all the domain analysis solutions presented until 2007. The goal of the review is to analyze the level of industrial application and/or empirical validation of the proposed solutions with the purpose of mapping maturity in terms of industrial application, as well as to what extent proposed solutions might have been evaluated in terms of usability and usefulness. The finding of this review indicates that, although many new domain analysis solutions for software product lines have been proposed over the years, the absence of qualitative and quantitative results from empirical application and/or validation makes it hard to evaluate the potential of proposed solutions with respect to their usability and/or usefulness for industry adoption. The detailed results of the systematic review can be used by individual researchers to see large gaps in research that give opportunities for future work, and from a general research perspective lessons can be learned from the absence of validation as well as from good examples presented. From an industry practitioner view, the results can be used to gauge as to what extent solutions have been applied and/or validated and in what manner, both valuable as input prior to industry adoption of a domain analysis solution.}
}

@article{rayyan-727967333,
  title={A systematic review of requirements change management},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={93},
  pages={163-185},
  author={Jayatilleke, Shalinka and Lai, Richard},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917304664},
  keywords={Systematic review, Agile, Requirements change management},
  abstract={Context Software requirements are often not set in concrete at the start of a software development project; and requirements changes become necessary and sometimes inevitable due to changes in customer requirements and changes in business rules and operating environments; hence, requirements development, which includes requirements changes, is a part of a software process. Previous work has shown that failing to manage software requirements changes well is a main contributor to project failure. Given the importance of the subject, there's a plethora of research work that discuss the management of requirements change in various directions, ways and means. An examination of these works suggests that there's a room for improvement. Objective In this paper, we present a systematic review of research in Requirements Change Management (RCM) as reported in the literature. Method We use a systematic review method to answer four key research questions related to requirements change management. The questions are: (1) What are the causes of requirements changes? (2) What processes are used for requirements change management? (3) What techniques are used for requirements change management? and (4) How do organizations make decisions regarding requirements changes? These questions are aimed at studying the various directions in the field of requirements change management and at providing suggestions for future research work. Results The four questions were answered; and the strengths and weaknesses of existing techniques for RCM were identified. Conclusions This paper has provided information about the current state-of-the-art techniques and practices for RCM and the research gaps in existing work. Benefits, risks and difficulties associated with RCM are also made available to software practitioners who will be in a position of making better decisions on activities related to RCM. Better decisions will lead to better planning which will increase the chance of project success.}
}

@article{rayyan-727967334,
  title={Requirements engineering: A systematic mapping study in agile software development},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={139},
  pages={32-50},
  author={Curcio, Karina and Navarro, Tiago and Malucelli, Andreia and Reinehr, Sheila},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218300141},
  keywords={Systematic mapping study, Requirements engineering, Agile software development, Software},
  abstract={Context Requirements engineering in agile software development is a relatively recent software engineering topic and it is not completely explored and understood. The understanding of how this process works on agile world needs a deeper analysis. Objective The goal of this paper is to map the subject area of requirements engineering in agile context to identify the main topics that have been researched and to identify gaps to develop future researches. It is also intended to identify the obstacles that practitioners face when using agile requirements engineering. Method A systematic mapping study was conducted and as a result 2171 papers were initially identified and further narrowed to 104 by applying exclusion criteria and analysis. Conclusion After completing the classification and the analysis of the selected studies it was possible to identify 15 areas (13 based on SWEBOK) where researches were developed. Five of such areas points to the need of future researches, among them are requirements elicitation, change management, measuring requirements, software requirements tools and comparative studies between traditional and agile requirements. In this research, some obstacles that practitioners face dealing with requirements engineering in agile context were also identified. They are related to environment, people and resources.}
}

@article{rayyan-727967335,
  title={Towards understanding the underlying structure of motivational factors for software engineers to guide the definition of motivational programs},
  year={2012},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={85},
  number={2},
  pages={216-226},
  author={da Silva, Fabio Q B and França, A César C},
  url={https://www.sciencedirect.com/science/article/pii/S0164121210003390},
  keywords={Empirical software engineering, Software development, Motivation, People management, Software},
  abstract={Aim In this article, factors influencing the motivation of software engineers is studied with the goal of guiding the definition of motivational programs. Method Using a set of 20 motivational factors compiled in a systematic literature review and a general theory of motivation, a survey questionnaire was created to evaluate the influence of these factors on individual motivation. Then, the questionnaire was applied on a semi-random sample of 176 software engineers from 20 software companies located in Recife-PE, Brazil. Results The survey results show the actual level of motivation for each motivator in the target population. Using principal component analysis on the values of all motivators, a five factor structure was identified and used to propose a guideline for the creation of motivational programs for software engineers. Conclusions The five factor structure provides an intuitive categorization for the set of variables and can be used to explain other motivational models presented in the literature. This contributes to a better understanding of motivation in software engineering.}
}

@article{rayyan-727967336,
  title={A systematic review of strategies and computer-based intervention (CBI) for reading comprehension of children with autism},
  year={2013},
  journal={Research in Autism Spectrum Disorders},
  issn={1750-9467},
  volume={7},
  number={9},
  pages={1111-1121},
  author={Khowaja, Kamran and Salim, Siti Salwah},
  url={https://www.sciencedirect.com/science/article/pii/S1750946713001116},
  keywords={Systematic review, Autism, Computer-based intervention, Reading comprehension, Vocabulary, Only Child, Child, Autistic Disorder},
  abstract={This paper presents a systematic review of relevant published studies on reading comprehension for children with autism, focusing on vocabulary instruction and text comprehension instruction from years 2000 to 2011. This systematic review attempts to address three specific research questions: strategies of vocabulary instruction and text comprehension instruction used, computer-based intervention (CBI) used or developed during study, and the effectiveness of using CBI for teaching children with autism. There are five strategies of vocabulary instruction and seven strategies of text comprehension instruction. Results indicate that two strategies of vocabulary instruction, multimedia methods and explicit instruction were found to be more commonly used than the other three. On the same note, question answering strategy of text comprehension instruction was discovered to be used more often than the other six. Results also indicate that children with autism can benefit from the strategies of reading comprehension and that the use of CBI as a mode of instruction for reading comprehension improved learning of children. This is clearly evident judging from the performance of children between pre-tests and post-tests of studies in which CBI was used. However, due to heterogeneity of participants, this is not always the case; a few studies reported no improvement in the learning of children with autism.}
}

@article{rayyan-727967337,
  title={A Systematic Mapping Study driven by the margin of error},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={144},
  pages={439-449},
  author={Kosar, Tomaž and Bohra, Sudev and Mernik, Marjan},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218301353},
  keywords={Software engineering, Systematic review, Systematic Mapping Study, Reliability, Margin of error},
  abstract={Until recently, many Systematic Literature Reviews (SLRs) and Systematic Mapping Studies (SMSs) have been proposed. However, when SMS is performed on a broad topic with a large amount of primary studies, the cost of assessment of all primary studies requires unjustified resources. In this paper, a new approach is introduced for performing SMSs, called SMS driven by the margin of error. The main objective of the described work was to decrease the assessment cost of primary studies by stopping the process of classification of primary studies when enough evidence has been collected. We introduced a statistical approach with random sampling and a margin of error into the design of SMSs when a topic under discussion is broad with a large number of primary studies. In this paper, SMS driven by the margin of error was applied on three different use cases: SMS on Domain-Specific Languages, SMS on Template-based Code Generation, and SMS on Software Reliability Modeling, where it was shown that the proposed approach reduced the cost of assessing primary studies and quantified the reliability of SMS.}
}

@article{rayyan-727967338,
  title={Explaining the privacy paradox: A systematic review of literature investigating privacy attitude and behavior},
  year={2018},
  journal={Computers & Security},
  issn={0167-4048},
  volume={77},
  pages={226-261},
  author={Gerber, Nina and Gerber, Paul and Volkamer, Melanie},
  url={https://www.sciencedirect.com/science/article/pii/S0167404818303031},
  keywords={Literature review, Information privacy, Predictor variables, Privacy paradox, User psychology, Privacy},
  abstract={Although survey results show that the privacy of their personal data is an important issue for online users worldwide, most users rarely make an effort to protect this data actively and often even give it away voluntarily. Privacy researchers have made several attempts to explain this dichotomy between privacy attitude and behavior, usually referred to as ‘privacy paradox'. While they proposed different theoretical explanations for the privacy paradox, as well as empirical study results concerning the relationship of individual factors on privacy behavior and attitude, no comprehensive explanation for the privacy paradox has been found so far. We aim to shed light on the privacy paradox phenomenon by summarizing the most popular theoretical privacy paradox explanations and identifying the factors that are most relevant for the prediction of privacy attitude and behavior. Since many studies focus on the behavioral intention instead of the actual behavior, we decided to consider this topic as well. Based on a literature review, we identify all factors that significantly predict one of the three privacy aspects and report the corresponding standardized effect sizes (β). The results provide strong evidence for the theoretical explanation approach called ‘privacy calculus', with possibly gained benefits being among the best predictors for disclosing intention as well as actual disclosure. Other strong predictors for privacy behavior are privacy intention, willingness to disclose, privacy concerns and privacy attitude. Demographic variables play a minor role, only gender was found to weakly predict privacy behavior. Privacy attitude was best predicted by internal variables like trust towards the website, privacy concerns or computer anxiety. Despite the multiplicity of survey studies dealing with user privacy, it is not easy to draw overall conclusions, because authors often refer to slightly different constructs. We suggest the privacy research community to agree on a shared definition of the different privacy constructs to allow for conclusions beyond individual samples and study designs.}
}

@article{rayyan-727967339,
  title={A systematic review of immersive virtual reality applications for higher education: Design elements, lessons learned, and research agenda},
  year={2020},
  journal={Computers & Education},
  issn={0360-1315},
  volume={147},
  pages={103778},
  author={Radianti, Jaziar and Majchrzak, Tim A and Fromm, Jennifer and Wohlgenannt, Isabell},
  url={https://www.sciencedirect.com/science/article/pii/S0360131519303276},
  keywords={Augmented and virtual reality, Cooperative/collaborative learning, Distance education and online learning, Human–computer interface, Media in education, Immersion},
  abstract={Researchers have explored the benefits and applications of virtual reality (VR) in different scenarios. VR possesses much potential and its application in education has seen much research interest lately. However, little systematic work currently exists on how researchers have applied immersive VR for higher education purposes that considers the usage of both high-end and budget head-mounted displays (HMDs). Hence, we propose using systematic mapping to identify design elements of existing research dedicated to the application of VR in higher education. The reviewed articles were acquired by extracting key information from documents indexed in four scientific digital libraries, which were filtered systematically using exclusion, inclusion, semi-automatic, and manual methods. Our review emphasizes three key points: the current domain structure in terms of the learning contents, the VR design elements, and the learning theories, as a foundation for successful VR-based learning. The mapping was conducted between application domains and learning contents and between design elements and learning contents. Our analysis has uncovered several gaps in the application of VR in the higher education sphere—for instance, learning theories were not often considered in VR application development to assist and guide toward learning outcomes. Furthermore, the evaluation of educational VR applications has primarily focused on usability of the VR apps instead of learning outcomes and immersive VR has mostly been a part of experimental and development work rather than being applied regularly in actual teaching. Nevertheless, VR seems to be a promising sphere as this study identifies 18 application domains, indicating a better reception of this technology in many disciplines. The identified gaps point toward unexplored regions of VR design for education, which could motivate future work in the field.}
}

@article{rayyan-727967340,
  title={A systematic review on search-based refactoring},
  year={2017},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={83},
  pages={14-34},
  author={Mariani, Thainá and Vergilio, Silvia Regina},
  url={https://www.sciencedirect.com/science/article/pii/S0950584916303779},
  keywords={Refactoring, Evolutionary algorithms, Search-based software engineering},
  abstract={Context: To find the best sequence of refactorings to be applied in a software artifact is an optimization problem that can be solved using search techniques, in the field called Search-Based Refactoring (SBR). Over the last years, the field has gained importance, and many SBR approaches have appeared, arousing research interest. Objective: The objective of this paper is to provide an overview of existing SBR approaches, by presenting their common characteristics, and to identify trends and research opportunities. Method: A systematic review was conducted following a plan that includes the definition of research questions, selection criteria, a search string, and selection of search engines. 71 primary studies were selected, published in the last sixteen years. They were classified considering dimensions related to the main SBR elements, such as addressed artifacts, encoding, search technique, used metrics, available tools, and conducted evaluation. Results: Some results show that code is the most addressed artifact, and evolutionary algorithms are the most employed search technique. Furthermore, most times, the generated solution is a sequence of refactorings. In this respect, the refactorings considered are usually the ones of the Fowler's Catalog. Some trends and opportunities for future research include the use of models as artifacts, the use of many objectives, the study of the bad smells effect, and the use of hyper-heuristics. Conclusions: We have found many SBR approaches, most of them published recently. The approaches are presented, analyzed, and grouped following a classification scheme. The paper contributes to the SBR field as we identify a range of possibilities that serve as a basis to motivate future researches.}
}

@article{rayyan-727967341,
  title={Prioritization based taxonomy of cloud-based outsource software development challenges: Fuzzy AHP analysis},
  year={2020},
  journal={Applied Soft Computing},
  issn={1568-4946},
  volume={95},
  pages={106557},
  author={Akbar, Muhammad Azeem and Shameem, Mohammad and Mahmood, Sajjad and Alsanad, Ahmed and Gumaei, Abdu},
  url={https://www.sciencedirect.com/science/article/pii/S1568494620304968},
  keywords={Challenges, Cloud-based outsource software development (COSD), Fuzzy analytical hierarchy process (FAHP), Software},
  abstract={Cloud-Based Outsource Software Development (COSD) is a new methodology adopted by organizations to develop software using teams of knowledge workers located across the globe using cloud computing services. However, there is a lack of understanding of challenges associated with successful execution of COSD projects. The objective of this study is to identify and prioritize the challenges that influence COSD projects. First, we conducted a Systematic Literature Review (SLR) and identified 21 challenges that impact COSD projects. Next, a questionnaire survey was developed based on the SLR findings to collect feedback from industry practitioners. Finally, we applied the Fuzzy Analytical Hierarchy Process (FAHP) to rank the identified challenges for COSD projects. We also present a prioritization-based taxonomy of the identified challenges which will help practitioners to focus on the critical areas for successful implementation of COSD projects.}
}

@article{rayyan-727967342,
  title={A systematic review of IP traceback schemes for denial of service attacks},
  year={2016},
  journal={Computers & Security},
  issn={0167-4048},
  volume={56},
  pages={111-139},
  author={Singh, Karanpreet and Singh, Paramvir and Kumar, Krishan},
  url={https://www.sciencedirect.com/science/article/pii/S0167404815000930},
  keywords={Systematic review, Distributed denial-of-service attacks, IP traceback, Packet logging, Packet marking},
  abstract={Internet has always been vulnerable to a variety of security threats as it was originally designed without apprehending the prospect of security concerns. Modern era has seen diverse nature of attacks possible on the Internet, including the most perilous attack, Distributed Denial of Service (DDoS) attacks. In such an attack, a large number of compromised systems coordinate with each other so as to direct gigantic magnitude of attack traffic toward the victim, depleting its tangible and intangible network resources. To further exacerbate the situation, these compromised systems usually disguise their identity by capitalizing on IP address spoofing. IP traceback is the class of techniques used to identify the actual source of network packets. In this paper, we followed a systematic approach to comprehensively review and categorize 275 works representing existing IP traceback literature. The paper also provides an in-depth analysis of different IP traceback approaches, their functional classes and the evaluation metrics. Based on the literature review, we also answered a set of research questions to understand the current trends in IP traceback. Various issues, challenges and avenues for future research in the area of IP traceback are also discussed.}
}

@article{rayyan-727967343,
  title={The use of bibliography enriched features for automatic citation screening},
  year={2019},
  journal={Journal of Biomedical Informatics},
  issn={1532-0464},
  volume={94},
  pages={103202},
  author={Olorisade, Babatunde Kazeem and Brereton, Pearl and Andras, Peter},
  url={https://www.sciencedirect.com/science/article/pii/S1532046419301200},
  keywords={Text mining, Systematic reviews, Citation screening automation, Computing methodologies, Feature enrichment},
  abstract={Context Citation screening (also called study selection) is a phase of systematic review process that has attracted a growing interest on the use of text mining (TM) methods to support it to reduce time and effort. Search results are usually imbalanced between the relevant and the irrelevant classes of returned citations. Class imbalance among other factors has been a persistent problem that impairs the performance of TM models, particularly in the context of automatic citation screening for systematic reviews. This has often caused the performance of classification models using the basic title and abstract data to ordinarily fall short of expectations. Objective In this study, we explore the effects of using full bibliography data in addition to title and abstract on text classification performance for automatic citation screening. Methods We experiment with binary and Word2vec feature representations and SVM models using 4 software engineering (SE) and 15 medical review datasets. We build and compare 3 types of models (binary-non-linear, Word2vec-linear and Word2vec-non-linear kernels) with each dataset using the two feature sets. Results The bibliography enriched data exhibited consistent improved performance in terms of recall, work saved over sampling (WSS) and Matthews correlation coefficient (MCC) in 3 of the 4 SE datasets that are fairly large in size. For the medical datasets, the results vary, however in the majority of cases the performance is the same or better. Conclusion Inclusion of the bibliography data provides the potential of improving the performance of the models but to date results are inconclusive.}
}

@article{rayyan-727967344,
  title={A review on predicting student's performance using data mining techniques},
  year={2015},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={72},
  pages={414-422},
  author={Shahiri, Amirah Mohamed and Husain, Wahidah and Rashid, Nur'aini Abdul},
  url={https://www.sciencedirect.com/science/article/pii/S1877050915036182},
  keywords={educational data mining, performance prediction, Student performance},
  abstract={Predicting students performance becomes more challenging due to the large volume of data in educational databases. Currently in Malaysia, the lack of existing system to analyze and monitor the student progress and performance is not being addressed. There are two main reasons of why this is happening. First, the study on existing prediction methods is still insufficient to identify the most suitable methods for predicting the performance of students in Malaysian institutions. Second is due to the lack of investigations on the factors affecting students achievements in particular courses within Malaysian context. Therefore, a systematical literature review on predicting student performance by using data mining techniques is proposed to improve students achievements. The main objective of this paper is to provide an overview on the data mining techniques that have been used to predict students performance. This paper also focuses on how the prediction algorithm can be used to identify the most important attributes in a students data. We could actually improve students achievement and success more effectively in an efficient way using educational data mining techniques. It could bring the benefits and impacts to students, educators and academic institutions.}
}

@article{rayyan-727967345,
  title={“Bad smells” in software analytics papers},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={112},
  pages={35-47},
  author={Menzies, Tim and Shepperd, Martin},
  url={https://www.sciencedirect.com/science/article/pii/S095058491930076X},
  keywords={Smell, Software},
  abstract={Context There has been a rapid growth in the use of data analytics to underpin evidence-based software engineering. However the combination of complex techniques, diverse reporting standards and poorly understood underlying phenomena are causing some concern as to the reliability of studies. Objective Our goal is to provide guidance for producers and consumers of software analytics studies (computational experiments and correlation studies). Method We propose using “bad smells”, i.e., surface indications of deeper problems and popular in the agile software community and consider how they may be manifest in software analytics studies. Results We list 12 “bad smells” in software analytics papers (and show their impact by examples). Conclusions We believe the metaphor of bad smell is a useful device. Therefore we encourage more debate on what contributes to the validity of software analytics studies (so we expect our list will mature over time).}
}

@article{rayyan-727967346,
  title={Understanding replication of experiments in software engineering: A classification},
  year={2014},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={56},
  number={8},
  pages={1033-1048},
  author={Gómez, Omar S and Juristo, Natalia and Vegas, Sira},
  url={https://www.sciencedirect.com/science/article/pii/S0950584914000858},
  keywords={Software engineering, Experimentation, Experimental software engineering, Replication, Software},
  abstract={Context Replication plays an important role in experimental disciplines. There are still many uncertainties about how to proceed with replications of SE experiments. Should replicators reuse the baseline experiment materials? How much liaison should there be among the original and replicating experimenters, if any? What elements of the experimental configuration can be changed for the experiment to be considered a replication rather than a new experiment? Objective To improve our understanding of SE experiment replication, in this work we propose a classification which is intend to provide experimenters with guidance about what types of replication they can perform. Method The research approach followed is structured according to the following activities: (1) a literature review of experiment replication in SE and in other disciplines, (2) identification of typical elements that compose an experimental configuration, (3) identification of different replications purposes and (4) development of a classification of experiment replications for SE. Results We propose a classification of replications which provides experimenters in SE with guidance about what changes can they make in a replication and, based on these, what verification purposes such a replication can serve. The proposed classification helped to accommodate opposing views within a broader framework, it is capable of accounting for less similar replications to more similar ones regarding the baseline experiment. Conclusion The aim of replication is to verify results, but different types of replication serve special verification purposes and afford different degrees of change. Each replication type helps to discover particular experimental conditions that might influence the results. The proposed classification can be used to identify changes in a replication and, based on these, understand the level of verification.}
}

@article{rayyan-727967347,
  title={Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions},
  year={2020},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={127},
  pages={106368},
  author={Lwakatare, Lucy Ellen and Raj, Aiswarya and Crnkovic, Ivica and Bosch, Jan and Olsson, Helena Holmström},
  url={https://www.sciencedirect.com/science/article/pii/S0950584920301373},
  keywords={Software engineering, SLR, Challenges, Industrial settings, Machine learning systems, Solutions},
  abstract={Background: Developing and maintaining large scale machine learning (ML) based software systems in an industrial setting is challenging. There are no well-established development guidelines, but the literature contains reports on how companies develop and maintain deployed ML-based software systems. Objective: This study aims to survey the literature related to development and maintenance of large scale ML-based systems in industrial settings in order to provide a synthesis of the challenges that practitioners face. In addition, we identify solutions used to address some of these challenges. Method: A systematic literature review was conducted and we identified 72 papers related to development and maintenance of large scale ML-based software systems in industrial settings. The selected articles were qualitatively analyzed by extracting challenges and solutions. The challenges and solutions were thematically synthesized into four quality attributes: adaptability, scalability, safety and privacy. The analysis was done in relation to ML workflow, i.e. data acquisition, training, evaluation, and deployment. Results: We identified a total of 23 challenges and 8 solutions related to development and maintenance of large scale ML-based software systems in industrial settings including six different domains. Challenges were most often reported in relation to adaptability and scalability. Safety and privacy challenges had the least reported solutions. Conclusion: The development and maintenance on large-scale ML-based systems in industrial settings introduce new challenges specific for ML, and for the known challenges characteristic for these types of systems, require new methods in overcoming the challenges. The identified challenges highlight important concerns in ML system development practice and the lack of solutions point to directions for future research.}
}

@article{rayyan-727967348,
  title={Inner source software development: Current thinking and an agenda for future research},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={163},
  pages={110520},
  author={Edison, Henry and Carroll, Noel and Morgan, Lorraine and Conboy, Kieran},
  url={https://www.sciencedirect.com/science/article/pii/S0164121220300030},
  keywords={Systematic literature review, Inner source, Inner source software development, Research agenda, Software, Thinking},
  abstract={Context Inner source software development (ISSD) has been viewed as an alternative approach in which organisations adopt open source software development (OSSD) practices and exploit its benefits internally. Objective In this paper, we aim to provide an extensive review of current research on ISSD and to establish a research agenda on this domain. Method The review is primarily performed using a systematic literature review protocol. Results We identified, critically evaluated and integrated the findings of 37 primary studies, describing 25 empirical research papers, 10 frameworks/methods, models and tools to support the implementation of inner source, as well as a set of benefits and challenges associated with ISSD. Conclusion This study presents four main contributions. First, the study provides an in-depth review of ISSD to date, i.e. the evolution of research across inner source, contributions of existing research developments, and theories, models and frameworks used to study inner source. Second, our review applies the OSSD approach framework as the lens to analyse ISSD. Third, the review updates the key challenges associated with ISSD from a management perspective. The final contribution is the establishment of a research agenda to advance knowledge on ISSD.}
}

@article{rayyan-727967349,
  title={Software product lines and variability modeling: A tertiary study},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={149},
  pages={485-510},
  author={Raatikainen, Mikko and Tiihonen, Juha and Männistö, Tomi},
  url={https://www.sciencedirect.com/science/article/pii/S016412121830284X},
  keywords={Tertiary study, Systematic literature review, Mapping study, Software product line, Variability, Variability modeling, Software},
  abstract={Context: A software product line is a means to develop a set of products in which variability is a central phenomenon captured in variability models. The field of SPLs and variability have been topics of extensive research over the few past decades. Objective: This research characterizes systematic reviews (SRs) in the field, studies how SRs analyze and use evidence-based results, and identifies how variability is modeled. Method: We conducted a tertiary study as a form of systematic review. Results: 86 SRs were included. SRs have become a widely adopted methodology covering the field broadly otherwise except for variability realization. Numerous variability models exist that cover different development artifacts, but the evidence is insufficient in quantity and immature, and we argue for better evidence. SRs perform well in searching and selecting studies and presenting data. However, their analysis and use of the quality of and evidence in the primary studies often remains shallow, merely presenting of what kinds of evidence exist. Conclusions: There is a need for actionable, context-sensitive, and evaluated solutions rather than novel ones. Different kinds of SRs (SLRs and Maps) need to be better distinguished, and evidence and quality need to be better used in the resulting syntheses.}
}

@article{rayyan-727967350,
  title={A survey of model transformation design patterns in practice},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={140},
  pages={48-73},
  author={Lano, Kevin and Kolahdouz-Rahimi, Shekoufeh and Yassipour-Tehrani, Sobhan and Sharbaf, Mohammadreza},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218300438},
  keywords={Design Patterns, Empirical Software Engineering, Model Transformations},
  abstract={Model transformation design patterns have been proposed by a number of researchers, but their usage appears to be sporadic and sometimes patterns are applied without recognition of the pattern. In this paper we provide a systematic literature review of transformation design pattern applications. We evaluate how widely patterns have been used, and how their use differs in different transformation languages and for different categories of transformation. We identify what benefits appear to arise from the use of patterns, and consider how the application of patterns can be improved. The paper also identifies several new patterns which have not previously been catalogued.}
}

@article{rayyan-727967351,
  title={Secondary studies in the academic context: A systematic mapping and survey},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={170},
  pages={110734},
  author={Felizardo, Katia Romero and de Souza, Érica Ferreira and Napoleão, Bianca Minetto and Vijaykumar, Nandamudi Lankalapalli and Baldassarre, Maria Teresa},
  url={https://www.sciencedirect.com/science/article/pii/S0164121220301655},
  keywords={Systematic literature review, Secondary studies, Systematic mapping, Education},
  abstract={Context: Several researchers have reported their experiences in applying secondary studies (Systematic Literature Reviews — SLRs and Systematic Mappings — SMs) in Software Engineering (SE). However, there is still a lack of studies discussing the value of performing secondary studies in an academic context. Goal: The main goal of this study is to provide an overview on the use of secondary studies in an academic context. Method: Two empirical research methods were used. Initially, we conducted a SM to identify the available and relevant studies on the use of secondary studies as a research methodology for conducting SE research projects. Secondly, a survey was performed with 64 SE researchers to identify their perception related to the value of performing secondary studies to support their research projects. Results: Our results show benefits of using secondary studies in the academic context, such as providing an overview of the literature as well as identifying relevant research literature on a research area enabling to find reasons to explain why a research project should be approved for a grant and/or supporting decisions made in a research project. Difficulties faced by SE graduate students with secondary studies are that they tend to be conducted by a team and it demands more effort than a traditional review. Conclusions: Secondary studies are valuable to graduate students. They should consider conducting a secondary study for their research project due to the benefits and contributions provided to develop the overall project. However, the advice of an experienced supervisor is essential to avoid bias. In addition, the acquisition of skills can increase student's motivation to pursue their research projects and prepare them for both academic or industrial careers.}
}

@article{rayyan-727967352,
  title={A systematic review on regression test selection techniques},
  year={2010},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={52},
  number={1},
  pages={14-30},
  author={Engström, Emelie and Runeson, Per and Skoglund, Mats},
  url={https://www.sciencedirect.com/science/article/pii/S0950584909001219},
  keywords={Systematic review, Empirical studies, Regression testing, Test selection},
  abstract={Regression testing is verifying that previously functioning software remains after a change. With the goal of finding a basis for further research in a joint industry-academia research project, we conducted a systematic review of empirical evaluations of regression test selection techniques. We identified 27 papers reporting 36 empirical studies, 21 experiments and 15 case studies. In total 28 techniques for regression test selection are evaluated. We present a qualitative analysis of the findings, an overview of techniques for regression test selection and related empirical evidence. No technique was found clearly superior since the results depend on many varying factors. We identified a need for empirical studies where concepts are evaluated rather than small variations in technical implementations.}
}

@article{rayyan-727967353,
  title={Reproducibility and replicability of software defect prediction studies},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={99},
  pages={148-163},
  author={Mahmood, Zaheed and Bowes, David and Hall, Tracy and Lane, Peter C R and Petrić, Jean},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917304202},
  keywords={Replication, Reproducibility, Software defect prediction, Software},
  abstract={Context: Replications are an important part of scientific disciplines. Replications test the credibility of original studies and can separate true results from those that are unreliable. Objective: In this paper we investigate the replication of defect prediction studies and identify the characteristics of replicated studies. We further assess how defect prediction replications are performed and the consistency of replication findings. Method: Our analysis is based on tracking the replication of 208 defect prediction studies identified by a highly cited Systematic Literature Review (SLR) [1]. We identify how often each of these 208 studies has been replicated and determine the type of replication carried out. We identify quality, citation counts, publication venue, impact factor, and data availability from all 208 SLR defect prediction papers to see if any of these factors are associated with the frequency with which they are replicated. Results: Only 13 (6%) of the 208 studies are replicated. Replication seems related to original papers appearing in the Transactions of Software Engineering (TSE) journal. The number of citations an original paper had was also an indicator of replications. In addition, studies conducted using closed source data seems to have more replications than those based on open source data. Where a paper has been replicated, 11 (38%) out of 29 studies revealed different results to the original study. Conclusion: Very few defect prediction studies are replicated. The lack of replication means that it remains unclear how reliable defect prediction is. We provide practical steps for improving the state of replication.}
}

@article{rayyan-727967354,
  title={Risks and risk mitigation in global software development: A tertiary study},
  year={2014},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={56},
  number={1},
  pages={54-78},
  author={Verner, J M and Brereton, O P and Kitchenham, B A and Turner, M and Niazi, M},
  url={https://www.sciencedirect.com/science/article/pii/S0950584913001341},
  keywords={Evidence, Systematic literature review, Global software development, Risk, Risk mitigation, Tertiary review, Software},
  abstract={Context There is extensive interest in global software development (GSD) which has led to a large number of papers reporting on GSD. A number of systematic literature reviews (SLRs) have attempted to aggregate information from individual studies. Objective We wish to investigate GSD SLR research with a focus on discovering what research has been conducted in the area and to determine if the SLRs furnish appropriate risk and risk mitigation advice to provide guidance to organizations involved with GSD. Method We performed a broad automated search to identify GSD SLRs. Data extracted from each study included: (1) authors, their affiliation and publishing venue, (2) SLR quality, (3) research focus, (4) GSD risks, (5) risk mitigation strategies and, (6) for each SLR the number of primary studies reporting each risk and risk mitigation strategy. Results We found a total of 37 papers reporting 24 unique GSD SLR studies. Major GSD topics covered include: (1) organizational environment, (2) project execution, (3) project planning and control and (4) project scope and requirements. We extracted 85 risks and 77 risk mitigation advice items and categorized them under four major headings: outsourcing rationale, software development, human resources, and project management. The largest group of risks was related to project management. GSD outsourcing rationale risks ranked highest in terms of primary study support but in many cases these risks were only identified by a single SLR. Conclusions The focus of the GSD SLRs we identified is mapping the research rather than providing evidence-based guidance to industry. Empirical support for the majority of risks identified is moderate to low, both in terms of the number of SLRs identifying the risk, and in the number of primary studies providing empirical support. Risk mitigation advice is also limited, and empirical support for these items is low.}
}

@article{rayyan-727967355,
  title={Analogy-based software development effort estimation: A systematic mapping and review},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={58},
  pages={206-230},
  author={Idri, Ali and azzahra Amazal, Fatima and Abran, Alain},
  url={https://www.sciencedirect.com/science/article/pii/S0950584914001815},
  keywords={Systematic literature review, Analogy, Mapping study, Case-based reasoning, Software development effort estimation, Software},
  abstract={Context Analogy-based Software development Effort Estimation (ASEE) techniques have gained considerable attention from the software engineering community. However, existing systematic map and review studies on software development effort prediction have not investigated in depth several issues of ASEE techniques, to the exception of comparisons with other types of estimation techniques. Objective The objective of this research is twofold: (1) to classify ASEE studies which primary goal is to propose new or modified ASEE techniques according to five criteria: research approach, contribution type, techniques used in combination with ASEE methods, and ASEE steps, as well as identifying publication channels and trends and (2) to analyze these studies from five perspectives: estimation accuracy, accuracy comparison, estimation context, impact of the techniques used in combination with ASEE methods, and ASEE tools. Method We performed a systematic mapping of studies for which the primary goal is to develop or to improve ASEE techniques published in the period 1990–2012, and reviewed them based on an automated search of four electronic databases. Results In total, we identified 65 studies published between 1990 and 2012, and classified them based on our predefined classification criteria. The mapping study revealed that most researchers focus on addressing problems related to the first step of an ASEE process, that is, feature and case subset selection. The results of our detailed analysis show that ASEE methods outperform the eight techniques with which they were compared, and tend to yield acceptable results especially when combining ASEE techniques with Fuzzy Logic (FL) or Genetic Algorithms (GA). Conclusion Based on the findings of this study, the use of other techniques such FL and GA in combination with an ASEE method is promising to generate more accurate estimates. However, the use of ASEE techniques by practitioners is still limited: developing more ASEE tools may facilitate the application of these techniques and then lead to increasing the use of ASEE techniques in industry.}
}

@article{rayyan-727967356,
  title={Process models in the practice of distributed software development: A systematic review of the literature},
  year={2010},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={52},
  number={8},
  pages={779-791},
  author={Prikladnicki, Rafael and Audy, Jorge Luis Nicolas},
  url={https://www.sciencedirect.com/science/article/pii/S0950584910000492},
  keywords={Distributed software development, Global software engineering, Offshoring, Process improvement, Process models, Software},
  abstract={Context Distributed Software Development (DSD) has recently become an active research area. Although considerable research effort has been made in this area, as yet, no agreement has been reached as to an appropriate process model for DSD. Purpose This paper is intended to identify and synthesize papers that describe process models for distributed software development in the context of overseas outsourcing, i.e. “offshoring”. Method We used a systematic review methodology to search seven digital libraries and one topic-specific conference. Results We found 27 primary studies describing stage-related DSD process models. Only five of such studies looked into outsourcing to a subsidiary company (i.e. “internal offshoring”). Nineteen primary studies addressed the need for DSD process models. Eight primary studies and three literature surveys described stage-based DSD process models, but only three of such models were empirically evaluated. Conclusion We need more research aimed at internal offshoring. Furthermore, proposed models need to be empirically validated.}
}

@article{rayyan-727967357,
  title={Student centered learning in statistics: Analysis of systematic review},
  year={2013},
  journal={Procedia - Social and Behavioral Sciences},
  issn={1877-0428},
  volume={103},
  pages={844-851},
  author={Judi, Hairulliza Mohamad and Sahari, Noraidah},
  url={https://www.sciencedirect.com/science/article/pii/S1877042813038512},
  keywords={systematic review, Statistics, Student-centered learning, Learning},
  abstract={This paper reports the initial results of research relatedto student-centered learning in statistics education. Student-centered learning (SCL) suggests students to engage actively as doers in education setting who are empowered to decide on what, when, where, and how to learn. Although SCL in statistics instruction research has rapidly increased, there is little study to evaluate and synthesize the results of relevant research in this area, specifically within the context of computer support education. The objective of this paper is to identify the direction of recent research in SCL usage in statistics teaching and learning. Four research questions were raised in this study: What are the important issues in student centered concept in statistics teaching and learning? What are the SCL methods in statistics course? Which methods are used in statistics education research? What type of computer supported material sources involved? This paper applies systematic review to summarize the research by performing synthesis on research resources. Results of the review were presented and discussed in the paper.}
}

@article{rayyan-727967358,
  title={Automatic text classification to support systematic reviews in medicine},
  year={2014},
  journal={Expert Systems with Applications},
  issn={0957-4174},
  volume={41},
  number={4},
  pages={1498-1508},
  author={García Adeva, J J and Pikatza Atxa, J M and Ubeda Carrillo, M and Ansuategi Zengotitabengoa, E},
  url={https://www.sciencedirect.com/science/article/pii/S0957417413006684},
  keywords={Text mining, Machine learning, Text classification, Medical systematic reviews},
  abstract={Medical systematic reviews answer particular questions within a very specific domain of expertise by selecting and analysing the current pertinent literature. As part of this process, the phase of screening articles usually requires a long time and significant effort as it involves a group of domain experts evaluating thousands of articles in order to find the relevant instances. Our goal is to support this process through automatic tools. There is a recent trend of applying text classification methods to semi-automate the screening phase by providing decision support to the group of experts, hence helping reduce the required time and effort. In this work, we contribute to this line of work by performing a comprehensive set of text classification experiments on a corpus resulting from an actual systematic review in the area of Internet-Based Randomised Controlled Trials. These experiments involved applying multiple machine learning algorithms combined with several feature selection techniques to different parts of the articles (i.e., titles, abstract, or both). Results are generally positive in terms of overall precision and recall measurements, reaching values of up to 84%. It is also revealing in terms of how using only article titles provides virtually as good results as when adding article abstracts. Based on the positive results, it is clear that text classification can support the screening stage of medical systematic reviews . However, selecting the most appropriate machine learning algorithms, related methods, and text sections of articles is a neglected but important requirement because of its significant impact to the end results.}
}

@article{rayyan-727967359,
  title={Software component and the semantic Web: An in-depth content analysis and integration history},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={125},
  pages={152-169},
  author={Kaur, Loveleen and Mishra, Ashutosh},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216302308},
  keywords={Ontology, Component-based software engineering, Linked Data, Reasoners, Semantic Web, Web services, Software, Semantics},
  abstract={With the advent of Component-based software engineering (CBSE), large software systems are being built by integrating pre-built software components. The Semantic Web in association with CBSE has shown to offer powerful representation facilities and reasoning techniques to enhance and support querying, reasoning, discovery, etc. of software components. The goal of this paper is to research the applicability of Semantic Web technologies in performing the various tasks of CBSE and review the experimental results of the same in an easy and effective manner. To the best of our knowledge, this is the first study which provides an extensive review of the application of Semantic Web in CBSE from different perspectives. A systematic literature review of the Semantic Web approaches, employed for use in CBSE, reported from 2001 until 2015, is conducted in this research article. Empirical results have been drawn through the question-answer based analysis of the research, which clearly tells the year wise trend of the research articles, with the possible justification of the usage of Semantic Web technology and tools for a particular phase of CBSE. To conclude, gaps in the current research and potential future prospects have been discussed.}
}

@article{rayyan-727967360,
  title={The influence of power distance on requirements engineering activities},
  year={2019},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={159},
  pages={2394-2403},
  author={Alsanoosy, Tawfeeq and Spichkova, Maria and Harland, James},
  url={https://www.sciencedirect.com/science/article/pii/S1877050919316187},
  keywords={software engineering, collaboration, cultural aspects, Requirement engineering},
  abstract={Requirements Engineering (RE) activities are inherently collaborative, requiring software stakeholders to have intensive communication to identify users' requirements. As the stakeholders' culture might heavily impact on the way how they collaborate and communicate, RE might be significantly influenced by their cultural background. To analyse stakeholders' culture, we use one of the most adopt and comprehensive cultural studies in software engineering: Hofstede's cultural model. Hofstede's model has six cultural dimensions, where one of them is Power Distance (PD). The aim of this study is to explore the influence of PD cultural aspects on RE activities. We conducted 32 interviews with software developments/requirements engineer practitioners from two different cultures: Saudi Arabia and Australia. Our results revealed that PD significantly affect RE collaboration among software stakeholders. In this paper, we address the core identified cultural aspects as well as the ways to overcome them. The results that we present in this paper would help to increase awareness of the influence of PD, and thus, improve collaboration within RE activities.}
}

@article{rayyan-727967361,
  title={A maturity model for secure requirements engineering},
  year={2020},
  journal={Computers & Security},
  issn={0167-4048},
  volume={95},
  pages={101852},
  author={Niazi, Mahmood and Saeed, Ashraf Mohammed and Alshayeb, Mohammad and Mahmood, Sajjad and Zafar, Saad},
  url={https://www.sciencedirect.com/science/article/pii/S0167404820301243},
  abstract={Security is considered to be a critical software quality attribute. Tackling security at the requirements phase helps to avoid the need to rework secure software development issues. The aim of this paper is to develop a Requirements Engineering (RE) Security Maturity Model (RESMM) to assist software development organizations to better specify the requirements for secure software development. To achieve this objective, first, we conducted a systematic literature review (SLR) to identify the requirement practices for secure software development. Then we modified Sommerville's requirements engineering practices. We also conducted a questionnaire survey based on the identified security requirements practices. Next, the RESMM was built based on the results of the SLR, the modified Sommerville practices and feedback from the security practitioners. Finally, two case studies were conducted to assess RESMM. RESMM has 79 practices classified into 7 RE categories. The case study results show that RESMM has a clear structure and is easy to comprehend and use. In addition, the case study participants recommended that software organizations adopt RESMM. RESMM has the ability to identify the RE security maturity levels in software organizations. RESMM can also help software development organizations deliver secure software.}
}

@article{rayyan-727967362,
  title={A survey on software coupling relations and tools},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={107},
  pages={159-178},
  author={Fregnan, Enrico and Baum, Tobias and Palomba, Fabio and Bacchelli, Alberto},
  url={https://www.sciencedirect.com/science/article/pii/S0950584918302441},
  keywords={Software engineering, Software metrics, Coupling relations, Software},
  abstract={Context Coupling relations reflect the dependencies between software entities and can be used to assess the quality of a program. For this reason, a vast amount of them has been developed, together with tools to compute their related metrics. However, this makes the coupling measures suitable for a given application challenging to find. Goals The first objective of this work is to provide a classification of the different kinds of coupling relations, together with the metrics to measure them. The second consists in presenting an overview of the tools proposed until now by the software engineering academic community to extract these metrics. Method This work constitutes a systematic literature review in software engineering. To retrieve the referenced publications, publicly available scientific research databases were used. These sources were queried using keywords inherent to software coupling. We included publications from the period 2002 to 2017 and highly cited earlier publications. A snowballing technique was used to retrieve further related material. Results Four groups of coupling relations were found: structural, dynamic, semantic and logical. A fifth set of coupling relations includes approaches too recent to be considered an independent group and measures developed for specific environments. The investigation also retrieved tools that extract the metrics belonging to each coupling group. Conclusion This study shows the directions followed by the research on software coupling: e.g., developing metrics for specific environments. Concerning the metric tools, three trends have emerged in recent years: use of visualization techniques, extensibility and scalability. Finally, some coupling metrics applications were presented (e.g., code smell detection), indicating possible future research directions. Public preprint [https://doi.org/10.5281/zenodo.2002001].}
}

@article{rayyan-727967363,
  title={Software startup engineering: A systematic mapping study},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={144},
  pages={255-274},
  author={Berg, Vebjørn and Birkeland, Jørgen and Nguyen-Duc, Anh and Pappas, Ilias O and Jaccheri, Letizia},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218301286},
  keywords={Systematic mapping study, Software engineering, Software development, Software startup, Startup, Software},
  abstract={Software startups have long been a significant driver in economic growth and innovation. The on-going failure of the major number of startups calls for a better understanding of state-of-the-practice of startup activities. Objective With a focus on engineering perspective, this study aims at identifying the change in focus of research area and thematic concepts operating startup research. A systematic mapping study on 74 primary papers (in which 27 papers are newly selected) from 1994 to 2017 was conducted with a comparison with findings from previous mapping studies. A classification schema was developed, and the primary studies were ranked according to their rigour. We discovered that most research has been conducted within the SWEBOK knowledge areas software engineering process, management, construction, design, and requirements, with the shift of focus towards process and management areas. We also provide an alternative classification for future startup research. We find that the rigour of the primary papers was assessed to be higher between 2013–2017 than that of 1994–2013. We also find an inconsistency of characterizing startups. Future work can focus on certain research themes, such as startup evolution models and human aspects, and consolidate the thematic concepts describing software startups.}
}

@article{rayyan-727967364,
  title={Basis for an integrated security ontology according to a systematic review of existing proposals},
  year={2011},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={33},
  number={4},
  pages={372-388},
  author={Blanco, Carlos and Lasheras, Joaquín and Fernández-Medina, Eduardo and Valencia-García, Rafael and Toval, Ambrosio},
  url={https://www.sciencedirect.com/science/article/pii/S0920548911000043},
  keywords={Systematic review, Security, Ontologies},
  abstract={The use of ontologies to represent knowledge provides us with organization, communication and reusability. The concepts and relations managed by any scientific community need to be formally defined. Since security in information technologies has evolved as a critical aspect and many related topics have been developed, this paper applies the method of systematic review for identifying, extracting and analyzing the principal proposals for security ontologies. The most mature proposals have been selected and compared by using a formal framework, extracting the key requirements that an integrated and unified security ontology should have, and providing the first steps towards its definition.}
}

@article{rayyan-727967365,
  title={Challenges and recommended practices for software architecting in global software development},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={106},
  pages={234-253},
  author={Sievi-Korte, Outi and Beecham, Sarah and Richardson, Ita},
  url={https://www.sciencedirect.com/science/article/pii/S0950584918302209},
  keywords={Systematic literature review, Software architecture, Software design, Global software development, Design practice, Software},
  abstract={Context Global software development (GSD), although now a norm in the software industry, carries with it enormous challenges mostly regarding communication and coordination. Aforementioned challenges are highlighted when there is a need to transfer knowledge between sites, particularly when software artifacts assigned to different sites depend on each other. The design of the software architecture and associated task dependencies play a major role in reducing some of these challenges. Objective The current literature does not provide a cohesive picture of how the distributed nature of software development is taken into account during the design phase: what to avoid, and what works in practice. The objective of this paper is to gain an understanding of software architecting in the context of GSD, in order to develop a framework of challenges and solutions that can be applied in both research and practice. Method We conducted a systematic literature review (SLR) that synthesises (i) challenges which GSD imposes on software architecture design, and (ii) recommended practices to alleviate these challenges. Results We produced a comprehensive set of guidelines for performing software architecture design in GSD based on 55 selected studies. Our framework comprises nine key challenges with 28 related concerns, and nine recommended practices, with 22 related concerns for software architecture design in GSD. These challenges and practices were mapped to a thematic conceptual model with the following concepts: Organization (Structure and Resources), Ways of Working (Architecture Knowledge Management, Change Management and Quality Management), Design Practices, Modularity and Task Allocation. Conclusion The synthesis of findings resulted in a thematic conceptual model of the problem area, a mapping of the key challenges to practices, and a concern framework providing concrete questions to aid the design process in a distributed setting. This is a first step in creating more concrete architecture design practices and guidelines.}
}

@article{rayyan-727967366,
  title={The impact of course-taking on academic achievements a systematic review and Meta analysis},
  year={2010},
  journal={Procedia - Social and Behavioral Sciences},
  issn={1877-0428},
  volume={2},
  number={2},
  pages={3401-3406},
  author={Shulruf, Boaz and Keuskamp, Dominic and Brake, Dulcie},
  url={https://www.sciencedirect.com/science/article/pii/S187704281000563X},
  keywords={academic achievements, Australia, educational career, Meta analysis, Systematically reviews},
  abstract={Students choose subjects in secondary schools that can be major determinants for their future educational career. This paper systematically reviews the literature on the effect of secondary school course-taking on educational outcomes in secondary and tertiary institutions. The findings of the review and the meta analysis suggest that course-taking predicts a number of educational outcomes such as further course-taking in secondary school, tertiary course-taking, and secondary school grades. However, these effects could not be easily disentangled from the diversity of factors, such as ethnicity, socioeconomic status (SES) and prior achievement that are co-correlated with course-taking and its outcomes.}
}

@article{rayyan-727967367,
  title={The roles of conceptual modelling in improving construction simulation studies: A comprehensive review},
  year={2020},
  journal={Advanced Engineering Informatics},
  issn={1474-0346},
  volume={46},
  pages={101175},
  author={Abdelmegid, M A and González, V A and O'Sullivan, M and Walker, C G and Poshdar, M and Ying, F},
  url={https://www.sciencedirect.com/science/article/pii/S1474034620301464},
  keywords={Systematic literature review, Benefits, Conceptual modeling, Construction simulation},
  abstract={The conceptual modelling phase of simulation studies has proven to be effective in enhancing the impact of simulation modelling in different domains. However, this simulation phase did not receive much attention in the construction simulation domain. The objective of this paper is to identify the roles that conceptual modelling can play in advancing the engagement, accuracy, and adoption (among other things) of discrete-event simulation studies in construction. In this paper, a Systematic Literature Review (SLR) is conducted, which involves a comprehensive search of databases and researchers' profiles to identify journal papers, conference articles, books, and theses that have reported the benefits of conceptual modelling for discrete-event simulation studies. The review resulted in 82 documents that were published from 2000 to 2020. Results indicate that the benefits of conceptual modelling include facilitating communications between stakeholders, capturing sufficient information for the simulation model, improving the quality of simulation models, guiding other simulation modelling activities, and facilitating verification and validation of simulation models. By linking these benefits to the current research agenda in construction simulation, this paper shows the significance and potential of the conceptual modelling phase to enhance the impact of discrete-event simulation studies in construction.}
}

@article{rayyan-727967368,
  title={State of the art in the research of formal verification},
  year={2014},
  journal={Ingeniería, Investigación y Tecnología},
  issn={1405-7743},
  volume={15},
  number={4},
  pages={615-623},
  author={Edgar, Serna-M. and David, Morales-V.},
  url={https://www.sciencedirect.com/science/article/pii/S1405774314706596},
  keywords={software engineering, formal verification, enfoques de investigación, engineering techniques, formal methods, ingeniería de software, métodos formales, research approaches, técnicas de ingenieréa, verificación formal},
  abstract={In recent years research in formal verification of hardware and software has reached important progresses in the development of methodologies and tools to meet the increasing complexity of systems. The explicit role of Formal Verification is to find errors and to improve the reliability on the accuracy of system design, which implies a challenge for software engineering of this century. The purpose of this research is to perform a systematic review of the literature to establish the state of the art of research in formal verification during the last 10 years and to identify the approaches, methods, techniques and methodologies used, as well as the intensity of those research activities. During the process it was found that research in this field has doubled since 2005, and that the mean value of researches conducted year after year remains the same and that prevail the application in control and interaction systems. Additionally it was found that, the case study is the most used method and that empirical research is the most applied type. Resumen En años recientes, la investigación en verificación formal de hardware y software ha logrado importantes progresos en el desarrollo de metodologías y herramientas para hacer frente a la creciente complejidad de los sistemas. La función explícita de la verificación formal es encontrar errores y mejorar la confianza en la exactitud del diseño del sistema, lo que supone un reto para la ingeniería de software de este siglo. El objetivo de esta investigación fue realizar una revisión sistemática a la literatura para determinar el estado del arte de la investigación en verificación formal en los últimos 10 años e identificar los enfoques, métodos, técnicas y metodologías empleadas, lo mismo que la intensidad de esa investigación. En el proceso se encontró que la investigación en esta área se duplicó a partir del año 2005, que hasta el momento mantiene un número promedio de investigaciones año tras año y que predomina la aplicación en sistemas de control e interacción. Además, que el estudio de caso es el método más utilizado y que la investigación empírica es la más aplicada.}
}

@article{rayyan-727967369,
  title={Non-functional requirements in model-driven development of service-oriented architectures},
  year={2018},
  journal={Science of Computer Programming},
  issn={0167-6423},
  volume={168},
  pages={18-37},
  author={Ameller, David and Burgués, Xavier and Costal, Dolors and Farré, Carles and Franch, Xavier},
  url={https://www.sciencedirect.com/science/article/pii/S0167642318303034},
  keywords={Systematic literature review, Model-driven development, Service-oriented architecture, Non-functional requirements, Quality requirement},
  abstract={Any software development process needs to consider non-functional requirements (NFR) in order to deliver a system that complies with its stakeholders' expectations. In a previous mapping study about model-driven development (MDD) for service-oriented architectures (SOA) we found a limited number of approaches managing NFR. The present work aims at analyzing in detail the state of the art in the management of NFR in MDD processes which produce SOA. We have conducted a systematic literature review following a rigorous protocol. We have taken as initial point the mapping study mentioned above and have used the subset of the 31 papers from this study (clustered into 15 approaches) that referred to NFR. We have analyzed them qualitatively in order to answer six research questions. We have built a Software Engineering theory to formalize this analysis. As result we highlight that most of approaches focus exclusively on security and reliability and we observe that NFR are expressed mainly as annotations of functional models represented in UML. From our perspective, existing research on the topic of this study is still scarce and without any evidence of transferability to industry. This situation suggests the need for further investigation efforts in order to produce validated MDD methods capable of generating SOA satisfying NFR stated by stakeholders.}
}

@article{rayyan-727967370,
  title={Usability in agile software development: A tertiary study},
  year={2019},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={64},
  pages={61-77},
  author={Curcio, Karina and Santana, Rodolfo and Reinehr, Sheila and Malucelli, Andreia},
  url={https://www.sciencedirect.com/science/article/pii/S0920548918302587},
  keywords={Tertiary study, Systematic literature review, Agile software development, User-centered design, Usability, Software},
  abstract={In the last years the interest in developing research on integration of usability and agile software development has been increasing. The number of systematic literature reviews, systematic mapping studies and non-systematic reviews, related to this thematic has also increased. Nevertheless, there is no analysis on the quality of these published secondary studies, nor is there a consolidated research that brings the answer of how to integrate these two areas. The goal of this paper is to categorize secondary studies related to the integration of usability and agile software development and present a critical analysis on the quality of the selected studies. To accomplish this goal a tertiary study was performed to categorize the related studies selected. Initially 3,065 papers were identified and further narrowed to 14 by applying exclusion criteria and analysis. We classified the selected studies as systematic literature reviews, systematic mapping studies and non-systematic literature reviews to report the data analysis. As a result of this study different forms to integrate usability and agile software development were detected as well as the various challenges that must be overcome for the integration success. Six main categories were identified to represent ways of integrating usability into agile development: processes, techniques, practices, recommendations, principles and different approaches. Regarding to the challenges for the integration seven main categories were also identified: issues related to tests, time, work balance, modularization, feedback, prioritization, and documentation. Although the interest in researching the integration of usability and agile software development has increased in the last years, mostly of the analyzed studies neglected the quality criteria and presented difficulties to use methods to synthetize the research results. Despite this, it has been realized that the integration of usability with agile software development is possible and is strongly aligned with user-centered design. The initial studies indicated a separation of activities and roles into specific tracks with parallel work to treat usability in agile software development, but the trend is no longer to manage and control these activities in separate ways, so new challenges are becoming to appear. Although we have identified several points of tension, the integration does not become unfeasible.}
}

@article{rayyan-727967371,
  title={The impact of Use Cases in real-world software development projects: A systematic mapping study},
  year={2019},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={66},
  pages={103362},
  author={Barros-Justo, José L and Benitti, Fabiane B V and Tiwari, Saurabh},
  url={https://www.sciencedirect.com/science/article/pii/S0920548919301254},
  keywords={Evidence-based software engineering, Systematic mapping study, Software engineering, Impact in industry, UML Use Cases, Software},
  abstract={Context There is abundant literature on the application of UML Use Cases. However, the impact that these applications have had on real projects (in industry) is not well known. It is necessary to know what the impact of the Use Cases really is, so that both, researchers and professionals can make the most pertinent decisions. Objective To identify and classify the positive and negative impacts of using Use Cases in real-world settings. Method We conducted a systematic mapping study. The search strategies retrieved a set of 4431 papers out of which 47 were selected as primary studies. We defined four facets to classify these studies: 1) the positive impact (advantages), 2) the negative impact (disadvantages), 3) the industry's domain and 4) the type of research reported. Results Our study identified eight categories of advantages related to the application of Use Cases. The most mentioned were estimation, analysis and automation. These advantages had a heterogeneous distribution along the years. On the other hand, the granularity of the scenarios described in the Use Cases, the lack of a standardized format for specifying requirements, and the lack of appropriate guidelines for analysing them were the most mentioned disadvantages. We identified a variety of industry domains, grouped into seven categories. As we can expect most of the papers report experiences in the Information Technology domain, followed by financials applications. Almost half the papers applied evaluation research, including an empirical validation. Only one third of the analysed papers reported threats to validity, the most mentioned being generalizability (38%). Conclusions Use Cases have proven to be a useful tool in software development, particularly during the early stages. The positive effects far outweigh the few negative effects reported, although this may be due to the researchers' tendency of not reporting negative results.}
}

@article{rayyan-727967372,
  title={Success factors influencing requirements change management process in global software development},
  year={2019},
  journal={Journal of Computer Languages},
  issn={2590-1184},
  volume={51},
  pages={112-130},
  author={Akbar, Muhammad Azeem and Sang, Jun and Nasrullah and Khan, Arif Ali and Mahmood, Sajjad and Qadri, Syed Furqan and Hu, Haibo and Xiang, Hong},
  url={https://www.sciencedirect.com/science/article/pii/S1045926X18301411},
  keywords={Systematic literature review, Global software development, Client and vendor success factors, Requirement change management, Size-based classification, Time period based classification, Software},
  abstract={Planning and managing requirements changes in Global Software Development (GSD) is a challenging task. While requirements change has received much attention from researchers, Requirements Change Management (RCM) process is still an emerging area in GSD. The objective of this paper is to identify the success factors of RCM in the GSD environment. We applied the Systematic Literature Review (SLR) approach and identified 23 success factors that influence RCM in GSD projects. The findings of the SLR indicate that change impact analysis, change understanding, management support, RCM process awareness, standard for RCM, progress measure, updated requirements and minimize project failure risk are key factors that influence RCM in a GSD project. We present a comparison of success factors identified in client and vendor organizations. Moreover, we present a framework to classify the identified success factors for RCM process implementation. We believe GSD organizations can use the framework to better manage requirements change in GSD projects.}
}

@article{rayyan-727967373,
  title={Hybrid methods and practices associated with agile methods, method tailoring and delivery of projects in a non-software context},
  year={2018},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={138},
  pages={739-746},
  author={Papadakis, Emmanouil and Tsironis, Loukas},
  url={https://www.sciencedirect.com/science/article/pii/S1877050918317447},
  keywords={systematic literature review, Agile project management, hybrid methodologies, method tailoring, Software},
  abstract={Nowadays the business world is characterized by complexity since market environment is changing quickly. Delivery practice and methods for project, program and portfolio management have changed over the decades to align themselves to the changing environment. Unlike traditional methods used in delivery of projects and programs, agile methods are marked by responding to change over following a plan and by extensive collaboration with customer over contract negotiation and offering a variety of benefits that make them attractive to researchers. Although the latter methods [1] claimed to be beneficial presenting advantages such as accelerate time to market, increase in quality and productivity, new trends and hybrid methods and tailored processes are being discussed and developed. In this study the authors provide a report analysis of proposed tested frameworks already presented in the relative literature, tailored methodologies, a review of most used and popular agile practices and approaches and the trends in our subject area conducting a literature review ranging from 2000 to 2017. Our research strategy following a systematic approach [2] revealed 524 studies, of which 71 had been identified to answer our research questions. This is part of further work based on the first authors' PhD work. The results will be a guide to choose the most appropriate blend of practices for a given project, adapt them to the changing needs and develop an innovative framework methodology.}
}

@article{rayyan-727967374,
  title={Fintechs: A literature review and research agenda},
  year={2019},
  journal={Electronic Commerce Research and Applications},
  issn={1567-4223},
  volume={34},
  pages={100833},
  author={Milian, Eduardo Z and de M. Spinola, Mauro and de Carvalho, Marly M},
  url={https://www.sciencedirect.com/science/article/pii/S1567422319300109},
  keywords={Systematic literature review, Blockchain, Innovation, Crowdfunding, Cryptocurrency, Financial services, Financial technologies, Fintech, Loans, Payment technologies},
  abstract={Although the fintech subject has been widely discussed in the press and communications media, there is a lack of consensus on the definition of the term in the scientific literature and the key research topics and trends. Aiming to narrow this gap, the objective of this study is to investigate the concept of fintech, to map the literature and point out new routes and opportunities in the field. For this purpose, a Systematic Literature Review (SLR) is performed, attempting to describe the areas of fintech activities, propose a categorization for this literature, highlight the main issues dealt with to date in the sample publications, as well as to point out new questions for continuing research in this field. The results show a set of definitions for the term fintech and suggest as a comprehensive understanding of fintech, as innovative companies active in the financial industry making use of the availability of communication, the ubiquity of the internet, and the automated processing of information. Moreover, the literature focuses on financial services and innovations, dealing with issues of financial industry regulation and local legislation or the financial system globally. The innovation of research subcategories (technology adoption/network externalities), blockchain and security appear with great emphasis in this work and represent the current most sensitive aspects also linked to the more global theme of digital transformation. Finally, subjects related to financial services operation particularly deal with risks of financial loss related to different factors involved in the business environment of these organizations.}
}

@article{rayyan-727967375,
  title={Finding reusable structured resources for the integration of environmental research data},
  year={2020},
  journal={Environmental Modelling & Software},
  issn={1364-8152},
  volume={133},
  pages={104813},
  author={Campos, Patricia M C and Reginato, Cassio C and Almeida, João Paulo A and Barcellos, Monalessa P and de Almeida Falbo, Ricardo and Silva Souza, Vítor E and Guizzardi, Giancarlo},
  url={https://www.sciencedirect.com/science/article/pii/S1364815219307078},
  keywords={Ontology, Data integration, Reuse, Environmental research data, Knowledge resources, Systematic search},
  abstract={Successful data integration requires careful examination of data semantics, a task that has often been approached with the use of ontologies. However, there are some barriers to build ontologies for data integration in complex domains such as the environmental one. A relevant problem is the development of new ontologies disregarding previous knowledge resources such as reference models and vocabularies. This paper addresses this challenge by proposing a systematic approach (dubbed CLeAR) for the identification and selection of reusable artifacts for building ontologies with the purpose of research data integration. CLeAR follows some principles of the systematic literature reviews, supporting the search for structured resources in the scientific literature. We apply CLeAR to the environmental domain. A total of 543 publications were surveyed. The results obtained provide a set of 75 structured resources for the environmental domain, evaluated according domain coverage and some quality attributes (e.g., proper documentation, community acceptance).}
}

@article{rayyan-727967376,
  title={Literature review of the situation research faces in the application of ITIL in Small and Medium Enterprises},
  year={2016},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={48},
  pages={124-138},
  author={Cruz-Hinojosa, Nancy Judith and Gutiérrez-de-Mesa, José Antonio},
  url={https://www.sciencedirect.com/science/article/pii/S0920548916300356},
  keywords={Systematic Literature Review, ITIL, Method, Small and Medium Enterprises},
  abstract={This paper carries out a review of the issues that Small and Medium Enterprises (SMEs) face when trying to ensure their alignment with Information Technology Infrastructure Library (ITIL) guidelines. It is well-known that SMEs experience different challenges to those experienced by Large Enterprises, however their demands are the same as larger companies. Given that they have less labor and technological resources, they must optimize their service levels and adapt the activities of their IT departments to the needs of the company without negatively impacting service commitments. In this context our main objective is to establish a complete review concerning the important information that exists in relation to ITIL and its use in Small and Medium Enterprises, evaluating methods for the collection of evidence and analysis. For this reason we have conducted a systematic literature review with the automated search in the range 2007–2015 which has led us to identify thirty-nine articles of relevance. It is noted that, although it was initially expected that there might be enough information that would help us validate and interpret the way that ITIL functions for Small and Medium Enterprises, the reality is that there are not many publications of relevance that deal with the topic of ITIL and SMEs.}
}

@article{rayyan-727967377,
  title={Twenty-eight years of component-based software engineering},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={111},
  pages={128-148},
  author={Vale, Tassio and Crnkovic, Ivica and de Almeida, Eduardo Santana and da Mota Silveira Neto, Paulo Anselmo and Cavalcanti, Yguaratã Cerqueira and de Lemos Meira, Silvio Romero},
  url={https://www.sciencedirect.com/science/article/pii/S0164121215002095},
  keywords={Systematic mapping study, Component-based software engineering, Component-based software development, Software component, Software},
  abstract={The idea of developing software components was envisioned more than forty years ago. In the past two decades, Component-Based Software Engineering (CBSE) has emerged as a distinguishable approach in software engineering, and it has attracted the attention of many researchers, which has led to many results being published in the research literature. There is a huge amount of knowledge encapsulated in conferences and journals targeting this area, but a systematic analysis of that knowledge is missing. For this reason, we aim to investigate the state-of-the-art of the CBSE area through a detailed literature review. To do this, 1231 studies dating from 1984 to 2012 were analyzed. Using the available evidence, this paper addresses five dimensions of CBSE: main objectives, research topics, application domains, research intensity and applied research methods. The main objectives found were to increase productivity, save costs and improve quality. The most addressed application domains are homogeneously divided between commercial-off-the-shelf (COTS), distributed and embedded systems. Intensity of research showed a considerable increase in the last fourteen years. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas that call for further research.}
}

@article{rayyan-727967378,
  title={A systematic literature review of actionable alert identification techniques for automated static code analysis},
  year={2011},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={53},
  number={4},
  pages={363-387},
  author={Heckman, Sarah and Williams, Laurie},
  url={https://www.sciencedirect.com/science/article/pii/S0950584910002235},
  keywords={Systematic literature review, Actionable alert identification, Actionable alert prediction, Automated static analysis, Unactionable alert mitigation, Warning prioritization},
  abstract={Context Automated static analysis (ASA) identifies potential source code anomalies early in the software development lifecycle that could lead to field failures. Excessive alert generation and a large proportion of unimportant or incorrect alerts (unactionable alerts) may cause developers to reject the use of ASA. Techniques that identify anomalies important enough for developers to fix (actionable alerts) may increase the usefulness of ASA in practice. Objective The goal of this work is to synthesize available research results to inform evidence-based selection of actionable alert identification techniques (AAIT). Method Relevant studies about AAITs were gathered via a systematic literature review. Results We selected 21 peer-reviewed studies of AAITs. The techniques use alert type selection; contextual information; data fusion; graph theory; machine learning; mathematical and statistical models; or dynamic detection to classify and prioritize actionable alerts. All of the AAITs are evaluated via an example with a variety of evaluation metrics. Conclusion The selected studies support (with varying strength), the premise that the effective use of ASA is improved by supplementing ASA with an AAIT. Seven of the 21 selected studies reported the precision of the proposed AAITs. The two studies with the highest precision built models using the subject program's history. Precision measures how well a technique identifies true actionable alerts out of all predicted actionable alerts. Precision does not measure the number of actionable alerts missed by an AAIT or how well an AAIT identifies unactionable alerts. Inconsistent use of evaluation metrics, subject programs, and ASAs in the selected studies preclude meta-analysis and prevent the current results from informing evidence-based selection of an AAIT. We propose building on an actionable alert identification benchmark for comparison and evaluation of AAIT from literature on a standard set of subjects and utilizing a common set of evaluation metrics.}
}

@article{rayyan-727967379,
  title={Distributed pair programming: A systematic literature review},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={63},
  pages={1-10},
  author={da Silva Estácio, Bernardo José and Prikladnicki, Rafael},
  url={https://www.sciencedirect.com/science/article/pii/S0950584915000476},
  keywords={Distributed Software Development, Distributed Pair Programming, Extreme Programming, Pair Programming},
  abstract={Context Geographically distributed teams have adopted agile practices as a work strategy. One of these practices is Distributed Pair Programming (DPP). DPP consists in two developers working remotely on the same design, algorithm or code. Objective In this paper we sought to identify and synthesize papers that describe and analyze DPP both from teaching and practice perspectives. Method We conducted a Systematic Literature Review to search for empirical evidence in eight digital libraries. Results Most of the 34 DPP primary studies identified explore DPP from a teaching perspective. We found that DPP requires a specific infrastructure, but the existing studies do not explore the impact of the distribution in the details. There are many tools proposed that support DPP practice, but few of them are evaluated within a software development team. Conclusion We need more studies that explore the effects of Pair Programming in the context of Distributed Software Development, such as coordination and communication. Most of the studies do not empirically evaluate DPP in industry. There is also a need to propose guidelines to use DPP in industry and as a teaching strategy.}
}

@article{rayyan-727967380,
  title={Time series forecasting using artificial neural networks methodologies: A systematic review},
  year={2018},
  journal={Future Computing and Informatics Journal},
  issn={2314-7288},
  volume={3},
  number={2},
  pages={334-340},
  author={Tealab, Ahmed},
  url={https://www.sciencedirect.com/science/article/pii/S2314728817300715},
  keywords={Neural networks, Forecasting, Moving averages, Nonlinear time series, Nerve Net, Neural Networks (Computer)},
  abstract={This paper studies the advances in time series forecasting models using artificial neural network methodologies in a systematic literature review. The systematic review has been done using a manual search of the published papers in the last 11 years (2006–2016) for the time series forecasting using new neural network models and the used methods are displayed. In the covered period in the study, the results obtained found 17 studies that meet all the requirements of the search criteria. Only three of the obtained proposals considered a process different to the autoregressive of a neural networks model. These results conclude that, although there are many studies that presented the application of neural network models, but few of them proposed new neural networks models for forecasting that considered theoretical support and a systematic procedure in the construction of model. This leads to the importance of formulating new models of neural networks.}
}

@article{rayyan-727967381,
  title={Recruitment, engagement and feedback in empirical software engineering studies in industrial contexts},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={98},
  pages={161-172},
  author={Salleh, Norsaremah and Hoda, Rashina and Su, Moon Ting and Kanij, Tanjila and Grundy, John},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917303786},
  keywords={Empirical software engineering, Case study, Survey, Challenges, Solutions, Grounded theory, Industry, Quasi-experiment, Recommendations, Research, Software, Feedback},
  abstract={Context Research carried out in industrial contexts are recognized as important to the advancement of software engineering knowledge and practice. However, several challenges present themselves in the three key phases of research carried out in industrial contexts, recruitment, engagement and feedback. Objective The aim of this paper is to report the challenges related to each of the three phases of research carried out in industrial contexts, and the associated solutions we have found useful from our combined body of industrial empirical software engineering research studies spanning four case studies, five grounded theory studies, seven survey studies and two quasi-experimental studies involving a total of over 400 industrial participants in the past decade. Method We designed an instrument to gather details of our studies carried out in industrial contexts and performed thematic analysis to synthesise and draw out the most prominent challenges faced. Results We present a set of recommendations around study design, conduct and reporting to try and mitigate some of these challenges as they apply specifically to industrial empirical research. Conclusion These recommendations can guide researchers, novice and experienced, working in close collaboration with industry stakeholders to make the most of their industrial software engineering research.}
}

@article{rayyan-727967382,
  title={Business intelligence and analytics in small and medium-sized enterprises: A systematic literature review},
  year={2017},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={121},
  pages={194-205},
  author={Llave, Marilex Rea},
  url={https://www.sciencedirect.com/science/article/pii/S1877050917322184},
  keywords={analytics, BI&A adoption, BI&A benefits, BI&A implementation, BI&A review, BI&A solutions, Business intelligence, SMEs, Intelligence},
  abstract={Despite much interest in business intelligence and analytics (BI&A), empirical research shows that small and medium-sized enterprises (SMEs) are still lagging behind in the proliferation of BI&A. However, there are no studies found on literature reviewing research on BI&A in SMEs. This paper collects, categorizes, synthesizes, and analyzes 62 articles related to BI&A in SMEs. The identified research topics being addressed in BI&A include: BI&A components, BI&A solutions, Mobile BI&A, Cloud BI&A, BI&A application, BI&A adoption, BI&A implementation, and BI&A benefits. Further, research gaps and directions for future research are presented to facilitate the progression of BI&A in SMEs research.}
}

@article{rayyan-727967383,
  title={Toward the tools selection in model based system engineering for embedded systems—A systematic literature review},
  year={2015},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={106},
  pages={150-163},
  author={Rashid, Muhammad and Anwar, Muhammad Waseem and Khan, Aamir M},
  url={https://www.sciencedirect.com/science/article/pii/S016412121500103X},
  keywords={Tools, Embedded systems, MBSE},
  abstract={Model based system engineering (MBSE) is a systematic approach of modeling which is frequently used to support requirement specification, design, verification and validation activities of system development. However, it is difficult to customize MBSE approach for the development of embedded systems due to their diverse behavioral aspects. Furthermore, appropriate tools selection to perform particular MBSE activities is always challenging. This paper focuses on the identification and classification of recent research practices pertaining to embedded systems development through MBSE approach. Consequently, a comprehensive analysis of various MBSE tools has been presented. Systematic literature review (SLR) has been used to identify 61 research practices published during 2008–2014. The identified researches have been classified into six different categories to analyze various aspects of MBSE approach for embedded systems. Consequently, 39 preliminary tools are identified that have been used in recent researches. Furthermore, classification and evaluation of tools have been presented. This research highlights important trends and approaches of MBSE to support development of embedded systems. A comprehensive investigation of tools in this article facilitates researchers, practitioners and developers to select appropriate tools according to their requirements.}
}

@article{rayyan-727967384,
  title={A systematic review of software robustness},
  year={2013},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={55},
  number={1},
  pages={1-17},
  author={Shahrokni, Ali and Feldt, Robert},
  url={https://www.sciencedirect.com/science/article/pii/S0950584912001048},
  keywords={Systematic review, Robustness, Software robustness, Software},
  abstract={Context With the increased use of software for running key functions in modern society it is of utmost importance to understand software robustness and how to support it. Although there have been many contributions to the field there is a lack of a coherent and summary view. Objective To address this issue, we have conducted a literature review in the field of robustness. Method This review has been conducted by following guidelines for systematic literature reviews. Systematic reviews are used to find and classify all existing and available literature in a certain field. Results From 9193 initial papers found in three well-known research databases, the 144 relevant papers were extracted through a multi-step filtering process with independent validation in each step. These papers were then further analyzed and categorized based on their development phase, domain, research, contribution and evaluation type. The results indicate that most existing results on software robustness focus on verification and validation of Commercial of the shelf (COTS) or operating systems or propose design solutions for robustness while there is a lack of results on how to elicit and specify robustness requirements. The research is typically solution proposals with little to no evaluation and when there is some evaluation it is primarily done with small, toy/academic example systems. Conclusion We conclude that there is a need for more software robustness research on real-world, industrial systems and on software development phases other than testing and design, in particular on requirements engineering.}
}

@article{rayyan-727967385,
  title={Software development project success and failure from the supplier's perspective: A systematic literature review},
  year={2012},
  journal={International Journal of Project Management},
  issn={0263-7863},
  volume={30},
  number={4},
  pages={458-469},
  author={Savolainen, Paula and Ahonen, Jarmo J and Richardson, Ita},
  url={https://www.sciencedirect.com/science/article/pii/S0263786311000901},
  keywords={Literature review, Project management, Customer, Outsourcing, Project failure, Project success, Software development project, Supplier, Software},
  abstract={In this paper, we consider software development project success and failure from the supplier's perspective. First we clarified concepts in order to be able to exclude review articles on in-house projects, continuous services, the customer's perspective, and software product development, with the aim of providing valid results for supplier firms. We divided success criteria into project success and project management (PM) success, and, in seven articles, identified three success criteria from the supplier's perspective: customer satisfaction, short-term business benefits, and long-term business benefits. In contrast, no definition of software development project failure was found. Articles were found in seven different journals, showing that knowledge on software development project success from the supplier's perspective is fragmented. This impedes the growth of knowledge on this topic.}
}

@article{rayyan-727967386,
  title={Software clone detection: A systematic review},
  year={2013},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={55},
  number={7},
  pages={1165-1199},
  author={Rattan, Dhavleesh and Bhatia, Rajesh and Singh, Maninder},
  url={https://www.sciencedirect.com/science/article/pii/S0950584913000323},
  keywords={Systematic literature review, Clone detection, Model based clone, Semantic clones, Software clone, Software},
  abstract={Context Reusing software by means of copy and paste is a frequent activity in software development. The duplicated code is known as a software clone and the activity is known as code cloning. Software clones may lead to bug propagation and serious maintenance problems. Objective This study reports an extensive systematic literature review of software clones in general and software clone detection in particular. Method We used the standard systematic literature review method based on a comprehensive set of 213 articles from a total of 2039 articles published in 11 leading journals and 37 premier conferences and workshops. Results Existing literature about software clones is classified broadly into different categories. The importance of semantic clone detection and model based clone detection led to different classifications. Empirical evaluation of clone detection tools/techniques is presented. Clone management, its benefits and cross cutting nature is reported. Number of studies pertaining to nine different types of clones is reported. Thirteen intermediate representations and 24 match detection techniques are reported. Conclusion We call for an increased awareness of the potential benefits of software clone management, and identify the need to develop semantic and model clone detection techniques. Recommendations are given for future research.}
}

@article{rayyan-727967387,
  title={A systematic review of gamification in e-Health},
  year={2017},
  journal={Journal of Biomedical Informatics},
  issn={1532-0464},
  volume={71},
  pages={31-48},
  author={Sardi, Lamyae and Idri, Ali and Fernández-Alemán, José Luis},
  url={https://www.sciencedirect.com/science/article/pii/S1532046417301065},
  keywords={Systematic literature review, e-Health, Serious game, Gamification, Application},
  abstract={Gamification is a relatively new trend that focuses on applying game mechanics to non-game contexts in order to engage audiences and to inject a little fun into mundane activities besides generating motivational and cognitive benefits. While many fields such as Business, Marketing and e-Learning have taken advantage of the potential of gamification, the digital healthcare domain has also started to exploit this emerging trend. This paper aims to summarize the current knowledge regarding gamified e-Health applications. A systematic literature review was therefore conducted to explore the various gamification strategies employed in e-Health and to address the benefits and the pitfalls of this emerging discipline. A total of 46 studies from multiple sources were then considered and thoroughly investigated. The results show that the majority of the papers selected reported gamification and serious gaming in health and wellness contexts related specifically to chronic disease rehabilitation, physical activity and mental health. Although gamification in e-Health has attracted a great deal of attention during the last few years, there is still a dearth of valid empirical evidence in this field. Moreover, most of the e-Health applications and serious games investigated have been proven to yield solely short-term engagement through extrinsic rewards. For gamification to reach its full potential, it is therefore necessary to build e-Health solutions on well-founded theories that exploit the core experience and psychological effects of game mechanics.}
}

@article{rayyan-727967388,
  title={Analyzing the concept of technical debt in the context of agile software development: A systematic literature review},
  year={2017},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={82},
  pages={139-158},
  author={Behutiye, Woubshet Nema and Rodríguez, Pilar and Oivo, Markku and Tosun, Ayşe},
  url={https://www.sciencedirect.com/science/article/pii/S0950584916302890},
  keywords={Systematic literature review, Agile software development, Technical debt, Technical debt management, Software},
  abstract={Context Technical debt (TD) is a metaphor that is used to communicate the consequences of poor software development practices to non-technical stakeholders. In recent years, it has gained significant attention in agile software development (ASD). Objective The purpose of this study is to analyze and synthesize the state of the art of TD, and its causes, consequences, and management strategies in the context of ASD. Research Method Using a systematic literature review (SLR), 38 primary studies, out of 346 studies, were identified and analyzed. Results We found five research areas of interest related to the literature of TD in ASD. Among those areas, “managing TD in ASD” received the highest attention, followed by “architecture in ASD and its relationship with TD”. In addition, eight categories regarding the causes and five categories regarding the consequences of incurring TD in ASD were identified. “Focus on quick delivery” and “architectural and design issues” were the most popular causes of incurring TD in ASD. “Reduced productivity”, “system degradation” and “increased maintenance cost” were identified as significant consequences of incurring TD in ASD. Additionally, we found 12 strategies for managing TD in the context of ASD, out of which “refactoring” and “enhancing the visibility of TD” were the most significant. Conclusion The results of this study provide a structured synthesis of TD and its management in the context of ASD as well as potential research areas for further investigation.}
}

@article{rayyan-727967389,
  title={Virtual and digital outcrops in the petroleum industry: A systematic review},
  year={2020},
  journal={Earth-Science Reviews},
  issn={0012-8252},
  volume={208},
  pages={103260},
  author={Marques, Ademir and Horota, Rafael Kenji and de Souza, Eniuce Menezes and Kupssinskü, Lucas and Rossa, Pedro and Aires, Alysson Soares and Bachi, Leonardo and Veronez, Mauricio Roberto and Gonzaga, Luiz and Cazarin, Caroline Lessio},
  url={https://www.sciencedirect.com/science/article/pii/S0012825220303068},
  keywords={Systematic, Modeling, Review, 3D model, Digital outcrop, Fluid flow, LiDAR, Petroleum, Photogrammetry, Reservoir, UAV, Virtual outcrop},
  abstract={The study of outcrop analogues of petroleum reservoirs is well established in the petroleum industry through the use of digital outcrop models (DOMs). These models, which are also known as virtual outcrop models (VOMs) or 3D outcrops, are of great importance for understanding the behavior of actual reservoirs. This topic has been reviewed by many authors, and the studies vary in detail according to the technologies involved. The present study applies systematic review methodology traversing a number of articles to find the trends in studies utilizing DOMs. The articles included in this review indicate that the technologies used to generate DOMs are still predominantly classified as Light Detection and Ranging (LiDAR) and digital photogrammetry, with the first being present in most of the works, and the latter attracting attention owing to the popularity of unmanned aerial vehicles (UAVs). These studies have attracted a significant amount of attention to outcrop analysis, and the information acquired can be used to better fit reservoir simulations. Furthermore, a trend is identified with a focus on outcrop geometry and structural data. This work also discusses some of the available opportunities related to the generation of DOMs as well as emerging technologies that can improve the quality of the outcrop models in order to provide better reservoir simulations. Finally, this work discusses the findings and highlights of the articles answering the initially raised research questions.}
}

@article{rayyan-727967390,
  title={On the representation and aggregation of evidence in software engineering: A theory and belief-based perspective},
  year={2013},
  journal={Electronic Notes in Theoretical Computer Science},
  issn={1571-0661},
  volume={292},
  pages={95-118},
  author={Medeiros dos Santos, Paulo Sérgio and Travassos, Guilherme Horta},
  url={https://www.sciencedirect.com/science/article/pii/S1571066113000108},
  keywords={theory, software engineering, belief functions, dempster-shafer theory, evidence aggregation, evidence representation, post-mortem, research synthesis, Software},
  abstract={An adequate representation and a feasible aggregation procedure of evidence represents a challenging problem in many disciplines. The right representation can help scientists discuss and present the results of their findings and, if it is simple enough, it can be useful for practitioners to base their decisions on improvement implementations. The aggregation strengthens confidence in comparison to single evidence and is an important contribution to the body of knowledge. In this paper, we present a preliminary proposal to use empirically-based theories and belief functions as a means to represent and aggregate evidence. By having evidence explained by the same theory, we used belief functions to combine them in a way that the theory propositions (cause-effect values) result from combined evidence. We suggest this can be an useful way to obtain a good estimate of multiple evidence combination. In addition, we indicate its possible usefulness for practitioners to formalize and reuse their experiences. A real-case application of the approach is presented by formulating a theory for Usage-Based Reading inspection technique and aggregating the evidence acquired in three related empirical studies. This application indicated that the approach can give compatible results with the aggregated evidence.}
}

@article{rayyan-727967391,
  title={The experimental applications of search-based techniques for model-based testing: Taxonomy and systematic literature review},
  year={2016},
  journal={Applied Soft Computing},
  issn={1568-4946},
  volume={49},
  pages={1094-1117},
  author={Saeed, Aneesa and Ab Hamid, Siti Hafizah and Mustafa, Mumtaz Begum},
  url={https://www.sciencedirect.com/science/article/pii/S1568494616304240},
  keywords={Software testing, Systematic literature review, Model-based testing, Taxonomy, Search-based techniques, Test case generation},
  abstract={Context Model-based testing (MBT) aims to generate executable test cases from behavioral models of software systems. MBT gains interest in industry and academia due to its provision of systematic, automated, and comprehensive testing. Researchers have successfully applied search-based techniques (SBTs) by automating the search for an optimal set of test cases at reasonable cost compared to other more expensive techniques. Thus, there is a recent surge toward the applications of SBTs for MBT because the generated test cases are optimal and have low computational cost. However, successful, future SBTs for MBT applications demand deep insight into its existing experimental applications that underlines stringent issues and challenges, which is lacking in the literature. Objective The objective of this study is to comprehensively analyze the current state-of-the-art of the experimental applications of SBTs for MBT and present the limitations of the current literature to direct future research. Method We conducted a systematic literature review (SLR) using 72 experimental papers from six data sources. We proposed a taxonomy based on the literature to categorize the characteristics of the current applications. Results The results indicate that the majority of the existing applications of SBTs for MBT focus on functional and structural coverage purposes, as opposed to stress testing, regression testing and graphical user interface (GUI) testing. We found research gaps in the existing applications in five areas: applying multi-objective SBTs, proposing hybrid techniques, handling complex constraints, addressing data and requirement-based adequacy criteria, and adapting landscape visualization. Only twelve studies proposed and empirically evaluated the SBTs for complex systems in MBT. Conclusion This extensive systematic analysis of the existing literature based on the proposed taxonomy enables to assist researchers in exploring the existing research efforts and reveal the limitations that need additional investigation.}
}

@article{rayyan-727967392,
  title={The impact of knowledge management processes on information systems: A systematic review},
  year={2018},
  journal={International Journal of Information Management},
  issn={0268-4012},
  volume={43},
  pages={173-187},
  author={Al-Emran, Mostafa and Mezhuyev, Vitaliy and Kamaludin, Adzhar and Shaalan, Khaled},
  url={https://www.sciencedirect.com/science/article/pii/S0268401217308186},
  keywords={Systematic review, Information systems, Knowledge management processes, Information Systems},
  abstract={Knowledge Management (KM) processes play a significant role in the implementation of various Information Systems (IS). Several review studies were carried out to afford a better understanding of the current research trend of KM processes. However, this issue still needs to be examined from other perspectives. It is observed that previous research neglects the examination of KM processes studies with regard to ISs. The current study systematically reviews and sheds the light on KM processes studies related to ISs aiming to provide a comprehensive analysis of 41 research articles published in peer-reviewed journals from 2001 to 2018. The main findings of this study indicate that knowledge sharing is the most frequent KM process studied, followed by knowledge acquisition and knowledge application. Besides, questionnaire surveys were found to be the primarily relied research methods for data collection in the context of KM processes. In addition, 78% of the analyzed studies registered positive research outcomes. In terms of IS type, most of the analyzed studies focused on investigating the impact of KM processes on E-business systems, knowledge management systems, and IS outsourcing, respectively. Additionally, in terms of data collection, the majority of the analyzed studies were primarily focused on the participants who are IS executives/managers. Furthermore, most of the analyzed studies that achieved positive outcomes were carried out in China. To that end, this review study attempts to demonstrate and detail the recent increase in the interest and the advancement made in KM processes research considering ISs studies, which form an essential reference for scholars in KM field.}
}

@article{rayyan-727967393,
  title={Factors influencing clients in the selection of offshore software outsourcing vendors: An exploratory study using a systematic literature review},
  year={2011},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={84},
  number={4},
  pages={686-699},
  author={Khan, Siffat Ullah and Niazi, Mahmood and Ahmad, Rashid},
  url={https://www.sciencedirect.com/science/article/pii/S0164121210003298},
  keywords={Systematic literature review, Offshore software development outsourcing (OSDO), SOVRM, Vendors, Software},
  abstract={Context Offshore software development outsourcing is a modern business strategy for developing high quality software at low cost. Objective The objective of this research paper is to identify and analyse factors that are important in terms of the competitiveness of vendor organisations in attracting outsourcing projects. Method We performed a systematic literature review (SLR) by applying our customised search strings which were derived from our research questions. We performed all the SLR steps, such as the protocol development, initial selection, final selection, quality assessment, data extraction and data synthesis. Results We have identified factors such as cost-saving, skilled human resource, appropriate infrastructure, quality of product and services, efficient outsourcing relationships management, and an organisation's track record of successful projects which are generally considered important by the outsourcing clients. Our results indicate that appropriate infrastructure, cost-saving, and skilled human resource are common in three continents, namely Asia, North America and Europe. We identified appropriate infrastructure, cost-saving, and quality of products and services as being common in three types of organisations (small, medium and large). We have also identified four factors-appropriate infrastructure, cost-saving, quality of products and services, and skilled human resource as being common in the two decades (1990–1999 and 2000–mid 2008). Conclusions Cost-saving should not be considered as the driving factor in the selection process of software development outsourcing vendors. Vendors should rather address other factors in order to compete in the OSDO business, such as skilled human resource, appropriate infrastructure and quality of products and services.}
}

@article{rayyan-727967394,
  title={Robotics applications grounded in learning theories on tertiary education: A systematic review},
  year={2017},
  journal={Computers & Education},
  issn={0360-1315},
  volume={112},
  pages={97-107},
  author={Spolaôr, Newton and Benitti, Fabiane B.Vavassori},
  url={https://www.sciencedirect.com/science/article/pii/S0360131517300970},
  keywords={Improving classroom teaching, Evaluation methodologies, Post-secondary education, Teaching/learning strategies, Robotics, Learning},
  abstract={Empirical evidence suggests the effectiveness of robotics as a learning complementary tool in tertiary education. In this context, some experiences benefited from the link between educational practice and theory. However, a comprehensive survey on initiatives that explores this link in universities and colleges is missing. This work systematically reviews quantitatively assessed robots applications, grounded in learning theories, in tertiary institutions. By applying a protocol review in different bibliographic databases, 15 papers were selected for synthesis. As a result, experiences developing non-robotic concepts and skills in universities and colleges were found. In most of the cases, Computer Science and Engineering undergraduate courses were involved. In addition, empirical results reported by the selected publications suggest that some literature proposals can be useful in practice. Based on the panorama obtained, this work also points out future directions for practitioners and researchers in education.}
}

@article{rayyan-727967395,
  title={Static analysis of android apps: A systematic literature review},
  year={2017},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={88},
  pages={67-95},
  author={Li, Li and Bissyandé, Tegawendé F and Papadakis, Mike and Rasthofer, Siegfried and Bartel, Alexandre and Octeau, Damien and Klein, Jacques and Traon, Le},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917302987},
  keywords={Methyltestosterone},
  abstract={Context Static analysis exploits techniques that parse program source code or bytecode, often traversing program paths to check some program properties. Static analysis approaches have been proposed for different tasks, including for assessing the security of Android apps, detecting app clones, automating test cases generation, or for uncovering non-functional issues related to performance or energy. The literature thus has proposed a large body of works, each of which attempts to tackle one or more of the several challenges that program analyzers face when dealing with Android apps. Objective We aim to provide a clear view of the state-of-the-art works that statically analyze Android apps, from which we highlight the trends of static analysis approaches, pinpoint where the focus has been put, and enumerate the key aspects where future researches are still needed. Method We have performed a systematic literature review (SLR) which involves studying 124 research papers published in software engineering, programming languages and security venues in the last 5 years (January 2011–December 2015). This review is performed mainly in five dimensions: problems targeted by the approach, fundamental techniques used by authors, static analysis sensitivities considered, android characteristics taken into account and the scale of evaluation performed. Results Our in-depth examination has led to several key findings: 1) Static analysis is largely performed to uncover security and privacy issues; 2) The Soot framework and the Jimple intermediate representation are the most adopted basic support tool and format, respectively; 3) Taint analysis remains the most applied technique in research approaches; 4) Most approaches support several analysis sensitivities, but very few approaches consider path-sensitivity; 5) There is no single work that has been proposed to tackle all challenges of static analysis that are related to Android programming; and 6) Only a small portion of state-of-the-art works have made their artifacts publicly available. Conclusion The research community is still facing a number of challenges for building approaches that are aware altogether of implicit-Flows, dynamic code loading features, reflective calls, native code and multi-threading, in order to implement sound and highly precise static analyzers.}
}

@article{rayyan-727967396,
  title={A systematic review on the profiling of digital news portal for big data veracity},
  year={2015},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={72},
  pages={390-397},
  author={Jamil, Normala Binti Che Eembi @ and Ishak, Iskandar Bin and Sidi, Fatimah and Affendey, Lilly Suriani and Mamat, Ali},
  url={https://www.sciencedirect.com/science/article/pii/S1877050915036157},
  keywords={Big Data, Profiling, Ranking, Veracity},
  abstract={Currently, digital news portals have been one of the most important news sources for Internet users. However, the way it is written depends on the direction of the content. One approach to news reporting is through manipulative writing. Such method of writing has created a number of adverse outcomes such as political unrest, slander and negative perception towards the particular organization, personnel, and country. It is important for readers to choose and select news portal that is reporting positively and to neglect portals which practices manipulative writing approach for their own gains or causing negative impact towards the community. The aim of this study is to structure and analyzed the literature related to data veracity research that can be used to the profile of digital news portal. The method that has been used in this paper is to classify and define data veracity; a systematic literature review is a conduct. It includes journal and conference proceedings. The results come out with objectives in data veracity, the structure of research topics, research trends with publications and framework veracity model validated. This paper provides a complete review of literature related to profiling digital news portal in data veracity.}
}

@article{rayyan-727967397,
  title={A systematic review and an expert survey on capabilities supporting multi product lines},
  year={2012},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={54},
  number={8},
  pages={828-852},
  author={Holl, Gerald and Grünbacher, Paul and Rabiser, Rick},
  url={https://www.sciencedirect.com/science/article/pii/S095058491200033X},
  keywords={Systematic literature review, Large-scale systems, Multi product lines, Product line engineering},
  abstract={Context Complex software-intensive systems comprise many subsystems that are often based on heterogeneous technological platforms and managed by different organizational units. Multi product lines (MPLs) are an emerging area of research addressing variability management for such large-scale or ultra-large-scale systems. Despite the increasing number of publications addressing MPLs the research area is still quite fragmented. Objective The aims of this paper are thus to identify, describe, and classify existing approaches supporting MPLs and to increase the understanding of the underlying research issues. Furthermore, the paper aims at defining success-critical capabilities of infrastructures supporting MPLs. Method Using a systematic literature review we identify and analyze existing approaches and research issues regarding MPLs. Approaches described in the literature support capabilities needed to define and operate MPLs. We derive capabilities supporting MPLs from the results of the systematic literature review. We validate and refine these capabilities based on a survey among experts from academia and industry. Results The paper discusses key research issues in MPLs and presents basic and advanced capabilities supporting MPLs. We also show examples from research approaches that demonstrate how these capabilities can be realized. Conclusions We conclude that approaches supporting MPLs need to consider both technical aspects like structuring large models and defining dependencies between product lines as well as organizational aspects such as distributed modeling and product derivation by multiple stakeholders. The identified capabilities can help to build, enhance, and evaluate MPL approaches.}
}

@article{rayyan-727967398,
  title={The contribution that empirical studies performed in industry make to the findings of systematic reviews: A tertiary study},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={94},
  pages={234-244},
  author={Budgen, David and Brereton, Pearl and Williams, Nikki and Drummond, Sarah},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917303798},
  keywords={Systematic review, Case study, Industry study, Primary study},
  abstract={Context Systematic reviews can provide useful knowledge for software engineering practice, by aggregating and synthesising empirical studies related to a specific topic. Objective We sought to assess how far the findings of systematic reviews addressing practice-oriented topics have been derived from empirical studies that were performed in industry or that used industry data. Method We drew upon and augmented the data obtained from a tertiary study that performed a systematic review of systematic reviews published in the period up to the end of 2015, seeking to identify those with findings that are relevant for teaching and practice. For the supplementary analysis reported here, we then examined the profiles of the primary studies as reported in each systematic review. Results We identified 48 systematic reviews as candidates for further analysis. The many differences that arise between systematic reviews, together with the incompleteness of reporting for these, mean that our counts should be treated as indicative rather than definitive. However, even when allowing for problems of classification, the findings from the majority of these systematic reviews were predominantly derived from using primary studies conducted in industry. There was also an emphasis upon the use of case studies, and a number of the systematic reviews also made some use of weaker ‘experience' or even ‘opinion' papers. Conclusions Primary studies from industry play an important role as inputs to systematic reviews. Using more rigorous industry-based primary studies can give greater authority to the findings of the systematic reviews, and should help with the creation of a corpus of sound empirical data to support evidence-informed decisions.}
}

@article{rayyan-727967399,
  title={Architectural tactics for cyber-foraging: Results of a systematic literature review},
  year={2015},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={107},
  pages={158-186},
  author={Lewis, Grace and Lago, Patricia},
  url={https://www.sciencedirect.com/science/article/pii/S0164121215001211},
  keywords={Software architecture, Cyber-foraging, Mobile cloud computing},
  abstract={Mobile devices have become for many the preferred way of interacting with the Internet, social media and the enterprise. However, mobile devices still do not have the computing power and battery life that will allow them to perform effectively over long periods of time, or for executing applications that require extensive communication, computation, or low latency. Cyber-foraging is a technique to enable mobile devices to extend their computing power and storage by offloading computation or data to more powerful servers located in the cloud or in single-hop proximity. This article presents the results of a systematic literature review (SLR) on architectures that support cyber-foraging. Elements of the identified architectures were codified in the form of Architectural Tactics for Cyber-Foraging. These tactics will help architects extend their design reasoning toward cyber-foraging as a way to support the mobile applications of the present and the future.}
}

@article{rayyan-727967400,
  title={Clinical text classification research trends: Systematic literature review and open issues},
  year={2019},
  journal={Expert Systems with Applications},
  issn={0957-4174},
  volume={116},
  pages={494-520},
  author={Mujtaba, Ghulam and Shuib, Liyana and Idris, Norisma and Hoo, Wai Lam and Raj, Ram Gopal and Khowaja, Kamran and Shaikh, Khairunisa and Nweke, Henry Friday},
  url={https://www.sciencedirect.com/science/article/pii/S0957417418306110},
  keywords={Performance metrics, Clinical text classification, Feature engineering, Rule-based text classification, Supervised machine learning},
  abstract={The pervasive use of electronic health databases has increased the accessibility of free-text clinical reports for supplementary use. Several text classification approaches, such as supervised machine learning (SML) or rule-based approaches, have been utilized to obtain beneficial information from free-text clinical reports. In recent years, many researchers have worked in the clinical text classification field and published their results in academic journals. However, to the best of our knowledge, no comprehensive systematic literature review (SLR) has recapitulated the existing primary studies on clinical text classification in the last five years. Thus, the current study aims to present SLR of academic articles on clinical text classification published from January 2013 to January 2018. Accordingly, we intend to maximize the procedural decision analysis in six aspects, namely, types of clinical reports, data sets and their characteristics, pre-processing and sampling techniques, feature engineering, machine learning algorithms, and performance metrics. To achieve our objective, 72 primary studies from 8 bibliographic databases were systematically selected and rigorously reviewed from the perspective of the six aspects. This review identified nine types of clinical reports, four types of data sets (i.e., homogeneous–homogenous, homogenous–heterogeneous, heterogeneous–homogenous, and heterogeneous–heterogeneous), two sampling techniques (i.e., over-sampling and under-sampling), and nine pre-processing techniques. Moreover, this review determined bag of words, bag of phrases, and bag of concepts features when represented by either term frequency or term frequency with inverse document frequency, thereby showing improved classification results. SML-based or rule-based approaches were generally employed to classify the clinical reports. To measure the performance of these classification approaches, we used precision, recall, F-measure, accuracy, AUC, and specificity in binary class problems. In multi-class problems, we primarily used micro or macro-averaging precision, recall, or F-measure. Lastly, open research issues and challenges are presented for future scholars who are interested in clinical text classification. This SLR will definitely be a beneficial resource for researchers engaged in clinical text classification.}
}

@article{rayyan-727967401,
  title={A visual analysis approach to validate the selection review of primary studies in systematic reviews},
  year={2012},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={54},
  number={10},
  pages={1079-1091},
  author={Felizardo, Katia R and Andery, Gabriel F and Paulovich, Fernando V and Minghim, Rosane and Maldonado, José C},
  url={https://www.sciencedirect.com/science/article/pii/S0950584912000742},
  keywords={Citation document map, Content document map, Information visualization, Systematic Literature Review (SLR), Visual Text Mining (VTM)},
  abstract={Context Systematic Literature Reviews (SLRs) are an important component to identify and aggregate research evidence from different empirical studies. Despite its relevance, most of the process is conducted manually, implying additional effort when the Selection Review task is performed and leading to reading all studies under analysis more than once. Objective We propose an approach based on Visual Text Mining (VTM) techniques to assist the Selection Review task in SLR. It is implemented into a VTM tool (Revis), which is freely available for use. Method We have selected and implemented appropriate visualization techniques into our approach and validated and demonstrated its usefulness in performing real SLRs. Results The results have shown that employment of VTM techniques can successfully assist in the Selection Review task, speeding up the entire SLR process in comparison to the conventional approach. Conclusion VTM techniques are valuable tools to be used in the context of selecting studies in the SLR process, prone to speed up some stages of SLRs.}
}

@article{rayyan-727967402,
  title={Higher order mutation testing: A systematic literature review},
  year={2017},
  journal={Computer Science Review},
  issn={1574-0137},
  volume={25},
  pages={29-48},
  author={Ghiduk, Ahmed S and Girgis, Moheb R and Shehata, Marwa H},
  url={https://www.sciencedirect.com/science/article/pii/S1574013716301095},
  keywords={First-order mutants, Higher-order mutants, Higher-order mutation testing, Mutation testing, Mutation},
  abstract={Mutation testing is the process whereby a fault is deliberately inserted into a software system, in order to assess the quality of test data, in terms of its ability to find this fault. Mutation testing is also used as a way to drive the test data development process. Traditionally, faults were inserted one by one into a software system, but more recently there has been an upsurge of interest by the area of higher-order mutation, in which multiple faults are inserted into the system at once. Originally, this was thought to be too expensive, as there was already a concern that the size of the pool of mutants for traditional mutation was already too large to handle. However, following a seminal publication in 2008, it was realized that the space of higher-order mutants (HOMs) could be searched for useful mutants that drive testing harder, and to reduce the overall test effort, by clever combination of first-order mutants. As a result, many authors examined the way in which HOM testing could find subtle hard to kill faults, capture partial fault masking, reduce equivalent mutants problem, reduce test effort while increasing effectiveness, and capture more realistic faults than those captured by simple insertion of first-order mutants. Because of the upsurge of interest in the previous issues, this paper presents the first Systematic Literature Review research specifically targeted at a higher-order mutation. This Systematic Literature Review analyzes the results of more than one hundred sixty research articles in this area. The current paper presents qualitative results and bibliometric analysis for the surveyed articles. In addition, it augments these results with scientific findings and quantitative results from the primary literature. As a result of this work, this SLR presents an outline for many future work.}
}

@article{rayyan-727967403,
  title={A systematic literature review: Refactoring for disclosing code smells in object oriented software},
  year={2018},
  journal={Ain Shams Engineering Journal},
  issn={2090-4479},
  volume={9},
  number={4},
  pages={2129-2151},
  author={Singh, Satwinder and Kaur, Sharanpreet},
  url={https://www.sciencedirect.com/science/article/pii/S2090447917300412},
  keywords={Code smells, Refactoring, Anti-patterns, Smell, Software},
  abstract={Context Reusing a design pattern is not always in the favor of developers. Thus, the code starts smelling. The presence of “Code Smells” leads to more difficulties for the developers. This racket of code smells is sometimes called Anti-Patterns. Objective The paper aimed at a systematic literature review of refactoring with respect to code smells. However the review of refactoring is done in general and the identification of code smells and anti-patterns is performed in depth. Method A systematic literature survey has been performed on 238 research items that includes articles from leading Conferences, Workshops and premier journals, theses of researchers and book chapters. Results Several data sets and tools for performing refactoring have been revealed under the specified research questions. Conclusion The work done in the paper is an addition to prior systematic literature surveys. With the study of paper the attentiveness of readers about code smells and anti-patterns will be enhanced.}
}

@article{rayyan-727967404,
  title={Obstacles in data distribution service middleware: A systematic review},
  year={2017},
  journal={Future Generation Computer Systems},
  issn={0167-739X},
  volume={68},
  pages={191-210},
  author={Köksal, Ömer and Tekinerdogan, Bedir},
  url={https://www.sciencedirect.com/science/article/pii/S0167739X1630351X},
  keywords={Systematic literature review, Data Distribution Service (DDS), Middleware},
  abstract={Context: Data Distribution Service (DDS) is a standard data-centric publish–subscribe programming model and specification for distributed systems. DDS has been applied for the development of high performance distributed systems such as in the defense, finance, automotive, and simulation domains. Various papers have been written on the application of DDS, however, there has been no attempt to systematically review and categorize the identified obstacles. Objective: The overall objective of this paper is to identify the state of the art of DDS, and describe the main lessons learned and obstacles in applying DDS. In addition, we aim to identify the important open research issues. Method: A systematic literature review (SLR) is conducted by a multiphase study selection process using the published literature since the introduction of DDS in 2003. Results: We reviewed 468 papers that are discovered using a well-planned review protocol, and 34 of them were assessed as primary studies related to our research questions. Conclusions: We have identified 11 basic categories for describing the identified obstacles and the corresponding research challenges that can be used to depict the state-of-the-art in DDS and provide a vision for further research.}
}

@article{rayyan-727967405,
  title={Experiences using systematic review guidelines},
  year={2007},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={80},
  number={9},
  pages={1425-1437},
  author={Staples, Mark and Niazi, Mahmood},
  url={https://www.sciencedirect.com/science/article/pii/S0164121206002962},
  keywords={Empirical software engineering, Systematic review},
  abstract={Systematic review is a method to identify, assess and analyse published primary studies to investigate research questions. We critique recently published guidelines for performing systematic reviews on software engineering, and comment on systematic review generally with respect to our experience conducting one. Overall we recommend the guidelines. We recommend researchers clearly and narrowly define research questions to reduce overall effort, and to improve selection and data extraction. We suggest that “complementary” research questions can help clarify the main questions and define selection criteria. We show our project timeline, and discuss possibilities for automating and increasing the acceptance of systematic review.}
}

@article{rayyan-727967406,
  title={On strategies for testing software product lines: A systematic literature review},
  year={2014},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={56},
  number={10},
  pages={1183-1199},
  author={do Carmo Machado, Ivan and McGregor, John D and Cavalcanti, Yguaratã Cerqueira and de Almeida, Eduardo Santana},
  url={https://www.sciencedirect.com/science/article/pii/S0950584914000834},
  keywords={Software product lines, Software testing, Systematic literature review, Software quality, Software},
  abstract={Context Testing plays an important role in the quality assurance process for software product line engineering. There are many opportunities for economies of scope and scale in the testing activities, but techniques that can take advantage of these opportunities are still needed. Objective The objective of this study is to identify testing strategies that have the potential to achieve these economies, and to provide a synthesis of available research on SPL testing strategies, to be applied towards reaching higher defect detection rates and reduced quality assurance effort. Method We performed a literature review of two hundred seventy-six studies published from the year 1998 up to the 1st semester of 2013. We used several filters to focus the review on the most relevant studies and we give detailed analyses of the core set of studies. Results The analysis of the reported strategies comprised two fundamental aspects for software product line testing: the selection of products for testing, and the actual test of products. Our findings indicate that the literature offers a large number of techniques to cope with such aspects. However, there is a lack of reports on realistic industrial experiences, which limits the inferences that can be drawn. Conclusion This study showed a number of leveraged strategies that can support both the selection of products, and the actual testing of products. Future research should also benefit from the problems and advantages identified in this study.}
}

@article{rayyan-727967407,
  title={Selecting component sourcing options: A survey of software engineering's broader make-or-buy decisions},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={112},
  pages={18-34},
  author={Borg, Markus and Chatzipetrou, Panagiota and Wnuk, Krzysztof and Alégroth, Emil and Gorschek, Tony and Papatheocharous, Efi and Shah, Syed Muhammad Ali and Axelsson, Jakob},
  url={https://www.sciencedirect.com/science/article/pii/S0950584919300710},
  keywords={Decision making, Software architecture, Survey, Component-based software engineering, Sourcing, Software},
  abstract={Context Component-based software engineering (CBSE) is a common approach to develop and evolve contemporary software systems. When evolving a system based on components, make-or-buy decisions are frequent, i.e., whether to develop components internally or to acquire them from external sources. In CBSE, several different sourcing options are available: (1) developing software in-house, (2) outsourcing development, (3) buying commercial-off-the-shelf software, and (4) integrating open source software components. Objective Unfortunately, there is little available research on how organizations select component sourcing options (CSO) in industry practice. In this work, we seek to contribute empirical evidence to CSO selection. Method We conduct a cross-domain survey on CSO selection in industry, implemented as an online questionnaire. Results Based on 188 responses, we find that most organizations consider multiple CSOs during software evolution, and that the CSO decisions in industry are dominated by expert judgment. When choosing between candidate components, functional suitability acts as an initial filter, then reliability is the most important quality. Conclusion We stress that future solution-oriented work on decision support has to account for the dominance of expert judgment in industry. Moreover, we identify considerable variation in CSO decision processes in industry. Finally, we encourage software development organizations to reflect on their decision processes when choosing whether to make or buy components, and we recommend using our survey for a first benchmarking.}
}

@article{rayyan-727967408,
  title={Systematic literature review and empirical investigation of barriers to process improvement in global software development: Client–vendor perspective},
  year={2017},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={87},
  pages={180-205},
  author={Khan, Arif Ali and Keung, Jacky and Niazi, Mahmood and Hussain, Shahid and Ahmad, Awais},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917302483},
  keywords={Systematic literature review, Software process improvement, Global software development, Barriers, Client, Vendor, Software},
  abstract={Context Increasingly, software development organizations are adopting global software development (GSD) strategies, mainly because of the significant return on investment they produce. However, there are many challenges associated with GSD, particularly with regards to software process improvement (SPI). SPI can play a significant role in the successful execution of GSD projects. Objective The aim of the present study was to identify barriers that can negatively affect SPI initiatives in GSD organizations from both client and vendor perspectives. Method A systematic literature review (SLR) and survey questionnaire were used to identify and validate the barriers. Results Twenty-two barriers to successful SPI programs were identified. Results illustrate that the barriers identified using SLR and survey approaches have more similarities However, there were significant differences between the ranking of these barriers in the SLR and survey approaches, as indicated by the results of t-tests (for instance, t = 2.28, p = 0.011 ¡ 0.05). Our findings demonstrate that there is a moderate positive correlation between the ranks obtained from the SLR and the empirical study (rs (22)= 0.567, p = 0.006). Conclusions The identified barriers can assist both client and vendor GSD organizations during initiation of an SPI program. Client-vendor classification was used to provide a broad picture of SPI programs, and their respective barriers. The top-ranked barriers can be used as a guide for GSD organizations prior to the initiation of an SPI program. We believe that the results of this study can be useful in tackling the problems associated with the implementation of SPI, which is vital to the success and progression of GSD organizations.}
}

@article{rayyan-727967409,
  title={Agile requirements engineering: A systematic literature review},
  year={2017},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={49},
  pages={79-91},
  author={Schön, Eva-Maria and Thomaschewski, Jörg and Escalona, María José},
  url={https://www.sciencedirect.com/science/article/pii/S0920548916300708},
  keywords={Systematic literature review, Agile software development, Human-computer interaction, Requirements Engineering, User-centered design},
  abstract={Nowadays, Agile Software Development (ASD) is used to cope with increasing complexity in system development. Hybrid development models, with the integration of User-Centered Design (UCD), are applied with the aim to deliver competitive products with a suitable User Experience (UX). Therefore, stakeholder and user involvement during Requirements Engineering (RE) are essential in order to establish a collaborative environment with constant feedback loops. The aim of this study is to capture the current state of the art of the literature related to Agile RE with focus on stakeholder and user involvement. In particular, we investigate what approaches exist to involve stakeholder in the process, which methodologies are commonly used to present the user perspective and how requirements management is been carried out. We conduct a Systematic Literature Review (SLR) with an extensive quality assessment of the included studies. We identified 27 relevant papers. After analyzing them in detail, we derive deep insights to the following aspects of Agile RE: stakeholder and user involvement, data gathering, user perspective, integrated methodologies, shared understanding, artifacts, documentation and Non-Functional Requirements (NFR). Agile RE is a complex research field with cross-functional influences. This study will contribute to the software development body of knowledge by assessing the involvement of stakeholder and user in Agile RE, providing methodologies that make ASD more human-centric and giving an overview of requirements management in ASD.}
}

@article{rayyan-727967410,
  title={Characterizing testing methods for context-aware software systems: Results from a quasi-systematic literature review},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={131},
  pages={1-21},
  author={Matalonga, Santiago and Rodrigues, Felyppe and Travassos, Guilherme Horta},
  url={https://www.sciencedirect.com/science/article/pii/S016412121730095X},
  keywords={Software testing, Systematic literature review, Context-aware, Test case design, Software},
  abstract={Context-Aware Software Systems (CASS) use environmental information to provide better service to the systems' actors to fulfill their goals. Testing of ubiquitous software systems can be challenging since it is unlikely that, while designing the test cases, the tester can identify all possible context variations. A quasi-Systematic Literature Review has been undertaken to characterize the methods usually used for testing CASS. The analysis and generation of knowledge in this work rely on classifying the extracted information. Established taxonomies of software testing and context-aware were used to characterize and interpret the findings. The results show that, although it is possible to observe the utilization of some software testing methods, few empirical studies are evaluating such methods when testing CASS. The selected technical literature conveys a lack of consensus on the understanding of context and CASS, and on the meaning of software testing. Furthermore, context variation in CASS has only been partially addressed by the identified approaches. They either rely on simulating context or in fixing the values of context variables during testing. We argue that the tests of context-aware software systems need to deal with the diversity of context instead of mitigating their effects.}
}

@article{rayyan-727967411,
  title={Agile development in the cloud computing environment: A systematic review},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={103},
  pages={142-158},
  author={Younas, Muhammad and Jawawi, Dayang N A and Ghani, Imran and Fries, Terrence and Kazmi, Rafaqut},
  url={https://www.sciencedirect.com/science/article/pii/S0950584918301319},
  keywords={Systematic review, Cloud computing, Agile software development, Agile, Agile methodology},
  abstract={Background: Agile software development is based on a set of values and principles. The twelve principles are inferred from agile values. Agile principles are composition of evolutionary requirement, simple design, continuous delivery, self-organizing team and face-to-face communication. Due to changing market demand, agile methodology faces problems such as scalability, more effort and cost required in setting up hardware and software infrastructure, availability of skilled resource and ability to build application from multiple locations. Twelve (12) principles may be practiced more appropriately with the support of cloud computing. This merger of agile and cloud computing may provide infrastructure optimization and automation benefits to agile practitioners. Objective: This Systematic Literature Review (SLR) identifies the techniques employed in cloud computing environment that are useful for agile development. In addition, SLR discusses the significance of cloud and its challenges. Method: By applying the SLR procedure, the authors select thirty-seven (37) studies out of six-hundred-forty-seven (647) from 2010 to 2017. Result: The result of SLR shows that the techniques using existing tools were reported in 35%, simulations in 20% and application developed in 15% of the studies. Evaluation of techniques was reported in 32% of the studies. The impact of cloud computing was measured by the classification of four major categories such as transparency 32%, collaboration 50%, development infrastructure 29% and cloud quality attributes in 39%. Furthermore, a large number of tools were reported in primary studies. The challenges posed by cloud adoption in agile was reported as interoperability 13%, security & privacy 18% and rest of the primary studies did not report any other research gaps. Conclusions: The study concludes that agile development in cloud computing environment is an important area in software engineering. There are many open challenges and gaps. In particular, more quality tools, evaluation research and empirical studies are required in this area.}
}

@article{rayyan-727967412,
  title={Guest Editorial: Special section on the selected papers from 14th International Conference on Evaluation and Assessment in Software Engineering (EASE 2010)},
  year={2011},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={53},
  number={6},
  pages={615},
  author={Niazi, Mahmood and Turner, Mark},
  url={https://www.sciencedirect.com/science/article/pii/S0950584910002247},
  keywords={Cesarean Section, Software}
}

@article{rayyan-727967413,
  title={Requirements engineering for software product lines: A systematic literature review},
  year={2010},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={52},
  number={8},
  pages={806-820},
  author={Alves, Vander and Niu, Nan and Alves, Carina and Valença, George},
  url={https://www.sciencedirect.com/science/article/pii/S0950584910000625},
  keywords={Software product lines, Systematic literature review, Requirements engineering, Software},
  abstract={Context Software product line engineering (SPLE) is a growing area showing promising results in research and practice. In order to foster its further development and acceptance in industry, it is necessary to assess the quality of the research so that proper evidence for adoption and validity are ensured. This holds in particular for requirements engineering (RE) within SPLE, where a growing number of approaches have been proposed. Objective This paper focuses on RE within SPLE and has the following goals: assess research quality, synthesize evidence to suggest important implications for practice, and identify research trends, open problems, and areas for improvement. Method A systematic literature review was conducted with three research questions and assessed 49 studies, dated from 1990 to 2009. Results The evidence for adoption of the methods is not mature, given the primary focus on toy examples. The proposed approaches still have serious limitations in terms of rigor, credibility, and validity of their findings. Additionally, most approaches still lack tool support addressing the heterogeneity and mostly textual nature of requirements formats as well as address only the proactive SPLE adoption strategy. Conclusions Further empirical studies should be performed with sufficient rigor to enhance the body of evidence in RE within SPLE. In this context, there is a clear need for conducting studies comparing alternative methods. In order to address scalability and popularization of the approaches, future research should be invested in tool support and in addressing combined SPLE adoption strategies.}
}

@article{rayyan-727967414,
  title={On evaluating commercial Cloud services: A systematic review},
  year={2013},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={86},
  number={9},
  pages={2371-2393},
  author={Li, Zheng and Zhang, He and O'Brien, Liam and Cai, Rainbow and Flint, Shayne},
  url={https://www.sciencedirect.com/science/article/pii/S0164121213000915},
  keywords={Systematic literature review, Cloud Computing, Cloud service evaluation},
  abstract={Background Cloud Computing is increasingly booming in industry with many competing providers and services. Accordingly, evaluation of commercial Cloud services is necessary. However, the existing evaluation studies are relatively chaotic. There exists tremendous confusion and gap between practices and theory about Cloud services evaluation. Aim To facilitate relieving the aforementioned chaos, this work aims to synthesize the existing evaluation implementations to outline the state-of-the-practice and also identify research opportunities in Cloud services evaluation. Method Based on a conceptual evaluation model comprising six steps, the systematic literature review (SLR) method was employed to collect relevant evidence to investigate the Cloud services evaluation step by step. Results This SLR identified 82 relevant evaluation studies. The overall data collected from these studies essentially depicts the current practical landscape of implementing Cloud services evaluation, and in turn can be reused to facilitate future evaluation work. Conclusions Evaluation of commercial Cloud services has become a world-wide research topic. Some of the findings of this SLR identify several research gaps in the area of Cloud services evaluation (e.g., Elasticity and Security evaluation of commercial Cloud services could be a long-term challenge), while some other findings suggest the trend of applying commercial Cloud services (e.g., compared with PaaS, IaaS seems more suitable for customers and is particularly important in industry). This SLR study itself also confirms some previous experiences and records new evidence-based software engineering (EBSE) lessons.}
}

@article{rayyan-727967415,
  title={Knowledge transfer challenges and mitigation strategies in global software development—A systematic literature review and industrial validation},
  year={2013},
  journal={International Journal of Information Management},
  issn={0268-4012},
  volume={33},
  number={2},
  pages={333-355},
  author={Nidhra, Srinivas and Yanamadala, Muralidhar and Afzal, Wasif and Torkar, Richard},
  url={https://www.sciencedirect.com/science/article/pii/S0268401212001466},
  keywords={Systematic literature review, Global software development, Interviews, Knowledge transfer, Software},
  abstract={Context In this article we considered knowledge transfer (KT) in global software development (GSD) from two perspectives, state-of-the-art and state-of-the-practice, in order to identify what are the challenges that hamper the success of KT in global software teams, as well as to find out what are the mitigation strategies that can be used to overcome such challenges. Objectives The overall aim of this work is to provide a body of knowledge for enabling successful KT in GSD settings. This is achieved by an in-depth understanding of KT challenges and mitigation strategies, both from the perspective of literature and industry. It also identifies the similarities and differences in challenges and strategies gathered from literature studies and industrial experts. Methods In order to fulfill the aim of the research, we collected data through a systematic literature review (SLR) and conducted interviews with industrial experts. Through the SLR we found 35 primary studies relevant to our objectives. We also conducted eight interviews of experienced industrial professionals from eight different multinational companies world-wide. For analyzing the data we used grounded theory and cross-case analysis. Results In total, 60 different challenges and 79 unique mitigation strategies are identified from both SLR and interview results. The challenges and mitigation strategies are grouped into three core categories of personnel, project and technology factors, thus giving rise to a conceptualization called as 2PT factors. There are greater numbers of challenges and mitigation strategies in the project and personnel factors, highlighting the complex interplay of project-related and human-intensive issues in GSD projects, while the technology factor plays the role as facilitator in transferring knowledge. The study also maps the mitigation strategies to challenges, which can guide practitioners in their selection of strategies to use for overcoming KT challenges in GSD. Conclusions We conclude that effective management of project and personnel factors, facilitated by technological factors, are crucial for a successful transfer of knowledge in GSD projects. Thus in future, the researchers and practitioners need to focus on the 2PT factors for ensuring effective KT in GSD settings.}
}

@article{rayyan-727967416,
  title={Automotive software engineering: A systematic mapping study},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={128},
  pages={25-55},
  author={Haghighatkhah, Alireza and Banijamali, Ahmad and Pakanen, Olli-Pekka and Oivo, Markku and Kuvaja, Pasi},
  url={https://www.sciencedirect.com/science/article/pii/S0164121217300560},
  keywords={Systematic mapping study, Embedded systems, Literature survey, Automotive software engineering, Automotive systems, Software-intensive systems, Software},
  abstract={The automotive industry is going through a fundamental change by moving from a mechanical to a software-intensive industry in which most innovation and competition rely on software engineering competence. Over the last few decades, the importance of software engineering in the automotive industry has increased significantly and has attracted much attention from both scholars and practitioners. A large body-of-knowledge on automotive software engineering has accumulated in several scientific publications, yet there is no systematic analysis of that knowledge. This systematic mapping study aims to classify and analyze the literature related to automotive software engineering in order to provide a structured body-of-knowledge, identify well-established topics and potential research gaps. The review includes 679 articles from multiple research sub-area, published between 1990 and 2015. The primary studies were analyzed and classified with respect to five different dimensions. Furthermore, potential research gaps and recommendations for future research are presented. Three areas, namely system/software architecture and design, qualification testing, and reuse were the most frequently addressed topics in the literature. There were fewer comparative and validation studies, and the literature lacks practitioner-oriented guidelines. Overall, research activity on automotive software engineering seems to have high industrial relevance but is relatively lower in its scientific rigor.}
}

@article{rayyan-727967417,
  title={Identifying refactoring opportunities in object-oriented code: A systematic literature review},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={58},
  pages={231-249},
  author={Al Dallal, Jehad},
  url={https://www.sciencedirect.com/science/article/pii/S0950584914001918},
  keywords={Systematic literature review, Refactoring activity, Refactoring opportunity},
  abstract={Context Identifying refactoring opportunities in object-oriented code is an important stage that precedes the actual refactoring process. Several techniques have been proposed in the literature to identify opportunities for various refactoring activities. Objective This paper provides a systematic literature review of existing studies identifying opportunities for code refactoring activities. Method We performed an automatic search of the relevant digital libraries for potentially relevant studies published through the end of 2013, performed pilot and author-based searches, and selected 47 primary studies (PSs) based on inclusion and exclusion criteria. The PSs were analyzed based on a number of criteria, including the refactoring activities, the approaches to refactoring opportunity identification, the empirical evaluation approaches, and the data sets used. Results The results indicate that research in the area of identifying refactoring opportunities is highly active. Most of the studies have been performed by academic researchers using nonindustrial data sets. Extract Class and Move Method were found to be the most frequently considered refactoring activities. The results show that researchers use six primary existing approaches to identify refactoring opportunities and six approaches to empirically evaluate the identification techniques. Most of the systems used in the evaluation process were open-source, which helps to make the studies repeatable. However, a relatively high percentage of the data sets used in the empirical evaluations were small, which limits the generality of the results. Conclusions It would be beneficial to perform further studies that consider more refactoring activities, involve researchers from industry, and use large-scale and industrial-based systems.}
}

@article{rayyan-727967418,
  title={Variability in quality attributes of service-based software systems: A systematic literature review},
  year={2013},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={55},
  number={2},
  pages={320-343},
  author={Mahdavi-Hezavehi, Sara and Galster, Matthias and Avgeriou, Paris},
  url={https://www.sciencedirect.com/science/article/pii/S0950584912001772},
  keywords={Systematic literature review, Quality attributes, Variability, Service-based systems, Software},
  abstract={Context Variability is the ability of a software artifact (e.g., a system, component) to be adapted for a specific context, in a preplanned manner. Variability not only affects functionality, but also quality attributes (e.g., security, performance). Service-based software systems consider variability in functionality implicitly by dynamic service composition. However, variability in quality attributes of service-based systems seems insufficiently addressed in current design practices. Objective We aim at (a) assessing methods for handling variability in quality attributes of service-based systems, (b) collecting evidence about current research that suggests implications for practice, and (c) identifying open problems and areas for improvement. Method A systematic literature review with an automated search was conducted. The review included studies published between the year 2000 and 2011. We identified 46 relevant studies. Results Current methods focus on a few quality attributes, in particular performance and availability. Also, most methods use formal techniques. Furthermore, current studies do not provide enough evidence for practitioners to adopt proposed approaches. So far, variability in quality attributes has mainly been studied in laboratory settings rather than in industrial environments. Conclusions The product line domain as the domain that traditionally deals with variability has only little impact on handling variability in quality attributes. The lack of tool support, the lack of practical research and evidence for the applicability of approaches to handle variability are obstacles for practitioners to adopt methods. Therefore, we suggest studies in industry (e.g., surveys) to collect data on how practitioners handle variability of quality attributes in service-based systems. For example, results of our study help formulate hypotheses and questions for such surveys. Based on needs in practice, new approaches can be proposed.}
}

@article{rayyan-727967419,
  title={Analyzing and documenting the systematic review results of software testing ontologies},
  year={2020},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={123},
  pages={106298},
  author={Tebes, Guido and Peppino, Denis and Becker, Pablo and Matturro, Gerardo and Solari, Martin and Olsina, Luis},
  url={https://www.sciencedirect.com/science/article/pii/S0950584920300495},
  keywords={Systematic literature review, Secondary study, Analysis, Software testing ontology, Systematic literature review process, Testing strategy, Software},
  abstract={Context Software testing is a complex area since it has a large number of specific methods, processes and strategies, involving a lot of domain concepts. Therefore, it would be valuable to have a conceptualized software testing ontology that explicitly and unambiguously defines the concepts. Consequently, it is important to find out the available evidence in the literature on primary studies for software testing ontologies. In particular, we are looking for research that has a rich ontological coverage that includes Non-Functional Requirements (NFRs) and Functional Requirements (FRs) concepts in conjunction with static and dynamic testing concepts, which can be used in method and process specifications for a family of testing strategies. Objective The main goal for this secondary study is to identify, evaluate and synthesize the available primary studies on conceptualized software testing ontologies. Method To conduct this study, we use the Systematic Literature Review (SLR) approach, which follows our enhanced SLR process. We set three research questions. Additionally, to quantitatively evaluate the quality of the selected conceptualized ontologies, we designed a NFRs tree and its associated metrics and indicators. Results We obtained 12 primary studies documenting conceptualized testing ontologies by using three different retrieval methods. In general, we noted that most of them have a lack of NFRs and static testing terminological coverage. Finally, we observe that none of them is directly linked with FRs and NFRs conceptual components. Conclusion A general benefit of having the suitable software testing ontology is to minimize the current heterogeneity, ambiguity and incompleteness problems in terms, properties and relationships. We have confirmed that exists heterogeneity, ambiguity, and incompleteness for concepts dealing with testing artifacts, roles, activities, and methods. Moreover, we did not find the suitable ontology for our aim since none of the conceptualized ontologies are directly linked with NFRs and FRs components.}
}

@article{rayyan-727967420,
  title={Requirements traceability technologies and technology transfer decision support: A systematic review},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={146},
  pages={59-79},
  author={Wang, Bangchao and Peng, Rong and Li, Yuanbang and Lai, Han and Wang, Zhuo},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218301754},
  keywords={Systematic literature review, Quality assessment, Requirements traceability challenges, Requirements traceability technology, Technology transfer},
  abstract={Requirements traceability (RT) is a core activity in Requirements Engineering. Various types of RT technologies have been extensively studied for decades. In this paper, we present a systematic literature review from 114 papers between 2006 and 2016 on RT techniques. We summarized 10 major challenges in current RT activities, and categorized existing RT techniques into 6 groups and 25 sub-groups. Moreover, we built mapping relations between these challenges and techniques, and identified 7 potential future research directions. Based on 83 empirical studies, the evaluations for technology transfer are conducted. The main conclusions are: (1) The “trustworthy” and “automated” challenges are the most widely investigated ones, while “scalable”, “coordinated”, “dynamic” and “lightweight” challenges receive much less attention; (2) “Trace link generation”, especially information retrieval-based (IR-based) methods, are the most studied techniques; (3) IR-based methods have the most potential to be adopted by industry, as they have been validated from multiple viewpoints; (4) Seven promising future research directions are identified, which include developing scalable, dynamic and lightweight tracing techniques, introducing new approaches in other disciplines to meet the RT challenges, improving the express ability of trace links, promoting the industry adoption of RT technologies and developing new techniques to support developers' coordination.}
}

@article{rayyan-727967421,
  title={A systematic literature review of software visualization evaluation},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={144},
  pages={165-180},
  author={Merino, L and Ghafari, M and Anslow, C and Nierstrasz, O},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218301237},
  keywords={Literature review, Evaluation, Software visualisation, Software},
  abstract={Context:Software visualizations can help developers to analyze multiple aspects of complex software systems, but their effectiveness is often uncertain due to the lack of evaluation guidelines. Objective: We identify common problems in the evaluation of software visualizations with the goal of formulating guidelines to improve future evaluations. Method:We review the complete literature body of 387 full papers published in the SOFTVIS/VISSOFT conferences, and study 181 of those from which we could extract evaluation strategies, data collection methods, and other aspects of the evaluation. Results:Of the proposed software visualization approaches, 62% lack a strong evaluation. We argue that an effective software visualization should not only boost time and correctness but also recollection, usability, engagement, and other emotions. Conclusion:We call on researchers proposing new software visualizations to provide evidence of their effectiveness by conducting thorough (i) case studies for approaches that must be studied in situ, and when variables can be controlled, (ii) experiments with randomly selected participants of the target audience and real-world open source software systems to promote reproducibility and replicability. We present guidelines to increase the evidence of the effectiveness of software visualization approaches, thus improving their adoption rate.}
}

@article{rayyan-727967422,
  title={An extensive systematic review on the Model-Driven Development of secure systems},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={68},
  pages={62-81},
  author={Nguyen, Phu H and Kramer, Max and Klein, Jacques and Traon, Yves Le},
  url={https://www.sciencedirect.com/science/article/pii/S0950584915001482},
  keywords={Systematic review, Model-Driven Engineering, MDE, MDS, Model-Driven Security, Software security engineering},
  abstract={Context: Model-Driven Security (MDS) is as a specialised Model-Driven Engineering research area for supporting the development of secure systems. Over a decade of research on MDS has resulted in a large number of publications. Objective: To provide a detailed analysis of the state of the art in MDS, a systematic literature review (SLR ) is essential. Method: We conducted an extensive SLR on MDS. Derived from our research questions, we designed a rigorous, extensive search and selection process to identify a set of primary MDS studies that is as complete as possible. Our three-pronged search process consists of automatic searching, manual searching, and snowballing. After discovering and considering more than thousand relevant papers, we identified, strictly selected, and reviewed 108 MDS publications. Results: The results of our SLR show the overall status of the key artefacts of MDS, and the identified primary MDS studies. For example, regarding security modelling artefact, we found that developing domain-specific languages plays a key role in many MDS approaches. The current limitations in each MDS artefact are pointed out and corresponding potential research directions are suggested. Moreover, we categorise the identified primary MDS studies into 5 significant MDS studies, and other emerging or less common MDS studies. Finally, some trend analyses of MDS research are given. Conclusion: Our results suggest the need for addressing multiple security concerns more systematically and simultaneously, for tool chains supporting the MDS development cycle, and for more empirical studies on the application of MDS methodologies. To the best of our knowledge, this SLR is the first in the field of Software Engineering that combines a snowballing strategy with database searching. This combination has delivered an extensive literature study on MDS.}
}

@article{rayyan-727967423,
  title={Problems, causes and solutions when adopting continuous delivery—A systematic literature review},
  year={2017},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={82},
  pages={55-79},
  author={Laukkanen, Eero and Itkonen, Juha and Lassenius, Casper},
  url={https://www.sciencedirect.com/science/article/pii/S0950584916302324},
  keywords={Systematic literature review, Continuous integration, Continuous deployment, Continuous delivery},
  abstract={Context: Continuous delivery is a software development discipline in which software is always kept releasable. The literature contains instructions on how to adopt continuous delivery, but the adoption has been challenging in practice. Objective: In this study, a systematic literature review is conducted to survey the faced problems when adopting continuous delivery. In addition, we identify causes for and solutions to the problems. Method: By searching five major bibliographic databases, we identified 293 articles related to continuous delivery. We selected 30 of them for further analysis based on them containing empirical evidence of adoption of continuous delivery, and focus on practice instead of only tooling. We analyzed the selected articles qualitatively and extracted problems, causes and solutions. The problems and solutions were thematically synthesized into seven themes: build design, system design, integration, testing, release, human and organizational and resource. Results: We identified a total of 40 problems, 28 causal relationships and 29 solutions related to adoption of continuous delivery. Testing and integration problems were reported most often, while the most critical reported problems were related to testing and system design. Causally, system design and testing were most connected to other themes. Solutions in the system design, resource and human and organizational themes had the most significant impact on the other themes. The system design and build design themes had the least reported solutions. Conclusions: When adopting continuous delivery, problems related to system design are common, critical and little studied. The found problems, causes and solutions can be used to solve problems when adopting continuous delivery in practice.}
}

@article{rayyan-727967424,
  title={Systematic literature review of usability capability/maturity models},
  year={2018},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={55},
  pages={95-105},
  author={Lacerda, Thaísa C and von Wangenheim, Christiane Gresse},
  url={https://www.sciencedirect.com/science/article/pii/S0920548916302355},
  keywords={Literature review, Usability, Capability/maturity model, Human-centered-design, Process assessment},
  abstract={A world becoming more digitally transformed and connected poses significant challenges for IT organizations, requiring increased attention to the usability of their software products and, consequently, to the systematic establishment of usability engineering (UE) processes. Typically, the establishment of software processes is guided by software process capability/maturity models (SPCMMs), such as CMMI or ISO/IEC 15504. However, it seems that these commonly adopted models do not explicitly cover usability engineering (UE) processes. Thus, a question that arises is, if there exist process capability/maturity models focusing explicitly on usability engineering? If yes, to which degree do they assist in the assessment process? To answer this, we conducted a systematic literature review on usability capability/maturity models (UCMMs). A total of 15 UCMMs were identified and analyzed, synthesizing information on their measurement framework and process reference model, usage support and how they have been developed/validated. We observed that most of the models are based on consolidated SPCMMs, such as CMMI or ISO/IEC 15504. Only few UCMMs customized for specific contexts were found. Although all UCMMs propose or reference a measurement framework, only 5 UCMMs define a proper process reference model. Most of the models also do not offer support for their usage, which may hinder their larger scale adoption in practice. Furthermore, we noted a lack of information on how most of the models have been developed and validated, which leaves their validity questionable. These results indicate the need for further research on UCMMs taking into consideration the increased importance of usability in software product quality.}
}

@article{rayyan-727967425,
  title={Taxonomies in software engineering: A Systematic mapping study and a revised taxonomy development method},
  year={2017},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={85},
  pages={43-59},
  author={Usman, Muhammad and Britto, Ricardo and Börstler, Jürgen and Mendes, Emilia},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917300472},
  keywords={Systematic mapping study, Software engineering, Classification, Taxonomy, Software},
  abstract={Context: Software Engineering (SE) is an evolving discipline with new subareas being continuously developed and added. To structure and better understand the SE body of knowledge, taxonomies have been proposed in all SE knowledge areas. Objective: The objective of this paper is to characterize the state-of-the-art research on SE taxonomies. Method: A systematic mapping study was conducted, based on 270 primary studies. Results: An increasing number of SE taxonomies have been published since 2000 in a broad range of venues, including the top SE journals and conferences. The majority of taxonomies can be grouped into the following SWEBOK knowledge areas: construction (19.55%), design (19.55%), requirements (15.50%) and maintenance (11.81%). Illustration (45.76%) is the most frequently used approach for taxonomy validation. Hierarchy (53.14%) and faceted analysis (39.48%) are the most frequently used classification structures. Most taxonomies rely on qualitative procedures to classify subject matter instances, but in most cases (86.53%) these procedures are not described in sufficient detail. The majority of the taxonomies (97%) target unique subject matters and many taxonomy-papers are cited frequently. Most SE taxonomies are designed in an ad-hoc way. To address this issue, we have revised an existing method for developing taxonomies in a more systematic way. Conclusion: There is a strong interest in taxonomies in SE, but few taxonomies are extended or revised. Taxonomy design decisions regarding the used classification structures, procedures and descriptive bases are usually not well described and motivated.}
}

@article{rayyan-727967426,
  title={Systematic literature review of machine learning based software development effort estimation models},
  year={2012},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={54},
  number={1},
  pages={41-59},
  author={Wen, Jianfeng and Li, Shixian and Lin, Zhiyong and Hu, Yong and Huang, Changqin},
  url={https://www.sciencedirect.com/science/article/pii/S0950584911001832},
  keywords={Systematic literature review, Machine learning, Software effort estimation, Software},
  abstract={Context Software development effort estimation (SDEE) is the process of predicting the effort required to develop a software system. In order to improve estimation accuracy, many researchers have proposed machine learning (ML) based SDEE models (ML models) since 1990s. However, there has been no attempt to analyze the empirical evidence on ML models in a systematic way. Objective This research aims to systematically analyze ML models from four aspects: type of ML technique, estimation accuracy, model comparison, and estimation context. Method We performed a systematic literature review of empirical studies on ML model published in the last two decades (1991–2010). Results We have identified 84 primary studies relevant to the objective of this research. After investigating these studies, we found that eight types of ML techniques have been employed in SDEE models. Overall speaking, the estimation accuracy of these ML models is close to the acceptable level and is better than that of non-ML models. Furthermore, different ML models have different strengths and weaknesses and thus favor different estimation contexts. Conclusion ML models are promising in the field of SDEE. However, the application of ML models in industry is still limited, so that more effort and incentives are needed to facilitate the application of ML models. To this end, based on the findings of this review, we provide recommendations for researchers as well as guidelines for practitioners.}
}

@article{rayyan-727967427,
  title={A systematic review of comparative evidence of aspect-oriented programming},
  year={2010},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={52},
  number={9},
  pages={871-887},
  author={Ali, Muhammad Sarmad and Ali Babar, Muhammad and Chen, Lianping and Stol, Klaas-Jan},
  url={https://www.sciencedirect.com/science/article/pii/S0950584910000819},
  keywords={Evidence-based software engineering, Systematic literature review, Aspect-oriented programming},
  abstract={Context Aspect-oriented programming (AOP) promises to improve many facets of software quality by providing better modularization and separation of concerns, which may have system wide affect. There have been numerous claims in favor and against AOP compared with traditional programming languages such as Objective Oriented and Structured Programming Languages. However, there has been no attempt to systematically review and report the available evidence in the literature to support the claims made in favor or against AOP compared with non-AOP approaches. Objective This research aimed to systematically identify, analyze, and report the evidence published in the literature to support the claims made in favor or against AOP compared with non-AOP approaches. Method We performed a systematic literature review of empirical studies of AOP based development, published in major software engineering journals and conference proceedings. Results Our search strategy identified 3307 papers, of which 22 were identified as reporting empirical studies comparing AOP with non-AOP approaches. Based on the analysis of the data extracted from those 22 papers, our findings show that for performance, code size, modularity, and evolution related characteristics, a majority of the studies reported positive effects, a few studies reported insignificant effects, and no study reported negative effects; however, for cognition and language mechanism, negative effects were reported. Conclusion AOP is likely to have positive effect on performance, code size, modularity, and evolution. However its effect on cognition and language mechanism is less likely to be positive. Care should be taken using AOP outside the context in which it has been validated.}
}

@article{rayyan-727967428,
  title={The use of software product lines for business process management: A systematic literature review},
  year={2013},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={55},
  number={8},
  pages={1355-1373},
  author={dos Santos Rocha, Roberto and Fantinato, Marcelo},
  url={https://www.sciencedirect.com/science/article/pii/S0950584913000402},
  keywords={Software product line, BPM, Business process management, PL, Software},
  abstract={Context Business Process Management (BPM) is a potential domain in which Software Product Line (PL) can be successfully applied. Including the support of Service-oriented Architecture (SOA), BPM and PL may help companies achieve strategic alignment between business and IT. Objective Presenting the results of a study undertaken to seek and assess PL approaches for BPM through a Systematic Literature Review (SLR). Moreover, identifying the existence of dynamic PL approaches for BPM. Method A SLR was conducted with four research questions formulated to evaluate PL approaches for BPM. Results 63 papers were selected as primary studies according to the criteria established. From these primary studies, only 15 papers address the specific dynamic aspects in the context evaluated. Moreover, it was found that PLs only partially address the BPM lifecycle since the last business process phase is not a current concern on the found approaches. Conclusions The found PL approaches for BPM only cover partially the BPM lifecycle, not taking into account the last phase which restarts the lifecycle. Moreover, no wide dynamic PL proposal was found for BPM, but only the treatment of specific dynamic aspects. The results indicate that PL approaches for BPM are still at an early stage and gaining maturity.}
}

@article{rayyan-727967429,
  title={Introduction to section most cited journal articles in Software Engineering},
  year={2009},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={51},
  number={1},
  pages={1},
  author={Wohlin, Claes},
  url={https://www.sciencedirect.com/science/article/pii/S0950584908001353},
  keywords={Software, Cesarean Section}
}

@article{rayyan-727967430,
  title={Using CMMI together with agile software development: A systematic review},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={58},
  pages={20-43},
  author={Selleri Silva, Fernando and Soares, Felipe Santana Furtado and Peres, Angela Lima and de Azevedo, Ivanildo Monteiro and Vasconcelos, Ana Paula L F and Kamei, Fernando Kenji and de Lemos Meira, Silvio Romero},
  url={https://www.sciencedirect.com/science/article/pii/S0950584914002110},
  keywords={Systematic review, Software process improvement, CMMI, Benefits, Agile methodology, Limitations, Software},
  abstract={Background The search for adherence to maturity levels by using lightweight processes that require low levels of effort is regarded as a challenge for software development organizations. Objective This study seeks to evaluate, synthesize, and present results on the use of the Capability Maturity Model Integration (CMMI) in combination with agile software development, and thereafter to give an overview of the topics researched, which includes a discussion of their benefits and limitations, the strength of the findings, and the implications for research and practice. Methods The method applied was a Systematic Literature Review on studies published up to (and including) 2011. Results The search strategy identified 3193 results, of which 81 included studies on the use of CMMI together with agile methodologies. The benefits found were grouped into two main categories: those related to the organization in general and those related to the development process, and were organized into subcategories, according to the area to which they refer. The limitations were also grouped into these categories. Using the criteria defined, the strength of the evidence found was considered low. The implications of the results for research and practice are discussed. Conclusion Agile methodologies can be used by companies to reduce efforts in getting to levels 2 and 3 of CMMI, there even being reports of applying agile practices that led to achieving level 5. However, agile methodologies alone, according to the studies, were not sufficient to obtain a rating at a given level, it being necessary to resort to additional practices to do so.}
}

@article{rayyan-727967431,
  title={Theory-oriented software engineering},
  year={2015},
  journal={Science of Computer Programming},
  issn={0167-6423},
  volume={101},
  pages={79-98},
  author={Stol, Klaas-Jan and Fitzgerald, Brian},
  url={https://www.sciencedirect.com/science/article/pii/S0167642314005425},
  keywords={Empirical research, Software engineering research, Theory building, Theory fragment, Theory-oriented software engineering, Software},
  abstract={There has been a growing interest in the role of theory within Software Engineering (SE) research. For several decades, researchers within the SE research community have argued that, to become a ‘real' engineering science, SE needs to develop stronger theoretical foundations. However, so far, the role of theory is neither fully appreciated nor well understood in SE research. Without a good common understanding of what theory is, what it constitutes in SE research, and the various roles it can play in SE research, it is difficult to appreciate how theory building can help to strengthen SE research. In this paper we discuss the importance of theory and conceptualization, and review the key components that comprise a theory. We then present the Research Path Schema (RPS), which is an adaptation of an analytical framework from the social sciences. The RPS defines a research study as consisting of three components: some phenomenon, system or substance that a researcher is interested in; some technique or method to study that substance; and some form of conceptualization or theory that provides an explanation for, or abstraction of the observations made in a study. Different research studies have a different archetypical ‘architecture,' depending on the selection of these three components. Consequently, the role of the conceptualization or theory will be different for each archetypical study design, or selected research path. We conclude this paper by outlining a number of implications for future SE research, and argue for a Theory-Oriented Software Engineering research perspective, which can complement the recent focus on Evidence Based Software Engineering.}
}

@article{rayyan-727967432,
  title={A systematic literature review on enterprise architecture implementation methodologies},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={62},
  pages={1-20},
  author={Rouhani, Babak Darvish and Mahrin, Mohd Naz'ri and Nikpay, Fatemeh and Ahmad, Rodina Binti and Nikfard, Pourya},
  url={https://www.sciencedirect.com/science/article/pii/S0950584915000282},
  keywords={SLR, Methodology, EAIM, Enterprise Architecture, Enterprise Architecture Implementation Methodology},
  abstract={Context Enterprise Architecture (EA) is a strategy to align business and Information Technology (IT) within an enterprise. EA is managed, developed, and maintained throughout the EA Implementation Methodology (EAIM). Objective The aims of this study are to identify the existing effective practices that are used by existing EAIMs, identify the factors that affect the effectiveness of EAIM, identify the current tools that are used by existing EAIMs, and identify the open problems and areas related to EAIM for improvement. Method A Systematic Literature Review (SLR) was carried out. 669 papers were retrieved by a manual search in 6 databases and 46 primary studies were finally included. Result From these studies 33% were journal articles, 41% were conference papers while 26% were contributions from the studies consisted of book chapters. Consequently, 28 practices, 19 factors, and 15 tools were identified and analysed. Conclusion Several rigorous researches have been done in order to provide effective EAIM, however there are still problems in components of EAIM, including: there is lack of tool support for whole part of EA implementation, there are deficiency in addressing the EAIM's practices especially in modeling, management, and maintenance, there is lack of consideration on non-functional requirement in existing EAIM, there is no appropriate consideration on requirement analysis in most existing EAIM. This review provides researchers with some guidelines for future research on this topic. It also provides broad information on EAIM, which could be useful for practitioners.}
}

@article{rayyan-727967433,
  title={Drivers, barriers and social considerations for AI adoption in business and management: A tertiary study},
  year={2020},
  journal={Technology in Society},
  issn={0160-791X},
  volume={62},
  pages={101257},
  author={Cubric, Marija},
  url={https://www.sciencedirect.com/science/article/pii/S0160791X19307171},
  keywords={Artificial intelligence, Tertiary study, Systematic literature review, Machine learning, Business, Management},
  abstract={The number of academic papers in the area of Artificial Intelligence (AI) and its applications across business and management domains has risen significantly in the last decade, and that rise has been followed by an increase in the number of systematic literature reviews. The aim of this study is to provide an overview of existing systematic reviews in this growing area of research and to synthesise the findings related to drivers, barriers and social implications of the AI adoption in business and management. The methodology used for this tertiary study is based on Kitchenham and Charter's guidelines [14], resulting in a selection of 30 reviews published between 2005 and 2019 which are reporting results of 2021 primary studies. These reviews cover the AI adoption across various business sectors (healthcare, information technology, energy, agriculture, apparel industry, engineering, smart cities, tourism and transport), management and business functions (HR, customer services, supply chain, health and safety, project management, decision-support, systems management and technology adoption). While the drivers for the AI adoption in these areas are mainly economic, the barriers are related to the technical aspects (e.g. availability of data, reusability of models) as well as the social considerations such as, increased dependence on non-humans, job security, lack of knowledge, safety, trust and lack of multiple stakeholders'perspectives. Very few reviews outside of the healthcare management domain consider human, organisational and wider societal factors of the AI adoption. In addition to increased focus on social implications of AI, the reviews are recommending more rigorous evaluation, increased use of hybrid solutions (AI and non-AI) and multidisciplinary approach to AI design and evaluation. Furthermore, this study found that there is a lack of systematic reviews in some of the early AI adoption sectors such as financial industry and retail.}
}

@article{rayyan-727967434,
  title={A systematic literature review on software measurement programs},
  year={2016},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={73},
  pages={101-121},
  author={Tahir, Touseef and Rasool, Ghulam and Gencel, Cigdem},
  url={https://www.sciencedirect.com/science/article/pii/S0950584916300131},
  keywords={Software measurement, Systematic Literature Review, GQM, Software measurement program, Software metrics, Software},
  abstract={Context Software measurement programs (MPs) are an important means for understanding, evaluating, managing, and improving software processes, products and resources. However, implementing successful MPs still remains a challenge. Objectives To make a comprehensive review of the studies on MPs for bringing into light the existing measurement planning models and tools used for implementing MPs,the accumulated knowledge on the success/failure factors of MPs and mitigation strategies to address their challenges. Methods A Systematic Literature Review (SLR) was conducted. In total, 65primary studies were reviewed and analyzed. Results We identified 35 measurement planning models and 11 associated tools, most of which either proposed extensions or improvements for goal based approaches. The identified success factors include (a) organizational adoption of MP, (b) integration of MP with SDLC, (c) synchronization of MP with SPI and (d) design of MP. The mostly mentioned mitigation strategies for addressing challenges are effective change management and measurement stakeholder management, automated tool support and incorporation of engineering mechanisms for designing sustainable, effective, scalable and extendible MPs, and measurement expertise and standards development. Conclusion Most of the success factors and mitigation strategies have interdependencies. Therefore, for successful MP implementation, software organizations should consider these factors in combination and make a feasibility study at the very beginning.}
}

@article{rayyan-727967435,
  title={Highly-cited papers in software engineering: The top-100},
  year={2016},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={71},
  pages={108-128},
  author={Garousi, Vahid and Fernandes, João M},
  url={https://www.sciencedirect.com/science/article/pii/S0950584915001871},
  keywords={Bibliometrics, Software engineering, Highly-cited papers, Most cited, Most frequently cited, Top cited, Software},
  abstract={Context According to the search reported in this paper, as of this writing (May 2015), a very large number of papers (more than 70,000) have been published in the area of Software Engineering (SE) since its inception in 1968. Citations are crucial in any research area to position the work and to build on the work of others. Identification and characterization of highly-cited papers are common and are regularly reported in various disciplines. Objective The objective of this study is to identify the papers in the area of SE that have influenced others the most as measured by citation count. Studying highly-cited SE papers helps researchers to see the type of approaches and research methods presented and applied in such papers, so as to be able to learn from them to write higher quality papers which will likely receive high citations. Method To achieve the above objective, we conducted a study, comprised of five research questions, to identify and classify the top-100 highly-cited SE papers in terms of two metrics: total number of citations and average annual number of citations. Results By total number of citations, the top paper is "A metrics suite for object-oriented design", cited 1817 times and published in 1994. By average annual number of citations, the top paper is "QoS-aware middleware for Web services composition", cited 154.2 times on average annually and published in 2004. Conclusion It is concluded that it is important to identify the highly-cited SE papers and also to characterize the overall citation landscape in the SE field. We hope that this paper will encourage further discussions in the SE community towards further analysis and formal characterization of the highly-cited SE papers.}
}

@article{rayyan-727967436,
  title={Design science research contribution to business intelligence in the cloud — A systematic literature review},
  year={2016},
  journal={Future Generation Computer Systems},
  issn={0167-739X},
  volume={63},
  pages={108-122},
  author={Sangupamba Mwilu, Odette and Comyn-Wattiau, Isabelle and Prat, Nicolas},
  url={https://www.sciencedirect.com/science/article/pii/S0167739X15003623},
  keywords={Systematic literature review, Cloud computing, Design science research, Business intelligence, Analytics},
  abstract={Business intelligence (BI) helps managers make informed decisions. In the age of big data, BI technology provides essential support for decision making. Cloud computing also attracts many organizations because of its potential: ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g. networks, servers, storage, applications, and services). This paper focuses on the deployment of BI in the cloud, from the vantage point of design science research (DSR). We produce a state of the art of research pertaining to BI in the cloud, following the methodology of systematic literature review. This literature review especially exhibits the different artifacts proposed by design science researchers regarding BI in the cloud. To structure the literature review, we propose a framework composed of two dimensions: artifact type and BI step. In particular, we propose a typology of artifact types, refining the coarse-grained typology commonly used in DSR. We use the two-dimensional framework both to map the current state of DSR regarding BI in the cloud, and to elicit future research avenues in terms of design science artifacts for BI in the cloud. The contribution is threefold: the literature review may help DSR researchers get an overview of this active research domain; the two-dimensional framework facilitates the understanding of different research streams; finally, the proposed future topics may guide researchers in identifying promising research avenues.}
}

@article{rayyan-727967437,
  title={Introduction to the EASE 2016 special section: Evidence-based software engineering: Past, present, and future},
  year={2017},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={89},
  pages={14-18},
  author={Beecham, Sarah and Bowes, David and Stol, Klaas-Jan},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917303877},
  keywords={Evidence-based software engineering, Empirical research, Software, Cesarean Section},
  abstract={The International Conference on Evaluation and Assessment in Software Engineering (EASE) had its twentieth anniversary in 2016, with that year's edition hosted in Limerick, Ireland. Founded in 1997, the EASE conference was the first event solely dedicated to encouraging empirical research in software engineering, and its founders have been longtime advocates of evidence-based software engineering (EBSE). In this editorial, we briefly look back at the history of EBSE and the EASE conference. We then introduce the four articles which are revised and extended versions of papers presented at EASE 2016. We conclude by looking at the future of EBSE, and provide some suggestions for conducting and reporting empirical research.}
}

@article{rayyan-727967438,
  title={25 years of software engineering in Brazil: Beyond an insider's view},
  year={2013},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={86},
  number={4},
  pages={872-889},
  author={da Mota Silveira Neto, Paulo Anselmo and Gomes, Joás Sousa and de Almeida, Eduardo Santana and Leite, Jair Cavalcanti and Batista, Thais Vasconcelos and Leite, Larissa},
  url={https://www.sciencedirect.com/science/article/pii/S0164121212002981},
  keywords={Software engineering, Mapping study, Expert opinion survey, Brazil, Software},
  abstract={The software engineering area is facing a growing number of challenges due to the continuing increase in software size and complexity. The challenges are addressed by the very relevant and high quality publications of the Brazilian Symposium on Software Engineering (SBES), in the past 25 editions. This article summarizes the findings from two different mapping studies about these 25 SBES editions. It also reports the results of an expert opinion survey with the most important Brazilian researchers in the software engineering (SE) area. The survey reinforces the findings of the mapping studies. It also provides guidance for future research. In addition, the studies report several findings that confirmed the validity of the research methods applied. All of these findings are important input to the current Brazilian SE scenario. Our findings also suggest that greater attention should be given to the SE area, by improving researchers' interaction with industry and increasing collaboration between researchers, especially internationally.}
}

@article{rayyan-727967439,
  title={Introduction to special section on evaluation and assessment in software engineering EASE06},
  year={2007},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={80},
  number={9},
  pages={1423-1424},
  author={Kitchenham, Barbara and Brereton, Pearl},
  url={https://www.sciencedirect.com/science/article/pii/S0164121206002950},
  keywords={Cesarean Section, Software}
}

@article{rayyan-727967440,
  title={Dealing with noise problem in machine learning data-sets: A systematic review},
  year={2019},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={161},
  pages={466-474},
  author={Gupta, Shivani and Gupta, Atul},
  url={https://www.sciencedirect.com/science/article/pii/S1877050919318575},
  keywords={Classification, Attribute noise, Class noise, Noise, Noise handling techniques, Noise identification techniques, Types of noise},
  abstract={The occurrences of noisy data in data set can significantly impact prediction of any meaningful information. Many empirical studies have shown that noise in data set dramatically led to decreased classification accuracy and poor prediction results. Therefore, the problem of identifying and handling noise in prediction application has drawn considerable attention over past many years. In our study, we performed a systematic literature review of noise identification and handling studies published in various conferences and journals between January 1993 to July 2018. We have identified 79 primary studies are of noise identification and noise handling techniques. After investigating these studies, we found that among the noise identification schemes, the accuracy of identification of noisy instances by using ensemble-based techniques are better than other techniques. But regarding efficiency, usually single based techniques method is better; it is more suitable for noisy data sets. Among noise handling techniques, polishing techniques generally improve classification accuracy than filtering and robust techniques, but it introduced some errors in the data sets.}
}

@article{rayyan-727967441,
  title={Incorrect results in software engineering experiments: How to improve research practices},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={116},
  pages={133-145},
  author={Jørgensen, Magne and Dybå, Tore and Liestøl, Knut and Sjøberg, Dag I K},
  url={https://www.sciencedirect.com/science/article/pii/S0164121215000679},
  keywords={Empirical software engineering, Controlled experiments, Statistical hypothesis testing, Software},
  abstract={Context The trustworthiness of research results is a growing concern in many empirical disciplines. Aim The goals of this paper are to assess how much the trustworthiness of results reported in software engineering experiments is affected by researcher and publication bias, given typical statistical power and significance levels, and to suggest improved research practices. Method First, we conducted a small-scale survey to document the presence of researcher and publication biases in software engineering experiments. Then, we built a model that estimates the proportion of correct results for different levels of researcher and publication bias. A review of 150 randomly selected software engineering experiments published in the period 2002–2013 was conducted to provide input to the model. Results The survey indicates that researcher and publication bias is quite common. This finding is supported by the observation that the actual proportion of statistically significant results reported in the reviewed papers was about twice as high as the one expected assuming no researcher and publication bias. Our models suggest a high proportion of incorrect results even with quite conservative assumptions. Conclusion Research practices must improve to increase the trustworthiness of software engineering experiments. A key to this improvement is to avoid conducting studies with unsatisfactory low statistical power.}
}

@article{rayyan-727967442,
  title={Social network data to alleviate cold-start in recommender system: A systematic review},
  year={2018},
  journal={Information Processing & Management},
  issn={0306-4573},
  volume={54},
  number={4},
  pages={529-544},
  author={Gonzalez Camacho, Lesly Alejandra and Alves-Souza, Solange Nice},
  url={https://www.sciencedirect.com/science/article/pii/S0306457317306544},
  keywords={Systematic literature review, Cold start, Collaborative filtering, Recommender system, Social network, Social Support, Cold Temperature},
  abstract={Recommender Systems are currently highly relevant for helping users deal with the information overload they suffer from the large volume of data on the web, and automatically suggest the most appropriate items that meet users needs. However, in cases in which a user is new to Recommender System, the system cannot recommend items that are relevant to her/him because of lack of previous information about the user and/or the user-item rating history that helps to determine the users preferences. This problem is known as cold-start, which remains open because it does not have a final solution. Social networks have been employed as a good source of information to determine users preferences to mitigate the cold-start problem. This paper presents the results of a Systematic Literature Review on Collaborative Filtering-based Recommender System that uses social network data to mitigate the cold-start problem. This Systematic Literature Review compiled the papers published between 2011–2017, to select the most recent studies in the area. Each selected paper was evaluated and classified according to the depth which social networks used to mitigate the cold-start problem. The final results show that there are several publications that use the information of the social networks within the Recommender System; however, few research papers currently use this data to mitigate the cold-start problem.}
}

@article{rayyan-727967443,
  title={The impact of global dispersion on coordination, team performance and software quality – A systematic literature review},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={57},
  pages={277-294},
  author={Nguyen-Duc, Anh and Cruzes, Daniela S and Conradi, Reidar},
  url={https://www.sciencedirect.com/science/article/pii/S0950584914001414},
  keywords={Systematic literature review, Software quality, Performance, Global software development, Meta analysis, Global dispersion, Software},
  abstract={Context Global software development (GSD) contains different context setting dimensions, which are essential for effective teamwork and success of projects. Although considerable research effort has been made in this area, as yet, no agreement has been reached about the impact of these dispersion dimensions on team coordination and project outcomes. Objective This paper summarizes empirical evidence on the impact of global dispersion dimensions on coordination, team performance and project outcomes. Method We performed a systematic literature review of 46 publications from 25 journals and 19 conference and workshop proceedings, which were published between 2001 and 2013. Thematic analysis was used to identify global dimensions and their measures. Vote counting was used to decide on the impact trends of dispersion dimensions on team performance and software quality. Results Global dispersion dimensions are consistently conceptualized, but quantified in many different ways. Different dispersion dimensions are associated with a distinct set of coordination challenges. Overall, geographical dispersion tends to have a negative impact on team performance and software quality. Temporal dispersion tends to have a negative impact on software quality, but its impact on team performance is inconsistent and can be explained by type of performance. Conclusion For researchers, we reveal several opportunities for future research, such as coordination challenges in inter-organizational software projects, impact of processes and practices mismatches on project outcomes, evolution of coordination needs and mechanism over time and impact of dispersion dimensions on open source project outcomes. For practitioners, they should consider the tradeoff between cost and benefits while dispersing tasks, alignment impact of dispersion dimensions with individual and organizational objectives, coordination mechanisms as situational approaches and collocation of development activities of high quality demand components in GSD projects.}
}

@article{rayyan-727967444,
  title={Systematic review for network survivability analysis in MANETS},
  year={2015},
  journal={Procedia - Social and Behavioral Sciences},
  issn={1877-0428},
  volume={195},
  pages={1872-1881},
  author={Azni, A H and Ahmad, Rabiah and Noh, Zul Azri Mohamad and Hazwani, Farida and Hayaati, Najwa},
  url={https://www.sciencedirect.com/science/article/pii/S1877042815039038},
  keywords={Systematic Literature Review, MANETS, Network Survivability, Survival Analysis},
  abstract={Network survivability analysis in MANETs was hardly an issue in the early years of wireless technology because there were no critical network system that depended on wireless technology yet. Today, network survivability analysis is an essential aspect of reliable communication especially in MANETs. Although various methods have been proposed to measure network survivability analysis in MANETs, no related review has been published as to date for this topic. Thus, a comprehensive review of this body of work would be beneficial to researchers to have an overview of the current state of research trend in this area. This paper provides a systematic literature review (SLR) of the state of the art approach in network survivability analysis in MANETs. We used studies from a number of relevant article sources, and our results showed the existence of twenty six (26) articles. From this SLR we found that the existing of analysis method is focusing on individual node in which the node is treated as independent event. Furthermore, the analysis also reveals the less popular methods in analyzing network survivability are with statistical methods such as regression analysis and survival analysis. The implication of this study is to give a clear direction to future researchers in this area for a better and accurate analysis in measuring network survivability in MANETs.}
}

@article{rayyan-727967445,
  title={A systematic review of knowledge sharing challenges and practices in global software development},
  year={2016},
  journal={International Journal of Information Management},
  issn={0268-4012},
  volume={36},
  number={6},
  pages={995-1019},
  author={Zahedi, Mansooreh and Shahin, Mojtaba and Ali Babar, Muhammad},
  url={https://www.sciencedirect.com/science/article/pii/S026840121630384X},
  keywords={Empirical software engineering, Global software development (GSD), Systematic literature review (SLR), Knowledge management (KM), Knowledge sharing, Software},
  abstract={Context Global Software Development (GSD) presents significant challenges to share and understand knowledge required for developing software. Organizations are expected to implement appropriate practices to address knowledge-sharing challenges in GSD. With the growing literature on GSD and its widespread adoption, it is important to build a body of knowledge to support future research and effective knowledge sharing practices. Objective We aimed at systematically identifying and synthesizing knowledge sharing challenges and practices. We also intended to classify the recurrent challenges and most frequently reported practices in different contextual settings. Method We used Systematic Literature Review (SLR) for reviewing 61 primary studies that were selected after searching the GSD literature published over the last 14 years (2000–September 2014). We applied thematic analysis method for analysing the data extracted from the reviewed primary studies. Results Our findings revealed that knowledge sharing challenges and practices in GSD could be classified in 6 main themes: management, team structure, work processes/practices, team cognition, social attributes and technology. In regard to contextual settings, we found empirical studies were mainly conducted in an offshore outsourcing collaboration model distributed between two sites. Most of the studied organizations were large enterprises. Many of the studies did not report any information for several contextual attributes that made it difficult to analyse the reported challenges and practices with respect to their respective contexts. Conclusion We can conclude: (a) there is a higher tendency among researchers to report practices than challenges of knowledge sharing in GSD. (b) Given our analysis, most of the reported knowledge sharing challenges and practices fall under the theme of “work practices”. (c) The technology related knowledge-sharing challenges are the least reported; we discussed the available technologies for supporting knowledge sharing needs in GSD. (d) The organizational contextual information is missing from a large number of studies; hence, it was not possible to investigate the potential relations between knowledge sharing challenges/practices and the contextual attributes of GSD teams. We assert the need of exploring knowledge sharing in the context of small/medium sized organizations to avoid the risk of findings being biased by specific empirical setting (e.g., large enterprises distributed between US and India).}
}

@article{rayyan-727967446,
  title={Software requirements selection and prioritization using SBSE approaches: A systematic review and mapping of the literature},
  year={2015},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={103},
  pages={267-280},
  author={Pitangueira, Antônio Mauricio and Maciel, Rita Suzana P and Barros, Márcio},
  url={https://www.sciencedirect.com/science/article/pii/S0164121214002118},
  keywords={Systematic review, Requirements prioritization, Requirements selection, Software},
  abstract={The selection and prioritization of software requirements represents an area of interest in Search-Based Software Engineering (SBSE) and its main focus is finding and selecting a set of requirements that may be part of a software release. This paper presents a systematic review and mapping that investigated, analyzed, categorized and classified the SBSE approaches that have been proposed to address software requirement selection and prioritization problems, reporting quantitative and qualitative assessment. Initially 39 papers returned from our search strategy in this area and they were analyzed by 18 previously established quality criteria. The results of this systematic review show which aspects of the requirements selection and prioritization problems were addressed by researchers, which approaches and search techniques are currently adopted to address these problems, as well as the strengths and weaknesses in this research area highlighted from the quality criteria.}
}

@article{rayyan-727967447,
  title={A systematic literature review: Opinion mining studies from mobile app store user reviews},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={125},
  pages={207-219},
  author={Genc-Nayebi, Necmiye and Abran, Alain},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216302291},
  keywords={Systematic literature review, Requirements engineering, App stores opinion mining, Mobile application},
  abstract={As mobile devices have overtaken fixed Internet access, mobile applications and distribution platforms have gained in importance. App stores enable users to search for, purchase and install mobile applications and then give feedback in the form of reviews and ratings. A review might contain information about the user's experience with the app and opinion of it, feature requests and bug reports. Hence, reviews are valuable not only to users who would like to find out what others think about an app, but also to developers and software companies interested in customer feedback. The rapid increase in the number of applications and total app store revenue has accelerated app store data mining and opinion aggregation studies. While development companies and app store regulators have pursued upfront opinion mining studies for business intelligence and marketing purposes, research interest into app ecosystem and user reviews is relatively new. In addition to studies examining online product reviews, there are now some academic studies focused on mobile app stores and user reviews. The objectives of this systematic literature review are to identify proposed solutions for mining online opinions in app store user reviews, challenges and unsolved problems in the domain, any new contributions to software requirements evolution and future research direction.}
}

@article{rayyan-727967448,
  title={Empirical studies of agile software development: A systematic review},
  year={2008},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={50},
  number={9},
  pages={833-859},
  author={Dybå, Tore and Dingsøyr, Torgeir},
  url={https://www.sciencedirect.com/science/article/pii/S0950584908000256},
  keywords={Evidence-based software engineering, Empirical software engineering, Systematic review, Agile software development, Scrum, Research synthesis, XP, Extreme programming, Software},
  abstract={Agile software development represents a major departure from traditional, plan-based approaches to software engineering. A systematic review of empirical studies of agile software development up to and including 2005 was conducted. The search strategy identified 1996 studies, of which 36 were identified as empirical studies. The studies were grouped into four themes: introduction and adoption, human and social factors, perceptions on agile methods, and comparative studies. The review investigates what is currently known about the benefits and limitations of, and the strength of evidence for, agile methods. Implications for research and practice are presented. The main implication for research is a need for more and better empirical studies of agile software development within a common research agenda. For the industrial readership, the review provides a map of findings, according to topic, that can be compared for relevance to their own settings and situations.}
}

@article{rayyan-727967449,
  title={A systematic literature review of use case specifications research},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={67},
  pages={128-158},
  author={Tiwari, Saurabh and Gupta, Atul},
  url={https://www.sciencedirect.com/science/article/pii/S0950584915001081},
  keywords={Guidelines, Systematic reviews, Quality, Evolution, Use case specifications, Use case templates},
  abstract={Context Use cases have been widely accepted and acknowledged as a specification tool for specifying the functional requirements of a software system. Many variations of use cases exist which tries to address the issues such as their completeness, degree of formalism, automated information extraction, usability, and pertinence. Objective The aim of this systematic review is to examine the existing literature for the evolution of the use cases, their applications, quality assessments, open issues, and the future directions. Method We perform keyword-based extensive search to identify the relevant studies related to use case specifications research reported in journal articles, conference papers, workshop papers, bulletins and book chapters. Results The specified search process resulted 119 papers, which were published between 1992 and February 2014. This included, 54 journal articles, 42 conference papers, 2 ACM/IEEE bulletins, 12 book chapters, 6 workshop papers and 3 white papers. We found that as many as twenty use case templates have been proposed and applied for various software specification problems ranging from informal descriptions with paragraph-style text to more formal keyword-oriented templates. Conclusion Use cases have been evolved from initial plain, semi-formal textual descriptions to a more formal template structure facilitating automated information extraction in various software development life cycle activities such as requirement documentation, requirement analysis, requirement validation, domain modeling, test case generation, planning and estimation, and maintenance. The issues that remain to be sorted out are (1) the right degree of formalism, (2) the efficient change management, (3) the industrial relevance, and (4) assessment of the quality of the specification. Additionally, its synergy with other software models that are used in the development processes is an issue that needs to be addressed.}
}

@article{rayyan-727967450,
  title={Using mapping studies as the basis for further research – A participant-observer case study},
  year={2011},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={53},
  number={6},
  pages={638-651},
  author={Kitchenham, Barbara A and Budgen, David and Pearl Brereton, O},
  url={https://www.sciencedirect.com/science/article/pii/S0950584910002272},
  keywords={Systematic literature review, Software engineering, Case study, Mapping studies},
  abstract={Context We are strong advocates of evidence-based software engineering (EBSE) in general and systematic literature reviews (SLRs) in particular. We believe it is essential that the SLR methodology is used constructively to support software engineering research. Objective This study aims to assess the value of mapping studies which are a form of SLR that aims to identify and categorise the available research on a broad software engineering topic. Method We used a multi-case, participant-observer case study using five examples of studies that were based on preceding mapping studies. We also validated our results by contacting two other researchers who had undertaken studies based on preceding mapping studies and by assessing review comments related to our follow-on studies. Results Our original case study identified 11 unique benefits that can accrue from basing research on a preceding mapping study of which only two were case specific. We also identified nine problems associated with using preceding mapping studies of which two were case specific. These results were consistent with the information obtained from the validation activities. We did not find an example of an independent research group making use of a mapping study produced by other researchers. Conclusion Mapping studies can save time and effort for researchers and provide baselines to assist new research efforts. However, they must be of high quality in terms of completeness and rigour if they are to be a reliable basis for follow-on research.}
}

@article{rayyan-727967451,
  title={Mental health ubiquitous monitoring supported by social situation awareness: A systematic review},
  year={2020},
  journal={Journal of Biomedical Informatics},
  issn={1532-0464},
  volume={107},
  pages={103454},
  author={Moura, Ivan and Teles, Ariel and Silva, Francisco and Viana, Davi and Coutinho, Luciano and Barros, Flávio and Endler, Markus},
  url={https://www.sciencedirect.com/science/article/pii/S1532046420300824},
  keywords={Mental health, Mental states, Sociability, Social behavior, Social situation awareness, Ubiquitous computing, Social Support, Mental Health},
  abstract={Traditionally, the process of monitoring and evaluating social behavior related to mental health has based on self-reported information, which is limited by the subjective character of responses and various cognitive biases. Today, however, there is a growing amount of studies that have provided methods to objectively monitor social behavior through ubiquitous devices and have used this information to support mental health services. In this paper, we present a Systematic Literature Review (SLR) to identify, analyze and characterize the state of the art about the use of ubiquitous devices to monitor users' social behavior focused on mental health. For this purpose, we performed an exhaustive literature search on the six main digital libraries. A screening process was conducted on 160 peer-reviewed publications by applying suitable selection criteria to define the appropriate studies to the scope of this SLR. Next, 20 selected studies were forwarded to the data extraction phase. From an analysis of the selected studies, we recognized the types of social situations identified, the process of transforming contextual data into social situations, the use of social situation awareness to support mental health monitoring, and the methods used to evaluate proposed solutions. Additionally, we identified the main trends presented by this research area, as well as open questions and perspectives for future research. Results of this SLR showed that social situation-aware ubiquitous systems represent promising assistance tools for patients and mental health professionals. However, studies still present limitations in methodological rigor and restrictions in experiments, and solutions proposed by them have limitations to be overcome.}
}

@article{rayyan-727967452,
  title={Towards pragmatic interoperability to support collaboration: A systematic review and mapping of the literature},
  year={2016},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={72},
  pages={137-150},
  author={Neiva, Frâncila Weidt and David, José Maria N and Braga, Regina and Campos, Fernanda},
  url={https://www.sciencedirect.com/science/article/pii/S0950584916000021},
  keywords={Collaboration, Collaborative systems, Groupware, Interoperability, Pragmatic interoperability},
  abstract={Context: Many researchers have argued that providing interoperability support only considering the format and meaning (i.e. syntax and semantic) of data exchange is not enough to achieve complete, effective and meaningful collaboration. Pragmatic interoperability has been highlighted as a key requirement to enhance collaboration. However, fulfilling this requirement is not a trivial task and there is a lack of works discussing solutions to achieve this level of interoperability. Objectives: The aim of this study is to present a systematic review and mapping of the literature in order to identify, analyse and classify the published solutions to achieve pragmatic interoperability. Method: To conduct a systematic review and mapping in accordance with the guidelines proposed in the evidence-based software engineering literature. Results: Our study identified 13 papers reporting pragmatic interoperability computational solutions. The first paper in our set of selected papers was published in 2004; the main strategies used to address pragmatic interoperability issues were service discovery, composition and/or selection and ontologies. The application domain of the identified solutions was mainly e-business. In addition, most of the identified solutions were software architectures. Conclusion: Mature proposals addressing pragmatic interoperability are still rare in the literature. Although many works have discussed the importance of pragmatic interoperability, it is necessary that researchers report solutions that implement and evaluate pragmatic interoperability in order to make progress in this area.}
}

@article{rayyan-727967453,
  title={Leveraging Software Product Lines Engineering in the development of external DSLs: A systematic literature review},
  year={2016},
  journal={Computer Languages, Systems & Structures},
  issn={1477-8424},
  volume={46},
  pages={206-235},
  author={Méndez-Acuña, David and Galindo, José A and Degueule, Thomas and Combemale, Benoît and Baudry, Benoît},
  url={https://www.sciencedirect.com/science/article/pii/S1477842416300768},
  keywords={Variability management, Domain-specific languages, Software language engineering, Software Product Lines Engineering, Software},
  abstract={The use of domain-specific languages (DSLs) has become a successful technique in the development of complex systems. Consequently, nowadays we can find a large variety of DSLs for diverse purposes. However, not all these DSLs are completely different; many of them share certain commonalities coming from similar modeling patterns – such as state machines or petri nets – used for several purposes. In this scenario, the challenge for language designers is to take advantage of the commonalities existing among similar DSLs by reusing, as much as possible, formerly defined language constructs. The objective is to leverage previous engineering efforts to minimize implementation from scratch. To this end, recent research in software language engineering proposes the use of product line engineering, thus introducing the notion of language product lines. Nowadays, there are several approaches that result useful in the construction of language product lines. In this article, we report on an effort for organizing the literature on language product line engineering. More precisely, we propose a definition for the life-cycle of language product lines, and we use it to analyze the capabilities of current approaches. In addition, we provide a mapping between each approach and the technological space it supports.}
}

@article{rayyan-727967454,
  title={Software engineering article types: An analysis of the literature},
  year={2008},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={81},
  number={10},
  pages={1694-1714},
  author={Montesi, Michela and Lago, Patricia},
  url={https://www.sciencedirect.com/science/article/pii/S0164121207003184},
  keywords={Software engineering, Survey, Article genres, Article types, Requirements for publication, Software},
  abstract={The software engineering (SE) community has recently recognized that the field lacks well-established research paradigms and clear guidance on how to write good research reports. With no comprehensive guide to the different article types in the field, article writing and reviewing heavily depends on the expertise and the understanding of the individual SE actors. In this work, we classify and describe the article types published in SE with an emphasis on what is required for publication in journals and conference proceedings. Theoretically, we consider article types as genres, because we assume that each type of article has a specific function and a particular communicative purpose within the community, which the members of the community can recognize. We draw on written sources available, i.e. the instructions to authors/reviewers of major SE journals, the calls for papers of major SE conferences, and previous research published on the topic. Despite the fragmentation and limitations of the sources studied, we are able to propose a classification of different SE article types. Such classification helps in guiding the reader through the SE literature, and in making the researcher reflect on directions for improvements.}
}

@article{rayyan-727967455,
  title={Safety for mobile robotic systems: A systematic mapping study from a software engineering perspective},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={151},
  pages={150-179},
  author={Bozhinoski, Darko and Di Ruscio, Davide and Malavolta, Ivano and Pelliccione, Patrizio and Crnkovic, Ivica},
  url={https://www.sciencedirect.com/science/article/pii/S0164121219300317},
  keywords={Software, Systematic mapping study, Safety for mobile robots, Robotics},
  abstract={Robotic research is making huge progress. However, existing solutions are facing a number of challenges preventing them from being used in our everyday tasks: (i) robots operate in unknown environments, (ii) robots collaborate with each other and even with humans, and (iii) robots shall never injure people or create damages. Researchers are targeting those challenges from various perspectives, producing a fragmented research landscape. We aim at providing a comprehensive and replicable picture of the state of the art from a software engineering perspective on existing solutions aiming at managing safety for mobile robotic systems. We apply the systematic mapping methodology on an initial set of 1274 potentially relevant research papers, we selected 58 primary studies and analyzed them according to a systematically-defined classification framework. This work contributes with (i) a classification framework for methods or techniques for managing safety when dealing with the software of mobile robotic systems (MSRs), (ii) a map of current software methods or techniques for software safety for MRSs, (iii) an elaboration on emerging challenges and implications for future research, and (iv) a replication package for independent replication and verification of this study. Our results confirm that generally existing solutions are not yet ready to be used in everyday life. There is the need of turn-key solutions ready to deal with all the challenges mentioned above.}
}

@article{rayyan-727967456,
  title={Equality in cumulative voting: A systematic review with an improvement proposal},
  year={2013},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={55},
  number={2},
  pages={267-287},
  author={Riņķevičs, K and Torkar, R},
  url={https://www.sciencedirect.com/science/article/pii/S0950584912001589},
  keywords={Requirements engineering, Prioritization, Compositional data, Cumulative voting, Log-ratio},
  abstract={Context Prioritization is an essential part of requirements engineering, software release planning and many other software engineering disciplines. Cumulative Voting (CV) is known as a relatively simple method for prioritizing requirements on a ratio scale. Historically, CV has been applied in decision-making in government elections, corporate governance, and forestry. However, CV prioritization results are of a special type of data—compositional data. Objectives The purpose of this study is to aid decision-making by collecting knowledge on the empirical use of CV and develop a method for detecting prioritization items with equal priority. Methods We present a systematic literature review of CV and CV analysis methods. The review is based on searching electronic databases and snowball sampling of the found primary studies. Relevant studies are selected based on titles, abstracts, and full text inspection. Additionally, we propose Equality of Cumulative Votes (ECVs)—a CV result analysis method that identifies prioritization items with equal priority. Results CV has been used in not only requirements prioritization and release planning but also in e.g. software process improvement, change impact analysis and model driven software development. The review presents a collection of state of the practice studies and CV result analysis methods. In the end, ECV was applied to 27 prioritization cases from 14 studies and identified nine groups of equal items in three studies. Conclusions We believe that the analysis of the collected studies and the CV result analysis methods can help in the adoption of CV prioritization method. The evaluation of ECV indicates that it is able to detect prioritization items with equal priority and thus provide the practitioner with a more fine-grained analysis.}
}

@article{rayyan-727967457,
  title={Technology acceptance model in m-learning context: A systematic review},
  year={2018},
  journal={Computers & Education},
  issn={0360-1315},
  volume={125},
  pages={389-412},
  author={Al-Emran, Mostafa and Mezhuyev, Vitaliy and Kamaludin, Adzhar},
  url={https://www.sciencedirect.com/science/article/pii/S0360131518301519},
  keywords={Systematic literature review, Technology acceptance model, Mobile learning, Learning},
  abstract={Various review studies were conducted to provide valuable insights into the current research trend of the Technology Acceptance Model (TAM). Nevertheless, this issue still needs to be investigated from further directions. It has been noticed that research overlooks the investigation of TAM with regard to Mobile learning (M-learning) studies from the standpoint of different perspectives. The present study systematically reviews and synthesizes the TAM studies related to M-learning aiming to provide a comprehensive analysis of 87 research articles from 2006 to 2018. The main findings include that most of the TAM studies involving M-learning focused on extending the TAM with external variables, followed by the studies that extended the model by factors from other theories/models. In addition, the main research problem that was frequently tackled among all the analyzed studies was to examine the acceptance of M-learning among students. Moreover, questionnaire surveys were the primarily relied research methods for data collection. Additionally, most of the analyzed studies were undertaken in Taiwan, this is followed by Spain, China, and Malaysia, respectively among the other countries. Besides, most of the analyzed studies were frequently conducted in humanities and educational context, followed by IT and computer science context, respectively among the other contexts. Most of the analyzed studies were carried out in the higher educational settings. To that end, the findings of this review study provide an insight into the current trend of TAM research involving M-learning studies and form an essential reference for scholars in the M-learning context.}
}

@article{rayyan-727967458,
  title={Artificial Immune Systems approaches to secure the internet of things: A systematic review of the literature and recommendations for future research},
  year={2020},
  journal={Journal of Network and Computer Applications},
  issn={1084-8045},
  volume={157},
  pages={102537},
  author={Aldhaheri, Sahar and Alghazzawi, Daniyal and Cheng, Li and Barnawi, Ahmed and Alzahrani, Bandar A},
  url={https://www.sciencedirect.com/science/article/pii/S1084804520300114},
  keywords={Artificial intelligence, Internet of things, IoT, Artificial immune networks, Artificial immune system, Clonal selection, Cyber security, Danger theory, Dendritic cell, Negative selection, Network security, Internet, Immune System},
  abstract={As the Internet of Things (IoT) recently attains tremendous popularity, this promising technology leads to a variety of security challenges. The traditional solutions do not fit the new challenges brought by the IoT ecosystem. Although the development's area of Artificial Immune Systems (AIS) provides an opportunity to improve security issues and create a fertile and exciting environment for further research and experiments, there is not any systematic and comprehensive study about analyzing its importance for IoT environment. Therefore, this work aims to identify, evaluate, and perform a comprehensive study of empirical research on the studies of AIS approaches to secure the IoT environment. The relevant and high-quality studies are addressing using three research questions about the main research motivations, existing solutions, and future gaps and directions. The AIS approaches have been divided into three main categories based on IoT layers, and detailed classifications have also been included based on different parameters. To achieve this aim, the authors use a systematic literature review (SLR) as a powerful method to collect and critically analyze the research papers. Also, the authors discuss the selected studies and their main techniques, as well as their benefits and drawbacks in general. This research process strives to build a knowledge base for AIS solutions under the umbrella of IoT security and suggest directions for future research.}
}

@article{rayyan-727967459,
  title={A systematic review on the code smell effect},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={144},
  pages={450-477},
  author={Santos, José Amancio M and Rocha-Junior, João B and Prates, Luciana Carla Lins and do Nascimento, Rogeres Santos and Freitas, Mydiã Falcão and de Mendonça, Manoel Gomes},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218301444},
  keywords={Systematic review, Code smell, Thematic synthesis, Smell},
  abstract={Context: Code smell is a term commonly used to describe potential problems in the design of software. The concept is well accepted by the software engineering community. However, some studies have presented divergent findings about the usefulness of the smell concept as a tool to support software development tasks. The reasons of these divergences have not been considered because the studies are presented independently. Objective: To synthesize current knowledge related to the usefulness of the smell concept. We focused on empirical studies investigating how smells impact the software development, the code smell effect. Method: A systematic review about the smell effect is carried out. We grouped the primary studies findings in a thematic map. Result: The smell concept does not support the evaluation of quality design in practice activities of software development. There is no strong evidence correlating smells and some important software development attributes, such as effort in maintenance. Moreover, the studies point out that human agreement on smell detection is low. Conclusion: In order to improve analysis on the subject, the area needs to better outline: (i) factors affecting human evaluation of smells; and (ii) a classification of types of smells, grouping them according to relevant characteristics.}
}

@article{rayyan-727967460,
  title={Cloud computing service composition: A systematic literature review},
  year={2014},
  journal={Expert Systems with Applications},
  issn={0957-4174},
  volume={41},
  number={8},
  pages={3809-3824},
  author={Jula, Amin and Sundararajan, Elankovan and Othman, Zalinda},
  url={https://www.sciencedirect.com/science/article/pii/S0957417413009925},
  keywords={Systematic literature review, QoS, Cloud computing service composition, Importance percentage of quality of service parameters, Quality of service parameter, Research objectives},
  abstract={The increasing tendency of network service users to use cloud computing encourages web service vendors to supply services that have different functional and nonfunctional (quality of service) features and provide them in a service pool. Based on supply and demand rules and because of the exuberant growth of the services that are offered, cloud service brokers face tough competition against each other in providing quality of service enhancements. Such competition leads to a difficult and complicated process to provide simple service selection and composition in supplying composite services in the cloud, which should be considered an NP-hard problem. How to select appropriate services from the service pool, overcome composition restrictions, determine the importance of different quality of service parameters, focus on the dynamic characteristics of the problem, and address rapid changes in the properties of the services and network appear to be among the most important issues that must be investigated and addressed. In this paper, utilizing a systematic literature review, important questions that can be raised about the research performed in addressing the above-mentioned problem have been extracted and put forth. Then, by dividing the research into four main groups based on the problem-solving approaches and identifying the investigated quality of service parameters, intended objectives, and developing environments, beneficial results and statistics are obtained that can contribute to future research.}
}

@article{rayyan-727967461,
  title={A systematic review of evaluation of variability management approaches in software product lines},
  year={2011},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={53},
  number={4},
  pages={344-362},
  author={Chen, Lianping and Ali Babar, Muhammad},
  url={https://www.sciencedirect.com/science/article/pii/S0950584910002223},
  keywords={Systematic literature reviews, Software product line, Empirical studies, Variability management, Software},
  abstract={Context Variability management (VM) is one of the most important activities of software product-line engineering (SPLE), which intends to develop software-intensive systems using platforms and mass customization. VM encompasses the activities of eliciting and representing variability in software artefacts, establishing and managing dependencies among different variabilities, and supporting the exploitation of the variabilities for building and evolving a family of software systems. Software product line (SPL) community has allocated huge amount of effort to develop various approaches to dealing with variability related challenges during the last two decade. Several dozens of VM approaches have been reported. However, there has been no systematic effort to study how the reported VM approaches have been evaluated. Objective The objectives of this research are to review the status of evaluation of reported VM approaches and to synthesize the available evidence about the effects of the reported approaches. Method We carried out a systematic literature review of the VM approaches in SPLE reported from 1990s until December 2007. Results We selected 97 papers according to our inclusion and exclusion criteria. The selected papers appeared in 56 publication venues. We found that only a small number of the reviewed approaches had been evaluated using rigorous scientific methods. A detailed investigation of the reviewed studies employing empirical research methods revealed significant quality deficiencies in various aspects of the used quality assessment criteria. The synthesis of the available evidence showed that all studies, except one, reported only positive effects. Conclusion The findings from this systematic review show that a large majority of the reported VM approaches have not been sufficiently evaluated using scientifically rigorous methods. The available evidence is sparse and the quality of the presented evidence is quite low. The findings highlight the areas in need of improvement, i.e., rigorous evaluation of VM approaches. However, the reported evidence is quite consistent across different studies. That means the proposed approaches may be very beneficial when they are applied properly in appropriate situations. Hence, it can be concluded that further investigations need to pay more attention to the contexts under which different approaches can be more beneficial.}
}

@article{rayyan-727967462,
  title={Requirements modeling languages for software product lines: A systematic literature review},
  year={2016},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={69},
  pages={16-36},
  author={Sepúlveda, Samuel and Cravero, Ania and Cachero, Cristina},
  url={https://www.sciencedirect.com/science/article/pii/S0950584915001494},
  keywords={Software product lines, Systematic literature review, Requirements engineering, Modeling languages, Software},
  abstract={Context: Software product lines (SPLs) have reached a considerable level of adoption in the software industry, having demonstrated their cost-effectiveness for developing higher quality products with lower costs. For this reason, in the last years the requirements engineering community has devoted much effort to the development of a myriad of requirements modelling languages for SPLs. Objective: In this paper, we review and synthesize the current state of research of requirements modelling languages used in SPLs with respect to their degree of empirical validation, origin and context of use, level of expressiveness, maturity, and industry adoption. Method: We have conducted a systematic literature review with six research questions that cover the main objective. It includes 54 studies, published from 2000 to 2013. Results: The mean level of maturity of the modelling languages is 2.59 over 5, with 46% of them falling within level 2 or below -no implemented abstract syntax reported-. They show a level of expressiveness of 0.7 over 1.0. Some constructs (feature, mandatory, optional, alternative, exclude and require) are present in all the languages, while others (cardinality, attribute, constraint and label) are less common. Only 6% of the languages have been empirically validated, 41% report some kind of industry adoption and 71% of the languages are independent from any development process. Last but not least, 57% of the languages have been proposed by the academia, while 43% have been the result of a joint effort between academia and industry. Conclusions: Research on requirements modeling languages for SPLs has generated a myriad of languages that differ in the set of constructs provided to express SPL requirements. Their general lack of empirical validation and adoption in industry, together with their differences in maturity, draws the picture of a discipline that still needs to evolve.}
}

@article{rayyan-727967463,
  title={Choreography in the embedded systems domain: A systematic literature review},
  year={2017},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={91},
  pages={82-101},
  author={Taušan, Nebojša and Markkula, Jouni and Kuvaja, Pasi and Oivo, Markku},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917304469},
  keywords={Systematic literature review, Service-oriented architecture, Embedded systems, Choreography},
  abstract={Context Software companies that develop their products on a basis of service-oriented architecture can expect various improvements as a result of choreography. Current choreography practices, however, are not yet used extensively in the embedded systems domain even though service-oriented architecture is increasingly used in this domain. Objective The objective of this study is to identify current features of the use of choreography in the embedded systems domain for practitioners and researchers by systematically analysing current developments in the scientific literature, strategies for choreography adaption, choreography specification and execution types, and implicit assumptions about choreography. Method To fulfil this objective, a systematic literature review of scientific publications that focus on the use of choreography in the embedded systems domain was carried out. After a systematic screening of 6823 publications, 48 were selected as primary studies and analysed using thematic synthesis. Results The main results of the study showed that there are differences in how choreography is used in embedded and non-embedded systems domain. In the embedded systems domain, it is used to capture the service interactions of a single organisation, while, for example, in the enterprise systems domain it captures the service interactions among multiple organisations. Additionally, the results indicate that the use of choreography can lead to improvements in system performance and that the languages that are used for choreography modelling in the embedded systems domain are insufficiently expressive to capture the complexities that are typical in this domain. Conclusion The selection of the key information resources and the identified gaps in the existing literature offer researchers a foundation for further investigations and contribute to the advancement of the use of choreography in the embedded systems domain. The study results facilitate the work of practitioners by allowing them to make informed decisions about the applicability of choreography in their organisations.}
}

@article{rayyan-727967464,
  title={Design criteria for visualization of energy consumption: A systematic literature review},
  year={2015},
  journal={Sustainable Cities and Society},
  issn={2210-6707},
  volume={18},
  pages={1-12},
  author={Murugesan, Latha Karthigaa and Hoda, Rashina and Salcic, Zoran},
  url={https://www.sciencedirect.com/science/article/pii/S2210670715000499},
  keywords={Visualization, Energy, Grounded Theory},
  abstract={Visualizing energy consumption is widely considered an important way to motivate end-users to conserve energy. Designing effective visualizations, however, is a non-trivial software design challenge. In particular, there are no clear criteria for designing visualizations of energy consumption for end-users. This paper presents systematic literature review findings from a total of 22 primary studies selected after applying quality and relevance filters. The results were synthesized using Grounded Theory's open coding and constant comparison procedures and led to the emergence of design criteria for visualization as the central theme across all primary studies. The key categories comprising this central theme include: (a) functional criteria, which include information displayed in the visualization, modes of visualization, and visualization techniques, and (b) non-functional criteria, which include hardware and software considerations such as integrality, extensibility and portability. Together, these criteria provide clear guidelines based on research evidence for software engineers and researchers designing visualizations of energy consumption for end-users.}
}

@article{rayyan-727967465,
  title={Systematic review on machine learning (ML) methods for manufacturing processes – Identifying artificial intelligence (AI) methods for field application},
  year={2020},
  journal={Procedia CIRP},
  issn={2212-8271},
  volume={93},
  pages={413-418},
  author={Fahle, Simon and Prinz, Christopher and Kuhlenkötter, Bernd},
  url={https://www.sciencedirect.com/science/article/pii/S2212827120307435},
  keywords={machine learning, Artificial Intelligence, factory operation, production systems, Intelligence},
  abstract={Artificial Intelligence (AI) and especially machine learning (ML) become increasingly more frequently applicable in factory operations. This paper presents a systematic review of today's applications of ML techniques in the factory environment. The utilization of ML methods related to manufacturing process planning and control, predictive maintenance, quality control, in situ process control and optimization, logistics, robotics, assistance and learning systems for shopfloor employees are being analyzed. Moreover, an overview of ML training concepts in learning factories is given. Furthermore, these concepts will be analyzed regarding the implemented ML method. Finally, research gaps are identified.}
}

@article{rayyan-727967466,
  title={Software test process improvement approaches: A systematic literature review and an industrial case study},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={111},
  pages={1-33},
  author={Afzal, Wasif and Alone, Snehal and Glocksien, Kerstin and Torkar, Richard},
  url={https://www.sciencedirect.com/science/article/pii/S0164121215001910},
  keywords={Systematic literature review, Case study, Software test process improvement, Software},
  abstract={Software test process improvement (STPI) approaches are frameworks that guide software development organizations to improve their software testing process. We have identified existing STPI approaches and their characteristics (such as completeness of development, availability of information and assessment instruments, and domain limitations of the approaches) using a systematic literature review (SLR). Furthermore, two selected approaches (TPI NEXT and TMMi) are evaluated with respect to their content and assessment results in industry. As a result of this study, we have identified 18 STPI approaches and their characteristics. A detailed comparison of the content of TPI NEXT and TMMi is done. We found that many of the STPI approaches do not provide sufficient information or the approaches do not include assessment instruments. This makes it difficult to apply many approaches in industry. Greater similarities were found between TPI NEXT and TMMi and fewer differences. We conclude that numerous STPI approaches are available but not all are generally applicable for industry. One major difference between available approaches is their model representation. Even though the applied approaches generally show strong similarities, differences in the assessment results arise due to their different model representations.}
}

@article{rayyan-727967467,
  title={A systematic review of software architecture visualization techniques},
  year={2014},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={94},
  pages={161-185},
  author={Shahin, Mojtaba and Liang, Peng and Babar, Muhammad Ali},
  url={https://www.sciencedirect.com/science/article/pii/S0164121214000831},
  keywords={Software architecture, Visualization techniques, Software architecture visualization, Software},
  abstract={Context Given the increased interest in using visualization techniques (VTs) to help communicate and understand software architecture (SA) of large scale complex systems, several VTs and tools have been reported to represent architectural elements (such as architecture design, architectural patterns, and architectural design decisions). However, there is no attempt to systematically review and classify the VTs and associated tools reported for SA, and how they have been assessed and applied. Objective This work aimed at systematically reviewing the literature on software architecture visualization to develop a classification of VTs in SA, analyze the level of reported evidence and the use of different VTs for representing SA in different application domains, and identify the gaps for future research in the area. Method We used systematic literature review (SLR) method of the evidence-based software engineering (EBSE) for reviewing the literature on VTs for SA. We used both manual and automatic search strategies for searching the relevant papers published between 1 February 1999 and 1 July 2011. Results We selected 53 papers from the initially retrieved 23,056 articles for data extraction, analysis, and synthesis based on pre-defined inclusion and exclusion criteria. The results from the data analysis enabled us to classify the identified VTs into four types based on the usage popularity: graph-based, notation-based, matrix-based, and metaphor-based VTs. The VTs in SA are mostly used for architecture recovery and architectural evolution activities. We have also identified ten purposes of using VTs in SA. Our results also revealed that VTs in SA have been applied to a wide range of application domains, among which “graphics software” and “distributed system” have received the most attention. Conclusion SA visualization has gained significant importance in understanding and evolving software-intensive systems. However, only a few VTs have been employed in industrial practice. This review has enabled us to identify the following areas for further research and improvement: (i) it is necessary to perform more research on applying visualization techniques in architectural analysis, architectural synthesis, architectural implementation, and architecture reuse activities; (ii) it is essential to pay more attention to use more objective evaluation methods (e.g., controlled experiment) for providing more convincing evidence to support the promised benefits of using VTs in SA; (iii) it is important to conduct industrial surveys for investigating how software architecture practitioners actually employ VTs in architecting process and what are the issues that hinder and prevent them from adopting VTs in SA.}
}

@article{rayyan-727967468,
  title={A systematic review on the functional testing of semantic web services},
  year={2013},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={86},
  number={11},
  pages={2877-2889},
  author={Tahir, Abbas and Tosi, Davide and Morasca, Sandro},
  url={https://www.sciencedirect.com/science/article/pii/S0164121213001659},
  keywords={Systematic literature review, Functional testing, Semantic web services, Testing approach, Semantics},
  abstract={Semantic web services are gaining more attention as an important element of the emerging semantic web. Therefore, testing semantic web services is becoming a key concern as an essential quality assurance measure. The objective of this systematic literature review is to summarize the current state of the art of functional testing of semantic web services by providing answers to a set of research questions. The review follows a predefined procedure that involves automatically searching 5 well-known digital libraries. After applying the selection criteria to the results, a total of 34 studies were identified as relevant. Required information was extracted from the studies and summarized. Our systematic literature review identified some approaches available for deriving test cases from the specifications of semantic web services. However, many of the approaches are either not validated or the validation done lacks credibility. We believe that a substantial amount of work remains to be done to improve the current state of research in the area of testing semantic web services.}
}

@article{rayyan-727967469,
  title={A systematic review of scholar context-aware recommender systems},
  year={2015},
  journal={Expert Systems with Applications},
  issn={0957-4174},
  volume={42},
  number={3},
  pages={1743-1758},
  author={Champiri, Zohreh Dehghani and Shahamiri, Seyed Reza and Salim, Siti Salwah Binti},
  url={https://www.sciencedirect.com/science/article/pii/S0957417414005569},
  keywords={Academic digital library, Context-aware recommender system, Context-awareness, Contextual information, Awareness},
  abstract={Incorporating contextual information in recommender systems is an effective approach to create more accurate and relevant recommendations. This review has been conducted to identify the contextual information and methods used for making recommendations in digital libraries as well as the way researchers understood and used relevant contextual information from the years 2001 to 2013 based on the Kitchenham systematic review methodology. The results indicated that contextual information incorporated into recommendations can be categorised into three contexts, namely users' context, document's context, and environment context. In addition, the classical approaches such as collaborative filtering were employed more than the other approaches. Researchers have understood and exploited relevant contextual information through four ways, including citation of past studies, citation of past definitions, self-definitions, and field-query researches; however, citation of the past studies has been the most popular method. This review highlights the need for more investigations on the concept of context from user viewpoint in scholarly domains. It also discusses the way a context-aware recommender system can be effectively designed and implemented in digital libraries. Additionally, a few recommendations for future investigations on scholarly recommender systems are proposed.}
}

@article{rayyan-727967470,
  title={A systematic literature review of studies on business process modeling quality},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={58},
  pages={187-205},
  author={Moreno-Montes de Oca, Isel and Snoeck, Monique and Reijers, Hajo A and Rodríguez-Morffi, Abel},
  url={https://www.sciencedirect.com/science/article/pii/S0950584914001797},
  keywords={Systematic literature review, Business process modeling, Modeling quality},
  abstract={Context Business process modeling is an essential part of understanding and redesigning the activities that a typical enterprise uses to achieve its business goals. The quality of a business process model has a significant impact on the development of any enterprise and IT support for that process. Objective Since the insights on what constitutes modeling quality are constantly evolving, it is unclear whether research on business process modeling quality already covers all major aspects of modeling quality. Therefore, the objective of this research is to determine the state of the art on business process modeling quality: What aspects of process modeling quality have been addressed until now and which gaps remain to be covered? Method We performed a systematic literature review of peer reviewed articles as published between 2000 and August 2013 on business process modeling quality. To analyze the contributions of the papers we use the Formal Concept Analysis technique. Results We found 72 studies addressing quality aspects of business process models. These studies were classified into different dimensions: addressed model quality type, research goal, research method, and type of research result. Our findings suggest that there is no generally accepted framework of model quality types. Most research focuses on empirical and pragmatic quality aspects, specifically with respect to improving the understandability or readability of models. Among the various research methods, experimentation is the most popular one. The results from published research most often take the form of intangible knowledge. Conclusion We believe there is a lack of an encompassing and generally accepted definition of business process modeling quality. This evidences the need for the development of a broader quality framework capable of dealing with the different aspects of business process modeling quality. Different dimensions of business process quality and of the process of modeling still require further research.}
}

@article{rayyan-727967471,
  title={Empirical software engineering: From discipline to interdiscipline},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={148},
  pages={170-179},
  author={Méndez Fernández, Daniel and Passoth, Jan-Hendrik},
  url={https://www.sciencedirect.com/science/article/pii/S016412121830253X},
  keywords={Empirical software engineering, Interdisciplinary research, Science & technology studies, Symmetrical collaboration, Software},
  abstract={Empirical software engineering has received much attention in recent years and coined the shift from a more design-science-driven engineering discipline to an insight-oriented, and theory-centric one. Yet, we still face many challenges, among which some increase the need for interdisciplinary research. This is especially true for the investigation of social, cultural and human-centric aspects of software engineering. Although we can already observe an increased recognition of the need for more interdisciplinary research in (empirical) software engineering, such research configurations come with challenges barely discussed from a scientific point of view. In this position paper, we critically reflect upon the epistemological setting of empirical software engineering and elaborate its configuration as an Interdiscipline. In particular, we (1) elaborate a pragmatic view on empirical research for software engineering reflecting a cyclic process for knowledge creation, (2) motivate a path towards symmetrical interdisciplinary research, and (3) adopt five rules of thumb from other interdisciplinary collaborations in our field before concluding with new emerging challenges. This supports to elevate empirical software engineering from a developing discipline moving towards a paradigmatic stage of normal science to one that configures interdisciplinary teams and research methods symmetrically.}
}

@article{rayyan-727967472,
  title={Adoption of open source software in software-intensive organizations – A systematic literature review},
  year={2010},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={52},
  number={11},
  pages={1133-1154},
  author={Hauge, Øyvind and Ayala, Claudia and Conradi, Reidar},
  url={https://www.sciencedirect.com/science/article/pii/S0950584910000972},
  keywords={Systematic literature review, Software development, Open source software, Organizations, Software},
  abstract={Context Open source software (OSS) is changing the way organizations develop, acquire, use, and commercialize software. Objective This paper seeks to identify how organizations adopt OSS, classify the literature according to these ways of adopting OSS, and with a focus on software development evaluate the research on adoption of OSS in organizations. Method Based on the systematic literature review method we reviewed publications from 24 journals and seven conference and workshop proceedings, published between 1998 and 2008. From a population of 24,289 papers, we identified 112 papers that provide empirical evidence on how organizations actually adopt OSS. Results We show that adopting OSS involves more than simply using OSS products. We moreover provide a classification framework consisting of six distinctly different ways in which organizations adopt OSS. This framework is used to illustrate some of the opportunities and challenges organizations meet when approaching OSS, to show that OSS can be adopted successfully in different ways, and to organize and review existing research. We find that existing research on OSS adoption does not sufficiently describe the context of the organizations studied, and it fails to benefit fully from related research fields. While existing research covers a large number of topics, it contains very few closely related studies. To aid this situation, we offer directions for future research. Conclusion The implications of our findings are twofold. On the one hand, practitioners should embrace the many opportunities OSS offers, but consciously evaluate the consequences of adopting it in their own context. They may use our framework and the success stories provided by the literature in their own evaluations. On the other hand, researchers should align their work, and perform more empirical research on topics that are important to organizations. Our framework may be used to position this research and to describe the context of the organization they are studying.}
}

@article{rayyan-727967473,
  title={Complexity metrics for process models – A systematic literature review},
  year={2017},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={51},
  pages={104-117},
  author={Polančič, Gregor and Cegnar, Blaž},
  url={https://www.sciencedirect.com/science/article/pii/S0920548916302276},
  keywords={Systematic literature review, Process model, Metrics, Complexity measurement, Process diagram, Metronidazole},
  abstract={Context One of the focal purposes of using ‘visual' process models (i.e. process diagrams) is to ensure easier, universally understood and unambiguous diagrammatic communication. Thus the models should be easy to comprehend and maintain, which is directly related to their complexity. In order to systematically address process models complexity, it has to be measured. Objective The goal of our work was to provide a better overview and understanding in the field of process models complexity and to provide an overview of the corresponding metrics. Method A systematic literature review (SLR) was conducted, being the most suitable method for achieving aforementioned goals. In addition, to answer the stated research questions, different techniques for qualitative and quantitative data analysis and synthesis were used. Results We identified 43 relevant articles which were systematically analyzed according to a pre-defined process and data acquisition form. Out of these articles we collected 66 process models complexity metrics. Conclusion Modelers can use the ‘catalogue' of process complexity metrics to establish and ensure good quality of diagrams, whereas researches can relate to or extend the ‘catalogue' by providing new metrics or new insights to existing ones.}
}

@article{rayyan-727967474,
  title={Preliminary systematic literature review of software and systems traceability},
  year={2017},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={112},
  pages={1141-1150},
  author={Kaiya, Haruhiko and Sato, Ryohei and Hazeyama, Atsuo and Ogata, Shinpei and Okubo, Takao and Tanaka, Takafumi and Yoshioka, Nobukazu and Washizaki, Hironori},
  url={https://www.sciencedirect.com/science/article/pii/S1877050917315090},
  keywords={Software, Systematic Literature Review, ACM, IEEE CPS, Systems Traceability},
  abstract={Traceability is important knowledge for improving the artifacts of software and systems and processes related to them. Even in a single system, various kinds of artifacts exist. Various kinds of processes also exist, and each of them relates to different kinds of artifacts. Traceability over them has thus large diversity. In addition, developers in each process have different types of purposes to improve their artifacts and process. Research results in traceability have to be categorized and analyzed so that such a developer can choose one of them to achieve his/her purposes. In this paper, we report on the results of Systematic Literature Review (SLR) related to software and systems traceability. Our SLR is preliminary one because we only analyzed articles in ACM digital library and IEEE computer society digital library. We found several interesting trends in traceability research. For example, researches related to creating or maintaining traceability are larger than those related to using it or thinking its strategy. Various kinds of traceability purposes are addressed or assumed in many researches, but some researches do not specify purposes. Purposes related to changes and updates are dominant.}
}

@article{rayyan-727967475,
  title={Systematic literature review on agile practices in global software development},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={96},
  pages={161-180},
  author={Vallon, Raoul and da Silva Estácio, Bernardo José and Prikladnicki, Rafael and Grechenig, Thomas},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917302975},
  keywords={Systematic literature review, Agile software development, Scrum, XP, Global software development, Distributed software development, Global software engineering, Extreme programming, Agile practices, Software},
  abstract={Context Developing software in distributed development environments exhibits coordination, control and communication challenges. Agile practices, which demand frequent communication and self-organization between remote sites, are increasingly found in global software development (GSD) to mitigate said challenges. Objective We aim to provide detailed insight into what is reported on the successful application of agile practices in GSD from 1999 to 2016 and also identify the most frequently applied agile practices and reported distribution scenarios. We further strive to uncover research opportunities and gaps in the field of agile GSD. Method We build our systematic literature review on top of a previous review, which investigated studies published between 1999 and 2009, and extend the review by years 2010–2016, for which we conduct both a quantitative and a qualitative analysis. Results Our results show that the majority of the cases studied is global and involves complex distribution scenarios with Scrum or combined Scrum/Extreme Programming being the most used agile methods. Key results include that in contrast to 1999–2009, where four Extreme Programming practices were among the ten most frequently used agile practices, in 2010–2016 Scrum is in the center of agile GSD implementations with eight Scrum-based practices in the top ten agile practices used in GSD. Conclusion Agile GSD is a maturing research field with higher quality contributions and a greater variety of publication types and methods from 2010 to 2016 than before from 1999 to 2009. However, researchers need to report full empirical contextual details of their studied cases in order to improve the generalizability of results and allow the future creation of stronger frameworks to drive the implementation of agile practices in GSD.}
}

@article{rayyan-727967476,
  title={How games for computing education are evaluated? A systematic literature review},
  year={2017},
  journal={Computers & Education},
  issn={0360-1315},
  volume={107},
  pages={68-90},
  author={Petri, Giani and Gresse von Wangenheim, Christiane},
  url={https://www.sciencedirect.com/science/article/pii/S0360131517300040},
  keywords={Systematic literature review, Evaluation, Computer science, Computing education, Educational game},
  abstract={Educational games are assumed to be an effective and efficient instructional strategy for computing education. However, it is essential to systematically evaluate such games in order to obtain sound evidence of their impact. Thus, the objective of this article is to present the state of the art on how games for computing education are evaluated. Therefore, we performed a systematic literature review of a sample of 3617 articles from which 112 relevant articles have been identified, describing 117 studies on the evaluation of games for computing education. Based on these studies we analyzed how evaluations are defined (the analysis factors evaluated, research designs, evaluation models/methods used, kind of data collection instruments, etc.), how they have been executed (sample size and replications) and analyzed (data analysis methods used). As a result, we can confirm that most evaluations use a simple research design in which, typically, the game is used and afterwards subjective feedback is collected via questionnaires from the learners. The majority of the evaluations are run with small samples, without replication, using mostly qualitative methods for data analysis. We also observed that most studies do not use a well-defined evaluation model or method. This shows that there is a need for more rigorous evaluations as well as methodological support in order to assist game creators and instructors to improve such games as well as to systematically support decisions on when or how to include them within instructional units.}
}

@article{rayyan-727967477,
  title={A systematic literature review of software requirements reuse approaches},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={93},
  pages={223-245},
  author={Irshad, Mohsin and Petersen, Kai and Poulding, Simon},
  url={https://www.sciencedirect.com/science/article/pii/S0950584916303615},
  keywords={Artefact reuse, Relevance, Requirements reuse, Reusability, Rigor, Software requirements, Software},
  abstract={Context Early software reuse is considered as the most beneficial form of software reuse. Hence, previous research has focused on supporting the reuse of software requirements. Objective This study aims to identify and investigate the current state of the art with respect to (a) what requirement reuse approaches have been proposed, (b) the methods used to evaluate the approaches, (c) the characteristics of the approaches, and (d) the quality of empirical studies on requirements reuse with respect to rigor and relevance. Method We conducted a systematic review and a combination of snowball sampling and database search have been used to identify the studies. The rigor and relevance scoring rubric has been used to assess the quality of the empirical studies. Multiple researchers have been involved in each step to increase the reliability of the study. Results Sixty-nine studies were identified that describe requirements reuse approaches. The majority of the approaches used structuring and matching of requirements as a method to support requirements reuse and text-based artefacts were commonly used as an input to these approaches. Further evaluation of the studies revealed that the majority of the approaches are not validated in the industry. The subset of empirical studies (22 in total) was analyzed for rigor and relevance and two studies achieved the maximum score for rigor and relevance based on the rubric. It was found that mostly text-based requirements reuse approaches were validated in the industry. Conclusion From the review, it was found that a number of approaches already exist in literature, but many approaches are not validated in industry. The evaluation of rigor and relevance of empirical studies show that these do not contain details of context, validity threats, and the industrial settings, thus highlighting the need for the industrial evaluation of the approaches.}
}

@article{rayyan-727967478,
  title={Factors influencing the understandability of process models: A systematic literature review},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={93},
  pages={112-129},
  author={Dikici, Ahmet and Turetken, Oktay and Demirors, Onur},
  url={https://www.sciencedirect.com/science/article/pii/S0950584916302889},
  keywords={Systematic literature review, Understandability, Business process model, Comprehension, Process model understandability, Fibrinogen},
  abstract={Context Process models are key in facilitating communication in organizations and in designing process-aware information systems. Organizations are facing increasingly larger and more complex processes, which pose difficulties to the understandability of process models. The literature reports several factors that are considered to influence the understandability of process models. However, these studies typically focus on testing of a limited set of factors. A work that collects, abstracts and synthesizes an in-depth summary of the current literature will help in developing the research in this field. Objective We conducted a systematic literature review (SLR) focusing on the empirical studies in the existing literature in order to better understand the state of the research on process model understandability, and identify the gaps and opportunities for future research. Method We searched the studies between the years 1995 and 2015 in established electronic libraries. Out of 1066 publications retrieved initially, we selected 45 publications for thorough analysis. We identified, analyzed and categorized factors that are considered to influence the understandability of process models as studied in the literature using empirical methods. We also analyzed the indicators that are used to quantify process model understandability. Results Our analysis identifies several gaps in the field, as well as issues of inconsistent findings regarding the effect of some factors, unbalanced emphasis on certain indicators, and methodological concerns. Conclusions The existing research calls for comprehensive empirical studies to contribute to a better understanding of the factors of process model understandability. Our study is a comprehensive source for researchers working on the understandability of process models and related fields, and a useful guide for practitioners aiming to generate understandable process models.}
}

@article{rayyan-727967479,
  title={Agile methods tailoring – A systematic literature review},
  year={2015},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={110},
  pages={85-100},
  author={Campanelli, Amadeu Silveira and Parreiras, Fernando Silva},
  url={https://www.sciencedirect.com/science/article/pii/S0164121215001843},
  keywords={Agile method tailoring, Agile practice selection, Software method tailoring},
  abstract={Background: The software development industry has been adopting agile methods instead of traditional software development methods because they are more flexible and can bring benefits such as handling requirements changes, productivity gains and business alignment. Objective: This study seeks to evaluate, synthesize, and present aspects of research on agile methods tailoring including the method tailoring approaches adopted and the criteria used for agile practice selection. Method: The method adopted was a Systematic Literature Review (SLR) on studies published from 2002 to 2014. Results: 56 out of 783 papers have been identified as describing agile method tailoring approaches. These studies have been identified as case studies regarding the empirical research, as solution proposals regarding the research type, and as evaluation studies regarding the research validation type. Most of the papers used method engineering to implement tailoring and were not specific to any agile method on their scope. Conclusion: Most of agile methods tailoring research papers proposed or improved a technique, were implemented as case studies analyzing one case in details and validated their findings using evaluation. Method engineering was the base for tailoring, the approaches are independent of agile method and the main criteria used are internal environment and objectives variables.}
}

@article{rayyan-727967480,
  title={Risk management in the software life cycle: A systematic literature review},
  year={2020},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={71},
  pages={103431},
  author={Masso, Jhon and Pino, Francisco J and Pardo, César and García, Félix and Piattini, Mario},
  url={https://www.sciencedirect.com/science/article/pii/S0920548919300881},
  keywords={Systematic literature review, ISO 12207, ISO 31000, Risk management activities, Software life cycle processes, Software risk, Risk Management, Software},
  abstract={Risk management (RM) plays a key role in project management, as it allows identification and prompt management of threats that may arise during project execution. Furthermore, project management within the software industry is evolving rapidly nowadays, a fact that implies new challenges, because the emergence and use of fresh approaches has brought a greater degree of complexity to the RM process. The objective of this paper is to carry out a systematic literature review (SLR) in the field of software risk, in an attempt to characterize and present the state of the art of this field, identifying gaps and opportunities for further research. From the analysis of the results of this SLR it could be observed that interest on the part of the scientific community has turned away from the definition of research work that addressed an integrated risk management process, to pay attention to work that concentrates on specific activities of this process. It was also possible to see that there is a clear lack of scientific rigour as regards the process of validation in the different studies, and a deficiency in the use of standards or of de facto models to define these.}
}

@article{rayyan-727967481,
  title={A systematic literature review of techniques and metrics to reduce the cost of mutation testing},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={157},
  pages={110388},
  author={Pizzoleto, Alessandro Viola and Ferrari, Fabiano Cutigi and Offutt, Jeff and Fernandes, Leo and Ribeiro, Márcio},
  url={https://www.sciencedirect.com/science/article/pii/S0164121219301554},
  keywords={Systematic review, Mutation testing, Cost reduction, Mutation analysis, Mutation, Metronidazole},
  abstract={Historically, researchers have proposed and applied many techniques to reduce the cost of mutation testing. It has become difficult to find all techniques and to understand the cost-benefit tradeoffs among them, which is critical to transitioning this technology to practice. This paper extends a prior workshop paper to summarize and analyze the current knowledge about reducing the cost of mutation testing through a systematic literature review. We selected 175 peer-reviewed studies, from which 153 present either original or updated contributions. Our analysis resulted in six main goals for cost reduction and 21 techniques. In the last decade, a growing number of studies explored techniques such as selective mutation, evolutionary algorithms, control-flow analysis, and higher-order mutation. Furthermore, we characterized 18 metrics, with particular interest in the number of mutants to be executed, test cases required, equivalent mutants generated and detected, and mutant execution speedup. We found that cost reduction for mutation is increasingly becoming interdisciplinary, often combining multiple techniques. Additionally, measurements vary even for studies that use the same techniques. Researchers can use our results to find more detailed information about particular techniques, and to design comparable and reproducible experiments.}
}

@article{rayyan-727967482,
  title={Software process modeling languages: A systematic literature review},
  year={2014},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={56},
  number={2},
  pages={103-116},
  author={García-Borgoñón, L and Barcelona, M A and García-García, J A and Alba, M and Escalona, M J},
  url={https://www.sciencedirect.com/science/article/pii/S0950584913001894},
  keywords={Systematic literature review, Software process language, Software process modeling, Software},
  abstract={Context Organizations working in software development are aware that processes are very important assets as well as they are very conscious of the need to deploy well-defined processes with the goal of improving software product development and, particularly, quality. Software process modeling languages are an important support for describing and managing software processes in software-intensive organizations. Objective This paper seeks to identify what software process modeling languages have been defined in last decade, the relationships and dependencies among them and, starting from the current state, to define directions for future research. Method A systematic literature review was developed. 1929 papers were retrieved by a manual search in 9 databases and 46 primary studies were finally included. Results Since 2000 more than 40 languages have been first reported, each of which with a concrete purpose. We show that different base technologies have been used to define software process modeling languages. We provide a scheme where each language is registered together with the year it was created, the base technology used to define it and whether it is considered a starting point for later languages. This scheme is used to illustrate the trend in software process modeling languages. Finally, we present directions for future research. Conclusion This review presents the different software process modeling languages that have been developed in the last ten years, showing the relevant fact that model-based SPMLs (Software Process Modeling Languages) are being considered as a current trend. Each one of these languages has been designed with a particular motivation, to solve problems which had been detected. However, there are still several problems to face, which have become evident in this review. This let us provide researchers with some guidelines for future research on this topic.}
}

@article{rayyan-727967483,
  title={Application of artificial intelligence methods in vital signs analysis of hospitalized patients: A systematic literature review},
  year={2020},
  journal={Applied Soft Computing},
  issn={1568-4946},
  volume={96},
  pages={106612},
  author={Kaieski, Naira and da Costa, Cristiano André and da Rosa Righi, Rodrigo and Lora, Priscila Schmidt and Eskofier, Björn},
  url={https://www.sciencedirect.com/science/article/pii/S1568494620305500},
  keywords={Artificial intelligence, Machine learning, Health informatics, Vital signs},
  abstract={In a hospital environment, patients are monitored continuously by electronic devices and health professionals. Therefore, a large amount of data is collected and stored in electronic health records systems for each patient. Among such data, vital signs are one of the most common and relevant types of information monitored to assess a patient's health status. Artificial intelligence techniques can be used to analyze and learn useful standards from clinical datasets to provide better evidence to support the decisions of health professionals and thus help to improve patient health outcomes in hospitals. This systematic literature review aims to provide an updated computational perspective of how artificial intelligence has been applied to analyze the vital signs of adult hospitalized patients and the outcomes obtained. To this end, we reviewed 2899 scientific articles published between 2008 and 2018 and selected 78 articles that met our inclusion criteria to answer the research questions. Moreover, we used the information found in the reviewed articles to propose a taxonomy and identified the main concerns, challenges, and opportunities in this field. Our findings demonstrate that many researchers are exploring the use of artificial intelligence methods in tasks related to improving the health outcomes of hospitalized patients in distinct units. Additionally, although vital signs are significant predictors of clinical deterioration, they are not analyzed in isolation to predict or identify a clinical outcome. Our taxonomy and discussion contribute to the achievement of a significant degree of coverage regarding the aspects related to using machine learning to improve health outcomes in hospital environments, while highlighting gaps in the literature for future research.}
}

@article{rayyan-727967484,
  title={Does the technology acceptance model predict actual use? A systematic literature review},
  year={2010},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={52},
  number={5},
  pages={463-479},
  author={Turner, Mark and Kitchenham, Barbara and Brereton, Pearl and Charters, Stuart and Budgen, David},
  url={https://www.sciencedirect.com/science/article/pii/S0950584909002055},
  keywords={Evidence-based software engineering, Systematic literature review, Literature review, Actual usage, Technology acceptance model (TAM)},
  abstract={Context The technology acceptance model (TAM) was proposed in 1989 as a means of predicting technology usage. However, it is usually validated by using a measure of behavioural intention to use (BI) rather than actual usage. Objective This review examines the evidence that the TAM predicts actual usage using both subjective and objective measures of actual usage. Method We performed a systematic literature review based on a search of six digital libraries, along with vote-counting meta-analysis to analyse the overall results. Results The search identified 79 relevant empirical studies in 73 articles. The results show that BI is likely to be correlated with actual usage. However, the TAM variables perceived ease of use (PEU) and perceived usefulness (PU) are less likely to be correlated with actual usage. Conclusion Care should be taken using the TAM outside the context in which it has been validated.}
}

@article{rayyan-727967485,
  title={Agent-based simulation of unmanned aerial vehicles in civilian applications: A systematic literature review and research directions},
  year={2019},
  journal={Future Generation Computer Systems},
  issn={0167-739X},
  volume={100},
  pages={344-364},
  author={Mualla, Yazan and Najjar, Amro and Daoud, Alaa and Galland, Stéphane and Nicolle, Christophe and Yasar, Ansar-Ul-Haque and Shakshuki, Elhadi},
  url={https://www.sciencedirect.com/science/article/pii/S0167739X18328462},
  keywords={Systematic literature review, Agent-based simulation, Civilian applications, Multi-agent systems, Unmanned aerial vehicle},
  abstract={Recently, the civilian applications of Unmanned Aerial Vehicles (UAVs) are gaining more interest in several domains. Due to operational costs, safety concerns, and legal regulations, Agent-Based Simulation (ABS) is commonly used to design models and conduct tests. This has resulted in numerous research works addressing ABS in civilian UAV applications. This paper aims to provide a comprehensive overview of the ABS contribution in civilian UAV applications by conducting a Systematic Literature Review (SLR) on the relevant research in the previous ten years. Following the SLR methodology, this objective is broken down into several research questions aiming to (i) understand the evolution of ABS use in civilian UAV applications and identify the related hot research topics, (ii) identify the underlying artificial intelligence systems used in the literature, (iii) understand how and when ABS is integrated in broader and more complex internet of things & ubiquitous computing environments, and (iv) identity the communication technologies, tools, and evaluation techniques used to design, implement, and test the proposed ABS models. From the SLR results, key research directions are highlighted including problems related to autonomy, explainability, security, flight duration, integration within smart cities, regulations, and validation & verification of the UAV behavior.}
}

@article{rayyan-727967486,
  title={Antecedents to IT personnel's intentions to leave: A systematic literature review},
  year={2011},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={84},
  number={2},
  pages={238-249},
  author={Ghapanchi, Amir Hossein and Aurum, Aybuke},
  url={https://www.sciencedirect.com/science/article/pii/S0164121210002645},
  keywords={Systematic review, Employee retention, Employee turnover, Intention to leave, IT personnel},
  abstract={This paper undertakes a systematic review to gain insight into existing studies on the turnover of information technology (IT) personnel. Our systematic review of 72 studies from 1980 to 2008 examines the background and trend of research into IT personnel's intentions to leave their workplaces, in addition to providing a taxonomy of the determinants of their intentions to quit as captured in IT literature. We note a huge growth in the number of academic papers on the topic since 1998. Moreover, most of the research on IT turnover has been undertaken in North America, followed by Asia. Based on the 72 extracted studies, we found a total of 70 conceptually distinct IT turnover drivers. We classified them into the 5 broad categories of individual, organisational, job-related, psychological, and environmental, each containing three to four sub-categories. Finally, this paper presents insightful recommendations for IT practitioners as well as for the research community.}
}

@article{rayyan-727967487,
  title={A systematic literature review on agile requirements engineering practices and challenges},
  year={2015},
  journal={Computers in Human Behavior},
  issn={0747-5632},
  volume={51},
  pages={915-929},
  author={Inayat, Irum and Salim, Siti Salwah and Marczak, Sabrina and Daneva, Maya and Shamshirband, Shahaboddin},
  url={https://www.sciencedirect.com/science/article/pii/S074756321400569X},
  keywords={Collaboration, Systematic review, Agile requirements engineering, Agile software development methods, Traditional requirements engineering},
  abstract={Unlike traditional software development methods, agile methods are marked by extensive collaboration, i.e. face-to-face communication. Although claimed to be beneficial, the software development community as a whole is still unfamiliar with the role of the requirements engineering practices in agile methods. The term “agile requirements engineering” is used to define the “agile way” of planning, executing and reasoning about requirements engineering activities. Moreover, not much is known about the challenges posed by collaboration-oriented agile way of dealing with requirements engineering activities. Our goal is to map the evidence available about requirements engineering practices adopted and challenges faced by agile teams in order to understand how traditional requirements engineering issues are resolved using agile requirements engineering. We conducted a systematic review of literature published between 2002 and June 2013 and identified 21 papers, that discuss agile requirements engineering. We formulated and applied specific inclusion and exclusion criteria in two distinct rounds to determine the most relevant studies for our research goal. The review identified 17 practices of agile requirements engineering, five challenges traceable to traditional requirements engineering that were overcome by agile requirements engineering, and eight challenges posed by the practice of agile requirements engineering. However, our findings suggest that agile requirements engineering as a research context needs additional attention and more empirical results are required to better understand the impact of agile requirements engineering practices e.g. dealing with non-functional requirements and self-organising teams.}
}

@article{rayyan-727967488,
  title={Software architecture knowledge management approaches and their support for knowledge management activities: A systematic literature review},
  year={2016},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={80},
  pages={265-286},
  author={Weinreich, Rainer and Groher, Iris},
  url={https://www.sciencedirect.com/science/article/pii/S0950584916301707},
  keywords={Systematic literature review, Software architecture, Software architecture knowledge management, Software architecture knowledge management activities, Software architecture knowledge management approaches, Software},
  abstract={Context: Numerous approaches for Software Architecture Knowledge Management (SAKM) have been developed by the research community over the last decade. Still, these approaches have not yet found widespread use in practice. Objective: This work identifies existing approaches to SAKM and analyzes them in terms of their support for central architecture knowledge management activities, i.e., capturing, using, maintaining, sharing, and reuse of architectural knowledge, along with presenting the evidence provided for this support. Method: A systematic literature review has been conducted for identifying and analyzing SAKM approaches, covering work published between January 2004 and August 2015. We identified 56 different approaches to SAKM based on 115 studies. We analyzed each approach in terms of its focus and support for important architecture knowledge management activities and in terms of the provided level of evidence for each supported activity. Results: Most of the developed approaches focus on using already-captured knowledge. Using is also the best-validated activity. The problem of efficient capturing is still not sufficiently addressed, and only a few approaches specifically address reuse, sharing, and, especially, maintaining. Conclusions: Without adequate support for other core architecture knowledge management activities besides using, the adoption of SAKM in practice will remain an elusive target. The problem of efficient capturing is still unsolved, as is the problem of maintaining captured knowledge over the long term. We also need more case studies and replication studies providing evidence for the usefulness of developed support for SAKM activities, as well as better reporting on these case studies.}
}

@article{rayyan-727967489,
  title={A systematic literature review on the industrial use of software process simulation},
  year={2014},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={97},
  pages={65-85},
  author={Ali, Nauman Bin and Petersen, Kai and Wohlin, Claes},
  url={https://www.sciencedirect.com/science/article/pii/S0164121214001502},
  keywords={Systematic literature review, Evidence based software engineering, Software process simulation, Software},
  abstract={Context Software process simulation modelling (SPSM) captures the dynamic behaviour and uncertainty in the software process. Existing literature has conflicting claims about its practical usefulness: SPSM is useful and has an industrial impact; SPSM is useful and has no industrial impact yet; SPSM is not useful and has little potential for industry. Objective To assess the conflicting standpoints on the usefulness of SPSM. Method A systematic literature review was performed to identify, assess and aggregate empirical evidence on the usefulness of SPSM. Results In the primary studies, to date, the persistent trend is that of proof-of-concept applications of software process simulation for various purposes (e.g. estimation, training, process improvement, etc.). They score poorly on the stated quality criteria. Also only a few studies report some initial evaluation of the simulation models for the intended purposes. Conclusion There is a lack of conclusive evidence to substantiate the claimed usefulness of SPSM for any of the intended purposes. A few studies that report the cost of applying simulation do not support the claim that it is an inexpensive method. Furthermore, there is a paramount need for improvement in conducting and reporting simulation studies with an emphasis on evaluation against the intended purpose.}
}

@article{rayyan-727967490,
  title={Cloud service evaluation method-based Multi-Criteria Decision-Making: A systematic literature review},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={139},
  pages={161-188},
  author={Alabool, Hamzeh and Kamil, Ahmad and Arshad, Noreen and Alarabiat, Deemah},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218300244},
  keywords={Systematic literature review, Cloud computing service, Cloud service evaluation method, Evaluation theory, Multi-Criteria Decision-Making (MCDM), Decision Making},
  abstract={A substantial effort has been made to solve the cloud-service evaluation problem. Different Cloud Service Evaluation Methods (CSEMs) have been developed to address the problem. Cloud services are evaluated against multiple criteria, which leads to a Multi-Criteria Decision-Making (MCDM) problem. Yet, studies that assess, analyse, and summarize the unresolved problems and shortcomings of current CSEM-based MCDM are limited. In the existing review studies, only individual parts of CSEMs, rarely the full solution, are reviewed and examined. To investigate CSEMs comprehensively, we present a systematic literature review based on Evaluation Theory, a theory that generalizes six evaluation components, target, criteria, yardstick, data gathering techniques, synthesis techniques, and evaluation process. These six evaluation components and the CSEMs validation approach are the seven dimensions used to assess and analyse 77 papers published from 2006 to 2016. Sixteen research deficiencies were identified. The results confirm that the majority of the studies of the proposed CSEMs were either incomplete or lacked sufficient evidence. This research not only provides the relative strengths and weaknesses of the different CSEMs but also offers a basis for researchers and decision makers to develop improved CSEMs.}
}

@article{rayyan-727967491,
  title={Identification of personal traits in adaptive learning environment: Systematic literature review},
  year={2019},
  journal={Computers & Education},
  issn={0360-1315},
  volume={130},
  pages={168-190},
  author={Afini Normadhi, Nur Baiti and Shuib, Liyana and Md Nasir, Hairul Nizam and Bimba, Andrew and Idris, Norisma and Balakrishnan, Vimala},
  url={https://www.sciencedirect.com/science/article/pii/S0360131518303026},
  keywords={Interactive learning environments, Cooperative/collaborative learning, Intelligent tutoring systems, Navigation},
  abstract={An adaptive learning environment provides personalised information to the learner through self-directed study. An adaptive learning environment model can be subdivided into a learner model, domain model, instructional model and adaptive engine. Personal traits comprise part of the components in a learner model and can be identified either explicitly or implicitly in an adaptive learning environment. In such an environment, the e-learning system should adapt to a learner's needs. However, even though academic research on adaptive learning environments has increased, the field lacks a comprehensive literature analysis of learners' personal traits in these environments. This study conducts a systematic literature review to identify the most commonly used personal traits in modelling the learner and the existing techniques suitable for identifying personal traits in an adaptive learning environment. A total of 140 articles spanning the years 2010–2017 are initially reviewed, from which 78 are selected based on the inclusion and exclusion criteria relevant to this study. This study provides an overview of learners' personal traits and the techniques used to identify them to provide a basis for improving adaptive learning environments. The findings indicate that most of the previous works used a learning style from the cognition learning domain category to model individual personal traits, while the computer-based detection technique was commonly applied to identify a learner's personal traits in adaptive learning environments. This study reveals the common learner characteristics used to develop learner models and the techniques for implementing such models. The findings of this paper can guide other researchers to recognise various personal traits and the identification technique for further studies, as well as assist developers in the development of the adaptive learning system.}
}

@article{rayyan-727967492,
  title={Software product line evolution: A systematic literature review},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={105},
  pages={190-208},
  author={Marques, Maíra and Simmonds, Jocelyn and Rossel, Pedro O and Bastarrica, María Cecilia},
  url={https://www.sciencedirect.com/science/article/pii/S0950584918301848},
  keywords={Systematic literature review, Software product line, Software reuse, Evolution, Software},
  abstract={Context: Software Product Lines (SPL) evolve when there are changes in the requirements, product structure or the technology being used. Different approaches have been proposed for managing SPL assets and some also address how evolution affects these assets. Existing mapping studies have focused on specific aspects of SPL evolution, but there is no cohesive body of work that gives an overview of the area as a whole. Objective: The goals of this work are to review the characteristics of the approaches reported as supporting SPL evolution, and to synthesize the evidence provided by primary studies about the nature of their processes, as well as how they are reported and validated. Method: We conducted a systematic literature review, considering six research questions formulated to evaluate evolution approaches for SPL. We considered journal, conference and workshop papers published up until March 2017 in leading digital libraries for computer science. Results: After a thorough analysis of the papers retrieved from the digital libraries, we ended up with a set of 60 primary studies. Feature models are widely used to represent SPLs, so feature evolution is frequently addressed. Other assets are less frequently addressed. The area has matured over time: papers presenting more rigorous work are becoming more common. The processes used to support SPL evolution are systematic, but with a low level of automation. Conclusions: Our research shows that there is no consensus about SPL formalization, what assets can evolve, nor how and when these evolve. Case studies are quite popular, but few industrial-sized case studies are publicly available. Also, few of the proposed techniques offer tool support. We believe that the SPL community needs to work together to improve the state of the art, creating methods and tools that support SPL evolution in a more comparable manner.}
}

@article{rayyan-727967493,
  title={A systematic literature review of software quality cost research},
  year={2011},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={84},
  number={3},
  pages={415-427},
  author={Karg, Lars M and Grottke, Michael and Beckhaus, Arne},
  url={https://www.sciencedirect.com/science/article/pii/S0164121210003146},
  keywords={Systematic literature review, Software development, Prevention appraisal failure cost scheme, Quality costs, Software},
  abstract={Abstract Software quality costs have not received as much attention from the research community as other economic aspects of software development. Over the last three decades, a number of articles on this topic have appeared in a range of journals, but comprehensive overviews of this body of research are not available. For the detailed review of software quality cost research presented in this article, we collect 87 articles published between 1980 and 2009 in 60 leading computing journals. We study the distribution of these articles across research disciplines and journals as well as over time. Moreover, we identify the predominant researchers in the software quality cost domain and the related research clusters. We also classify the articles according to three properties, namely, research topic, research scope, and research approach. This categorization enables us to identify aspects emphasized by previous research on software quality costs and to point out promising future research directions. Our review shows that prevention costs have gained the least attention, in spite of their big cost impact. It also reveals that only one article has targeted multiple companies. Further, we observe that many articles do not empirically validate their findings. This is especially true for those articles dealing with an entire firm.}
}

@article{rayyan-727967494,
  title={Approaches to strategic alignment of software process improvement: A systematic literature review},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={123},
  pages={45-63},
  author={Vasconcellos, Francisco J S and Landre, Geraldo B and Cunha, José Adson O G and Oliveira, Juliano L and Ferreira, Ronaldo A and Vincenzi, Auri M R},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216301893},
  keywords={Systematic literature review, Software process improvement, Business alignment, Strategic alignment, Software},
  abstract={Context: Software process improvement (SPI) aims to increase the effectiveness of a software organization. Many studies indicate that the strategic alignment is a critical factor for the SPI success. However, little is known about practical approaches to achieving and maintaining such alignment. Objective: The goal of this study is to evaluate the validation evidence of the existing approaches to the strategic alignment of SPI. Method: We develop a search protocol that combines database search and snowballing to perform the systematic literature review and evaluate empirical studies by applying rigor and relevance criteria. To evaluate the efficiency of our protocol, we use a “quasi-gold standard” to compute the sensitivity and precision of the search. Result: We identified 30 studies (18 empirical) and 19 approaches to strategic alignment of SPI from 495 retrieved studies. Only three out of the 18 empirical studies were rated as high in the categories rigor and relevance, suggesting the need for a stronger validation of the approaches. Conclusion: We conclude that the lack of empirical validation indicates that the results of the existing approaches have not been adequately transferred to practitioners yet, calling for more rigorous studies on the subject.}
}

@article{rayyan-727967495,
  title={Reliability of search in systematic reviews: Towards a quality assessment framework for the automated-search strategy},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={99},
  pages={133-147},
  author={Ali, Nauman Bin and Usman, Muhammad},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917304263},
  keywords={Secondary studies, Systematic literature reviews, Guidelines, Reliability, Credibility, Search strategies},
  abstract={Context The trust in systematic literature reviews (SLRs) to provide credible recommendations is critical for establishing evidence-based software engineering (EBSE) practice. The reliability of SLR as a method is not a given and largely depends on the rigor of the attempt to identify, appraise and aggregate evidence. Previous research, by comparing SLRs on the same topic, has identified search as one of the reasons for discrepancies in the included primary studies. This affects the reliability of an SLR, as the papers identified and included in it are likely to influence its conclusions. Objective We aim to propose a comprehensive evaluation checklist to assess the reliability of an automated-search strategy used in an SLR. Method Using a literature review, we identified guidelines for designing and reporting automated-search as a primary search strategy. Using the aggregated design, reporting and evaluation guidelines, we formulated a comprehensive evaluation checklist. The value of this checklist was demonstrated by assessing the reliability of search in 27 recent SLRs. Results Using the proposed evaluation checklist, several additional issues (not captured by the current evaluation checklist) related to the reliability of search in recent SLRs were identified. These issues severely limit the coverage of literature by the search and also the possibility to replicate it. Conclusion Instead of solely relying on expensive replications to assess the reliability of SLRs, this work provides means to objectively assess the likely reliability of a search-strategy used in an SLR. It highlights the often-assumed aspect of repeatability of search when using automated-search. Furthermore, by explicitly considering repeatability and consistency as sub-characteristics of a reliable search, it provides a more comprehensive evaluation checklist than the ones currently used in EBSE.}
}

@article{rayyan-727967496,
  title={Evolution of statistical analysis in empirical software engineering research: Current state and steps forward},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={156},
  pages={246-267},
  author={de Oliveira Neto, Francisco Gomes and Torkar, Richard and Feldt, Robert and Gren, Lucas and Furia, Carlo A and Huang, Ziwei},
  url={https://www.sciencedirect.com/science/article/pii/S0164121219301451},
  keywords={Empirical software engineering, Practical significance, Semi-automated literature review, Statistical methods, Software},
  abstract={Software engineering research is evolving and papers are increasingly based on empirical data from a multitude of sources, using statistical tests to determine if and to what degree empirical evidence supports their hypotheses. To investigate the practices and trends of statistical analysis in empirical software engineering (ESE), this paper presents a review of a large pool of papers from top-ranked software engineering journals. First, we manually reviewed 161 papers and in the second phase of our method, we conducted a more extensive semi-automatic classification of papers spanning the years 2001–2015 and 5196 papers. Results from both review steps was used to: i) identify and analyse the predominant practices in ESE (e.g., using t-test or ANOVA), as well as relevant trends in usage of specific statistical methods (e.g., nonparametric tests and effect size measures) and, ii) develop a conceptual model for a statistical analysis workflow with suggestions on how to apply different statistical methods as well as guidelines to avoid pitfalls. Lastly, we confirm existing claims that current ESE practices lack a standard to report practical significance of results. We illustrate how practical significance can be discussed in terms of both the statistical analysis and in the practitioner's context.}
}

@article{rayyan-727967497,
  title={Systematic literature review of mobile application development and testing effort estimation},
  year={2018},
  journal={Journal of King Saud University - Computer and Information Sciences},
  issn={1319-1578},
  author={Kaur, Anureet and Kaur, Kulwant},
  url={https://www.sciencedirect.com/science/article/pii/S1319157818306074},
  keywords={Systematic literature review, Agile, Estimation, Test effort, Mobile Applications},
  abstract={In the recent years, the advances in mobile technology have brought an exorbitant change in daily lifestyle of individuals. Smartphones/mobile devices are rampant in all aspects of human life. This has led to an extreme demand for developing software that runs on mobile devices. The developers have to keep up with this high demand and deliver high-quality app on time and within budget. For this, estimation of development and testing of apps play a pivotal role. In this paper, a Systematic Literature Review (SLR) is conducted to highlight development and testing estimation process for software/application. The goal of the present literature survey is to identify and compare existing test estimation techniques for traditional software (desktop/laptop) and for mobile software/application. The characteristics that make mobile software/application different from traditional software are identified in this literature survey. Further, the trend for developing the software is towards agile, thus this study also presents and compares estimation techniques used in agile software development for mobile applications. The analysis of literature review suggests filling a research gap to present formal models for estimating mobile application considering specific characteristics of mobile software.}
}

@article{rayyan-727967498,
  title={A business model for commercial open source software: A systematic literature review},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={103},
  pages={202-214},
  author={Shahrivar, Shahrokh and Elahi, Shaban and Hassanzadeh, Alireza and Montazer, Gholamali},
  url={https://www.sciencedirect.com/science/article/pii/S0950584918301277},
  keywords={Systematic literature review, Business model, Commercial open source software (COSS), Software},
  abstract={Context Commercial open source software (COSS) and community open source software (OSS) are two types of open source software. The former is the newer concept with the grounds for research such as business model. However, in the literature of open source software, the revenue model has been studied as a business model, which is one component of the business model. Therefore, there is a need for a more complete review of the COSS business model with all components. Objective The purpose of this research is to describe and present the COSS business model with all its components. Method A systematic literature review of the COSS business model was conducted and 1157 studies were retrieved through search in six academic databases. The result of the process of selecting the primary studies was 21 studies. By backward snowballing, we discovered 10 other studies, and thus a total of 31 studies were found. Then, the grounded theory coding procedures were used to determine the characteristics and components of the COSS business model. Results The COSS business model was presented with value proposition, value creation & delivery, and value capture. This business model includes eight components: COSS products and complementarities, COSS clients and users, COSS competitive strategies, organizational aspects of COSS, position of COSS producers in the value network, resources and capabilities of COSS business, COSS revenue sources, and COSS cost-benefit. Conclusion This study provides a complete illustration of the COSS business model. Identifies COSS generic competitive strategies. By cost-benefit component, we have considered both tangible and intangible components. This business model is especially effective in developing countries. In future research, it is necessary to review the management of the COSS community, the organization, the new revenue models for disruptive ability of open source software, and the localization of open source software.}
}

@article{rayyan-727967499,
  title={A systematic review of statistical power in software engineering experiments},
  year={2006},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={48},
  number={8},
  pages={745-755},
  author={Dybå, Tore and Kampenes, Vigdis By and Sjøberg, Dag I K},
  url={https://www.sciencedirect.com/science/article/pii/S0950584905001333},
  keywords={Empirical software engineering, Systematic review, Controlled experiment, Effect size, Statistical power, Software},
  abstract={Statistical power is an inherent part of empirical studies that employ significance testing and is essential for the planning of studies, for the interpretation of study results, and for the validity of study conclusions. This paper reports a quantitative assessment of the statistical power of empirical software engineering research based on the 103 papers on controlled experiments (of a total of 5,453 papers) published in nine major software engineering journals and three conference proceedings in the decade 1993–2002. The results show that the statistical power of software engineering experiments falls substantially below accepted norms as well as the levels found in the related discipline of information systems research. Given this study's findings, additional attention must be directed to the adequacy of sample sizes and research designs to ensure acceptable levels of statistical power. Furthermore, the current reporting of significance tests should be enhanced by also reporting effect sizes and confidence intervals.}
}

@article{rayyan-727967500,
  title={Identifying relevant studies in software engineering},
  year={2011},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={53},
  number={6},
  pages={625-637},
  author={Zhang, He and Babar, Muhammad Ali and Tell, Paolo},
  url={https://www.sciencedirect.com/science/article/pii/S0950584910002260},
  keywords={Evidence-based software engineering, Systematic literature review, Quasi-gold standard, Search strategy, Software},
  abstract={Context Systematic literature review (SLR) has become an important research methodology in software engineering since the introduction of evidence-based software engineering (EBSE) in 2004. One critical step in applying this methodology is to design and execute appropriate and effective search strategy. This is a time-consuming and error-prone step, which needs to be carefully planned and implemented. There is an apparent need for a systematic approach to designing, executing, and evaluating a suitable search strategy for optimally retrieving the target literature from digital libraries. Objective The main objective of the research reported in this paper is to improve the search step of undertaking SLRs in software engineering (SE) by devising and evaluating systematic and practical approaches to identifying relevant studies in SE. Method We have systematically selected and analytically studied a large number of papers (SLRs) to understand the state-of-the-practice of search strategies in EBSE. Having identified the limitations of the current ad-hoc nature of search strategies used by SE researchers for SLRs, we have devised a systematic and evidence-based approach to developing and executing optimal search strategies in SLRs. The proposed approach incorporates the concept of ‘quasi-gold standard' (QGS), which consists of collection of known studies, and corresponding ‘quasi-sensitivity' into the search process for evaluating search performance. Results We conducted two participant–observer case studies to demonstrate and evaluate the adoption of the proposed QGS-based systematic search approach in support of SLRs in SE research. Conclusion We report their findings based on the case studies that the approach is able to improve the rigor of search process in an SLR, as well as it can serve as a supplement to the guidelines for SLRs in EBSE. We plan to further evaluate the proposed approach using a series of case studies on varying research topics in SE.}
}

@article{rayyan-727967501,
  title={A systematic literature review on automated log abstraction techniques},
  year={2020},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={122},
  pages={106276},
  author={El-Masri, Diana and Petrillo, Fabio and Guéhéneuc, Yann-Gaël and Hamou-Lhadj, Abdelwahab and Bouziane, Anas},
  url={https://www.sciencedirect.com/science/article/pii/S0950584920300264},
  keywords={Systematic literature review, Data mining, Systematic survey, AIOps, Log Abstraction Techniques, Log Analysis, Log Management, Log Mining, Log Parsing, Quality Model, Software Analysis, Software Log},
  abstract={Context: Logs are often the first and only information available to software engineers to understand and debug their systems. Automated log-analysis techniques help software engineers gain insights into large log data. These techniques have several steps, among which log abstraction is the most important because it transforms raw log-data into high-level information. Thus, log abstraction allows software engineers to perform further analyses. Existing log-abstraction techniques vary significantly in their designs and performances. To the best of our knowledge, there is no study that examines the performances of these techniques with respect to the following seven quality aspects concurrently: mode, coverage, delimiter independence, efficiency,scalability, system knowledge independence, and parameter tuning effort. Objectives: We want (1) to build a quality model for evaluating automated log-abstraction techniques and (2) to evaluate and recommend existing automated log-abstraction techniques using this quality model. Method: We perform a systematic literature review (SLR) of automated log-abstraction techniques. We review 89 research papers out of 2,864 initial papers. Results: Through this SLR, we (1) identify 17 automated log-abstraction techniques, (2) build a quality model composed of seven desirable aspects: mode, coverage, delimiter independence, efficiency, scalability, system knowledge independence, and parameter tuning effort, and (3) make recommendations for researchers on future research directions. Conclusion: Our quality model and recommendations help researchers learn about the state-of-the-art automated log-abstraction techniques, identify research gaps to enhance existing techniques, and develop new ones. We also support software engineers in understanding the advantages and limitations of existing techniques and in choosing the suitable technique to their unique use cases.}
}

@article{rayyan-727967502,
  title={Software fault prediction metrics: A systematic literature review},
  year={2013},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={55},
  number={8},
  pages={1397-1418},
  author={Radjenović, Danijel and Heričko, Marjan and Torkar, Richard and Živkovič, Aleš},
  url={https://www.sciencedirect.com/science/article/pii/S0950584913000426},
  keywords={Systematic literature review, Software metric, Software fault prediction, Software, Metronidazole},
  abstract={Context Software metrics may be used in fault prediction models to improve software quality by predicting fault location. Objective This paper aims to identify software metrics and to assess their applicability in software fault prediction. We investigated the influence of context on metrics' selection and performance. Method This systematic literature review includes 106 papers published between 1991 and 2011. The selected papers are classified according to metrics and context properties. Results Object-oriented metrics (49%) were used nearly twice as often compared to traditional source code metrics (27%) or process metrics (24%). Chidamber and Kemerer's (CK) object-oriented metrics were most frequently used. According to the selected studies there are significant differences between the metrics used in fault prediction performance. Object-oriented and process metrics have been reported to be more successful in finding faults compared to traditional size and complexity metrics. Process metrics seem to be better at predicting post-release faults compared to any static code metrics. Conclusion More studies should be performed on large industrial software systems to find metrics more relevant for the industry and to answer the question as to which metrics should be used in a given context.}
}

@article{rayyan-727967503,
  title={Systematic literature review of ensemble effort estimation},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={118},
  pages={151-175},
  author={Idri, Ali and Hosni, Mohamed and Abran, Alain},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216300450},
  keywords={Systematic literature review, Software development effort estimation, Ensemble effort estimation},
  abstract={The need to overcome the weaknesses of single estimation techniques for prediction tasks has given rise to ensemble methods in software development effort estimation (SDEE). An ensemble effort estimation (EEE) technique combines several of the single/classical models found in the SDEE literature. However, to the best of our knowledge, no systematic review has yet been performed with a focus on the use of EEE techniques in SDEE. The purpose of this review is to analyze EEE techniques from six viewpoints: single models used to construct ensembles, ensemble estimation accuracy, rules used to combine single estimates, accuracy comparison of EEE techniques with single models, accuracy comparison between EEE techniques and methodologies used to construct ensemble methods. We performed a systematic review of EEE studies published between 2000 and 2016, and we selected 24 of them to address the questions raised in this review. We found that EEE techniques may be separated into two types: homogeneous and heterogeneous, and that the machine learning single models are the most frequently employed in constructing EEE techniques. We also found that EEE techniques usually yield acceptable estimation accuracy, and in fact are more accurate than single models.}
}

@article{rayyan-727967504,
  title={A systematic literature review of iStar extensions},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={137},
  pages={1-33},
  author={Gonçalves, Enyo and Castro, Jaelson and Araújo, João and Heineck, Tiago},
  url={https://www.sciencedirect.com/science/article/pii/S0164121217302741},
  keywords={Systematic Literature Review, Goal modelling, iStar, Modelling language extensions},
  abstract={iStar is a general-purpose goal-based modelling language used to model requirements at early and late phases of software development. It has been used in industrial and academic projects. Often the language is extended to incorporate new constructs related to an application area. The language is currently undergoing standardisation, so several studies have focused on the analysis of iStar variations to identify the similarities and defining a core iStar. However, we believe it will continue to be extended and it is important to understand how iStar is extended. This paper contributes to this purpose through the identification and analysis of the existing extensions and its constructs. A Systematic Literature Review was conducted to guide identification and analysis. The results point to 96 papers and 307 constructs proposed. The extensions and constructs were analysed according to well-defined questions in three dimensions: a general analysis; model-based analysis (to characterise the extensions from semantic and syntactic definitions); and a third dimension related to semiotic clarity. The application area targeted by the iStar extensions and their evolutions are presented as results of our analysis. The results point to the need for more complete, consistent and careful development of iStar extensions. The paper concludes with some discussions and future directions for this research field.}
}

@article{rayyan-727967505,
  title={Threat modeling – A systematic literature review},
  year={2019},
  journal={Computers & Security},
  issn={0167-4048},
  volume={84},
  pages={53-69},
  author={Xiong, Wenjun and Lagerström, Robert},
  url={https://www.sciencedirect.com/science/article/pii/S0167404818307478},
  keywords={Literature review, Risk management, Cyber security, Cyber attacks, Threat modeling},
  abstract={Cyber security is attracting worldwide attention. With attacks being more and more common and often successful, no one is spared today. Threat modeling is proposed as a solution for secure application development and system security evaluations. Its aim is to be more proactive and make it more difficult for attackers to accomplish their malicious intents. However, threat modeling is a domain that lacks common ground. What is threat modeling, and what is the state-of-the-art work in this field? To answer these questions, this article presents a review of threat modeling based on systematic queries in four leading scientific databases. This is the first systematic literature review on threat modeling to the best of our knowledge. 176 articles were assessed, and 54 of them were selected for further analysis. We identified three separate clusters: (1) articles making a contribution to threat modeling, e.g., introducing a new method, (2) articles using an existing threat modeling approach, and (3) introductory articles presenting work related to the threat modeling process. The three clusters were analyzed in terms of a set of criteria, for instance: Is the threat modeling approach graphical or formal? Is it focused on a specific attack type and application? Is the contribution validated empirically or theoretically? We observe from the results that, most threat modeling work remains to be done manually, and there is limited assurance of their validations. The results can be used for researchers and practitioners who want to know the state-of-the-art threat modeling methods, and future research directions are discussed.}
}

@article{rayyan-727967506,
  title={Using metrics in Agile and Lean Software Development – A systematic literature review of industrial studies},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={62},
  pages={143-163},
  author={Kupiainen, Eetu and Mäntylä, Mika V and Itkonen, Juha},
  url={https://www.sciencedirect.com/science/article/pii/S095058491500035X},
  keywords={Systematic literature review, Software engineering, Measurement, Agile, Lean, Metrics, Metronidazole, Software},
  abstract={Context Software industry has widely adopted Agile software development methods. Agile literature proposes a few key metrics but little is known of the actual metrics use in Agile teams. Objective The objective of this paper is to increase knowledge of the reasons for and effects of using metrics in industrial Agile development. We focus on the metrics that Agile teams use, rather than the ones used from outside by software engineering researchers. In addition, we analyse the influence of the used metrics. Method This paper presents a systematic literature review (SLR) on using metrics in industrial Agile software development. We identified 774 papers, which we reduced to 30 primary studies through our paper selection process. Results The results indicate that the reasons for and the effects of using metrics are focused on the following areas: sprint planning, progress tracking, software quality measurement, fixing software process problems, and motivating people. Additionally, we show that although Agile teams use many metrics suggested in the Agile literature, they also use many custom metrics. Finally, the most influential metrics in the primary studies are Velocity and Effort estimate. Conclusion The use of metrics in Agile software development is similar to Traditional software development. Projects and sprints need to be planned and tracked. Quality needs to be measured. Problems in the process need to be identified and fixed. Future work should focus on metrics that had high importance but low prevalence in our study, as they can offer the largest impact to the software industry.}
}

@article{rayyan-727967507,
  title={The role and value of replication in empirical software engineering results},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={99},
  pages={120-132},
  author={Shepperd, Martin and Ajienka, Nemitari and Counsell, Steve},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917304305},
  keywords={Software engineering, Meta-analysis, Reliability, Replication, Experiment, Software},
  abstract={Context Concerns have been raised from many quarters regarding the reliability of empirical research findings and this includes software engineering. Replication has been proposed as an important means of increasing confidence. Objective We aim to better understand the value of replication studies, the level of confirmation between replication and original studies, what confirmation means in a statistical sense and what factors modify this relationship. Method We perform a systematic review to identify relevant replication experimental studies in the areas of (i) software project effort prediction and (ii) pair programming. Where sufficient details are provided we compute prediction intervals. Results Our review locates 28 unique articles that describe replications of 35 original studies that address 75 research questions. Of these 10 are external, 15 internal and 3 internal-same-article replications. The odds ratio of internal to external (conducted by independent researchers) replications of obtaining a ‘confirmatory' result is 8.64. We also found incomplete reporting hampered our ability to extract estimates of effect sizes. Where we are able to compute replication prediction intervals these were surprisingly large. Conclusion We show that there is substantial evidence to suggest that current approaches to empirical replications are highly problematic. There is a consensus that replications are important, but there is a need for better reporting of both original and replicated studies. Given the low power and incomplete reporting of many original studies, it can be unclear the extent to which a replication is confirmatory and to what extent it yields additional knowledge to the software engineering community. We recommend attention is switched from replication research to meta-analysis.}
}

@article{rayyan-727967508,
  title={Twitter as a tool for the management and analysis of emergency situations: A systematic literature review},
  year={2018},
  journal={International Journal of Information Management},
  issn={0268-4012},
  volume={43},
  pages={196-208},
  author={Martínez-Rojas, María and del Carmen Pardo-Ferreira, María and Rubio-Romero, Juan Carlos},
  url={https://www.sciencedirect.com/science/article/pii/S0268401218303499},
  keywords={Review, Management, Social network, Data, Emergencies, Twitter},
  abstract={The importance of timely, accurate and effective use of available information is essential to the proper management of emergency situations. In recent years, emerging technologies have provided new approaches towards the distribution and acquisition of crowdsourced information to facilitate situational awareness and management during emergencies. In this regard, internet and social networks have shown potential to be an effective tool in disseminating and obtaining up-to-date information. Among the most popular social networks, research has pointed to Twitter as a source of information that offers valuable real-time data for decision-making. The objective of this paper is to conduct a systematic literature review that provides an overview of the current state of research concerning the use of Twitter to emergencies management, as well as presents the challenges and future research directions.}
}

@article{rayyan-727967509,
  title={Re-identification attacks—A systematic literature review},
  year={2016},
  journal={International Journal of Information Management},
  issn={0268-4012},
  volume={36},
  number={6},
  pages={1184-1192},
  author={Henriksen-Bulmer, Jane and Jeary, Sheridan},
  url={https://www.sciencedirect.com/science/article/pii/S0268401215301262},
  keywords={Systematic literature review, Anonymisation, Anonymization, Re-identification},
  abstract={The publication of increasing amounts of anonymised open source data has resulted in a worryingly rising number of successful re-identification attacks. This has a number of privacy and security implications both on an individual and corporate level. This paper uses a systematic literature review to investigate the depth and extent of this problem as reported in peer reviewed literature. Using a detailed protocol,seven research portals were explored, 10,873 database entries were searched, from which a subset of 220 papers were selected for further review. From this total, 55 papers were selected as being within scope and to be included in the final review. The main review findings are that 72.7% of all successful re-identification attacks have taken place since 2009. Most attacks use multiple datasets. The majority of them have taken place on global datasets such as social networking data, and have been conducted by US based researchers. Furthermore, the number of datasets can be used as an attribute. Because privacy breaches have security, policy and legal implications (e.g. data protection, Safe Harbor etc.), the work highlights the need for new and improved anonymisation techniques or indeed, a fresh approach to open source publishing.}
}

@article{rayyan-727967510,
  title={Understanding Service-Oriented Architecture (SOA): A systematic literature review and directions for further investigation},
  year={2020},
  journal={Information Systems},
  issn={0306-4379},
  volume={91},
  pages={101491},
  author={Niknejad, Naghmeh and Ismail, Waidah and Ghani, Imran and Nazari, Behzad and Bahari, Mahadi and Hussin, Ab Razak Bin Che},
  url={https://www.sciencedirect.com/science/article/pii/S0306437920300028},
  keywords={Systematic literature review, Service-Oriented Architecture, SOA, Information systems, Success factors},
  abstract={Service-Oriented Architecture (SOA) has emerged as an architectural approach that enhances the service delivery performance of existing traditional systems while still retaining their most important features. This approach, due to its flexibility of adoption, has gained the attention of both academic and business entities, especially in the development of world-leading technologies such as Cloud Computing (CC) and the Internet of Things (IoT). Although many studies have listed the success factors of SOA, a few minor failures have also been reported in the literature. Despite the availability of rich material on SOA, there is a lack of systematic reviews covering the different aspects of the SOA concept in Information Systems (IS) research. Therefore, the central objective of this study is to review existing issues of SOA and share the findings with the academia. Hence, a systematic literature review (SLR) was conducted to analyse existing studies related to SOA and the factors that led to SOA success and failure from 2009 to 2019. To completely cover all SOA-related research in the IS field, a two-stage review protocol that included automatic and manual searching was applied, resulting in 103 primary studies. The articles were categorised into four research themes, namely: SOA Adoption, SOA Concepts, SOA Impact, and SOA Practice. The result shows that the academic research interest on SOA increased recently with most of the articles covering SOA Practice followed by SOA Adoption. Moreover, the findings of this review highlighted SOA Governance, SOA Strategy, Financial Issues and Costs, and Education and Training as the most significant factors of SOA adoption and implementation. Consequently, the outcomes will assist professionals and experts in organisations as well as academic researchers to focus more on these factors for successfully adopting and implementing SOA.}
}

@article{rayyan-727967511,
  title={Forty years of research on personality in software engineering: A mapping study},
  year={2015},
  journal={Computers in Human Behavior},
  issn={0747-5632},
  volume={46},
  pages={94-113},
  author={Cruz, Shirley and da Silva, Fabio Q B and Capretz, Luiz Fernando},
  url={https://www.sciencedirect.com/science/article/pii/S0747563214007237},
  keywords={Systematic literature review, Empirical software engineering, Mapping study, Human factors in software engineering, Software psychology, Software},
  abstract={In this article, we present a systematic mapping study of research on personality in software engineering. The goal is to plot the landscape of current published empirical and theoretical studies that deal with the role of personality in software engineering. We applied the systematic review method to search and select published articles, and to extract and synthesize data from the selected articles that reported studies about personality. Our search retrieved more than 19,000 articles, from which we selected 90 articles published between 1970 and 2010. Nearly 72% of the studies were published after 2002 and 83% of the studies reported empirical research findings. Data extracted from the 90 studies showed that education and pair programming were the most recurring research topics, and that MBTI was the most used test. Research related to pair programming, education, team effectiveness, software process allocation, software engineer personality characteristics, and individual performance concentrated over 88% of the studies, while team process, behavior and preferences, and leadership performance were the topics with the smallest number of studies. We conclude that the number of articles has grown in the last few years, but contradictory evidence was found that might have been caused by differences in context, research method, and versions of the tests used in the studies. While this raises a warning for practitioners that wish to use personality tests in practice, it shows several opportunities for the research community to improve and extend findings in this field.}
}

@article{rayyan-727967512,
  title={The effect of software engineers' personality traits on team climate and performance: A Systematic Literature Review},
  year={2016},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={73},
  pages={52-65},
  author={Soomro, Arjumand Bano and Salleh, Norsaremah and Mendes, Emilia and Grundy, John and Burch, Giles and Nordin, Azlin},
  url={https://www.sciencedirect.com/science/article/pii/S0950584916000082},
  keywords={Systematic literature review, Personality and software engineering, Software team climate, Team performance, Software},
  abstract={Context Over the past 50years numerous studies have investigated the possible effect that software engineers' personalities may have upon their individual tasks and teamwork. These have led to an improved understanding of that relationship; however, the analysis of personality traits and their impact on the software development process is still an area under investigation and debate. Further, other than personality traits, “team climate” is also another factor that has also been investigated given its relationship with software teams' performance. Objective The aim of this paper is to investigate how software professionals' personality is associated with team climate and team performance. Method In this paper we detail a Systematic Literature Review (SLR) of the effect of software engineers' personality traits and team climate on software team performance. Results Our main findings include 35 primary studies that have addressed the relationship between personality and team performance without considering team climate. The findings showed that team climate comprises a wide range of factors that fall within the fields of management and behavioral sciences. Most of the studies used undergraduate students as subjects and as surrogates of software professionals. Conclusions The findings from this SLR would be beneficial for understanding the personality assessment of software development team members by revealing the traits of personality taxonomy, along with the measurement of the software development team working environment. These measurements would be useful in examining the success and failure possibilities of software projects in development processes. General terms Human factors, performance.}
}

@article{rayyan-727967513,
  title={Testing scientific software: A systematic literature review},
  year={2014},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={56},
  number={10},
  pages={1219-1232},
  author={Kanewala, Upulee and Bieman, James M},
  url={https://www.sciencedirect.com/science/article/pii/S0950584914001232},
  keywords={Software testing, Systematic literature review, Software quality, Scientific software, Software},
  abstract={Context Scientific software plays an important role in critical decision making, for example making weather predictions based on climate models, and computation of evidence for research publications. Recently, scientists have had to retract publications due to errors caused by software faults. Systematic testing can identify such faults in code. Objective This study aims to identify specific challenges, proposed solutions, and unsolved problems faced when testing scientific software. Method We conducted a systematic literature survey to identify and analyze relevant literature. We identified 62 studies that provided relevant information about testing scientific software. Results We found that challenges faced when testing scientific software fall into two main categories: (1) testing challenges that occur due to characteristics of scientific software such as oracle problems and (2) testing challenges that occur due to cultural differences between scientists and the software engineering community such as viewing the code and the model that it implements as inseparable entities. In addition, we identified methods to potentially overcome these challenges and their limitations. Finally we describe unsolved challenges and how software engineering researchers and practitioners can help to overcome them. Conclusions Scientific software presents special challenges for testing. Specifically, cultural differences between scientist developers and software engineers, along with the characteristics of the scientific software make testing more difficult. Existing techniques such as code clone detection can help to improve the testing process. Software engineers should consider special challenges posed by scientific software such as oracle problems when developing testing techniques.}
}

@article{rayyan-727967514,
  title={Two decades of research on business intelligence system adoption, utilization and success – A systematic literature review},
  year={2019},
  journal={Decision Support Systems},
  issn={0167-9236},
  volume={125},
  pages={113113},
  author={Ain, NoorUl and Vaia, Giovanni and DeLone, William H and Waheed, Mehwish},
  url={https://www.sciencedirect.com/science/article/pii/S0167923619301423},
  keywords={Systematic literature review, Adoption, Business intelligence system, Success, Utilization},
  abstract={In the recent era of technological advances and hyper-competition, business intelligence (BI) systems have attracted significant attention from executives and decision makers due to their ability to provide complex and competitive information inputs for the decision process. Following the world of practice, research into the adoption, utilization and success of BI systems has grown substantially over the past two decades. The literature suggests that organizations have largely failed to capture the benefits of BI systems to their full extent and are seeking ways to leverage value from the implemented systems. However, prior studies do not have any comprehensive study that discusses the issues and challenges related to adoption, utilization and success of BI systems. In this study, using a systematic literature review, we present comprehensive knowledge about what has been found in the domain of BI system adoption, utilization and success. A total of 111 peer-reviewed studies, covering three categories – adoption, utilization and success – published between 2000 and 2019, were selected. The findings present the research methods, underpinning theories and key factors employed to study BI system adoption, utilization and success. In addition, the review identified the key issues related to BI adoption, utilization and success and highlighted the areas that have attracted more or less attention. This study also suggests future directions for researchers and practitioners in terms of unexplored themes that may help organizations to obtain value from BI systems.}
}

@article{rayyan-727967515,
  title={A systematic literature review on the semi-automatic configuration of extended product lines},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={144},
  pages={511-532},
  author={Ochoa, Lina and González-Rojas, Oscar and Juliana, Alves Pereira and Castro, Harold and Saake, Gunter},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218301511},
  keywords={Systematic literature review, Extended product line, Product configuration},
  abstract={Product line engineering has become essential in mass customisation given its ability to reduce production costs and time to market, and to improve product quality and customer satisfaction. In product line literature, mass customisation is known as product configuration. Currently, there are multiple heterogeneous contributions in the product line configuration domain. However, a secondary study that shows an overview of the progress, trends, and gaps faced by researchers in this domain is still missing. In this context, we provide a comprehensive systematic literature review to discover which approaches exist to support the configuration process of extended product lines and how these approaches perform in practice. Extend product lines consider non-functional properties in the product line modelling. We compare and classify a total of 66 primary studies from 2000 to 2016. Mainly, we give an in-depth view of techniques used by each work, how these techniques are evaluated and their main shortcomings. As main results, our review identified (i) the need to improve the quality of the evaluation of existing approaches, (ii) a lack of hybrid solutions to support multiple configuration constraints, and (iii) a need to improve scalability and performance conditions.}
}

@article{rayyan-727967516,
  title={Empirical studies omit reporting necessary details: A systematic literature review of reporting quality in model based testing},
  year={2018},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={55},
  pages={156-170},
  author={Khan, Muhammad Uzair and Iftikhar, Sidra and Iqbal, Muhammad Zohaib and Sherin, Salman},
  url={https://www.sciencedirect.com/science/article/pii/S0920548916302112},
  keywords={Empirical study, Model based testing, Reporting guidelines, Reporting quality},
  abstract={Context Empirical studies are essential in evaluating the effectiveness of Model-based Testing (MBT) research and should be reported properly to ensure their replication and to highlight the strengths and limitations of the MBT techniques being evaluated. Researchers have proposed guidelines detailing what information should be reported when presenting empirical studies and what should be the structure of such primary studies. There is a need to evaluate the reporting quality of the empirical studies in MBT literature. Objective To evaluate the reporting quality of empirical studies in the model based testing domain; identifying where the reported studies fail to follow the proposed guidelines and finding frequently omitted details. As an auxiliary goal we aim to quantify the percentage of empirical studies conducted in industrial context. Method We evaluate the reporting quality and the execution contexts of MBT empirical studies reported in literature. For our study we consider the MBT papers published in top ten software engineering journals over the last eighteen years. We evaluate the published primary studies using the empirical study reporting guidelines. Results We found 87 empirical in MBT that met our selection criteria. Initial results showed that the existing guidelines were not only too strict (for example they demand presence of specific sections rather than simply having the details present in the paper), they also did not adequately cover MBT specific details. Therefore, we propose modified the guidelines for reporting empirical studies in MBT and re-evaluated the selected studies. Results show that while only a few empirical studies follow the exact structure proposed by the guidelines, approximately half the papers contain at least 50% of the required details. Most of the papers omit details related to process and analysis leading to presented results. We found a positive trend of improving reporting quality of empirical studies in MBT over the last Eighteen years. Another important finding from the review is that few reported studies were conducted in real industrial context. Conclusions Model based testing community needs to be more aware of the reporting guidelines and more effort should be spent on reporting the necessary details. Furthermore, we found that only few studies that are conducted in industrial context and hence more focus should be given to empirical case studies in real industry context. However, the reporting quality of research papers presenting empirical evaluations is gradually improving.}
}

@article{rayyan-727967517,
  title={Supporting the semi-automatic semantic annotation of web services: A systematic literature review},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={61},
  pages={16-32},
  author={Tosi, Davide and Morasca, Sandro},
  url={https://www.sciencedirect.com/science/article/pii/S0950584915000154},
  keywords={Systematic literature review, Ontologies, Semantic web services, Functional and non-functional aspects, Semantics},
  abstract={Context Semantically annotating web services is gaining more attention as an important aspect to support the automatic matchmaking and composition of web services. Therefore, the support of well-known and agreed ontologies and tools for the semantical annotation of web services is becoming a key concern to help the diffusion of semantic web services. Objective The objective of this systematic literature review is to summarize the current state-of-the-art for supporting the semantical annotation of web services by providing answers to a set of research questions. Method The review follows a predefined procedure that involves automatically searching well-known digital libraries. As a result, a total of 35 primary studies were identified as relevant. A manual search led to the identification of 9 additional primary studies that were not reported during the automatic search of the digital libraries. Required information was extracted from these 44 studies against the selected research questions and finally reported. Results Our systematic literature review identified some approaches available for semantically annotating functional and non-functional aspects of web services. However, many of the approaches are either not validated or the validation done lacks credibility. Conclusion We believe that a substantial amount of work remains to be done to improve the current state of research in the area of supporting semantic web services.}
}

@article{rayyan-727967518,
  title={Models of motivation in software engineering},
  year={2009},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={51},
  number={1},
  pages={219-233},
  author={Sharp, Helen and Baddoo, Nathan and Beecham, Sarah and Hall, Tracy and Robinson, Hugh},
  url={https://www.sciencedirect.com/science/article/pii/S0950584908000827},
  keywords={Systematic literature review, Project management, Human factors, Motivation, Software},
  abstract={Motivation in software engineering is recognized as a key success factor for software projects, but although there are many papers written about motivation in software engineering, the field lacks a comprehensive overview of the area. In particular, several models of motivation have been proposed, but they either rely heavily on one particular model (the job characteristics model), or are quite disparate and difficult to combine. Using the results from our previous systematic literature review (SLR), we constructed a new model of motivation in software engineering. We then compared this new model with existing models and refined it based on this comparison. This paper summarises the SLR results, presents the important existing models found in the literature and explains the development of our new model of motivation in software engineering.}
}

@article{rayyan-727967519,
  title={Feature extraction approaches from natural language requirements for reuse in software product lines: A systematic literature review},
  year={2015},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={106},
  pages={132-149},
  author={Bakar, Noor Hasrina and Kasirun, Zarinah M and Salleh, Norsaremah},
  url={https://www.sciencedirect.com/science/article/pii/S0164121215001004},
  keywords={Software product lines, Systematic literature review, Requirements reuse, Feature extractions, Natural language requirements, Software},
  abstract={Requirements for implemented system can be extracted and reused for a production of a new similar system. Extraction of common and variable features from requirements leverages the benefits of the software product lines engineering (SPLE). Although various approaches have been proposed in feature extractions from natural language (NL) requirements, no related literature review has been published to date for this topic. This paper provides a systematic literature review (SLR) of the state-of-the-art approaches in feature extractions from NL requirements for reuse in SPLE. We have included 13 studies in our synthesis of evidence and the results showed that hybrid natural language processing approaches were found to be in common for overall feature extraction process. A mixture of automated and semi-automated feature clustering approaches from data mining and information retrieval were also used to group common features, with only some approaches coming with support tools. However, most of the support tools proposed in the selected studies were not made available publicly and thus making it hard for practitioners' adoption. As for the evaluation, this SLR reveals that not all studies employed software metrics as ways to validate experiments and case studies. Finally, the quality assessment conducted confirms that practitioners' guidelines were absent in the selected studies.}
}

@article{rayyan-727967520,
  title={Prognosis of dementia employing machine learning and microsimulation techniques: A systematic literature review},
  year={2016},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={100},
  pages={480-488},
  author={Dallora, Ana Luiza and Eivazzadeh, Shahryar and Mendes, Emilia and Berglund, Johan and Anderberg, Peter},
  url={https://www.sciencedirect.com/science/article/pii/S1877050916323535},
  keywords={machine learning, dementia, microsimulation, prognosis, Prognosis, Dementia},
  abstract={OBJECTIVE: The objective of this paper is to investigate the goals and variables employed in the machine learning and microsimulation studies for the prognosis of dementia. METHOD: According to preset protocols, the Pubmed, Socups and Web of Science databases were searched to find studies that matched the defined inclusion/exclusion criteria, and then its references were checked for new studies. A quality checklist assessed the selected studies, and removed the low quality ones. The remaining ones (included set) had their data extracted and summarized. RESULTS: The summary of the data of the 37 included studies showed that the most common goal of the selected studies was the prediction of the conversion from mild cognitive impairment to Alzheimer's Disease, for studies that used machine learning, and cost estimation for the microsimulation ones. About the variables, neuroimaging was the most frequent used. CONCLUSIONS: The systematic literature review showed clear trends in prognosis of dementia research in what concerns machine learning techniques and microsimulation.}
}

@article{rayyan-727967521,
  title={Current issue on knowledge management system for future research: a systematic literature review},
  year={2017},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={116},
  pages={68-80},
  author={Iskandar, Karto and Jambak, Muhammad Ikhwan and Kosala, Raymondus and Prabowo, Harjanto},
  url={https://www.sciencedirect.com/science/article/pii/S1877050917320483},
  keywords={Systematic Literature Review, Big Data Issue in KMS, Current Issues, KMS, Knowledge Management System},
  abstract={Nowadays, the number of papers on the topic of Knowledge Management and Knowledge Management System is still widely discussed. The study of Knowledge Management System (KMS) issues are based on Systematic Literature Review (SLR). It aims to analyze the state of the art, identify current popular issues on KMS, and offer directions for future research agenda. The methodology used in this paper is based on the systematic literature review to collect, synthesize and analyze numerous papers on a variety of topics that are closely related to knowledge management system issues that published in the last two decades. Based on fifty-four papers reviewed from six electronic databases, the result of this paper obtained fourteen current issues on knowledge management system. Moreover, the top three popular issues consist of the development of capabilities and features of KMS, Big Data issues on KMS, and adoption to new technology issue for KMS respectively. The conclusion of this study emphasized the big data phenomenon as the most contemporary topic for the future research area besides the growing of required KMS capabilities and features development.}
}

@article{rayyan-727967522,
  title={Web application testing: A systematic literature review},
  year={2014},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={91},
  pages={174-201},
  author={Doğan, Serdar and Betin-Can, Aysu and Garousi, Vahid},
  url={https://www.sciencedirect.com/science/article/pii/S0164121214000223},
  keywords={Systematic literature review, Testing, Web application},
  abstract={Context The web has had a significant impact on all aspects of our society. As our society relies more and more on the web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 193 papers in the area of web application testing, which have appeared between 2000 and 2013. Objective As this research area matures and the number of related papers increases, it is important to systematically identify, analyze, and classify the publications and provide an overview of the trends and empirical evidence in this specialized field. Methods We systematically review the body of knowledge related to functional testing of web application through a systematic literature review (SLR) study. This SLR is a follow-up and complimentary study to a recent systematic mapping (SM) study that we conducted in this area. As part of this study, we pose three sets of research questions, define selection and exclusion criteria, and synthesize the empirical evidence in this area. Results Our pool of studies includes a set of 95 papers (from the 193 retrieved papers) published in the area of web application testing between 2000 and 2013. The data extracted during our SLR study is available through a publicly-accessible online repository. Among our results are the followings: (1) the list of test tools in this area and their capabilities, (2) the types of test models and fault models proposed in this domain, (3) the way the empirical studies in this area have been designed and reported, and (4) the state of empirical evidence and industrial relevance. Conclusion We discuss the emerging trends in web application testing, and discuss the implications for researchers and practitioners in this area. The results of our SLR can help researchers to obtain an overview of existing web application testing approaches, fault models, tools, metrics and empirical evidence, and subsequently identify areas in the field that require more attention from the research community.}
}

@article{rayyan-727967523,
  title={A systematic literature review to identify and classify software requirement errors},
  year={2009},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={51},
  number={7},
  pages={1087-1109},
  author={Walia, Gursimran Singh and Carver, Jeffrey C},
  url={https://www.sciencedirect.com/science/article/pii/S0950584909000111},
  keywords={Systematic literature review, Software quality, Human errors, Software},
  abstract={Most software quality research has focused on identifying faults (i.e., information is incorrectly recorded in an artifact). Because software still exhibits incorrect behavior, a different approach is needed. This paper presents a systematic literature review to develop taxonomy of errors (i.e., the sources of faults) that may occur during the requirements phase of software lifecycle. This taxonomy is designed to aid developers during the requirement inspection process and to improve overall software quality. The review identified 149 papers from the software engineering, psychology and human cognition literature that provide information about the sources of requirements faults. A major result of this paper is a categorization of the sources of faults into a formal taxonomy that provides a starting point for future research into error-based approaches to improving software quality.}
}

@article{rayyan-727967524,
  title={Current state of research on cross-site scripting (XSS) – A systematic literature review},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={58},
  pages={170-186},
  author={Hydara, Isatou and Sultan, Abu Bakar Md. and Zulzalil, Hazura and Admodisastro, Novia},
  url={https://www.sciencedirect.com/science/article/pii/S0950584914001700},
  keywords={Systematic literature review, Security, Web applications, Cross-site scripting},
  abstract={Context Cross-site scripting (XSS) is a security vulnerability that affects web applications. It occurs due to improper or lack of sanitization of user inputs. The security vulnerability caused many problems for users and server applications. Objective To conduct a systematic literature review on the studies done on XSS vulnerabilities and attacks. Method We followed the standard guidelines for systematic literature review as documented by Barbara Kitchenham and reviewed a total of 115 studies related to cross-site scripting from various journals and conference proceedings. Results Research on XSS is still very active with publications across many conference proceedings and journals. Attack prevention and vulnerability detection are the areas focused on by most of the studies. Dynamic analysis techniques form the majority among the solutions proposed by the various studies. The type of XSS addressed the most is reflected XSS. Conclusion XSS still remains a big problem for web applications, despite the bulk of solutions provided so far. There is no single solution that can effectively mitigate XSS attacks. More research is needed in the area of vulnerability removal from the source code of the applications before deployment.}
}

@article{rayyan-727967525,
  title={Enactment of adaptation in data stream processing with latency implications—A systematic literature review},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={111},
  pages={1-21},
  author={Qin, Cui and Eichelberger, Holger and Schmid, Klaus},
  url={https://www.sciencedirect.com/science/article/pii/S0950584919300539},
  keywords={Systematic literature review, Big data, Enactment, Latency, Runtime adaptation, Stream processing},
  abstract={Context Stream processing is a popular paradigm to continuously process huge amounts of data. Runtime adaptation plays a significant role in supporting the optimization of data processing tasks. In recent years runtime adaptation has received significant interest in scientific literature. However, so far no categorization of the enactment approaches for runtime adaptation in stream processing has been established. Objective This paper identifies and characterizes different approaches towards the enactment of runtime adaptation in stream processing with a main focus on latency as quality dimension. Method We performed a systematic literature review (SLR) targeting five main research questions. An automated search, resulting in 244 papers, was conducted. 75 papers published between 2006 and 2018 were finally included. From the selected papers, we extracted data like processing problems, adaptation goals, enactment approaches of adaptation, enactment techniques, evaluation metrics as well as evaluation parameters used to trigger the enactment of adaptation in their evaluation. Results We identified 17 different enactment approaches and categorized them into a taxonomy. For each, we extracted the underlying technique used to implement this enactment approach. Further, we identified 9 categories of processing problems, 6 adaptation goals, 9 evaluation metrics and 12 evaluation parameters according to the extracted data properties. Conclusion We observed that the research interest on enactment approaches to the adaptation of stream processing has significantly increased in recent years. The most commonly applied enactment approaches are parameter adaptation to tune parameters or settings of the processing, load balancing used to re-distribute workloads, and processing scaling to dynamically scale up and down the processing. In addition to latency, most adaptations also address resource fluctuation / bottleneck problems. For presenting a dynamic environment to evaluate enactment approaches, researchers often change input rates or processing workloads.}
}

@article{rayyan-727967526,
  title={The impact of information security events to the stock market: A systematic literature review},
  year={2016},
  journal={Computers & Security},
  issn={0167-4048},
  volume={58},
  pages={216-229},
  author={Spanos, Georgios and Angelis, Lefteris},
  url={https://www.sciencedirect.com/science/article/pii/S0167404816300013},
  keywords={Literature review, Event study, Information security, Security breaches, Stock market},
  abstract={Information security is a highly critical aspect of information systems. Although the literature regarding security assurance is vast, the research on economic consequences of security incidents is quite limited. The purpose of this systematic review is to search, collect and classify event studies related to information security impact on stock prices. In total, 37 related papers conducting 45 studies were found by the systematic search of bibliographic sources. The majority (75.6%) of these studies report statistical significance of the impact of security events to the stock prices of firms.}
}

@article{rayyan-727967527,
  title={Machine learning techniques for code smell detection: A systematic literature review and meta-analysis},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={108},
  pages={115-138},
  author={Azeem, Muhammad Ilyas and Palomba, Fabio and Shi, Lin and Wang, Qing},
  url={https://www.sciencedirect.com/science/article/pii/S0950584918302623},
  keywords={Systematic literature review, Machine learning, Code smells, Smell},
  abstract={Background: Code smells indicate suboptimal design or implementation choices in the source code that often lead it to be more change- and fault-prone. Researchers defined dozens of code smell detectors, which exploit different sources of information to support developers when diagnosing design flaws. Despite their good accuracy, previous work pointed out three important limitations that might preclude the use of code smell detectors in practice: (i) subjectiveness of developers with respect to code smells detected by such tools, (ii) scarce agreement between different detectors, and (iii) difficulties in finding good thresholds to be used for detection. To overcome these limitations, the use of machine learning techniques represents an ever increasing research area. Objective: While the research community carefully studied the methodologies applied by researchers when defining heuristic-based code smell detectors, there is still a noticeable lack of knowledge on how machine learning approaches have been adopted for code smell detection and whether there are points of improvement to allow a better detection of code smells. Our goal is to provide an overview and discuss the usage of machine learning approaches in the field of code smells. Method: This paper presents a Systematic Literature Review (SLR) on Machine Learning Techniques for Code Smell Detection. Our work considers papers published between 2000 and 2017. Starting from an initial set of 2456 papers, we found that 15 of them actually adopted machine learning approaches. We studied them under four different perspectives: (i) code smells considered, (ii) setup of machine learning approaches, (iii) design of the evaluation strategies, and (iv) a meta-analysis on the performance achieved by the models proposed so far. Results: The analyses performed show that God Class, Long Method, Functional Decomposition, and Spaghetti Code have been heavily considered in the literature. Decision Trees and Support Vector Machines are the most commonly used machine learning algorithms for code smell detection. Models based on a large set of independent variables have performed well. JRip and Random Forest are the most effective classifiers in terms of performance. The analyses also reveal the existence of several open issues and challenges that the research community should focus on in the future. Conclusion: Based on our findings, we argue that there is still room for the improvement of machine learning techniques in the context of code smell detection. The open issues emerged in this study can represent the input for researchers interested in developing more powerful techniques.}
}

@article{rayyan-727967528,
  title={Identifying, categorizing and mitigating threats to validity in software engineering secondary studies},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={106},
  pages={201-230},
  author={Ampatzoglou, Apostolos and Bibi, Stamatia and Avgeriou, Paris and Verbeek, Marijn and Chatzigeorgiou, Alexander},
  url={https://www.sciencedirect.com/science/article/pii/S0950584918302106},
  keywords={Secondary studies, Empirical software engineering, Literature Review, Threats to Validity, Software Validation, Software},
  abstract={Context Secondary studies are vulnerable to threats to validity. Although, mitigating these threats is crucial for the credibility of these studies, we currently lack a systematic approach to identify, categorize and mitigate threats to validity for secondary studies. Objective In this paper, we review the corpus of secondary studies, with the aim to identify: (a) the trend of reporting threats to validity, (b) the most common threats to validity and corresponding mitigation actions, and (c) possible categories in which threats to validity can be classified. Method To achieve this goal we employ the tertiary study research method that is used for synthesizing knowledge from existing secondary studies. In particular, we collected data from more than 100 studies, published until December 2016 in top quality software engineering venues (both journals and conference). Results Our results suggest that in recent years, secondary studies are more likely to report their threats to validity. However, the presentation of such threats is rather ad hoc, e.g., the same threat may be presented with a different name, or under a different category. To alleviate this problem, we propose a classification schema for reporting threats to validity and possible mitigation actions. Both the classification of threats and the associated mitigation actions have been validated by an empirical study, i.e., Delphi rounds with experts. Conclusion Based on the proposed schema, we provide a checklist, which authors of secondary studies can use for identifying and categorizing threats to validity and corresponding mitigation actions, while readers of secondary studies can use the checklist for assessing the validity of the reported results.}
}

@article{rayyan-727967529,
  title={Integration between requirements engineering and safety analysis: A systematic literature review},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={125},
  pages={68-92},
  author={Vilela, Jéssyka and Castro, Jaelson and Martins, Luiz Eduardo G and Gorschek, Tony},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216302333},
  keywords={Systematic literature review, Requirements engineering, Integration, Communication, Safety analysis, Safety-critical systems},
  abstract={Context: Safety-Critical Systems (SCS) require more sophisticated requirements engineering (RE) approaches as inadequate, incomplete or misunderstood requirements have been recognized as a major cause in many accidents and safety-related catastrophes. Objective: In order to cope with the complexity of specifying SCS by RE, we investigate the approaches proposed to improve the communication or integration between RE and safety engineering in SCS development. We analyze the activities that should be performed by RE during safety analysis, the hazard/safety techniques it could use, the relationships between safety information that it should specify, the tools to support safety analysis as well as integration benefits between these areas. Method: We use a Systematic Literature Review (SLR) as the basis for our work. Results: We developed four taxonomies to help RE during specification of SCS that classify: techniques used in (1) hazard analysis; (2) safety analysis; (3) safety-related information and (4) a detailed set of information regarding hazards specification. Conclusions: This paper is a step towards developing a body of knowledge in safety concerns necessary to RE in the specification of SCS that is derived from a large-scale SLR. We believe the results will benefit both researchers and practitioners.}
}

@article{rayyan-727967530,
  title={A systematic literature review on QoS-aware service composition and selection in cloud environment},
  year={2018},
  journal={Journal of Network and Computer Applications},
  issn={1084-8045},
  volume={110},
  pages={52-74},
  author={Hayyolalam, Vahideh and Pourhaji Kazem, Ali Asghar},
  url={https://www.sciencedirect.com/science/article/pii/S1084804518300845},
  keywords={Cloud computing, Quality of service (QoS), QoS-aware service composition, QoS-aware service selection},
  abstract={Generally, cloud computing consists of providing virtualized and scalable resources as services through the Internet dynamically. According to the costumers' requests, various types of services which have the same functionality with different non-functionality features, are delivered in the cloud environment that often should be combined to satisfy the customer's complex requests. Recently, the composition of unique and loosely-coupled services into a preferred system is a prevalent industrial method and a commonly tracked research topic in academia. Service composition deals with generating new value-added services by merging some single existing services to provide an optimal composite service which includes formerly existing single and simple services aims to improve Quality of service (QoS). To the best of our knowledge, in spite of this issue's significance in cloud computing, there is not any comprehensive and systematic single research about this issue with a particular focus on QoS, which takes all metrics inspected in this paper into consideration. The most notable and impact of this paper is that it does not eliminate any paper in this scope, also it investigates more criteria than the current surveys. Hence, the purpose of this paper is to investigate the former mechanisms and techniques in terms of numerous factors. So, it adopts a systematic literature review, vital questions which can be enhanced by the research accomplished to address the stated problem have been extracted and raised. Afterwards, by classifying the researches into two primary groups (centralized and distributed) based on the environment of the problem and identifying the inspected QoS parameters, predefined goals, and developing environments, appropriate outcomes and statistics are attained that can contribute to upcoming works. In other words, this paper focuses to systematically categorize and evaluate the current research approaches and strategies on QoS-aware cloud service composition (published up to August 2017).}
}

@article{rayyan-727967531,
  title={Information technology service management models applied to medium and small organizations: A systematic literature review},
  year={2016},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={47},
  pages={120-127},
  author={Melendez, Karin and Dávila, Abraham and Pessoa, Marcelo},
  url={https://www.sciencedirect.com/science/article/pii/S0920548915001166},
  keywords={ISO/IEC 20000, ITIL, CMMI-SVC, Service process model, Small organization},
  abstract={(ANTECEDENT) The main responsibility of the Information Technology Service Management (ITSM) as an organization is to provide services in high level quality. That implies that the services will be an appropriate service and it will ensure continuity. In this context, the organization needs to adopt the best practices in service management to be more efficient and competitive. Some ITSM models collect the best practices of recognized organizations. These models are mainly applied by large organizations. (OBJECTIVE) The objective of this study is to gather experiences in the application of ITSM models in small organizations. (METHODS) To achieve this objective a systematic literature review was performed. (RESULTS) We found primary studies applied to IT areas from some large and medium companies but there is a few in small companies' context. (CONCLUSION) During the SLR we have identified some improvements and difficulties in many organizations, we have founded when applying ITSM models. The principal difficulty was the lack of knowledge of its personnel and consultants have, for adopting a model. On the other hand, companies who succeeded in the application of an ITSM model, had founded some benefits, such as processes improvement, higher user satisfaction, and service cost and time reduction.}
}

@article{rayyan-727967532,
  title={Investigations about replication of empirical studies in software engineering: A systematic mapping study},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={64},
  pages={76-101},
  author={de Magalhães, Cleyton V C and da Silva, Fabio Q B and Santos, Ronnie E S and Suassuna, Marcos},
  url={https://www.sciencedirect.com/science/article/pii/S0950584915000300},
  keywords={Systematic literature review, Software engineering, Experiments, Mapping study, Empirical studies, Replications, Software},
  abstract={Context Two recent mapping studies which were intended to verify the current state of replication of empirical studies in Software Engineering (SE) identified two sets of studies: empirical studies actually reporting replications (published between 1994 and 2012) and a second group of studies that are concerned with definitions, classifications, processes, guidelines, and other research topics or themes about replication work in empirical software engineering research (published between 1996 and 2012). Objective In this current article, our goal is to analyze and discuss the contents of the second set of studies about replications to increase our understanding of the current state of the work on replication in empirical software engineering research. Method We applied the systematic literature review method to build a systematic mapping study, in which the primary studies were collected by two previous mapping studies covering the period 1996–2012 complemented by manual and automatic search procedures that collected articles published in 2013. Results We analyzed 37 papers reporting studies about replication published in the last 17years. These papers explore different topics related to concepts and classifications, presented guidelines, and discuss theoretical issues that are relevant for our understanding of replication in our field. We also investigated how these 37 papers have been cited in the 135 replication papers published between 1994 and 2012. Conclusions Replication in SE still lacks a set of standardized concepts and terminology, which has a negative impact on the replication work in our field. To improve this situation, it is important that the SE research community engage on an effort to create and evaluate taxonomy, frameworks, guidelines, and methodologies to fully support the development of replications.}
}

@article{rayyan-727967533,
  title={Managing architectural technical debt: A unified model and systematic literature review},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={135},
  pages={1-16},
  author={Besker, Terese and Martini, Antonio and Bosch, Jan},
  url={https://www.sciencedirect.com/science/article/pii/S0164121217302121},
  keywords={Systematic literature review, Software architecture, Software maintenance, Architectural technical debt},
  abstract={Large Software Companies need to support the continuous and fast delivery of customer value in both the short and long term. However, this can be impeded if the evolution and maintenance of existing systems is hampered by what has been recently termed Technical Debt (TD). Specifically, Architectural TD has received increased attention in the last few years due to its significant impact on system success and, left unchecked, it can cause expensive repercussions. It is therefore important to understand the underlying factors of architectural TD. With this as background, there is a need for a descriptive model to illustrate and explain different architectural TD issues. The aim of this study is to synthesize and compile research efforts with the goal of creating new knowledge with a specific interest in the architectural TD field. The contribution of this paper is the presentation of a novel descriptive model, providing a comprehensive interpretation of the architectural TD phenomenon. This model categorizes the main characteristics of architectural TD and reveals their relations. The results show that, by using this model, different stakeholders could increase the system's success rate, and lower the rate of negative consequences, by raising awareness about architectural TD.}
}

@article{rayyan-727967534,
  title={Enhancing computing studies in high schools: A systematic literature review & UAE case study},
  year={2019},
  journal={Heliyon},
  issn={2405-8440},
  volume={5},
  number={2},
  pages={e01235},
  author={Talib, Manar Abu and Einea, Omar and Nasir, Qassim and Mowakeh, Mohamad Fouzi and Eltawil, Mohamed},
  url={https://www.sciencedirect.com/science/article/pii/S2405844018334807},
  keywords={Education, Computer science},
  abstract={Open source software (OSS) is increasingly being integrated into educational institutions, and many countries require the use of OSS in government departments. However, not much focus is placed on integrating it into the educational sector in a strategic and productive manner. This paper examines the existing literature on the use of OSS in terms of the potential enhancements it can provide for computer science studies in high schools in general, and those in the UAE more specifically. It also details a survey conducted among 400 high school teachers after teaching them about multiple types of OSS that might enhance their teaching experience. After examining more than 69 different research papers and taking the survey findings into account, we drafted a roadmap that can be used by any educational institute—especially high schools—to strategically integrate OSS into the educational system.}
}

@article{rayyan-727967535,
  title={Software ecosystems – A systematic literature review},
  year={2013},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={86},
  number={5},
  pages={1294-1306},
  author={Manikas, Konstantinos and Hansen, Klaus Marius},
  url={https://www.sciencedirect.com/science/article/pii/S016412121200338X},
  keywords={Systematic literature review, Software ecosystem, Software ecosystems, Software},
  abstract={A software ecosystem is the interaction of a set of actors on top of a common technological platform that results in a number of software solutions or services. Arguably, software ecosystems are gaining importance with the advent of, e.g., the Google Android, Apache, and Salesforce.com ecosystems. However, there exists no systematic overview of the research done on software ecosystems from a software engineering perspective. We performed a systematic literature review of software ecosystem research, analyzing 90 papers on the subject taken from a gross collection of 420. Our main conclusions are that while research on software ecosystems is increasing (a) there is little consensus on what constitutes a software ecosystem, (b) few analytical models of software ecosystems exist, and (c) little research is done in the context of real-world ecosystems. This work provides an overview of the field, while identifying areas for future research.}
}

@article{rayyan-727967536,
  title={Port sustainability and performance: A systematic literature review},
  year={2019},
  journal={Transportation Research Part D: Transport and Environment},
  issn={1361-9209},
  volume={72},
  pages={47-64},
  author={Lim, Sehwa and Pettit, Stephen and Abouarghoub, Wessam and Beresford, Anthony},
  url={https://www.sciencedirect.com/science/article/pii/S1361920918311520},
  abstract={Motivated by a lack of research on port sustainability performance and assessment, this paper uses a systematic literature review to identify trends, measurement methods, and mechanisms for the implementation of strategy and policy in this area. The paper provides a comprehensive and critical evaluation of port operational sustainability, focusing on ascertaining the impact of its implementation. The study analysed and synthesised established characteristics in the current literature regarding the performance of port sustainability and its evaluation in terms of operations and management. Successful performance measurement in port sustainability is driven by the dependence on establishing accurate indicators as the basis for measurement. Our clustering of analytical sustainability indicators reveals that environmental research is focused on pollution, social research is mainly focused on human resource management, while economic research is mainly on port management and borderline investment. Findings are discussed in four key areas of port sustainability performance and assessment: existing trends, implementation of measures, mechanisms for implementation, and assessment gaps and challenges. For existing trends, attempts to evaluate the applicability and practicality of green operations have improved the awareness and promotion of governmental green policies. Implementation measures relate to the utilisation of techniques that reveal optimal practices for practical sustainable operations while mechanisms largely relate to establishing indicators which increase understanding of performance. Finally, challenges in this field include achieving consistency among ports in how sustainability is measured. Future research should incentivise improvements in port operational practice and encourage self-examination in order to reprioritise activity.}
}

@article{rayyan-727967537,
  title={A systematic review on the engineering of software for ubiquitous systems},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={118},
  pages={251-276},
  author={Sánchez Guinea, Alejandro and Nain, Grégory and Le Traon, Yves},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216300553},
  keywords={Evidence-based software engineering, Empirical software engineering, Systematic review, Research synthesis, Development methods, Pervasive systems, Software development cycle, Ubiquitous systems, Software},
  abstract={Context: Software engineering for ubiquitous systems has experienced an important and rapid growth, however the vast research corpus makes it difficult to obtain valuable information from it. Objective: To identify, evaluate, and synthesize research about the most relevant approaches addressing the different phases of the software development life cycle for ubiquitous systems. Method: We conducted a systematic literature review of papers presenting and evaluating approaches for the different phases of the software development life cycle for ubiquitous systems. Approaches were classified according to the phase of the development cycle they addressed, identifying their main concerns and limitations. Results: We identified 128 papers reporting 132 approaches addressing issues related to different phases of the software development cycle for ubiquitous systems. Most approaches have been aimed at addressing the implementation, evolution/maintenance, and feedback phases, while others phases such as testing need more attention from researchers. Conclusion: We recommend to follow existing guidelines when conducting case studies to make the studies more reproducible and closer to real life cases. While some phases of the development cycle have been extensively explored, there is still room for research in other phases, toward a more agile and integrated cycle, from requirements to testing and feedback.}
}

@article{rayyan-727967538,
  title={Agile, web engineering and capability maturity model integration: A systematic literature review.},
  year={2016},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={71},
  pages={92-107},
  author={Torrecilla-Salinas, C J and Sedeño, J and Escalona, M J and Mejías, M},
  url={https://www.sciencedirect.com/science/article/pii/S095058491500186X},
  keywords={Software Engineering, Scrum, Agile, CMMI, Web Engineering},
  abstract={Context Agile approaches are an alternative for organizations developing software, particularly for those who develop Web applications. Besides, CMMI (Capability Maturity Model Integration) models are well-established approaches focused on assessing the maturity of an organization that develops software. Web Engineering is the field of Software Engineering responsible for analyzing and studying the specific characteristics of the Web. The suitability of an Agile approach to help organizations reach a certain CMMI maturity level in Web environments will be very interesting, as they will be able to keep the ability to quickly react and adapt to changes as long as their development processes get mature. Objective This paper responds to whether it is feasible or not, for an organization developing Web systems, to achieve a certain maturity level of the CMMI-DEV model using Agile methods. Method The proposal is analyzed by means of a systematic literature review of the relevant approaches in the field, defining a characterization schema in order to compare them to introduce the current state-of-the-art. Results The results achieved after the systematic literature review are presented, analyzed and compared against the defined schema, extracting relevant conclusions for the different dimensions of the problem: compatibility, compliance, experience, maturity and Web. Conclusion It is concluded that although the definition of an Agile approach to meet the different CMMI maturity levels goals could be possible for an organization developing Web systems, there is still a lack of detailed studies and analysis on the field.}
}

@article{rayyan-727967539,
  title={Looking over the research literature on software engineering from 2016 to 2018},
  year={2019},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={162},
  pages={712-719},
  author={Heradio, Ruben and Fernandez-Amoros, David and Galan, Daniel and Cabrerizo, Francisco Javier and Herrera-Viedma, Enrique},
  url={https://www.sciencedirect.com/science/article/pii/S1877050919320538},
  keywords={Software engineering, Bibliometric analysis, Scopus, Software},
  abstract={This paper carries out a bibliometric analysis to detect (i) what is the most influential research on software engineering at the moment, (ii) where is being published that relevant research, (iii) what are the most commonly researched topics, (iv) and where is being undertaken that research (i.e., in which countries and institutions). For that, 6,365 software engineering articles, published from 2016 to 2018 on a variety of conferences and journals, are examined.}
}

@article{rayyan-727967540,
  title={IT service management process improvement based on ISO/IEC 15504: A systematic review},
  year={2012},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={54},
  number={3},
  pages={239-247},
  author={Mesquida, Antoni Lluís and Mas, Antonia and Amengual, Esperança and Calvo-Manzano, Jose A},
  url={https://www.sciencedirect.com/science/article/pii/S0950584911002266},
  keywords={Systematic review, ISO/IEC 15504 (SPICE), IT Service Management (ITSM), Software Process Improvement (SPI)},
  abstract={Context In recent years, many software companies have considered Software Process Improvement (SPI) as essential for successful software development. These companies have also shown special interest in IT Service Management (ITSM). SPI standards have evolved to incorporate ITSM best practices. Objective This paper presents a systematic literature review of ITSM Process Improvement initiatives based on the ISO/IEC 15504 standard for process assessment and improvement. Method A systematic literature review based on the guidelines proposed by Kitchenham and the review protocol template developed by Biolchini et al. is performed. Results Twenty-eight relevant studies related to ITSM Process Improvement have been found. From the analysis of these studies, nine different ITSM Process Improvement initiatives have been detected. Seven of these initiatives use ISO/IEC 15504 conformant process assessment methods. Conclusion During the last decade, in order to satisfy the on-going demand of mature software development companies for assessing and improving ITSM processes, different models which use the measurement framework of ISO/IEC 15504 have been developed. However, it is still necessary to define a method with the necessary guidelines to implement both software development processes and ITSM processes reducing the amount of effort, especially because some processes of both categories are overlapped.}
}

@article{rayyan-727967541,
  title={Challenges and success factors for large-scale agile transformations: A systematic literature review},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={119},
  pages={87-108},
  author={Dikert, Kim and Paasivaara, Maria and Lassenius, Casper},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216300826},
  keywords={Systematic literature review, Agile software development, Challenges, Success factors, Adopting agile software development, Large-scale agile, Organizational transformation},
  abstract={Agile methods have become an appealing alternative for companies striving to improve their performance, but the methods were originally designed for small and individual teams. This creates unique challenges when introducing agile at scale, when development teams must synchronize their activities, and there might be a need to interface with other organizational units. In this paper we present a systematic literature review on how agile methods and lean software development has been adopted at scale, focusing on reported challenges and success factors in the transformation. We conducted a systematic literature review of industrial large-scale agile transformations. Our keyword search found 1875 papers. We included 52 publications describing 42 industrial cases presenting the process of taking large-scale agile development into use. Almost 90% of the included papers were experience reports, indicating a lack of sound academic research on the topic. We identified 35 reported challenges grouped into nine categories, and 29 success factors, grouped into eleven categories. The most salient success factor categories were management support, choosing and customizing the agile model, training and coaching, and mindset and alignment.}
}

@article{rayyan-727967542,
  title={A systematic literature review of blockchain cyber security},
  year={2020},
  journal={Digital Communications and Networks},
  issn={2352-8648},
  volume={6},
  number={2},
  pages={147-156},
  author={Taylor, Paul J and Dargahi, Tooska and Dehghantanha, Ali and Parizi, Reza M and Choo, Kim-Kwang Raymond},
  url={https://www.sciencedirect.com/science/article/pii/S2352864818301536},
  keywords={Blockchain, Smart contracts, IoT, Cryptocurrency, Cyber security, Bitcoin, Distributed ledger technology},
  abstract={Since the publication of Satoshi Nakamoto's white paper on Bitcoin in 2008, blockchain has (slowly) become one of the most frequently discussed methods for securing data storage and transfer through decentralized, trustless, peer-to-peer systems. This research identifies peer-reviewed literature that seeks to utilize blockchain for cyber security purposes and presents a systematic analysis of the most frequently adopted blockchain security applications. Our findings show that the Internet of Things (IoT) lends itself well to novel blockchain applications, as do networks and machine visualization, public-key cryptography, web applications, certification schemes and the secure storage of Personally Identifiable Information (PII). This timely systematic review also sheds light on future directions of research, education and practices in the blockchain and cyber security space, such as security of blockchain in IoT, security of blockchain for AI data, and sidechain security.}
}

@article{rayyan-727967543,
  title={Systematic literature review of industry 4.0 maturity model for manufacturing and logistics sectors},
  year={2020},
  journal={Procedia Manufacturing},
  issn={2351-9789},
  volume={52},
  pages={337-343},
  author={Angreani, Linda Salma and Vijaya, Annas and Wicaksono, Hendro},
  url={https://www.sciencedirect.com/science/article/pii/S2351978920322010},
  keywords={Systematic Literature Review, Production, Industry 4.0 Maturity Model, Industry 4.0 Readiness, Logistics},
  abstract={A maturity model is a wide technique to measure several aspects and identify the current state of processes in an organization, which can be used as a starting point for business improvement. In the Industry 4.0 context, several terms are used to express the model, such as readiness assessment model, roadmap, framework, and maturity index. They have the same purpose of measuring how the current state of an organization unit is capable of adopting and implementing the concept of industry 4.0 in the future. Many researchers had proposed maturity models for assessing Industry 4.0 readiness and maturity since 2011 when Industry 4.0 was commenced. However, there has been no attempt to analyze empirical evidence systematically. This paper aims to analyze currently available maturity models related to Industry 4.0 and provide a synthesis on those maturity models. This paper describes a systematic literature review (SLR) of empirical studies implemented on the maturity model published in several reputable and relevant sources. It focuses on the manufacturing and logistics sectors since the processes in both sectors can be highly improved through the introduction of technologies such as cyber-physical systems, internet of things, and artificial intelligence. In general, the primary purpose of the review is to address the following questions: (1) Based on what dimensions do researchers develop Industry 4.0 maturity models, and what are the most used and influencing parameters in those dimensions? (2) How do those maturity models compare to each other in terms of dimension complexity, techniques, maturity leveling, and kind of application sectors of the model? In conclusion, the maturity model in the context of Industry 4.0 is promising to guide the adoption of industry 4.0 technologies at the organization level. However, just having a maturity model is not enough. More efforts are needed to facilitate the application of it.}
}

@article{rayyan-727967544,
  title={Test case prioritization approaches in regression testing: A systematic literature review},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={93},
  pages={74-93},
  author={Khatibsyarbini, Muhammad and Isa, Mohd Adham and Jawawi, Dayang N A and Tumeng, Rooster},
  url={https://www.sciencedirect.com/science/article/pii/S0950584916304888},
  keywords={Software testing, Systematic literature review, Regression testing, Test case prioritization},
  abstract={Context Software quality can be assured by going through software testing process. However, software testing phase is an expensive process as it consumes a longer time. By scheduling test cases execution order through a prioritization approach, software testing efficiency can be improved especially during regression testing. Objective It is a notable step to be taken in constructing important software testing environment so that a system's commercial value can increase. The main idea of this review is to examine and classify the current test case prioritization approaches based on the articulated research questions. Method Set of search keywords with appropriate repositories were utilized to extract most important studies that fulfill all the criteria defined and classified under journal, conference paper, symposiums and workshops categories. 69 primary studies were nominated from the review strategy. Results There were 40 journal articles, 21 conference papers, three workshop articles, and five symposium articles collected from the primary studies. As for the result, it can be said that TCP approaches are still broadly open for improvements. Each approach in TCP has specified potential values, advantages, and limitation. Additionally, we found that variations in the starting point of TCP process among the approaches provide a different timeline and benefit to project manager to choose which approaches suite with the project schedule and available resources. Conclusion Test case prioritization has already been considerably discussed in the software testing domain. However, it is commonly learned that there are quite a number of existing prioritization techniques that can still be improved especially in data used and execution process for each approach.}
}

@article{rayyan-727967545,
  title={An overview of software functionality service: A systematic literature review},
  year={2017},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={124},
  pages={337-344},
  author={Salleh, Masrina A and Bahari, Mahadi and Zakaria, Nor Hidayati},
  url={https://www.sciencedirect.com/science/article/pii/S1877050917329319},
  keywords={Systematic Literature Review, Functionality Service, Okoli Guideline, Software Functionality Quality, Software},
  abstract={This study focuses in contributing a literature on software functionality service area. We aim to provide an overview of software functionality service related to its research activity, investigated the major themes and identiﬁed the focus on its sub-characteristics addressed. In doing this, we employed a Systematic Literature Review (SLR) approach by reviewing all relevant articles from four online databases (i.e., IEEE, Springer, ScienceDirect and EmaraldInsight) and finally identified only 79 relevant articles to our research questions. We categorized the articles into its major themes discussed and its sub-characteristics addressed. It is found that there are increased of researched related to software functionality service for the last five years mainly in functionality service development while there is still lack of research coverage on functional compliance sub-characteristics. Future work should be included of reviewing a greater number of articles from more various types of journal and workshop.}
}

@article{rayyan-727967546,
  title={Model-Driven Engineering as a new landscape for traceability management: A systematic literature review},
  year={2012},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={54},
  number={12},
  pages={1340-1356},
  author={Santiago, Iván and Jiménez, Álvaro and Vara, Juan Manuel and De Castro, Valeria and Bollati, Verónica A and Marcos, Esperanza},
  url={https://www.sciencedirect.com/science/article/pii/S0950584912001346},
  keywords={Systematic literature review, Model-Driven Engineering, Traceability},
  abstract={Context Model-Driven Engineering provides a new landscape for dealing with traceability in software development. Objective Our goal is to analyze the current state of the art in traceability management in the context of Model-Driven Engineering. Method We use the systematic literature review based on the guidelines proposed by Kitchenham. We propose five research questions and six quality assessments. Results Of the 157 relevant studies identified, 29 have been considered primary studies. These studies have resulted in 17 proposals. Conclusion The evaluation shows that the most addressed operations are storage, CRUD and visualization, while the most immature operations are exchange and analysis traceability information.}
}

@article{rayyan-727967547,
  title={Software component decision-making: In-house, OSS, COTS or outsourcing - A systematic literature review},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={121},
  pages={105-124},
  author={Badampudi, Deepika and Wohlin, Claes and Petersen, Kai},
  url={https://www.sciencedirect.com/science/article/pii/S0164121216301212},
  keywords={OSS, Component-based software engineering, Outsourcing, COTS, Decision-making, In-house development, Decision Making, Software},
  abstract={Context: Component-based software systems require decisions on component origins for acquiring components. A component origin is an alternative of where to get a component from. Objective: To identify factors that could influence the decision to choose among different component origins and solutions for decision-making (For example, optimization) in the literature. Method: A systematic review study of peer-reviewed literature has been conducted. Results: In total we included 24 primary studies. The component origins compared were mainly focused on in-house vs. COTS and COTS vs. OSS. We identified 11 factors affecting or influencing the decision to select a component origin. When component origins were compared, there was little evidence on the relative (either positive or negative) effect of a component origin on the factor. Most of the solutions were proposed for in-house vs. COTS selection and time, cost and reliability were the most considered factors in the solutions. Optimization models were the most commonly proposed technique used in the solutions. Conclusion: The topic of choosing component origins is a green field for research, and in great need of empirical comparisons between the component origins, as well of how to decide between different combinations of them.}
}

@article{rayyan-727967548,
  title={Metrics for analyzing variability and its implementation in software product lines: A systematic literature review},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={106},
  pages={1-30},
  author={El-Sharkawy, Sascha and Yamagishi-Eichler, Nozomi and Schmid, Klaus},
  url={https://www.sciencedirect.com/science/article/pii/S0950584918301873},
  keywords={Software product lines, Systematic literature review, Metrics, Implementation, SPL, Metronidazole, Software},
  abstract={Context: Software Product Line (SPL) development requires at least concepts for variability implementation and variability modeling for deriving products from a product line. These variability implementation concepts are not required for the development of single systems and, thus, are not considered in traditional software engineering. Metrics are well established in traditional software engineering, but existing metrics are typically not applicable to SPLs as they do not address variability management. Over time, various specialized product line metrics have been described in literature, but no systematic description of these metrics and their characteristics is currently available. Objective: This paper describes and analyzes variability-aware metrics, designed for the needs of software product lines. More precisely we restrict the scope of our study explicitly to metrics designed for variability models, code artifacts, and metrics taking both kinds of artifacts into account. Further, we categorize the purpose for which these metrics were developed. We also analyze to what extent these metrics were evaluated to provide a basis for researchers for selecting adequate metrics. Method: We conducted a systematic literature review to identify variability-aware implementation metrics. We discovered 42 relevant papers reporting metrics intended to measure aspects of variability models or code artifacts. Results: We identified 57 variability model metrics, 34 annotation-based code metrics, 46 code metrics specific to composition-based implementation techniques, and 10 metrics integrating information from variability model and code artifacts. For only 31 metrics, an evaluation was performed assessing their suitability to draw any qualitative conclusions. Conclusions: We observed several problematic issues regarding the definition and the use of the metrics. Researchers and practitioners benefit from the catalog of variability-aware metrics, which is the first of its kind. Also, the research community benefits from the identified observations in order to avoid those problems when defining new metrics.}
}

@article{rayyan-727967549,
  title={A systematic literature review on serious games evaluation: An application to software project management},
  year={2015},
  journal={Computers & Education},
  issn={0360-1315},
  volume={87},
  pages={396-422},
  author={Calderón, Alejandro and Ruiz, Mercedes},
  url={https://www.sciencedirect.com/science/article/pii/S0360131515300166},
  keywords={Systematic literature review, Software project management, Evaluation, Serious game, Software},
  abstract={Training that future practitioners receive in software project management is a topic of great importance. The objective of this systematic literature review is to summarize the current state of the art of the different methods and procedures used to assess serious games. The review follows a predefined procedure that involves automatically searching well-known digital databases. 1199 papers were found by the automatic searches in the digital databases and 102 papers were selected as primary studies. The process was complemented with manual searches using author and backward snowballing techniques. Our systematic literature review identified the main methods followed to assess serious games, the application domains in which the assessments took place, the categories of serious games assessed, the main features considered to assess the educational effectiveness of serious games, the procedures followed for the assessments and the size of the population that participated in the assessments. The results are useful to researchers and practitioners willing to assess serious games in different fields, but specially to those interested in assessing serious games in the area of software project management.}
}

@article{rayyan-727967550,
  title={Formal verification approaches for distributed algorithms: A systematic literature review},
  year={2018},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={126},
  pages={1551-1560},
  author={Fakhfakh, Faten and Tounsi, Mohamed and Mosbah, Mohamed and Kacem, Ahmed Hadj},
  url={https://www.sciencedirect.com/science/article/pii/S1877050918314066},
  keywords={formal verification, challenges, Distributed algorithms, dynamic networks, static, taxonomy, Algorithms},
  abstract={Distributed algorithms have become a rapidly growing field of research due to the advances of the network technologies. However, they are very difficult to implement correctly because they must meet many requirements. In this paper, we follow the guidelines of systematic literature reviews to provide a survey of the existing works ensuring the formal verification of distributed algorithms in static and dynamic networks. Then, we develop a taxonomy of these solutions based on some criteria. Also, a discussion on each criterion is shown with a focus on constraints, requirements and challenges. Finally, we identify some recommendations and open research areas which can motivate the development of more efficient solutions. So, throughout this present paper, we provide information for researchers and developers to understand the contributions and challenges of the existing solutions to pave the way for enhancing their reliability.}
}

@article{rayyan-727967551,
  title={Knowledge-based approaches in software documentation: A systematic literature review},
  year={2014},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={56},
  number={6},
  pages={545-567},
  author={Ding, Wei and Liang, Peng and Tang, Antony and van Vliet, Hans},
  url={https://www.sciencedirect.com/science/article/pii/S0950584914000196},
  keywords={Systematic literature review, Software documentation, Knowledge activity, Knowledge-based approach, Software architecture design, Software},
  abstract={Context Software documents are core artifacts produced and consumed in documentation activity in the software lifecycle. Meanwhile, knowledge-based approaches have been extensively used in software development for decades, however, the software engineering community lacks a comprehensive understanding on how knowledge-based approaches are used in software documentation, especially documentation of software architecture design. Objective The objective of this work is to explore how knowledge-based approaches are employed in software documentation, their influences to the quality of software documentation, and the costs and benefits of using these approaches. Method We use a systematic literature review method to identify the primary studies on knowledge-based approaches in software documentation, following a pre-defined review protocol. Results Sixty studies are finally selected, in which twelve quality attributes of software documents, four cost categories, and nine benefit categories of using knowledge-based approaches in software documentation are identified. Architecture understanding is the top benefit of using knowledge-based approaches in software documentation. The cost of retrieving information from documents is the major concern when using knowledge-based approaches in software documentation. Conclusions The findings of this review suggest several future research directions that are critical and promising but underexplored in current research and practice: (1) there is a need to use knowledge-based approaches to improve the quality attributes of software documents that receive less attention, especially credibility, conciseness, and unambiguity; (2) using knowledge-based approaches with the knowledge content in software documents which gets less attention in current applications of knowledge-based approaches in software documentation, to further improve the practice of software documentation activity; (3) putting more focus on the application of software documents using the knowledge-based approaches (knowledge reuse, retrieval, reasoning, and sharing) in order to make the most use of software documents; and (4) evaluating the costs and benefits of using knowledge-based approaches in software documentation qualitatively and quantitatively.}
}

@article{rayyan-727967552,
  title={Reporting systematic reviews: Some lessons from a tertiary study},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={95},
  pages={62-74},
  author={Budgen, David and Brereton, Pearl and Drummond, Sarah and Williams, Nikki},
  url={https://www.sciencedirect.com/science/article/pii/S0950584916303548},
  keywords={Systematic review, Reporting quality, Provenance of findings},
  abstract={Context Many of the systematic reviews published in software engineering are related to research or methodological issues and hence are unlikely to be of direct benefit to practitioners or teachers. Those that are relevant to practice and teaching need to be presented in a form that makes their findings usable with minimum interpretation. Objective We have examined a sample of the many systematic reviews that have been published over a period of six years, in order to assess how well these are reported and identify useful lessons about how this might be done. Method We undertook a tertiary study, performing a systematic review of systematic reviews. Our study found 178 systematic reviews published in a set of major software engineering journals over the period 2010–2015. Of these, 37 provided recommendations or conclusions of relevance to education and/or practice and we used the DARE criteria as well as other attributes related to the systematic review process to analyse how well they were reported. Results We have derived a set of 12 ‘lessons' that could help authors with reporting the outcomes of a systematic review in software engineering. We also provide an associated checklist for use by journal and conference referees. Conclusion There are several areas where better reporting is needed, including quality assessment, synthesis, and the procedures followed by the reviewers. Researchers, practitioners, teachers and journal referees would all benefit from better reporting of systematic reviews, both for clarity and also for establishing the provenance of any findings.}
}

@article{rayyan-727967553,
  title={CERSE - Catalog for empirical research in software engineering: A Systematic mapping study},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={105},
  pages={117-149},
  author={Molléri, Jefferson Seide and Petersen, Kai and Mendes, Emilia},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917301118},
  keywords={Mapping study, Empirical research, Empirical methods, Software},
  abstract={Context Empirical research in software engineering contributes towards developing scientific knowledge in this field, which in turn is relevant to inform decision-making in industry. A number of empirical studies have been carried out to date in software engineering, and the need for guidelines for conducting and evaluating such research has been stressed. Objective: The main goal of this mapping study is to identify and summarize the body of knowledge on research guidelines, assessment instruments and knowledge organization systems on how to conduct and evaluate empirical research in software engineering. Method: A systematic mapping study employing manual search and snowballing techniques was carried out to identify the suitable papers. To build up the catalog, we extracted and categorized information provided by the identified papers. Results: The mapping study comprises a list of 341 methodological papers, classified according to research methods, research phases covered, and type of instrument provided. Later, we derived a brief explanatory review of the instruments provided for each of the research methods. Conclusion: We provide: an aggregated body of knowledge on the state of the art relating to guidelines, assessment instruments and knowledge organization systems for carrying out empirical software engineering research; an exemplary usage scenario that can be used to guide those carrying out such studies is also provided. Finally, we discuss the catalog's implications for research practice and the needs for further research.}
}

@article{rayyan-727967554,
  title={Kanban in software engineering: A systematic mapping study},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={137},
  pages={96-113},
  author={Ahmad, Muhammad Ovais and Dennehy, Denis and Conboy, Kieran and Oivo, Markku},
  url={https://www.sciencedirect.com/science/article/pii/S0164121217302820},
  keywords={Software engineering, Software development, Kanban, Lean, Software},
  abstract={Following a well-established track record of success in other domains such as manufacturing, Kanban is increasingly used to achieve continuous development and delivery of value in the software industry. However, while research on Kanban in software is growing, these articles are largely descriptive, and there is limited rigorous research on its application and with little cohesive building of cumulative knowledge. As a result, it is extremely difficult to determine the true value of Kanban in software engineering. This study investigates the scientific evidence to date regarding Kanban by conducting a systematic mapping of Kanban literature in software engineering between 2006 and 2016. The search strategy resulted in 382 studies, of which 23 were identified as primary papers relevant to this research. This study is unique as it compares the findings of these primary papers with insights from a review of 23 Kanban experience reports during the same period. This study makes four important contributions, (i) a state-of-the-art of Kanban research is provided, (ii) the reported benefits and challenges are identified in both the primary papers and experience reports, (iii) recommended practices from both the primary papers and experience reports are listed and (iv) opportunities for future Kanban research are identified.}
}

@article{rayyan-727967555,
  title={An investigation into the best practices for the successful design and implementation of lightweight software process assessment methods: A systematic literature review},
  year={2015},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={101},
  pages={180-192},
  author={Zarour, Mohammad and Abran, Alain and Desharnais, Jean-Marc and Alarifi, Abdulrahman},
  url={https://www.sciencedirect.com/science/article/pii/S0164121214002726},
  keywords={systematic literature review, assessment method design, software process assessment, Process Assessment (Health Care), Software},
  abstract={Software process assessment (SPA) is an effective tool to understand an organization's process quality and to explore improvement opportunities. However, the knowledge that underlies the best practices required to develop assessment methods, either lightweight or heavyweight methods, is unfortunately scattered throughout the literature. This paper presents the results of a systematic literature review to organize those recognized as the best practices in a way that helps SPA researchers and practitioners in designing and implementing their assessment methods. Such practices are presented in the literature as assessment requirements, success factors, observations, and lessons learned. Consequently, a set of 38 best practices has been collected and classified into five main categories, namely practices related to SPA methods, support tools, procedures, documentation, and users. While this collected set of best practices is important for designing lightweight as well as heavyweight assessment methods, it is of utmost importance in designing lightweight assessment methods, as the design of which depends on individual experience.}
}

@article{rayyan-727967556,
  title={Three empirical studies on the agreement of reviewers about the quality of software engineering experiments},
  year={2012},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={54},
  number={8},
  pages={804-819},
  author={Kitchenham, Barbara Ann and Sjøberg, Dag I K and Dybå, Tore and Pfahl, Dietmar and Brereton, Pearl and Budgen, David and Höst, Martin and Runeson, Per},
  url={https://www.sciencedirect.com/science/article/pii/S0950584911002321},
  keywords={Software engineering, Quality evaluation, Empirical studies, Experimentation, Human-intensive experiments, Software},
  abstract={Context During systematic literature reviews it is necessary to assess the quality of empirical papers. Current guidelines suggest that two researchers should independently apply a quality checklist and any disagreements must be resolved. However, there is little empirical evidence concerning the effectiveness of these guidelines. Aims This paper investigates the three techniques that can be used to improve the reliability (i.e. the consensus among reviewers) of quality assessments, specifically, the number of reviewers, the use of a set of evaluation criteria and consultation among reviewers. We undertook a series of studies to investigate these factors. Method Two studies involved four research papers and eight reviewers using a quality checklist with nine questions. The first study was based on individual assessments, the second study on joint assessments with a period of inter-rater discussion. A third more formal randomised block experiment involved 48 reviewers assessing two of the papers used previously in teams of one, two and three persons to assess the impact of discussion among teams of different size using the evaluations of the “teams” of one person as a control. Results For the first two studies, the inter-rater reliability was poor for individual assessments, but better for joint evaluations. However, the results of the third study contradicted the results of Study 2. Inter-rater reliability was poor for all groups but worse for teams of two or three than for individuals. Conclusions When performing quality assessments for systematic literature reviews, we recommend using three independent reviewers and adopting the median assessment. A quality checklist seems useful but it is difficult to ensure that the checklist is both appropriate and understood by reviewers. Furthermore, future experiments should ensure participants are given more time to understand the quality checklist and to evaluate the research papers.}
}

@article{rayyan-727967557,
  title={Process models for service-based applications: A systematic literature review},
  year={2011},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={53},
  number={5},
  pages={424-439},
  author={Lane, Stephen and Richardson, Ita},
  url={https://www.sciencedirect.com/science/article/pii/S0950584910002211},
  keywords={Systematic literature review, SOA, Software process, Service-based application},
  abstract={Context Service-Oriented Computing (SOC) is a promising computing paradigm which facilitates the development of adaptive and loosely coupled service-based applications (SBAs). Many of the technical challenges pertaining to the development of SBAs have been addressed, however, there are still outstanding questions relating to the processes required to develop them. Objective The objective of this study is to systematically identify process models for developing service-based applications (SBAs) and review the processes within them. This will provide a useful starting point for any further research in the area. A secondary objective of the study is to identify process models which facilitate the adaptation of SBAs. Method In order to achieve this objective a systematic literature review (SLR) of the existing software engineering literature is conducted. Results During this research 722 studies were identified using a predefined search strategy, this number was narrowed down to 57 studies based on a set of strict inclusion and exclusion criteria. The results are reported both quantitatively in the form of a mapping study, as well as qualitatively in the form of a narrative summary of the key processes identified. Conclusion There are many process models reported for the development of SBAs varying in detail and maturity, this review has identified and categorised the processes within those process models. The review has also identified and evaluated process models which facilitate the adaptation of SBAs.}
}

@article{rayyan-727967558,
  title={Peer to peer (P2P) lending problems and potential solutions: A systematic literature review},
  year={2019},
  journal={Procedia Computer Science},
  issn={1877-0509},
  volume={161},
  pages={204-214},
  author={Suryono, Ryan Randy and Purwandari, Betty and Budi, Indra},
  url={https://www.sciencedirect.com/science/article/pii/S1877050919318265},
  keywords={Systematic Literature Review, Fintech, P2P Lending},
  abstract={There is a growing Financial Technology (Fintech) business model, such as Peer to Peer (P2P) Lending. P2P Lending allows individuals and businesses to borrow and lend money to each other. In its development, China has become the market with the most P2P lending platforms. However, there is a moral hazard that makes this business need to be monitored. This threat begins with verification of the borrower's data that is not appropriate. Whereas in Indonesia Fintech P2P Lending has received special attention, because its regulations and policies have not matured yet. Besides, P2P Lending is considered as a new business to flourish. Consequently, it requires investigation on problems from the implementation of the P2P Lending. This study aims to identify problems in P2P Lending and present alternative technical and non-technical solutions to the problems. By implementing the Kitchenham Systematic Literature Review (SLR) approach from the ACM, AIS, IEEE, SCOPUS, and Science Direct databases, this research finds a rich picture, creates a table of problem identification and alternative solutions.}
}

@article{rayyan-727967559,
  title={A systematic literature review of stakeholder identification methods in requirements elicitation},
  year={2012},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={85},
  number={9},
  pages={2171-2181},
  author={Pacheco, Carla and Garcia, Ivan},
  url={https://www.sciencedirect.com/science/article/pii/S0164121212001288},
  keywords={Software engineering, Systematic review, Requirements engineering, Requirements elicitation, Stakeholder identification},
  abstract={This paper presents a systematic review of relevant published studies related to topics in Requirements Engineering, specifically, concerning stakeholder identification methods in requirements elicitation, dated from 1984 to 2011. Addressing four specific research questions, this systematic literature review shows the following evidence gathered from these studies: current status of stakeholder identification in software requirement elicitation, the best practices recommended for its performance, consequences of incorrect identification in requirements quality, and, aspects which need to be improved. Our findings suggest that the analyzed approaches still have serious limitations in terms of covering all aspects of stakeholder identification as an important part of requirements elicitation. However, through correctly identifying and understanding the stakeholders, it is possible to develop high quality software.}
}

@article{rayyan-727967560,
  title={Model-driven architecture based testing: A systematic literature review},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={102},
  pages={30-48},
  author={Uzun, Burak and Tekinerdogan, Bedir},
  url={https://www.sciencedirect.com/science/article/pii/S0950584918300880},
  keywords={Systematic review, Software architecture, Model-based testing},
  abstract={Context Model-driven architecture based testing (MDABT) adopts architectural models of a system under test and/or its environment to derive test artifacts. In the literature, different MDABT approaches have been provided together with the corresponding lessons results and lessons learned. Objective The overall objective of this paper is to identify the published concerns for applying MDABT, identify the proposed solutions, and describe the current research directions for MDABT. Method To this end we have provided a systematic literature review (SLR) that is conducted by a multi-phase study selection process using the published literature in major software engineering journals and conference proceedings. Results We reviewed 739 papers that are discovered using a well-planned review protocol, and 31 of them were assessed as primary studies related to our research questions. Based on the analysis of the data extraction process, we discuss the primary trends and approaches and present the identified obstacles. Conclusion This study shows that although a generic process the approaches different in various ways with different goals, modeling abstractions and results. Further, based on the synthesis process in the SLR we can state that the potential of MDABT has not been fully exploited yet.}
}

@article{rayyan-727967561,
  title={Behavioral software engineering: A definition and systematic literature review},
  year={2015},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={107},
  pages={15-37},
  author={Lenberg, Per and Feldt, Robert and Wallgren, Lars Göran},
  url={https://www.sciencedirect.com/science/article/pii/S0164121215000989},
  keywords={Software engineering, Human aspects, Psychology, Software},
  abstract={Throughout the history of software engineering, the human aspects have repeatedly been recognized as important. Even though research that investigates them has been growing in the past decade, these aspects should be more generally considered. The main objective of this study is to clarify the research area concerned with human aspects of software engineering and to create a common platform for future research. In order to meet the objective, we propose a definition of the research area behavioral software engineering (BSE) and present results from a systematic literature review based on the definition. The result indicates that there are knowledge gaps in the research area of behavioral software engineering and that earlier research has been focused on a few concepts, which have been applied to a limited number of software engineering areas. The individual studies have typically had a narrow perspective focusing on few concepts from a single unit of analysis. Further, the research has rarely been conducted in collaboration by researchers from both software engineering and social science. Altogether, this review can help put a broader set of human aspects higher on the agenda for future software engineering research and practice.}
}

@article{rayyan-727967562,
  title={Taking the emotional pulse of software engineering — A systematic literature review of empirical studies},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={115},
  pages={23-43},
  author={Sánchez-Gordón, Mary and Colomo-Palacios, Ricardo},
  url={https://www.sciencedirect.com/science/article/pii/S0950584919301661},
  keywords={Systematic literature review, Affect, Behavioral software engineering, Emotion, Mood, Social aspects of software development, Software, Pulse},
  abstract={Context Over the past 50 years of Software Engineering, numerous studies have acknowledged the importance of human factors. However, software developers' emotions are still an area under investigation and debate that is gaining relevance in the software industry. Objective In this study, a systematic literature review (SLR) was carried out to identify, evaluate, and synthesize research published concerning software developers' emotions as well as the measures used to assess its existence. Method By searching five major bibliographic databases, authors identified 7172 articles related to emotions in Software Engineering. We selected 66 of these papers as primary studies. Then, they were analyzed in order to find empirical evidence of the intersection of emotions and software engineering. Results Studies report a total of 40 discrete emotions but the most frequent were: anger, fear, disgust, sadness, joy, love, and happiness. There are also 2 different dimensional approaches and 10 datasets related to this topic which are publicly available on the Web. The findings also showed that self-reported mood instruments (e.g., SAM, PANAS), physiological measures (e.g., heart rate, perspiration) or behavioral measures (e.g., keyboard use) are the least reported tools, although, there is a recognized intrinsic problem with the accuracy of current state of the art sentiment analysis tools. Moreover, most of the studies used software practitioners and/or datasets from industrial context as subjects. Conclusions The study of emotions has received a growing attention from the research community in the recent years, but the management of emotions has always been challenging in practice. Although it can be said that this field is not mature enough yet, our results provide a holistic view that will benefit researchers by providing the latest trends in this area and identifying the corresponding research gaps.}
}

@article{rayyan-727967563,
  title={Multiple fault localization of software programs: A systematic literature review},
  year={2020},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={124},
  pages={106312},
  author={Zakari, Abubakar and Lee, Sai Peck and Abreu, Rui and Ahmed, Babiker Hussien and Rasheed, Rasheed Abubakar},
  url={https://www.sciencedirect.com/science/article/pii/S0950584920300641},
  keywords={Fault localization, Multiple faults, One-bug-at-a-time (OBA), Parallel debugging, Program debugging, Software},
  abstract={Context Multiple fault localization (MFL) is the act of identifying the locations of multiple faults (more than one fault) in a faulty software program. This is known to be more complicated, tedious, and costly in comparison to the traditional practice of presuming that a software contains a single fault. Due to the increasing interest in MFL by the research community, a broad spectrum of MFL debugging approaches and solutions have been proposed and developed. Objective The aim of this study is to systematically review existing research on MFL in the software fault localization (SFL) domain. This study also aims to identify, categorize, and synthesize relevant studies in the research domain. Method Consequently, using an evidence-based systematic methodology, we identified 55 studies relevant to four research questions. The methodology provides a systematic selection and evaluation process with rigorous and repeatable evidence-based studies selection process. Result The result of the systematic review shows that research on MFL is gaining momentum with stable growth in the last 5 years. Three prominent MFL debugging approaches were identified, i.e. One-bug-at-a-time debugging approach (OBA), parallel debugging approach, and multiple-bug-at-a-time debugging approach (MBA), with OBA debugging approach being utilized the most. Conclusion The study concludes with some identified research challenges and suggestions for future research. Although MFL is becoming of grave concern, existing solutions in the field are less mature. Studies utilizing real faults in their experiments are scarce. Concrete solutions to reduce MFL debugging time and cost by adopting an approach such as MBA debugging approach are also less, which require more attention from the research community.}
}

@article{rayyan-727967564,
  title={Buhos: A web-based systematic literature review management software},
  year={2018},
  journal={SoftwareX},
  issn={2352-7110},
  volume={7},
  pages={360-372},
  author={Bustos Navarrete, Claudio and Morales Malverde, María Gabriela and Salcedo Lagos, Pedro and Díaz Mujica, Alejandro},
  url={https://www.sciencedirect.com/science/article/pii/S2352711018300293},
  keywords={Systematic literature review, Collaborative work, Ruby, Web-based software, Software},
  abstract={Software can significantly facilitate the management of the complete systematic literature review process (SLR). However, most specialized software for use in SLR processes is designed to meet the requirements of the health and medical sciences and software engineering, and there is a need for dedicated software for the specific research requirements of the social sciences. Furthermore, most of the software currently used is closed and the open source code alternatives require personnel with expertise in configuration and setup. We present Buhos, an application for managing the complete process of systematic literature reviews that is web-based and developed in Ruby. It offers functionalities for supporting the process of searching, screening, data extraction and reporting. Buhos can be used locally through an in-house web server, as well as in a distributed manner, integrated with other online services.}
}

@article{rayyan-727967565,
  title={Code smells and refactoring: A tertiary systematic review of challenges and observations},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={167},
  pages={110610},
  author={Lacerda, Guilherme and Petrillo, Fabio and Pimenta, Marcelo and Guéhéneuc, Yann Gaël},
  url={https://www.sciencedirect.com/science/article/pii/S0164121220300881},
  keywords={Code smells, Refactoring, Tertiary systematic review, Smell},
  abstract={Refactoring and smells have been well researched by the software-engineering research community these past decades. Several secondary studies have been published on code smells, discussing their implications on software quality, their impact on maintenance and evolution, and existing tools for their detection. Other secondary studies addressed refactoring, discussing refactoring techniques, opportunities for refactoring, impact on quality, and tools support. In this paper, we present a tertiary systematic literature review of previous surveys, secondary systematic literature reviews, and systematic mappings. We identify the main observations (what we know) and challenges (what we do not know) on code smells and refactoring. We perform this tertiary review using eight scientific databases, based on a set of five research questions, identifying 40 secondary studies between 1992 and 2018. We organize the main observations and challenges about code smell and their refactoring into: smells definitions, most common code-smell detection approaches, code-smell detection tools, most common refactoring, and refactoring tools. We show that code smells and refactoring have a strong relationship with quality attributes, i.e., with understandability, maintainability, testability, complexity, functionality, and reusability. We argue that code smells and refactoring could be considered as the two faces of a same coin. Besides, we identify how refactoring affects quality attributes, more than code smells. We also discuss the implications of this work for practitioners, researchers, and instructors. We identify 13 open issues that could guide future research work. Thus, we want to highlight the gap between code smells and refactoring in the current state of software-engineering research. We wish that this work could help the software-engineering research community in collaborating on future work on code smells and refactoring.}
}

@article{rayyan-727967566,
  title={Systematic literature reviews in software engineering—enhancement of the study selection process using Cohen's Kappa statistic},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={168},
  pages={110657},
  author={Pérez, Jorge and Díaz, Jessica and Garcia-Martin, Javier and Tabuenca, Bernardo},
  url={https://www.sciencedirect.com/science/article/pii/S0164121220301217},
  keywords={Systematic review, Evidence-based practice, Cohen's kappa, Software},
  abstract={Context: Systematic literature reviews (SLRs) rely on a rigorous and auditable methodology for minimizing biases and ensuring reliability. A common kind of bias arises when selecting studies using a set of inclusion/exclusion criteria. This bias can be decreased through dual revision, which makes the selection process more time-consuming and remains prone to generating bias depending on how each researcher interprets the inclusion/exclusion criteria. Objective: To reduce the bias and time spent in the study selection process, this paper presents a process for selecting studies based on the use of Cohen's Kappa statistic. We have defined an iterative process based on the use of this statistic during which the criteria are refined until obtain almost perfect agreement (k¿0.8). At this point, the two researchers interpret the selection criteria in the same way, and thus, the bias is reduced. Starting from this agreement, dual review can be eliminated; consequently, the time spent is drastically shortened. Method: The feasibility of this iterative process for selecting studies is demonstrated through a tertiary study in the area of software engineering on works that were published from 2005 to 2018. Results: The time saved in the study selection process was 28% (for 152 studies) and if the number of studies is sufficiently large, the time saved tend asymptotically to 50%. Conclusions: Researchers and students may take advantage of this iterative process for selecting studies when conducting SLRs to reduce bias in the interpretation of inclusion and exclusion criteria. It is especially useful for research with few resources.}
}

@article{rayyan-727967567,
  title={On the need to update systematic literature reviews},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={109},
  pages={40-42},
  author={Nepomuceno, Vilmar and Soares, Sergio},
  url={https://www.sciencedirect.com/science/article/pii/S0950584919300072},
  keywords={Systematic literature review, Systematic mapping, Evidence based software engineering, Tertiary studies, Updates},
  abstract={Context Many Systematic Literature Reviews (SLRs) were performed in the recent past, but just a few are being updated. Keeping SLRs updated is essential to prolong their lifespan. Objective To give a picture about how SLRs are being updated and what researchers think about SLRs updates. Method In this work, we present a Systematic Mapping (SM) study about SLRs updates and a survey with EBSE researchers that published their SLRs between 2011 and 2015. Results We included 22 studies in the SM, where 15 changed some artifact from the original study, including changes in research questions. We obtained 28 answers in our survey with SLRs authors that, in general, consolidate interpretations retrieved from the SM, but some answers did not. Conclusion SLRs may lose their impact over the years. Identifying actions to keep them updated is of great importance to SLR research field.}
}

@article{rayyan-727967568,
  title={Leveraging creativity in requirements elicitation within agile software development: A systematic literature review},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={157},
  pages={110396},
  author={Aldave, Ainhoa and Vara, Juan M and Granada, David and Marcos, Esperanza},
  url={https://www.sciencedirect.com/science/article/pii/S0164121219301712},
  keywords={Systematic review, Software development, Software project management, Requirements elicitation, Agile methodologies, Creative thinking, Software},
  abstract={Agile approaches tend to focus solely on scoping and simplicity rather than on problem solving and discovery. This hampers the development of innovative solutions. Additionally, little has been said about how to capture and represent the real user needs. To fill this gap, some authors argue in favor of the application of “Creative thinking” for requirements elicitation within agile software development. This synergy between creativeness and agility has arisen as a new means of bringing innovation and flexibility to increasingly demanding software. The aim of the present study is therefore to employ a systematic review to investigate the state-of-the-art of those approaches that leverage creativity in requirements elicitation within Agile Software Development, as well as the benefits, limitations and strength of evidence of these approaches. The review was carried out by following the guidelines proposed by Dr. Kitchenham. The search strategy identified 1451 studies, 17 of which were eventually classified as primary studies. The selected studies contained 13 different and unique proposals. These approaches provide evidence that enhanced creativity in requirements elicitation can be successfully implemented in real software projects. We specifically observed that projects related to user interface development, such as those for mobile or web applications, are good candidates for the use of these approaches. We have also found that agile methodologies such as Scrum, Extreme Programming or methodologies based on rapid modelling are preferred when introducing creativity into requirements elicitation. Despite this being a new research field, there is a mixture of techniques, tools and processes that have already been and are currently being successfully tested in industry. Finally, we have found that, although creativity is an important ingredient with which to bring about innovation, it is not always sufficient to generate new requirements because this needs to be followed by user engagement and a specific context in which proper conditions, such as flexibility, time or resources, have to be met.}
}

@article{rayyan-727967569,
  title={Software process simulation modeling: Systematic literature review},
  year={2020},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={70},
  pages={103425},
  author={García-García, J A and Enríquez, J G and Ruiz, M and Arévalo, C and Jiménez-Ramírez, A},
  url={https://www.sciencedirect.com/science/article/pii/S0920548919303095},
  keywords={Systematic literature review, Software engineering, Software process, Software Process Simulation Modeling, SPSM, Software},
  abstract={Changes and continuous progress in logistics and productive systems make the realization of improvements in decision making necessary. Simulation is a good support tool for this type of decisions because it allows reproducing processes virtually to study their behavior, to analyze the impact of possible changes or to compare different design alternatives without the high cost of scale experiments. Although process simulation is usually focused on industrial processes, over the last two decades, new proposals have emerged to bring simulation techniques into software engineering. This paper describes a Systematic Literature Review (SLR) which returned 8070 papers (published from 2013 to 2019) by a systematic search in 4 digital libraries. After conducting this SLR, 36 Software Process Simulation Modeling (SPSM) works were selected as primary studies and were documented following a specific characterization scheme. This scheme allows characterizing each proposal according to the paradigm used and its technology base as well as its future line of work. Our purpose is to identify trends and directions for future research on SPSM after identifying and studying which proposals in this topic have been defined and the relationships and dependencies between these proposals in the last five years. After finishing this review, it is possible to conclude that SPSM continues to be a topic that is very much addressed by the scientific community, but each contribution has been proposed with particular goals. This review also concludes that Agent-Based Simulation and System Dynamics paradigm is increasing and decreasing, respectively, its trend among SPSM proposals in the last five years. Regarding Discrete-Event Simulation paradigm, it seems that it is strengthening its position among research community in recent years to design new approaches.}
}

@article{rayyan-727967570,
  title={Systematic reviews in evidence-based software technology and software engineering},
  year={2005},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={47},
  number={1},
  pages={1},
  url={https://www.sciencedirect.com/science/article/pii/S0950584904001636},
  keywords={Software}
}

@article{rayyan-727967571,
  title={Scientific research ontology to support systematic review in software engineering},
  year={2007},
  journal={Advanced Engineering Informatics},
  issn={1474-0346},
  volume={21},
  number={2},
  pages={133-151},
  author={de Almeida Biolchini, Jorge Calmon and Mian, Paula Gomes and Natali, Ana Candida Cruz and Conte, Tayana Uchôa and Travassos, Guilherme Horta},
  url={https://www.sciencedirect.com/science/article/pii/S147403460600070X},
  keywords={Systematic review, Ontology, Experimental software engineering, Experimental study, Scientific method, Software},
  abstract={The term systematic review is used to refer to a specific methodology of research, developed in order to gather and evaluate the available evidence pertaining to a focused topic. It represents a secondary study that depends on primary study results to be accomplished. Several primary studies have been conducted in the field of Software Engineering in the last years, determining an increasing improvement in methodology. However, in most cases software is built with technologies and processes for which developers have insufficient evidence to confirm their suitability, limits, qualities, costs, and inherent risks. Conducting systematic reviews in Software Engineering consists in a major methodological tool to scientifically improve the validity of assertions that can be made in the field and, as a consequence, the reliability degree of the methods that are employed for developing software technologies and supporting software processes. This paper aims at discussing the significance of experimental studies, particularly systematic reviews, and their use in supporting software processes. A template designed to support systematic reviews in Software Engineering is presented, and the development of ontologies to describe knowledge regarding such experimental studies is also introduced.}
}

@article{rayyan-727967572,
  title={Testing and verification of neural-network-based safety-critical control software: A systematic literature review},
  year={2020},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={123},
  pages={106296},
  author={Zhang, Jin and Li, Jingyue},
  url={https://www.sciencedirect.com/science/article/pii/S0950584920300471},
  keywords={Systematic literature review, Neural network, Safety-critical control software, Software testing and verification, Neural Networks (Computer), Nerve Net, Software},
  abstract={Context: Neural Network (NN) algorithms have been successfully adopted in a number of Safety-Critical Cyber-Physical Systems (SCCPSs). Testing and Verification (T&V) of NN-based control software in safety-critical domains are gaining interest and attention from both software engineering and safety engineering researchers and practitioners. Objective: With the increase in studies on the T&V of NN-based control software in safety-critical domains, it is important to systematically review the state-of-the-art T&V methodologies, to classify approaches and tools that are invented, and to identify challenges and gaps for future studies. Method: By searching the six most relevant digital libraries, we retrieved 950 papers on the T&V of NN-based Safety-Critical Control Software (SCCS). Then we filtered the papers based on the predefined inclusion and exclusion criteria and applied snowballing to identify new relevant papers. Results: To reach our result, we selected 83 primary papers published between 2011 and 2018, applied the thematic analysis approach for analyzing the data extracted from the selected papers, presented the classification of approaches, and identified challenges. Conclusion: The approaches were categorized into five high-order themes, namely, assuring robustness of NNs, improving the failure resilience of NNs, measuring and ensuring test completeness, assuring safety properties of NN-based control software, and improving the interpretability of NNs. From the industry perspective, improving the interpretability of NNs is a crucial need in safety-critical applications. We also investigated nine safety integrity properties within four major safety lifecycle phases to investigate the achievement level of T&V goals in IEC 61508-3. Results show that correctness, completeness, freedom from intrinsic faults, and fault tolerance have drawn most attention from the research community. However, little effort has been invested in achieving repeatability, and no reviewed study focused on precisely defined testing configuration or defense against common cause failure.}
}

@article{rayyan-727967573,
  title={Landscaping systematic mapping studies in software engineering: A tertiary study},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={149},
  pages={396-436},
  author={Khan, Muhammad Uzair and Sherin, Salman and Iqbal, Muhammad Zohaib and Zahid, Rubab},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218302784},
  keywords={Tertiary study, Systematic mapping study, Software engineering, Survey, Secondary study, Software},
  abstract={Context A number of Systematic Mapping Studies (SMSs) that cover Software Engineering (SE) are reported in literature. Tertiary studies synthesize the secondary studies to provide a holistic view of an area. Objectives We synthesize SMSs in SE to provide insights into existing SE areas and to investigate the trends and quality of SMSs. Methodology We use Systematic Literature Review protocol to analyze and map the SMSs in SE, till August 2017, to SE Body of Knowledge (SWEBOK). Results We analyze 210 SMSs and results show that: (1) Software design and construction are most active areas in SE; (2) Some areas lack SMSs, including mathematical foundations, software configuration management, and SE tools; (3) The quality of SMSs is improving with time; (4) SMSs in journals have higher quality than SMSs in conferences and are cited more often; (5) Low quality in SMSs can be attributed to a lack of quality assessment in SMSs and not reporting information about the primary studies. Conclusion There is a potential for more SMSs in some SE areas. A number of SMSs do not provide the required information for an SMS, which leads to a low quality score.}
}

@article{rayyan-727967574,
  title={Analyzing the software architectures supporting HCI/HMI processes through a systematic review of the literature},
  year={2019},
  journal={Telematics and Informatics},
  issn={0736-5853},
  volume={38},
  pages={118-132},
  author={Cruz-Benito, Juan and García-Peñalvo, Francisco J and Therón, Roberto},
  url={https://www.sciencedirect.com/science/article/pii/S0736585318305392},
  keywords={Systematic Literature Review, Human-Computer Interaction, Human-Machine Interaction, Software Architectures, Software},
  abstract={Many researchers have dealt with Human-Computer Interaction or Human-Machine Interaction by building or designing software architectures that facilitate the users' interaction or recognize users' inputs to the generate proper responses. Many studies include these approaches in different research areas: from research in healthcare to mobile environments, robotics, etc. Interaction is seen as a critical concept, and the work for its improvement is a crucial factor for many platforms, systems, and business domains. The goal of this manuscript is to present a systematic review of the literature to identify, analyze and classify the published approaches to support or enhance Human-Computer Interaction or Human-Machine Interaction from the perspective of software architectures. The method followed is the systematic review following the guidelines related to Systematic Literature Reviews methods such as the one proposed by Kitchenham and other authors in the field of software engineering. As results, this study identified 39 papers that included software architectures to improve or analyze Human-Computer Interaction or Human-Machine Interaction. Three main approaches were found on software architectures: layered architectures, modular architectures, and architectures based on software agents, but they lacked standardization and were mainly ad-hoc solutions. The primary interfaces covered were those related to Graphical User Interfaces (GUIs) and multimodal/natural ones. The primary application domain detected were in multimodal systems. The main purpose of most of the papers was to support multimodal interaction. Some conclusions achieved are that the generic solutions to support or analyze HCI/HMI processes are still rare in the literature. Despite many works dealing with this topic and its issues and challenges, it is necessary to keep on improving the research in this area through the application of standard techniques and solutions, exploring new ways of analyzing and interpreting interaction, escaping from ad-hoc solutions or evaluating the solutions proposed.}
}

@article{rayyan-727967575,
  title={Software engineering process models for mobile app development: A systematic literature review},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={145},
  pages={98-111},
  author={Jabangwe, Ronald and Edison, Henry and Duc, Anh Nguyen},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218301638},
  keywords={Systematic literature review, Hybrid apps, Mobile application development, Mobile apps, Native apps, Software engineering process models, Software},
  abstract={Context: An effective development model can help improve competitive advantage and shorten release cycles, which is vital in the fast paced environment of mobile app development. Objective: The aim with this paper is to provide an extensive review of existing mobile app development models. Method: The review is done by following a systematic literature review process. Also presented is an assessment of the usefulness and relevance to industry of the models based on a rigor and relevance framework. Results: 20 primary studies were identified, each with distinct models. Agile methods or state-based principles are commonly adopted across the models. Relatively little effort focuses on deployment, maintenance, project evaluation activities. Conclusion: The review reveals that the contexts in which the identified models are intended to be used vary. This benefits practitioners as they are able to select a model that suits their contexts. However, the usefulness in industry of most of the models, based on the contexts in which the models were evaluated, is questionable. There is a need for evaluating mobile app models in contexts that resemble realistic contexts. The review also calls for further research addressing special constraints of mobile apps, e.g., testing apps on multiple-platforms, user involvement in release planning and continuous deployment.}
}

@article{rayyan-727967576,
  title={When to update systematic literature reviews in software engineering},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={167},
  pages={110607},
  author={Mendes, Emilia and Wohlin, Claes and Felizardo, Katia and Kalinowski, Marcos},
  url={https://www.sciencedirect.com/science/article/pii/S0164121220300856},
  keywords={Systematic literature reviews, Software engineering, Systematic literature review update, Software},
  abstract={[Context] Systematic Literature Reviews (SLRs) have been adopted by the Software Engineering (SE) community for approximately 15 years to provide meaningful summaries of evidence on several topics. Many of these SLRs are now potentially outdated, and there are no systematic proposals on when to update SLRs in SE. [Objective] The goal of this paper is to provide recommendations on when to update SLRs in SE. [Method] We evaluated, using a three-step approach, a third-party decision framework (3PDF) employed in other fields, to decide whether SLRs need updating. First, we conducted a literature review of SLR updates in SE and contacted the authors to obtain their feedback relating to the usefulness of the 3PDF within the context of SLR updates in SE. Second, we used these authors' feedback to see whether the framework needed any adaptation; none was suggested. Third, we applied the 3PDF to the SLR updates identified in our literature review. [Results] The 3PDF showed that 14 of the 20 SLRs did not need updating. This supports the use of a decision support mechanism (such as the 3PDF) to help the SE community decide when to update SLRs. [Conclusions] We put forward that the 3PDF should be adopted by the SE community to keep relevant evidence up to date and to avoid wasting effort with unnecessary updates.}
}

@article{rayyan-727967577,
  title={Ontology-based solutions for interoperability among product lifecycle management systems: A systematic literature review},
  year={2020},
  journal={Journal of Industrial Information Integration},
  issn={2452-414X},
  volume={20},
  pages={100176},
  author={Fraga, Alvaro Luis and Vegetti, Marcela and Leone, Horacio Pascual},
  url={https://www.sciencedirect.com/science/article/pii/S2452414X20300510},
  keywords={Ontology, Review, Interoperability, Product lifecycle management, Roles of ontology},
  abstract={During recent years, globalization has had an impact on the competitive capacity of industries, forcing them to integrate their productive processes with other, geographically distributed, facilities. This requires the information systems that support such processes to interoperate. Significant attention has been paid to the development of ontology-based solutions, which are meant to tackle issues from inconsistency to semantic interoperability and knowledge reusability. This paper looks into how the available technology, models and ontology-based solutions might interact within the manufacturing industry environment to achieve semantic interoperability among industrial information systems. Through a systematic literature review, this paper has aimed to identify the most relevant elements to consider in the development of an ontology-based solution and how these solutions are being deployed in industry. The research analyzed 54 studies in alignment with the specific requirements of our research questions. The most relevant results show that ontology-based solutions can be set up using OWL as the ontology language, Protégé as the ontology modeling tool, Jena as the application programming interface to interact with the built ontology, and different standards from the International Organization for Standardization Technical Committee 184, Subcommittee 4 or 5, to get the foundational concepts, axioms, and relationships to develop the knowledge base. We believe that the findings of this study make an important contribution to practitioners and researchers as they provide useful information about different projects and choices involved in undertaking projects in the field of industrial ontology application.}
}

@article{rayyan-727967578,
  title={Six years of systematic literature reviews in software engineering: An updated tertiary study},
  year={2011},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={53},
  number={9},
  pages={899-913},
  author={da Silva, Fabio Q B and Santos, André L M and Soares, Sérgio and França, A César C and Monteiro, Cleviton V F and Maciel, Felipe Farias},
  url={https://www.sciencedirect.com/science/article/pii/S0950584911001017},
  keywords={Software engineering, Systematic reviews, Mapping studies, Tertiary studies, Software},
  abstract={Context Since the introduction of evidence-based software engineering in 2004, systematic literature review (SLR) has been increasingly used as a method for conducting secondary studies in software engineering. Two tertiary studies, published in 2009 and 2010, identified and analysed 54 SLRs published in journals and conferences in the period between 1st January 2004 and 30th June 2008. Objective In this article, our goal was to extend and update the two previous tertiary studies to cover the period between 1st July 2008 and 31st December 2009. We analysed the quality, coverage of software engineering topics, and potential impact of published SLRs for education and practice. Method We performed automatic and manual searches for SLRs published in journals and conference proceedings, analysed the relevant studies, and compared and integrated our findings with the two previous tertiary studies. Results We found 67 new SLRs addressing 24 software engineering topics. Among these studies, 15 were considered relevant to the undergraduate educational curriculum, and 40 appeared of possible interest to practitioners. We found that the number of SLRs in software engineering is increasing, the overall quality of the studies is improving, and the number of researchers and research organisations worldwide that are conducting SLRs is also increasing and spreading. Conclusion Our findings suggest that the software engineering research community is starting to adopt SLRs consistently as a research method. However, the majority of the SLRs did not evaluate the quality of primary studies and fail to provide guidelines for practitioners, thus decreasing their potential impact on software engineering practice.}
}

@article{rayyan-727967579,
  title={On the generation of requirements specifications from software engineering models: A systematic literature review},
  year={2009},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={51},
  number={9},
  pages={1291-1307},
  author={Nicolás, Joaquín and Toval, Ambrosio},
  url={https://www.sciencedirect.com/science/article/pii/S0950584909000378},
  keywords={Systematic literature review, Requirements document generation from software engineering model, Specification generation from software engineering model, Textual requirements generation from software engineering model, Software},
  abstract={System and software requirements documents play a crucial role in software engineering in that they must both communicate requirements to clients in an understandable manner and define requirements in precise detail for system developers. The benefits of both lists of textual requirements (usually written in natural language) and software engineering models (usually specified in graphical form) can be brought together by combining the two approaches in the specification of system and software requirements documents. If, moreover, textual requirements are generated from models in an automatic or closely monitored form, the effort of specifying those requirements is reduced and the completeness of the specification and the management of the requirements traceability are improved. This paper presents a systematic review of the literature related to the generation of textual requirements specifications from software engineering models.}
}

@article{rayyan-727967580,
  title={Lessons from applying the systematic literature review process within the software engineering domain},
  year={2007},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={80},
  number={4},
  pages={571-583},
  author={Brereton, Pearl and Kitchenham, Barbara A and Budgen, David and Turner, Mark and Khalil, Mohamed},
  url={https://www.sciencedirect.com/science/article/pii/S016412120600197X},
  keywords={Systematic literature review, Empirical software engineering, Software},
  abstract={A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted. The basic systematic literature review process seems appropriate to software engineering and the preparation and validation of a review protocol in advance of a review activity is especially valuable. The paper highlights areas where some adaptation of the process to accommodate the domain-specific characteristics of software engineering is needed as well as areas where improvements to current software engineering infrastructure and practices would enhance its applicability. In particular, infrastructure support provided by software engineering indexing databases is inadequate. Also, the quality of abstracts is poor; it is usually not possible to judge the relevance of a study from a review of the abstract alone.}
}

@article{rayyan-727967581,
  title={Challenges and best practices in industry-academia collaborations in software engineering: A systematic literature review},
  year={2016},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={79},
  pages={106-127},
  author={Garousi, Vahid and Petersen, Kai and Ozkan, Baris},
  url={https://www.sciencedirect.com/science/article/pii/S0950584916301203},
  keywords={Systematic literature review, Software engineering, Challenges, Industry, Best practices, Industry-academia collaborations, Success patterns, Universities, Software},
  abstract={Context: The global software industry and the software engineering (SE) academia are two large communities. However, unfortunately, the level of joint industry-academia collaborations in SE is still relatively very low, compared to the amount of activity in each of the two communities. It seems that the two 'camps' show only limited interest/motivation to collaborate with one other. Many researchers and practitioners have written about the challenges, success patterns (what to do, i.e., how to collaborate) and anti-patterns (what not do do) for industry-academia collaborations. Objective: To identify (a) the challenges to avoid risks to the collaboration by being aware of the challenges, (b) the best practices to provide an inventory of practices (patterns) allowing for an informed choice of practices to use when planning and conducting collaborative projects. Method: A systematic review has been conducted. Synthesis has been done using grounded-theory based coding procedures. Results: Through thematic analysis we identified 10 challenge themes and 17 best practice themes. A key outcome was the inventory of best practices, the most common ones recommended in different contexts were to hold regular workshops and seminars with industry, assure continuous learning from industry and academic sides, ensure management engagement, the need for a champion, basing research on real-world problems, showing explicit benefits to the industry partner, be agile during the collaboration, and the co-location of the researcher on the industry side. Conclusion: Given the importance of industry-academia collaboration to conduct research of high practical relevance we provide a synthesis of challenges and best practices, which can be used by researchers and practitioners to make informed decisions on how to structure their collaborations.}
}

@article{rayyan-727967582,
  title={Systematic literature reviews in software engineering – A tertiary study},
  year={2010},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={52},
  number={8},
  pages={792-805},
  author={Kitchenham, Barbara and Pretorius, Rialette and Budgen, David and Pearl Brereton, O and Turner, Mark and Niazi, Mahmood and Linkman, Stephen},
  url={https://www.sciencedirect.com/science/article/pii/S0950584910000467},
  keywords={Tertiary study, Systematic literature review, Software engineering, Mapping study, Software},
  abstract={Context In a previous study, we reported on a systematic literature review (SLR), based on a manual search of 13 journals and conferences undertaken in the period 1st January 2004 to 30th June 2007. Objective The aim of this on-going research is to provide an annotated catalogue of SLRs available to software engineering researchers and practitioners. This study updates our previous study using a broad automated search. Method We performed a broad automated search to find SLRs published in the time period 1st January 2004 to 30th June 2008. We contrast the number, quality and source of these SLRs with SLRs found in the original study. Results Our broad search found an additional 35 SLRs corresponding to 33 unique studies. Of these papers, 17 appeared relevant to the undergraduate educational curriculum and 12 appeared of possible interest to practitioners. The number of SLRs being published is increasing. The quality of papers in conferences and workshops has improved as more researchers use SLR guidelines. Conclusion SLRs appear to have gone past the stage of being used solely by innovators but cannot yet be considered a main stream software engineering research methodology. They are addressing a wide range of topics but still have limitations, such as often failing to assess primary study quality.}
}

@article{rayyan-727967583,
  title={Similarity-based analyses on software applications: A systematic literature review},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={168},
  pages={110669},
  author={Auch, Maximilian and Weber, Manuel and Mandl, Peter and Wolff, Christian},
  url={https://www.sciencedirect.com/science/article/pii/S0164121220301278},
  keywords={Machine learning, Secondary study, Software similarity, Software},
  abstract={In empirical studies on processes, practices, and techniques of software engineering, automation and machine learning are gaining popularity. In order to extract knowledge from existing software projects, a sort of similarity analysis is often performed using different methodologies, data and metadata. This systematic literature review focuses therefore on existing approaches of similarity-, categorization- and relevance-based analysis on software applications. In total, 136 relevant publications and patents were identified between 2002 and 2019 according to the established inclusion and exclusion criteria, which perform a calculation of software similarity in general or to support certain software engineering phases.}
}

@article{rayyan-727967584,
  title={Motivation in software engineering: A systematic literature review},
  year={2008},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={50},
  number={9},
  pages={860-878},
  author={Beecham, Sarah and Baddoo, Nathan and Hall, Tracy and Robinson, Hugh and Sharp, Helen},
  url={https://www.sciencedirect.com/science/article/pii/S0950584907001097},
  keywords={Software Engineering, Systematic literature review, Motivation, Characteristics, Personality, Software Engineer, Software},
  abstract={Objective In this paper, we present a systematic literature review of motivation in Software Engineering. The objective of this review is to plot the landscape of current reported knowledge in terms of what motivates developers, what de-motivates them and how existing models address motivation. Methods We perform a systematic literature review of peer reviewed published studies that focus on motivation in Software Engineering. Systematic reviews are well established in medical research and are used to systematically analyse the literature addressing specific research questions. Results We found 92 papers related to motivation in Software Engineering. Fifty-six percent of the studies reported that Software Engineers are distinguishable from other occupational groups. Our findings suggest that Software Engineers are likely to be motivated according to three related factors: their ‘characteristics' (for example, their need for variety); internal ‘controls' (for example, their personality) and external ‘moderators' (for example, their career stage). The literature indicates that de-motivated engineers may leave the organisation or take more sick-leave, while motivated engineers will increase their productivity and remain longer in the organisation. Aspects of the job that motivate Software Engineers include problem solving, working to benefit others and technical challenge. Our key finding is that the published models of motivation in Software Engineering are disparate and do not reflect the complex needs of Software Engineers in their career stages, cultural and environmental settings. Conclusions The literature on motivation in Software Engineering presents a conflicting and partial picture of the area. It is clear that motivation is context dependent and varies from one engineer to another. The most commonly cited motivator is the job itself, yet we found very little work on what it is about that job that Software Engineers find motivating. Furthermore, surveys are often aimed at how Software Engineers feel about ‘the organisation', rather than ‘the profession'. Although models of motivation in Software Engineering are reported in the literature, they do not account for the changing roles and environment in which Software Engineers operate. Overall, our findings indicate that there is no clear understanding of the Software Engineers' job, what motivates Software Engineers, how they are motivated, or the outcome and benefits of motivating Software Engineers.}
}

@article{rayyan-727967585,
  title={On the application of search-based techniques for software engineering predictive modeling: A systematic review and future directions},
  year={2017},
  journal={Swarm and Evolutionary Computation},
  issn={2210-6502},
  volume={32},
  pages={85-109},
  author={Malhotra, Ruchika and Khanna, Megha and Raje, Rajeev R},
  url={https://www.sciencedirect.com/science/article/pii/S2210650216303418},
  keywords={Effort estimation, Software quality, Search-based techniques, Change prediction, Defect prediction, Maintainability prediction, Software},
  abstract={Software engineering predictive modeling involves construction of models, with the help of software metrics, for estimating quality attributes. Recently, the use of search-based techniques have gained importance as they help the developers and project-managers in the identification of optimal solutions for developing effective prediction models. In this paper, we perform a systematic review of 78 primary studies from January 1992 to December 2015 which analyze the predictive capability of search-based techniques for ascertaining four predominant software quality attributes, i.e., effort, defect proneness, maintainability and change proneness. The review analyses the effective use and application of search-based techniques by evaluating appropriate specifications of fitness functions, parameter settings, validation methods, accounting for their stochastic natures and the evaluation of developmental models with the use of well-known statistical tests. Furthermore, we compare the effectiveness of different models, developed using the various search-based techniques amongst themselves, and also with the prevalent machine learning techniques used in literature. Although there are very few studies which use search-based techniques for predicting maintainability and change proneness, we found that the results of the application of search-based techniques for effort estimation and defect prediction are encouraging. Hence, this comprehensive study and the associated results will provide guidelines to practitioners and researchers and will enable them to make proper choices for applying the search-based techniques to their specific situations.}
}

@article{rayyan-727967586,
  title={Reproducibility and credibility in empirical software engineering: A case study based on a systematic literature review of the use of the SZZ algorithm},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={99},
  pages={164-176},
  author={Rodríguez-Pérez, Gema and Robles, Gregorio and González-Barahona, Jesús M},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917304275},
  keywords={Systematic literature review, Reproducibility, Credibility, SZZ Algorithm, Case-Control Studies, Software, Algorithms},
  abstract={Context Reproducibility of Empirical Software Engineering (ESE) studies is an essential part for improving their credibility, as it offers the opportunity to the research community to verify, evaluate and improve their research outcomes. Objective We aim to study reproducibility and credibility in ESE with a case study, by investigating how they have been addressed in studies where SZZ, a widely-used algorithm by Śliwerski, Zimmermann and Zeller to detect the origin of a bug, has been applied. Methodology We have performed a systematic literature review to evaluate publications that use SZZ. In total, 187 papers have been analyzed for reproducibility, reporting of limitations and use of improved versions of the algorithm. Results We have found a situation with a lot of room for improvement in ESE as reproducibility is not commonly found; factors that undermine the credibility of results are common. We offer some lessons learned and guidelines for researchers and reviewers to address this problem. Conclusion Reproducibility and other related aspects that ensure a high quality scientific process should be taken more into consideration by the ESE community in order to increase the credibility of the research results.}
}

@article{rayyan-727967587,
  title={A systematic literature review of machine learning techniques for software maintainability prediction},
  year={2020},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={119},
  pages={106214},
  author={Alsolai, Hadeel and Roper, Marc},
  url={https://www.sciencedirect.com/science/article/pii/S0950584919302228},
  keywords={Systematic literature review, Machine learning, Dataset, Metric, Software maintainability prediction, Software},
  abstract={Context Software maintainability is one of the fundamental quality attributes of software engineering. The accurate prediction of software maintainability is a significant challenge for the effective management of the software maintenance process. Objective The major aim of this paper is to present a systematic review of studies related to the prediction of maintainability of object-oriented software systems using machine learning techniques. This review identifies and investigates a number of research questions to comprehensively summarize, analyse and discuss various viewpoints concerning software maintainability measurements, metrics, datasets, evaluation measures, individual models and ensemble models. Method The review uses the standard systematic literature review method applied to the most common computer science digital database libraries from January 1991 to July 2018. Results We survey 56 relevant studies in 35 journals and 21 conference proceedings. The results indicate that there is relatively little activity in the area of software maintainability prediction compared with other software quality attributes. CHANGE maintenance effort and the maintainability index were the most commonly used software measurements (dependent variables) employed in the selected primary studies, and most made use of class-level product metrics as the independent variables. Several private datasets were used in the selected studies, and there is a growing demand to publish datasets publicly. Most studies focused on regression problems and performed k-fold cross-validation. Individual prediction models were employed in the majority of studies, while ensemble models relatively rarely. Conclusion Based on the findings obtained in this systematic literature review, ensemble models demonstrated increased accuracy prediction over individual models, and have been shown to be useful models in predicting software maintainability. However, their application is relatively rare and there is a need to apply these, and other models to an extensive variety of datasets with the aim of improving the accuracy and consistency of results.}
}

@article{rayyan-727967588,
  title={A systematic review of quasi-experiments in software engineering},
  year={2009},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={51},
  number={1},
  pages={71-82},
  author={Kampenes, Vigdis By and Dybå, Tore and Hannay, Jo E and K. Sjøberg, Dag I},
  url={https://www.sciencedirect.com/science/article/pii/S0950584908000670},
  keywords={Empirical software engineering, Effect size, Field experiments, Quasi-experiments, Randomization, Selection bias, Software},
  abstract={Background: Experiments in which study units are assigned to experimental groups nonrandomly are called quasi-experiments. They allow investigations of cause–effect relations in settings in which randomization is inappropriate, impractical, or too costly. Problem outline: The procedure by which the nonrandom assignments are made might result in selection bias and other related internal validity problems. Selection bias is a systematic (not happening by chance) pre-experimental difference between the groups that could influence the results. By detecting the cause of the selection bias, and designing and analyzing the experiments accordingly, the effect of the bias may be reduced or eliminated. Research method: To investigate how quasi-experiments are performed in software engineering (SE), we conducted a systematic review of the experiments published in nine major SE journals and three conference proceedings in the decade 1993–2002. Results: Among the 113 experiments detected, 35% were quasi-experiments. In addition to field experiments, we found several applications for quasi-experiments in SE. However, there seems to be little awareness of the precise nature of quasi-experiments and the potential for selection bias in them. The term “quasi-experiment” was used in only 10% of the articles reporting quasi-experiments; only half of the quasi-experiments measured a pretest score to control for selection bias, and only 8% reported a threat of selection bias. On average, larger effect sizes were seen in randomized than in quasi-experiments, which might be due to selection bias in the quasi-experiments. Conclusion: We conclude that quasi-experimentation is useful in many settings in SE, but their design and analysis must be improved (in ways described in this paper), to ensure that inferences made from this kind of experiment are valid.}
}

@article{rayyan-727967589,
  title={Investigation on test effort estimation of mobile applications: Systematic literature review and survey},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={110},
  pages={56-77},
  author={Kaur, Anureet and Kaur, Kulwant},
  url={https://www.sciencedirect.com/science/article/pii/S095058491930031X},
  keywords={Software engineering, Mobile applications, Survey, Systematic literature review (SLR), Test effort estimation},
  abstract={Context In the last few years, the exigency of mobile devices has proliferated to prodigious heights. The process of developing the mobile software/application proceeds amidst testing phase to verify the correctness of the mobile app. The estimation of testing plays a vital role in the effective completion of testing. Objective To identify how estimation of test effort for mobile applications is distinct from other software via published literature and from mobile software organizations. Second is to recognize different issues in adapting traditional test estimation methods to the mobile domain and if suggestions from survey results could be helpful in providing an improved test estimation model for mobile applications. Method A systematic literature review is conducted followed by a survey through an online questionnaire filled from experienced mobile application developers and testers. Results The results from SLR cover identification of mobile app specific characteristics and reports test effort estimation techniques in the mobile domain. Findings from survey corroborate that a) Function Point/Test Point Analysis is highly adapted traditional test estimation technique to mobile domain; b) Challenges like uncertain requirements, no tool support for test estimation, complexity in testing, client miscommunication etc. are reported; c)Suggestions to improve test estimation process include proper test planning, adoption of agile methodology, healthier communication among client, developer, and tester etc.; d) On the basis of responses, Analytical Hierarchical Process (AHP) identifies “Diverse Devices and OS” along with “Type of App” as highly influential mobile app characteristic on the test estimation process. Conclusion Results conclude that the importance of identified mobile app characteristics from SLR cannot be ignored in the estimation process of mobile software testing. There might be a possibility to improve existing test estimation techniques for mobile apps by giving weight to mobile app specific characteristics and by considering suggestions from experienced developers and testers.}
}

@article{rayyan-727967590,
  title={Aligning software engineering education with industrial needs: A meta-analysis},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={156},
  pages={65-83},
  author={Garousi, Vahid and Giray, Görkem and Tüzün, Eray and Catal, Cagatay and Felderer, Michael},
  url={https://www.sciencedirect.com/science/article/pii/S0164121219301347},
  keywords={Meta-analysis, Systematic literature review (SLR), Important skills, Industry needs, Knowledge gap, Software engineering education, Software},
  abstract={Context According to various reports, many software engineering (SE) graduates often face difficulties when beginning their careers, which is mainly due to misalignment of the skills learned in university education with what is needed in the software industry. Objective Our objective is to perform a meta-analysis to aggregate the results of the studies published in this area to provide a consolidated view on how to align SE education with industry needs, to identify the most important skills and also existing knowledge gaps. Method To synthesize the body of knowledge, we performed a systematic literature review (SLR), in which we systematically selected a pool of 35 studies and then conducted a meta-analysis using data extracted from those studies. Results Via a meta-analysis and using data from 13 countries and over 4,000 data points, highlights of the SLR include: (1) software requirements, design, and testing are the most important skills; and (2) the greatest knowledge gaps are in configuration management, SE models and methods, SE process, design (and architecture), as well as in testing. Conclusion This paper provides implications for both educators and hiring managers by listing the most important SE skills and the knowledge gaps in the industry.}
}

@article{rayyan-727967591,
  title={Intelligent software engineering in the context of agile software development: A systematic literature review},
  year={2020},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={119},
  pages={106241},
  author={Perkusich, Mirko and Chaves e Silva, Lenardo and Costa, Alexandre and Ramos, Felipe and Saraiva, Renata and Freire, Arthur and Dilorenzo, Ednaldo and Dantas, Emanuel and Santos, Danilo and Gorgônio, Kyller and Almeida, Hyggo and Perkusich, Angelo},
  url={https://www.sciencedirect.com/science/article/pii/S0950584919302587},
  keywords={Artificial intelligence, Machine learning, Agile software development, Search-based software engineering, Bayesian networks, Intelligent software engineering, Software, Intelligence},
  abstract={CONTEXT: Intelligent Software Engineering (ISE) refers to the application of intelligent techniques to software engineering. We define an “intelligent technique” as a technique that explores data (from digital artifacts or domain experts) for knowledge discovery, reasoning, learning, planning, natural language processing, perception or supporting decision-making. OBJECTIVE: The purpose of this study is to synthesize and analyze the state of the art of the field of applying intelligent techniques to Agile Software Development (ASD). Furthermore, we assess its maturity and identify adoption risks. METHOD: Using a systematic literature review, we identified 104 primary studies, resulting in 93 unique studies. RESULTS: We identified that there is a positive trend in the number of studies applying intelligent techniques to ASD. Also, we determined that reasoning under uncertainty (mainly, Bayesian network), search-based solutions, and machine learning are the most popular intelligent techniques in the context of ASD. In terms of purposes, the most popular ones are effort estimation, requirements prioritization, resource allocation, requirements selection, and requirements management. Furthermore, we discovered that the primary goal of applying intelligent techniques is to support decision making. As a consequence, the adoption risks in terms of the safety of the current solutions are low. Finally, we highlight the trend of using explainable intelligent techniques. CONCLUSION: Overall, although the topic area is up-and-coming, for many areas of application, it is still in its infancy. So, this means that there is a need for more empirical studies, and there are a plethora of new opportunities for researchers.}
}

@article{rayyan-727967592,
  title={A systematic literature review on crowdsourcing in software engineering},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={153},
  pages={200-219},
  author={Sarı, Aslı and Tosun, Ayşe and Alptekin, Gülfem Işıklar},
  url={https://www.sciencedirect.com/science/article/pii/S0164121219300779},
  keywords={Systematic literature review, Empirical software engineering, Crowdsourcing, Crowdsourcing in software engineering, Software},
  abstract={Background Crowdsourcing outsources a task to large groups of people by open call format, and it recently plays significant role for software practitioners. Aim The purpose of this study is to conduct a comprehensive overview on crowdsourcing in software engineering (CSE), concerning business models, tools, platforms, software development processes, and software economics. Method We conducted a systematic literature review on CSE. We identified 158 relevant studies and 6 secondary studies. We further reviewed 67 primary studies that passed our quality assessment criteria. We defined 10 research questions and synthesized different approaches used in primary studies regarding each question. Results Majority of studies report the application of crowdsourcing for coding and testing tasks. Crowdsourcing follows a unique methodology in which project planning, task specification and deployment have more emphasis. There is not enough literature on effort estimation approaches in CSE and associated cost factors. Complexity of the task and its expected duration play significant role in estimation. Conclusions Future studies should focus more on economic models, experience reports, specific software development methodologies, and strategic pricing mechanism for CSE.}
}

@article{rayyan-727967593,
  title={Solutions in global software engineering: A systematic literature review},
  year={2013},
  journal={International Journal of Information Management},
  issn={0268-4012},
  volume={33},
  number={1},
  pages={119-132},
  author={Schneider, Stefan and Torkar, Richard and Gorschek, Tony},
  url={https://www.sciencedirect.com/science/article/pii/S0268401212000989},
  keywords={Systematic literature review, Process model, Solutions, Global software engineering, Distributed development, Solution, Software},
  abstract={Global software engineering (GSE) has received increased attention, as globalization enables and encourages increased distribution of product development. Many empirical studies and systematic literature reviews (SLRs) focus on the identification of challenges, this paper however presents the first SLR collecting and analyzing solutions associated with GSE, while also evaluating the level of empirical validation of said solutions. As a starting point the paper presents a GSE model, designed to categorize solutions into process areas, useful for the analysis of the research community's contributions to state-of-the-art and identifying fundamental gaps in research. In addition, the model categorizing the solutions is populated with references and good-examples, useful for practitioners, which can use the model to find solutions to overall challenges in various process areas. The overall results of the systematic review revealed more than 330 papers containing 127 solutions that were then identified and mapped to the model. The process areas of project management are highly populated, while other areas like product integration have received surprisingly little attention. In addition, selected process area is elaborated upon in terms of contents and deficiencies.}
}

@article{rayyan-727967594,
  title={Evidence-based software engineering},
  year={2016},
  journal={Perspectives on data science for software engineering},
  issn={978-0-12-804206-9},
  pages={149-153},
  author={Dybå, T and Bergersen, G R and Sjøberg, D I K and Menzies, Tim and Williams, Laurie and Zimmermann, Thomas},
  url={https://www.sciencedirect.com/science/article/pii/B9780128042069000295},
  publisher={Morgan Kaufmann},
  address={Boston},
  keywords={Contextualizing, Evidence-based software engineering (EBSE), GRADE, Reversing effect theory, Systematic literature reviews (SLRs), Software},
  abstract={A decade ago, Kitchenham, Dybå and Jørgensen coined the term and provided the foundations for evidence-based software engineering (EBSE). A trilogy of papers was written for researchers, practitioners, and educators. They suggested that practitioners consider EBSE as a mechanism to support and improve their technology adoption decisions, and that researchers should use systematic literature reviews as a methodology for performing unbiased aggregation of empirical results. This spurred significant international activity, and a renewed focus on research methods and theory, and on the future of empirical methods in SE research.}
}

@article{rayyan-727967595,
  title={What recommendation systems for software engineering recommend: A systematic literature review},
  year={2016},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={113},
  pages={101-113},
  author={Gasparic, Marko and Janes, Andrea},
  url={https://www.sciencedirect.com/science/article/pii/S0164121215002605},
  keywords={Systematic literature review, Recommendation system for software engineering, Software},
  abstract={A recommendation system for software engineering (RSSE) is a software application that provides information items estimated to be valuable for a software engineering task in a given context. Present the results of a systematic literature review to reveal the typical functionality offered by existing RSSEs, research gaps, and possible research directions. We evaluated 46 papers studying the benefits, the data requirements, the information and recommendation types, and the effort requirements of RSSE systems. We include papers describing tools that support source code related development published between 2003 and 2013. The results show that RSSEs typically visualize source code artifacts. They aim to improve system quality, make the development process more efficient and less expensive, lower developer's cognitive load, and help developers to make better decisions. They mainly support reuse actions and debugging, implementation, and maintenance phases. The majority of the systems are reactive. Unexploited opportunities lie in the development of recommender systems outside the source code domain. Furthermore, current RSSE systems use very limited context information and rely on simple models. Context-adapted and proactive behavior could improve the acceptance of RSSE systems in practice.}
}

@article{rayyan-727967596,
  title={Time pressure in software engineering: A systematic review},
  year={2020},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={121},
  pages={106257},
  author={Kuutila, Miikka and Mäntylä, Mika and Farooq, Umar and Claes, Maëlick},
  url={https://www.sciencedirect.com/science/article/pii/S0950584920300045},
  keywords={Software},
  abstract={Context Large project overruns and overtime work have been reported in the software industry, resulting in additional expense for companies and personal issues for developers. Experiments and case studies have investigated the relationship between time pressure and software quality and productivity. Objective The present work aims to provide an overview of studies related to time pressure in software engineering; specifically, existing definitions, possible causes, and metrics relevant to time pressure were collected, and a mapping of the studies to software processes and approaches was performed. Moreover, we synthesize results of existing quantitative studies on the effects of time pressure on software development, and offer practical takeaways for practitioners and researchers, based on empirical evidence. Method Our search strategy examined 5414 sources, found through repository searches and snowballing. Applying inclusion and exclusion criteria resulted in the selection of 102 papers, which made relevant contributions related to time pressure in software engineering. Results The majority of high quality studies report increased productivity and decreased quality under time pressure. The most frequent categories of studies focus on quality assurance, cost estimation, and process simulation. It appears that time pressure is usually caused by errors in cost estimation. The effect of time pressure is most often identified during software quality assurance. Conclusions The majority of empirical studies report increased productivity under time pressure, while the most cost estimation and process simulation models assume that compressing the schedule increases the total needed hours. We also find evidence of the mediating effect of knowledge on the effects of time pressure, and that tight deadlines impact tasks with an algorithmic nature more severely. Future research should better contextualize quantitative studies to account for the existing conflicting results and to provide an understanding of situations when time pressure is either beneficial or harmful.}
}

@article{rayyan-727967597,
  title={Threat analysis of software systems: A systematic literature review},
  year={2018},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={144},
  pages={275-294},
  author={Tuma, K and Calikli, G and Scandariato, R},
  url={https://www.sciencedirect.com/science/article/pii/S0164121218301304},
  keywords={Systematic literature review (SLR), Risk assessment, Security-by-design, Software systems, Threat analysis (modeling), Software},
  abstract={Architectural threat analysis has become an important cornerstone for organizations concerned with developing secure software. Due to the large number of existing techniques it is becoming more challenging for practitioners to select an appropriate threat analysis technique. Therefore, we conducted a systematic literature review (SLR) of the existing techniques for threat analysis. In our study we compare 26 methodologies for what concerns their applicability, characteristics of the required input for analysis, characteristics of analysis procedure, characteristics of analysis outcomes and ease of adoption. We also provide insight into the obstacles for adopting the existing approaches and discuss the current state of their adoption in software engineering trends (e.g. Agile, DevOps, etc.). As a summary of our findings we have observed that: the analysis procedure is not precisely defined, there is a lack of quality assurance of analysis outcomes and tool support and validation are limited.}
}

@article{rayyan-727967598,
  title={A critical appraisal tool for systematic literature reviews in software engineering},
  year={2019},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={112},
  pages={48-50},
  author={bin Ali, Nauman and Usman, Muhammad},
  url={https://www.sciencedirect.com/science/article/pii/S0950584919300771},
  keywords={Systematic literature reviews, Software engineering, Quality assessment, AMSTAR, Critical appraisal tools, Software},
  abstract={Context: Methodological research on systematic literature reviews (SLRs) in Software Engineering (SE) has so far focused on developing and evaluating guidelines for conducting systematic reviews. However, the support for quality assessment of completed SLRs has not received the same level of attention. Objective: To raise awareness of the need for a critical appraisal tool (CAT) for assessing the quality of SLRs in SE. To initiate a community-based effort towards the development of such a tool. Method: We reviewed the literature on the quality assessment of SLRs to identify the frequently used CATs in SE and other fields. Results: We identified that the CATs currently used is SE were borrowed from medicine, but have not kept pace with substantial advancements in the field of medicine. Conclusion: In this paper, we have argued the need for a CAT for quality appraisal of SLRs in SE. We have also identified a tool that has the potential for application in SE. Furthermore, we have presented our approach for adapting this state-of-the-art CAT for assessing SLRs in SE.}
}

@article{rayyan-727967599,
  title={A systematic literature review on the usage of eye-tracking in software engineering},
  year={2015},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={67},
  pages={79-107},
  author={Sharafi, Zohreh and Soh, Zéphyrin and Guéhéneuc, Yann-Gaël},
  url={https://www.sciencedirect.com/science/article/pii/S0950584915001196},
  keywords={Software engineering, Experiment, Eye-tracking, Software},
  abstract={Context Eye-tracking is a mean to collect evidence regarding some participants' cognitive processes. Eye-trackers monitor participants' visual attention by collecting eye-movement data. These data are useful to get insights into participants' cognitive processes during reasoning tasks. Objective The Evidence-based Software Engineering (EBSE) paradigm has been proposed in 2004 and, since then, has been used to provide detailed insights regarding different topics in software engineering research and practice. Systematic Literature Reviews (SLR) are also useful in the context of EBSE by bringing together all existing evidence of research and results about a particular topic. This SLR evaluates the current state of the art of using eye-trackers in software engineering and provides evidence on the uses and contributions of eye-trackers to empirical studies in software engineering. Method We perform a SLR covering eye-tracking studies in software engineering published from 1990 up to the end of 2014. To search all recognised resources, instead of applying manual search, we perform an extensive automated search using Engineering Village. We identify 36 relevant publications, including nine journal papers, two workshop papers, and 25 conference papers. Results The software engineering community started using eye-trackers in the 1990s and they have become increasingly recognised as useful tools to conduct empirical studies from 2006. We observe that researchers use eye-trackers to study model comprehension, code comprehension, debugging, collaborative interaction, and traceability. Moreover, we find that studies use different metrics based on eye-movement data to obtain quantitative measures. We also report the limitations of current eye-tracking technology, which threaten the validity of previous studies, along with suggestions to mitigate these limitations. Conclusion However, not withstanding these limitations and threats, we conclude that the advent of new eye-trackers makes the use of these tools easier and less obtrusive and that the software engineering community could benefit more from this technology.}
}

@article{rayyan-727967600,
  title={Task scheduling in big data platforms: A systematic literature review},
  year={2017},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={134},
  pages={170-189},
  author={Soualhia, Mbarka and Khomh, Foutse and Tahar, Sofiène},
  url={https://www.sciencedirect.com/science/article/pii/S0164121217301954},
  keywords={Systematic Literature Review, Hadoop, Mesos, Spark, Storm, Task Scheduling},
  abstract={Context: Hadoop, Spark, Storm, and Mesos are very well known frameworks in both research and industrial communities that allow expressing and processing distributed computations on massive amounts of data. Multiple scheduling algorithms have been proposed to ensure that short interactive jobs, large batch jobs, and guaranteed-capacity production jobs running on these frameworks can deliver results quickly while maintaining a high throughput. However, only a few works have examined the effectiveness of these algorithms. Objective: The Evidence-based Software Engineering (EBSE) paradigm and its core tool, i.e., the Systematic Literature Review (SLR), have been introduced to the Software Engineering community in 2004 to help researchers systematically and objectively gather and aggregate research evidences about different topics. In this paper, we conduct a SLR of task scheduling algorithms that have been proposed for big data platforms. Method: We analyse the design decisions of different scheduling models proposed in the literature for Hadoop, Spark, Storm, and Mesos over the period between 2005 and 2016. We provide a research taxonomy for succinct classification of these scheduling models. We also compare the algorithms in terms of performance, resources utilization, and failure recovery mechanisms. Results: Our searches identifies 586 studies from journals, conferences and workshops having the highest quality in this field. This SLR reports about different types of scheduling models (dynamic, constrained, and adaptive) and the main motivations behind them (including data locality, workload balancing, resources utilization, and energy efficiency). A discussion of some open issues and future challenges pertaining to improving the current studies is provided.}
}

@article{rayyan-727967601,
  title={Stakeholder quantification and prioritisation research: A systematic literature review},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={102},
  pages={85-99},
  author={Hujainah, Fadhl and Abu Bakar, Rohani Binti and Al-haimi, Basheer and Abdulgabber, Mansoor Abdullateef},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917302422},
  keywords={Systematic review, Stakeholders prioritisation, Stakeholders quantification},
  abstract={Context Stakeholder quantification and prioritisation (SQP) is executed to quantify and prioritise stakeholders of the system based on their impacts. Selecting and involving the appropriate stakeholders are considered one of the major factors for producing a successful system. Objective The objectives of this paper is to provide precise investigation regarding the SQP domain with respect to its impact on prioritising requirements, identifying SQP attributes, critically investigating the existing techniques, and presenting the challenges and recommended future works. Method The systematic literature review (SLR) guidelines proposed by Kitchenham are adopted to guide the review process. The identified related studies underwent a rigorous study selection process. Thus, 31 out of 210 identified studies were selected as primary studies to address adequately the formulated research questions. Results Findings demonstrate that SQP is a crucial process in requirement prioritisation (RP) because of its ability to identify stakeholders' impact on the systems requirements that lead to the production of a correctly prioritised list of requirements. Seventeen SQP attributes are revealed along with their description, usage impact, and degree of importance. Furthermore, nine techniques that focus on quantification and prioritisation of the stakeholders are identified and critically analysed in terms of their description, SQP process involved, SQP attributes used, types, and limitations. The findings reveal that these techniques face some challenges with respect to the lack of low-level implementation details, lack of automation and intelligence level, and heavy reliance on the involvement of experts. Conclusion SQP has been extensively discussed in stakeholder analysis and requirement prioritisation domains. Based on the findings, a new intelligent solution is suggested to minimise the need for expert participation in conducting the SQP process along with proposing measurement criteria for the attributes used to evaluate the stakeholders. The deficiency of research works regarding the selection of SQP techniques is also observed.}
}

@article{rayyan-727967602,
  title={Profiling of pornography addiction among children using EEG signals: A systematic literature review},
  year={2020},
  journal={Computers in Biology and Medicine},
  issn={0010-4825},
  volume={125},
  pages={103970},
  author={Kang, Xiaoxi and Handayani, Dini Oktarina Dwi and Chong, Pei Pei and Acharya, U Rajendra},
  url={https://www.sciencedirect.com/science/article/pii/S0010482520303024},
  keywords={Systematic literature review, Addiction, EEG, Pornography, Erotica, Only Child, Child, Electroencephalography},
  abstract={Nowadays human behavior has been affected with the advent of new digital technologies. Due to the rampant use of the Internet by children, many have been addicted to pornography. This addiction has negatively affected the behaviors of children including increased impulsiveness, learning ability to attention, poor decision-making, memory problems, and deficit in emotion regulation. The children with porn addiction can be identified by parents and medical practitioners as third-party observers. This systematic literature review (SLR) is conducted to increase the understanding of porn addiction using electroencephalogram (EEG) signals. We have searched five different databases namely IEEE, ACM, Science Direct, Springer and National Center for Biotechnology Information (NCBI) using addiction, porn, and EEG as keywords along with ‘OR ‘operation in between the expressions. We have selected 46 studies in this work by screening 815,554 papers from five databases. Our results show that it is possible to identify children with porn addiction using EEG signals.}
}

@article{rayyan-727967603,
  title={Introduction to the special section on enhancing credibility of empirical software engineering},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={99},
  pages={118-119},
  author={Madeyski, Lech and Kitchenham, Barbara and Wnuk, Krzysztof},
  url={https://www.sciencedirect.com/science/article/pii/S0950584918300557},
  keywords={systematic review, replication, empirical software engineering, reproducible research, research credibility, Software, Cesarean Section}
}

@article{rayyan-727967604,
  title={On the application of genetic programming for software engineering predictive modeling: A systematic review},
  year={2011},
  journal={Expert Systems with Applications},
  issn={0957-4174},
  volume={38},
  number={9},
  pages={11984-11997},
  author={Afzal, Wasif and Torkar, Richard},
  url={https://www.sciencedirect.com/science/article/pii/S0957417411004490},
  keywords={Systematic review, Modeling, Genetic programming, Symbolic regression, Software},
  abstract={The objective of this paper is to investigate the evidence for symbolic regression using genetic programming (GP) being an effective method for prediction and estimation in software engineering, when compared with regression/machine learning models and other comparison groups (including comparisons with different improvements over the standard GP algorithm). We performed a systematic review of literature that compared genetic programming models with comparative techniques based on different independent project variables. A total of 23 primary studies were obtained after searching different information sources in the time span 1995–2008. The results of the review show that symbolic regression using genetic programming has been applied in three domains within software engineering predictive modeling: (i) Software quality classification (eight primary studies). (ii) Software cost/effort/size estimation (seven primary studies). (iii) Software fault prediction/software reliability growth modeling (eight primary studies). While there is evidence in support of using genetic programming for software quality classification, software fault prediction and software reliability growth modeling; the results are inconclusive for software cost/effort/size estimation.}
}

@article{rayyan-727967605,
  title={Software product line applied to the internet of things: A systematic literature review},
  year={2020},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={124},
  pages={106293},
  author={Geraldi, Ricardo Theis and Reinehr, Sheila and Malucelli, Andreia},
  url={https://www.sciencedirect.com/science/article/pii/S0950584920300434},
  keywords={Software product line, Internet of things, Variability management, Families of systems, Product family engineering, Internet, Software},
  abstract={Context Internet of Things (IoT) is a promising paradigm due to the growing number of devices that may be connected, defined as “things”. Managing these “things” is still considered a challenge. One way to overcome this challenge may be by adopting the software product line (SPL) paradigm and the variability management (VM) activity. SPL engineering consists of mechanisms that provide identification, representation, and traceability, which may be helpful to “things” management supported by VM organizational and technical activities. Objective This research aims to investigate how SPL engineering has been applied along with the IoT paradigm, as well as how VM is being carried out. Method A systematic literature review (SLR) was conducted considering papers available until March 2019. This systematic review identified 1039 papers. After eliminating the duplicated titles and the ones not related to the review, 112 papers remained. The number of papers was narrowed to 56 after applying the exclusion criteria. Results The results provide evidence on the diversity of proposed SPLs used to specify approaches for managing IoT systems. However, most SPLs and research developed for IoT lack a systematic and detailed specification to ensure their quality, as well as tailoring guidelines for further use.}
}

@article{rayyan-727967606,
  title={Software project scheduling problem in the context of search-based software engineering: A systematic review},
  year={2019},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={155},
  pages={43-56},
  author={Rezende, Allan Vinicius and Silva, Leila and Britto, André and Amaral, Rodrigo},
  url={https://www.sciencedirect.com/science/article/pii/S0164121219301086},
  keywords={Systematic review, Search-based software engineering, Software project scheduling problem, Software},
  abstract={This work provides a systematic literature review of the software project scheduling problem, in the context of search-based software engineering, and summarizes the main models, techniques, search algorithms and evaluation criteria applied to solve this problem. We also discuss trends and research opportunities. Our keyword search found 438 papers, published in the last 20 years. After considering the inclusion and exclusion criteria and performing the snowballing procedure, we have analyzed 37 primary studies. The results show the predominance of the use of evolutionary algorithms. The static model, in which the scheduling is performed once during the project, is considered in the majority of the papers. Synthetic instances are commonly used to validate the heuristic and hypervolume and execution time are the mostly applied evaluating criteria.}
}

@article{rayyan-727967607,
  title={Software architectures of the convergence of cloud computing and the Internet of Things: A systematic literature review},
  year={2020},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={122},
  pages={106271},
  author={Banijamali, Ahmad and Pakanen, Olli-Pekka and Kuvaja, Pasi and Oivo, Markku},
  url={https://www.sciencedirect.com/science/article/pii/S0950584920300215},
  keywords={Cloud computing, Fog computing, Software architecture, Edge computing, Complex systems, Internet of Things (IoT), Internet, Software},
  abstract={Context Over the last few years, there has been an increasing interest in the convergence of cloud computing and the Internet of Things (IoT). Although software systems in this domain have attracted researchers to develop a large body of knowledge on software architecture designs, there is no systematic analysis of this knowledge. Objective This study aims to identify and synthesise state-of-the-art architectural elements including the design patterns, styles, views, quality attributes, and evaluation methodologies in the convergence of cloud computing and IoT. Method We used systematic literature review (SLR) methodology for a detailed analysis of 82 primary studies of a total of 1618 studies. Results We extracted six architectural design patterns in this domain; among them, edge connectivity patterns stand out as the most popular choice. The service-oriented architecture is the most frequently applied style in this context. Among all applicable quality attributes, scalability, timeliness, and security were the most investigated quality attributes. In addition, we included nine cross analyses to address the relationship between architectural patterns, styles, views, and evaluation methodologies with respect to different quality attributes and application areas. Conclusions Our findings indicate that research on software architectures in this domain is increasing. Although few studies were found in which industrial evaluations were presented, industry requires more scientific and empirically validated design frameworks to guide software engineering in this domain. This work provides an overview of the field while identifying areas for future research.}
}

@article{rayyan-727967608,
  title={Systematic literature reviews in agile software development: A tertiary study},
  year={2017},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={85},
  pages={60-70},
  author={Hoda, Rashina and Salleh, Norsaremah and Grundy, John and Tee, Hui Mien},
  url={https://www.sciencedirect.com/science/article/pii/S0950584917300538},
  keywords={Tertiary study, Systematic literature reviews, Agile software development, Mapping study, Software},
  abstract={Context A number of systematic literature reviews and mapping studies (SLRs) covering numerous primary research studies on various aspects of agile software development (ASD) exist. Objective The aim of this paper is to provide an overview of the SLRs on ASD research topics for software engineering researchers and practitioners. Method We followed the tertiary study guidelines by Kitchenham et al. to find SLRs published between late 1990s to December 2015. Results We found 28 SLRs focusing on ten different ASD research areas: adoption, methods, practices, human and social aspects, CMMI, usability, global software engineering (GSE), organizational agility, embedded systems, and software product line engineering. The number of SLRs on ASD topics, similar to those on software engineering (SE) topics in general, is on the rise. A majority of the SLRs applied standardized guidelines and the quality of these SLRs on ASD topics was found to be slightly higher for journal publications than for conferences. While some individuals and institutions seem to lead this area, the spread of authors and institutions is wide. With respect to prior review recommendations, significant progress was noticed in the area of connecting agile to established domains such as usability, CMMI, and GSE; and considerable progress was observed in focusing on management-oriented approaches as Scrum and sustaining ASD in different contexts such as embedded systems. Conclusion SLRs of ASD studies are on the rise and cover a variety of ASD aspects, ranging from early adoption issues to newer applications of ASD such as in product line engineering. ASD research can benefit from further primary and secondary studies on evaluating benefits and challenges of ASD methods, agile hybrids in large-scale setups, sustainability, motivation, teamwork, and project management; as well as a fresh review of empirical studies in ASD to cover the period post 2008.}
}

@article{rayyan-727967609,
  title={Requirements engineering for safety-critical systems: A systematic literature review},
  year={2016},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={75},
  pages={71-89},
  author={Martins, Luiz Eduardo G and Gorschek, Tony},
  url={https://www.sciencedirect.com/science/article/pii/S0950584916300568},
  keywords={Systematic literature review, Requirements engineering, Safety-critical systems, Accident, Hazard, Safety requirements},
  abstract={Context Safety-Critical Systems (SCS) are becoming increasingly present in our society. A considerable amount of research effort has been invested into improving the SCS requirements engineering process as it is critical to the successful development of SCS and, in particular, the engineering of safety aspects. Objective This article aims to investigate which approaches have been proposed to elicit, model, specify and validate safety requirements in the context of SCS, as well as to what extent such approaches have been validated in industrial settings. The paper will also investigate how the usability and usefulness of the reported approaches have been explored, and to what extent they enable requirements communication among the development project/team actors in the development of SCS. Method We conducted a systematic literature review by selecting 151 papers published between 1983 and 2014. The research methodology to conduct the SLR was based on the guidelines proposed by Kitchenham and Biolchini. Results The results of this systematic review should encourage further research into the design of studies to improve the requirements engineering for SCS, particularly to enable the communication of the safety requirements among the project team actors, and the adoption of other models for hazard and accident models. The presented results point to the need for more industry-oriented studies, particularly with more participation of practitioners in the validation of new approaches. Conclusion The most relevant findings from this review and their implications for further research are as follows: integration between requirements engineering and safety engineering areas; dominance of the traditional approaches; early mortality of new approaches; need for industry validation; lack of evidence for the usefulness and usability of most approaches; and the lack of studies that investigate how to improve the communication process throughout the lifecycle. Based on the findings, we suggest a research agenda to the community of researchers and advices to SCS practitioners.}
}

@article{rayyan-727967610,
  title={A systematic literature review on requirement prioritization techniques and their empirical evaluation},
  year={2020},
  journal={Computer Standards & Interfaces},
  issn={0920-5489},
  volume={69},
  pages={103389},
  author={Bukhsh, Faiza Allah and Bukhsh, Zaharah Allah and Daneva, Maya},
  url={https://www.sciencedirect.com/science/article/pii/S0920548919300789},
  keywords={Systematic literature review, Requirements engineering, Empirical study, Requirements prioritization, Empirical research method},
  abstract={[Context and Motivation] Many requirements prioritization approaches have been proposed, however not all of them have been investigated empirically in real-life settings. As a result, our knowledge of their applicability and actual use is incomplete. [Question/problem] A 2007 systematic review on requirements prioritization mapped out the landscape of proposed prioritization approaches and their prioritization criteria. To understand how this sub-field of requirements engineering has developed since 2007 and what evidence has been accumulated through empirical evaluations, we carried out a literature review that takes as input publications published between 2007 and 2019. [Principle ideas/results] We evaluated 102 papers that proposed and/or evaluated requirements prioritization methods. Our results show that the newly proposed requirements prioritization methods tend to use as basis fuzzy logic and machine learning algorithms. We also concluded that the Analytical Hierarchy Process is the most accurate and extensively used requirement prioritization method in industry. However, scalability is still its major limitation when requirements are large in number. We have found that machine learning has shown potential to deal with this limitation. Last, we found that experiments were the most used research method to evaluate the various aspects of the proposed prioritization approaches. [Contribution] This paper identified and evaluated requirements prioritization techniques proposed between 2007 and 2019, and derived some trends. Limitations of the proposals and implications for research and practice are identified as well.}
}

@article{rayyan-727967611,
  title={Attack surface definitions: A systematic literature review},
  year={2018},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={104},
  pages={94-103},
  author={Theisen, Christopher and Munaiah, Nuthan and Al-Zyoud, Mahran and Carver, Jeffrey C and Meneely, Andrew and Williams, Laurie},
  url={https://www.sciencedirect.com/science/article/pii/S0950584918301514},
  keywords={Systematic literature review, Software engineering, Attack surface, Vulnerabilities},
  abstract={Context Michael Howard conceptualized the attack surface of a software system as a metaphor for risk assessment during the development and maintenance of software. While the phrase attack surface is used in a variety of contexts in cybersecurity, professionals have different conceptions of what the phrase means. Objective The goal of this systematic literature review is to aid researchers and practitioners in reasoning about security in terms of attack surface by exploring various definitions of the phrase attack surface. Method We reviewed 644 works from prior literature, including research papers, magazine articles, and technical reports, that use the phrase attack surface and categorized them into those that provided their own definition; cited another definition; or expected the reader to intuitively understand the phrase. Results In our study, 71% of the papers used the phrase without defining it or citing another paper. Additionally, we found six themes of definitions for the phrase attack surface. Conclusion Based on our analysis, we recommend practitioners choose a definition of attack surface appropriate for their domain based on the six themes we identified in our study.}
}

@article{rayyan-727967612,
  title={Crowdsourced software testing: A systematic literature review},
  year={2020},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={127},
  pages={106363},
  author={Alyahya, Sultan},
  url={https://www.sciencedirect.com/science/article/pii/S0950584920301312},
  keywords={Software testing, Systematic literature review, Empirical software engineering, Crowdsourcing, Crowdsourced software testing, Software},
  abstract={Context Crowdsourced software testing (CST) refers to the use of crowdsourcing techniques in the domain of software testing. CST is an emerging area with its applications rapidly increasing in the last decade. Objective A comprehensive review on CST has been conducted to determine the current studies aiming to improve and assess the value of using CST as well as the challenges identified by these evaluation studies. Method We conducted a systematic literature review on CST by searching six popular databases. We identified 50 primary studies that passed our quality assessment criteria and defined two research questions covering the aim of the study. Results There are three main process activities that the current literature aims to improve, namely selection of suitable testers, reporting of defects, and validation of submitted defects. In addition, there are 23 CST evaluation studies and most of them involve a large group and real crowd. These studies have identified 27 different challenges encountered during the application of crowdsourcing in software testing. Conclusions The improvements achieved for the specific process activities in CST help explore other unexplored process activities. Similarly, knowing the characteristics of the evaluation studies can direct us on what other studies are worth investigating. Additionally, many of the challenges identified by the evaluation studies represent research problems that need better understanding and alternative solutions. This research also offers opportunities for practitioners to understand and apply new solutions proposed in the literature and the variations between them. Moreover, it provides awareness to the related parties regarding the challenges reported in the literature, which they may encounter during CST tasks.}
}

@article{rayyan-727967613,
  title={Empirical software product line engineering: A systematic literature review},
  year={2020},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={128},
  pages={106389},
  author={Chacón-Luna, Ana Eva and Gutiérrez, Antonio Manuel and Galindo, José A and Benavides, David},
  url={https://www.sciencedirect.com/science/article/pii/S0950584920301555},
  keywords={Software product lines, Systematic literature review, Case study, Experiment, Empirical strategies, Software},
  abstract={Context: The adoption of Software Product Line Engineering (SPLE) is usually only based on its theoretical benefits instead of empirical evidences. In fact, there is no work that synthesizes the empirical studies on SPLE. This makes it difficult for researchers to base their contributions on previous works validated with an empirical strategy. Objective: The objective of this work is to discover and summarize the studies that have used empirical evidences in SPLE limited to those ones with the intervention of humans. This will allow evaluating the quality and to know the scope of these studies over time. Doing so, research opportunities can arise Methods: A systematic literature review was conducted. The scope of the work focuses on those studies in which there is human intervention and were published between 2000 and 2018. We considered peer-reviewed papers from journals and top software engineering conferences. Results: Out of a total of 1880 studies in the initial set, a total of 62 primary studies were selected after applying a series of inclusion and exclusion criteria. We found that, approximately 56% of the studies used the empirical case study strategy while the rest used experimental strategies. Around 86% of the case studies were performed in an industrial environment showing the penetration of SPLE in industry. Conclusion: The interest of empirical studies has been growing since 2008. Around 95.16% of the studies address aspects related to domain engineering while application engineering received less attention. Most of the experiments and case study evaluated showed an acceptable level of quality. The first study found dates from 2005 and since then, the interest in the empirical SPLE has increased.}
}

@article{rayyan-727967614,
  title={A systematic literature review of model-driven security engineering for cyber–physical systems},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={169},
  pages={110697},
  author={Geismann, Johannes and Bodden, Eric},
  url={https://www.sciencedirect.com/science/article/pii/S0164121220301461},
  keywords={Systematic literature review, Cyber–physical systems, Literature survey, Model-driven security, Platform-specific, Security modeling},
  abstract={The last years have elevated the importance of cyber–physical systems like IoT applications, smart cars, or industrial control systems, and, therefore, these systems have also come into the focus of attackers. In contrast to software products running on PCs or smartphones, updating and maintaining cyber–physical systems presents a major challenge. This challenge, combined with the often decades-long lifetime of cyber–physical systems, and with their deployment in often safety-critical contexts, makes it particularly important to consider their security already at design time. When aiming to obtain a provably secure design, model-driven security approaches are key, as they allow to identify and mitigate threats in early phases of the development. As attacks may exploit both code-level as well as physical vulnerabilities, such approaches must consider not just the cyber layer but the physical layer as well. To find out which model-driven security approaches for cyber–physical systems exist considering both layers, we conducted a systematic literature review. From a set of 1160 initial papers, we extracted 69 relevant publications describing 17 candidate approaches. We found seven approaches specifically developed for cyber–physical systems. We provide a comprehensive description of these approaches, discuss them in particular detail, and determine their limitations. We found out that model-driven security is a relevant research area but most approaches focus only on specific security properties and even for CPS-specific approaches the platform is only rarely taken into account.}
}

@article{rayyan-727967615,
  title={Model composition in model driven engineering: A systematic literature review},
  year={2020},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={125},
  pages={106316},
  author={Abouzahra, Anas and Sabraoui, Ayoub and Afdel, Karim},
  url={https://www.sciencedirect.com/science/article/pii/S0950584920300689},
  keywords={Systematic literature review, Model composition, Model Driven Engineering},
  abstract={Context Model Driven Engineering (MDE) aims to alleviate complexity and improve reusability in software development. The development of complex software implies to divide it into independent parts before then assembled. This is how the problem of model composition has become an interesting and stills an emerging topic in MDE. Objective Our goal is to analyze the current state of the art in model composition in the context of Model Driven Engineering. Method We use the systematic literature review based on the guidelines proposed by Biolchini et al., Brereton et al., and Kitchenham and Charters. We propose five research questions and six quality assessments. Results Of the 9270 search results, 56 have been considered relevant studies. These studies have resulted in 36 primary studies. Conclusion The evaluation shows that most of approaches allow more than two models as inputs of the composition, allow composing heterogeneous models and enable the tuning of the composition schema, while the important limitations are about the maturity of implementations and the lack on the management of future evolutions or backwards compatibility.}
}

@article{rayyan-727967616,
  title={On the performance of hybrid search strategies for systematic literature reviews in software engineering},
  year={2020},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={123},
  pages={106294},
  author={Mourão, Erica and Pimentel, João Felipe and Murta, Leonardo and Kalinowski, Marcos and Mendes, Emilia and Wohlin, Claes},
  url={https://www.sciencedirect.com/science/article/pii/S0950584920300446},
  keywords={Software Engineering, Systematic literature review, Snowballing, Database Search, Search Strategy, Software},
  abstract={Context When conducting a Systematic Literature Review (SLR), researchers usually face the challenge of designing a search strategy that appropriately balances result quality and review effort. Using digital library (or database) searches or snowballing alone may not be enough to achieve high-quality results. On the other hand, using both digital library searches and snowballing together may increase the overall review effort. Objective The goal of this research is to propose and evaluate hybrid search strategies that selectively combine database searches with snowballing. Method We propose four hybrid search strategies combining database searches in digital libraries with iterative, parallel, or sequential backward and forward snowballing. We simulated the strategies over three existing SLRs in SE that adopted both database searches and snowballing. We compared the outcome of digital library searches, snowballing, and hybrid strategies using precision, recall, and F-measure to investigate the performance of each strategy. Results Our results show that, for the analyzed SLRs, combining database searches from the Scopus digital library with parallel or sequential snowballing achieved the most appropriate balance of precision and recall. Conclusion We put forward that, depending on the goals of the SLR and the available resources, using a hybrid search strategy involving a representative digital library and parallel or sequential snowballing tends to represent an appropriate alternative to be used when searching for evidence in SLRs.}
}

@article{rayyan-727967617,
  title={Obstacles and features of farm management information systems: A systematic literature review},
  year={2019},
  journal={Computers and Electronics in Agriculture},
  issn={0168-1699},
  volume={157},
  pages={189-204},
  author={Tummers, J and Kassahun, A and Tekinerdogan, B},
  url={https://www.sciencedirect.com/science/article/pii/S0168169918307944},
  keywords={Systematic literature review, Farm Management Information System, Features of FMIS, Obstacles to FMIS, Information Systems},
  abstract={Various Farm Management Information Systems (FMISs) have been developed to support the management of the farm businesses. These FMISs typically support the different domains of the agricultural sector, such as arable and dairy farming; and include different set of features, such as crop, field, and financial management. These FMISs also have to deal with diverse obstacles during their development and adoption, such as lack of standardized data, cost and usability. Though several papers have been published in the past several years on this topic, there has been no explicit attempt to systematically review these papers to identify and characterize the features and obstacles. The objective of this study is to identify and describe the state-of-the-art of FMISs and as such pave the way for further research and development of FMISs. We applied a systematic literature review protocol in which we included the literature published from 2008 to 2018. We found 1048 papers of which 38 papers were selected as primary studies that we analyzed further in detail. From the detailed analysis, we identified 81 unique FMIS features and 51 unique obstacles of FMISs. We have systematically ranked the identified features and obstacles and describe the key associated aspects. These aspects include the agricultural domains, modeling approaches, delivery models, and identified stakeholders.}
}

@article{rayyan-727967618,
  title={A systematic literature review on semantic web enabled software testing},
  year={2020},
  journal={Journal of Systems and Software},
  issn={0164-1212},
  volume={162},
  pages={110485},
  author={Dadkhah, Mahboubeh and Araban, Saeed and Paydar, Samad},
  url={https://www.sciencedirect.com/science/article/pii/S0164121219302596},
  keywords={Software testing, Systematic literature review, Ontology, Semantic web, Test generation, Software, Semantics},
  abstract={Software testing, as a major verification and validation activity which revolves around quality tests, is a knowledge-intensive activity. Hence, it is reasonable to expect that it can be improved by effective application of semantic web technologies, e.g., ontologies, which have been frequently used in knowledge engineering activities. The objective of this work is to investigate and provide a better understanding of how semantic web enabled techniques, i.e., the techniques that are based on the effective application of the semantic web technologies, have been used to support software testing activities. For this purpose, a Systematic Literature Review based on a predefined procedure is conducted. A total of 52 primary studies were identified as relevant, which have undergone a thorough meta-analysis with regards to our posed research questions. This study indicates the benefits of semantic web enabled software testing in both industry and academia. It also identifies main software testing activities that can benefit from the semantic web enabled techniques. Furthermore, contributions of such techniques to the testing process are thoroughly examined. Finally, potentials and difficulties of applying these techniques to software testing, along with the promising research directions are discussed.}
}

@article{rayyan-727967619,
  title={Data preprocessing for heart disease classification: A systematic literature review},
  year={2020},
  journal={Computer Methods and Programs in Biomedicine},
  issn={0169-2607},
  volume={195},
  pages={105635},
  author={Benhar, H and Idri, A and Fernández-Alemán, J L},
  url={https://www.sciencedirect.com/science/article/pii/S0169260720314681},
  keywords={Literature review, Data preprocessing, Cardiac datasets, Cardiology, Datamining},
  abstract={Context Early detection of heart disease is an important challenge since 17.3 million people yearly lose their lives due to heart diseases. Besides, any error in diagnosis of cardiac disease can be dangerous and risks an individual's life. Accurate diagnosis is therefore critical in cardiology. Data Mining (DM) classification techniques have been used to diagnosis heart diseases but still limited by some challenges of data quality such as inconsistencies, noise, missing data, outliers, high dimensionality and imbalanced data. Data preprocessing (DP) techniques were therefore used to prepare data with the goal of improving the performance of heart disease DM based prediction systems. Objective The purpose of this study is to review and summarize the current evidence on the use of preprocessing techniques in heart disease classification as regards: (1) the DP tasks and techniques most frequently used, (2) the impact of DP tasks and techniques on the performance of classification in cardiology, (3) the overall performance of classifiers when using DP techniques, and (4) comparisons of different combinations classifier-preprocessing in terms of accuracy rate. Method A systematic literature review is carried out, by identifying and analyzing empirical studies on the application of data preprocessing in heart disease classification published in the period between January 2000 and June 2019. A total of 49 studies were therefore selected and analyzed according to the aforementioned criteria. Results The review results show that data reduction is the most used preprocessing task in cardiology, followed by data cleaning. In general, preprocessing either maintained or improved the performance of heart disease classifiers. Some combinations such as (ANN + PCA), (ANN + CHI) and (SVM + PCA) are promising terms of accuracy. However the deployment of these models in real-world diagnosis decision support systems is subject to several risks and limitations due to the lack of interpretation.}
}

@article{rayyan-727967620,
  title={Systematic literature reviews in software engineering},
  year={2013},
  journal={Information and Software Technology},
  issn={0950-5849},
  volume={55},
  number={6},
  pages={919-920},
  author={Wohlin, Claes and Prikladnicki, Rafael},
  url={https://www.sciencedirect.com/science/article/pii/S0950584913000359},
  keywords={Software}
}

@article{rayyan-727967621,
  title={Experiences in using practitioner's checklists to evaluate the industrial relevance of requirements engineering experiments},
  year={2018},
  issn={978-1-4503-5736-4},
  pages={5-12},
  author={Daneva, Maya and Sikkel, Klaas and Condori-Fernandez, Nelly and Herrmann, Andrea},
  url={https://doi.org/10.1145/3193965.3193966},
  publisher={Association for Computing Machinery},
  series={CESI '18},
  keywords={experiments, practitioner's checklist, reliability, requirement specifications, validation},
  abstract={Background: A grand challenge for Requirement Engineering (RE) research is to help practitioners understand which RE methods work in what contexts and why. RE researchers recognize that for an RE method to be adopted in industry, RE practitioners should be able to evaluate the relevance of empirical studies to their practice. One possible approach to relevance evaluation is the set of perspective-based checklists proposed by Kitchenham et al. Specifically, the checklist from the practitioner's perspective seems to be a good candidate for evaluating the relevance of RE studies to RE practice. However, little is known about the applicability of this checklist to the RE field. Moreover, this checklist also requires a deeper analysis of its reliability. Aim: We propose a perspective-based checklist to the RE community that allows evaluating the relevance of experimental studies in RE from the practitioner's/consultant's viewpoint. Method: We followed an iterative design-science based approach in which we first analyzed the problems with a previously published checklist and then developed an operationalized proposal for a new checklist to counter these problems. We performed a reliability evaluation of this new checklist by having two practitioners apply the checklist on 24 papers that report experimental results on software requirements specifications' comprehensibility. Results: We report first-hand experiences of practitioners in evaluating the relevance of primary studies in RE, by using a perspective-based checklist. With respect to the reliability of the adjusted checklist, 9 of out 19 questions show an acceptable proportion of agreement (between two practitioners). Conclusions: Based on our experience, the contextualization and operationalization of a perspective-based checklist helps to make it more useful for the practitioners. However, to increase the reliability of the checklist, more reviewers and more discussion cycles are necessary.}
}

@article{rayyan-727967622,
  title={Understanding the different levels of challenges in global software development},
  year={2019},
  pages={66-67},
  author={Saleem, Nazish and Mathrani, Sanjay and Taskin, Nazim},
  url={https://doi.org/10.1109/ICGSE.2019.000-3},
  publisher={IEEE Press},
  series={ICGSE '19},
  keywords={⚠️ Invalid DOI, global software development, GSD challenges, levels of challenges, operational levels in GSD, Software},
  abstract={Global software development (GSD) faces several inherent challenges due to temporal, organizational, socio-cultural and geographical distances. Since GSD operates at different functional levels that include country, company and team levels, there is a need to understand and categorize GSD challenges at these levels. This paper aims to re-investigate GSD challenges and categorize them at the country, company and team level. It will help academia to look into GSD issues with respect to operational levels and allow software companies to improve their processes and management at these levels.}
}

@article{rayyan-727967623,
  title={Cognitive biases in software quality and testing},
  year={2016},
  issn={978-1-4503-4205-6},
  pages={823-826},
  author={Salman, Iflaah},
  url={https://doi.org/10.1145/2889160.2889265},
  publisher={Association for Computing Machinery},
  series={ICSE '16},
  keywords={cognitive bias, human factors, software psychology, software quality, software testing, Software, Bias (Epidemiology), Cognition},
  abstract={Humans are an integral entity for performing software quality and testing activities. The quality is compromised when human-thought process deviates from the laws of rational thinking, referred to as cognitive biases. The work carried out so far from this perspective in software quality and testing is very scarce and is limited to one cognitive bias only. This work aims to explore the phenomenon of cognitive biases in software quality and testing in more detail. Furthermore, investigating the factors that exist in an organisational context and that trigger the biases, which in turn deteriorate the quality of software, is also the focus of this work. Acquiring the knowledge of cognitive biases and the triggering factors will help in circumventing them, thus improving software quality.}
}

@article{rayyan-727967624,
  title={Assisted discovery of software vulnerabilities},
  year={2018},
  issn={978-1-4503-5663-3},
  pages={464-467},
  author={Munaiah, Nuthan},
  url={https://doi.org/10.1145/3183440.3183453},
  publisher={Association for Computing Machinery},
  series={ICSE '18},
  keywords={Software},
  abstract={As more aspects of our daily lives rely on technology, the software that enables the technology must be secure. Developers rely on practices such as threat modeling, static and dynamic analyses, code review, and fuzz and penetration testing to engineer secure software. These practices, while effective at identifying vulnerabilities in software, are limited in their ability to describe the potential reasons for the existence of vulnerabilities. In order to overcome this limitation, researchers have proposed empirically validated metrics to identify factors that may have led to the introduction of vulnerabilities in the past. Developers must be made aware of these factors so that they can proactively consider the security implications of each line of code that they contribute. The goal of our research is to assist developers in engineering secure software by providing a technique that generates scientific, interpretable, and actionable feedback on security as the software evolves. In this paper, we provide an overview of our proposed approach to accomplish this research goal through a series of three research studies in which we (1) systematize the knowledge on vulnerability discovery metrics, (2) leverage the metrics to generate feedback on security, and (3) implement a framework for providing automatically generated feedback on security using code reviews as a medium.}
}

@article{rayyan-727967625,
  title={A systematic survey on the design of self-adaptive software systems using control engineering approaches},
  year={2012},
  issn={978-1-4673-1787-0},
  pages={33-42},
  author={Patikirikorala, Tharindu and Colman, Alan and Han, Jun and Wang, Liuping},
  publisher={IEEE Press},
  series={SEAMS '12},
  keywords={Software},
  abstract={Control engineering approaches have been identified as a promising tool to integrate self-adaptive capabilities into software systems. Introduction of the feedback loop and controller into the management system potentially enables the software systems to achieve the runtime performance objectives and maintain the integrity of the system when they are operating in unpredictable and dynamic environments. There is a large body of literature that has proposed control engineering solutions for different application domains, handling different performance variables and control objectives. However, the relevant literature is scattered over different conference proceedings, journals and research communities. Consequently, conducting a survey to analyze and classify the existing literature is a useful, yet a challenging task. This paper presents the results of a systematic survey that includes classification and analysis of 161 papers in the existing literature. In order to capture the characteristics of the control solutions proposed in these papers we introduce a taxonomy as a basis for classification of all articles. Finally, survey results are presented, including quantitative, cross and trend analysis.}
}

@article{rayyan-727967626,
  title={A roadmap for software maintainability measurement},
  year={2013},
  issn={978-1-4673-3076-3},
  pages={1453-1455},
  author={Saraiva, Juliana},
  publisher={IEEE Press},
  series={ICSE '13},
  keywords={Software},
  abstract={Object-Oriented Programming (OOP) is one of the most used programming paradigms. Thus, researches dedicated in improvement of software quality that adhere to this paradigm are demanded. Complementarily, maintainability is considered a software attribute that plays an important role in its quality level. In this context, Object-Oriented Software Maintainability (OOSM) has been studied through years and several researchers proposed a high number of metrics to measure it. Nevertheless, there is no standardization or a catalogue to summarize all the information about these metrics, helping the researchers to make decision about which metrics can be adopted to perform their experiments in OOSM. Actually, distinct areas in both academic and industrial environment, such as Software Development, Project Management, and Software Research can adopt them to support decision-making processes. Thus, this work researched about the usage of OOSM metrics in academia and industry in order to help researchers in making decision about the metrics suite to be adopted. We found 570 OOSM metrics. Additionally, as a preliminary result we proposed a catalog with 36 metrics that were most used in academic works/experiments, trying to guide researchers with their decision-make about which metrics are more indicated to be adopted in their experiments.}
}

@article{rayyan-727967627,
  title={The use of empirical methods in open source software research: Facts, trends and future directions},
  year={2009},
  issn={978-1-4244-3720-7},
  pages={19-24},
  author={Stol, Klaas-Jan and Babar, Muhammad Ali and Russo, Barbara and Fitzgerald, Brian},
  url={https://doi.org/10.1109/FLOSS.2009.5071355},
  publisher={IEEE Computer Society},
  series={FLOSS '09},
  keywords={Software},
  abstract={Open Source Software (OSS) is a field of study with increasing interest of researchers. By its nature, OSS is especially suitable for empirical research. A great number of OSS related empirical studies have been conducted, but no effort has been made to systematically review the published evidence. This paper presents the results of a systematic review to investigate research topics and used methods in OSS related research. We present our results as facts and trends in this field and provide directions for future research.}
}

@article{rayyan-727967628,
  title={The quamoco product quality modelling and assessment approach},
  year={2012},
  issn={978-1-4673-1067-3},
  pages={1133-1142},
  author={Wagner, Stefan and Lochmann, Klaus and Heinemann, Lars and Kläs, Michael and Trendowicz, Adam and Plösch, Reinhold and Seidl, Andreas and Goeb, Andreas and Streit, Jonathan},
  publisher={IEEE Press},
  series={ICSE '12},
  abstract={Published software quality models either provide abstract quality attributes or concrete quality assessments. There are no models that seamlessly integrate both aspects. In the project Quamoco, we built a comprehensive approach with the aim to close this gap. For this, we developed in several iterations a meta quality model specifying general concepts, a quality base model covering the most important quality factors and a quality assessment approach. The meta model introduces the new concept of a product factor, which bridges the gap between concrete measurements and abstract quality aspects. Product factors have measures and instruments to operationalise quality by measurements from manual inspection and tool analysis. The base model uses the ISO 25010 quality attributes, which we refine by 200 factors and 600 measures for Java and C# systems. We found in several empirical validations that the assessment results fit to the expectations of experts for the corresponding systems. The empirical analyses also showed that several of the correlations are statistically significant and that the maintainability part of the base model has the highest correlation, which fits to the fact that this part is the most comprehensive. Although we still see room for extending and improving the base model, it shows a high correspondence with expert opinions and hence is able to form the basis for repeatable and understandable quality assessments in practice.}
}

@article{rayyan-727967629,
  title={Model-driven allocation engineering},
  year={2015},
  issn={978-1-5090-0024-1},
  pages={374-384},
  author={Pohlmann, Uwe and Hüwe, Marcus},
  url={https://doi.org/10.1109/ASE.2015.18},
  publisher={IEEE Press},
  series={ASE '15},
  abstract={Cyber-physical systems (CPSs) provide sophisticated functionality and are controlled by networked electronic control units (ECUs). Nowadays, software engineers use component-based development approaches to develop their software. Moreover, software components have to be allocated to an ECU to be executed. Engineers have to cope with topology-, software-, and timing-dependencies and memory-, scheduling-, and routing-constraints. Currently, engineers use linear programs to specify allocation constraints and to derive a feasible allocation automatically. However, encoding the allocation problem as a linear program is a complex and error-prone task. This paper contributes a model-driven, OCL-based allocation engineering approach for reducing the engineering effort and to avoid failures. We validate our approach with an automotive case study modeled with MechatronicUML. Our validation shows that we can specify allocation constraints with less engineering effort and are able to derive feasible allocations automatically.}
}

@article{rayyan-727967630,
  title={Beyond continuous delivery: An empirical investigation of continuous deployment challenges},
  year={2017},
  issn={978-1-5090-4039-1},
  pages={111-120},
  author={Shahin, Mojtaba and Babar, Muhammad Ali and Zahedi, Mansooreh and Zhu, Liming},
  url={https://doi.org/10.1109/ESEM.2017.18},
  publisher={IEEE Press},
  series={ESEM '17},
  keywords={DevOps, continuous delivery, continuous deployment, empirical study},
  abstract={Context: A growing number of software organizations have been adopting Continuous DElivery (CDE) and Continuous Deployment (CD) practices. Researchers have started investing significant efforts in studying different aspects of CDE and CD. Many studies refer to CDE (i.e., where an application is potentially capable of being deployed) and CD (i.e., where an application is automatically deployed to production on every update) as synonyms and do not distinguish them from each other. Despite CDE being successfully adopted by a large number of organizations, it is not empirically known why organizations still are unable or demotivated to have automatic and continuous deployment (i.e., CD practice). Goal: This study aims at empirically investigating and classifying the factors that may impact on adopting and implementing CD practice. Method: We conducted a mixed-method empirical study consisting of interviewing 21 software practitioners, followed by a survey with 98 respondents. Results: Our study reveals 11 confounding factors that limit or demotivate software organizations to push changes automatically and continuously to production. The most important ones are "lack of automated (user) acceptance test", "manual quality check", "deployment as business decision", "insufficient level of automated test coverage", and "highly bureaucratic deployment process". Conclusion: Our findings highlight several areas for future research and provide suggestions for practitioners to streamline deployment process.}
}

@article{rayyan-727967631,
  title={Using a follow-on survey to investigate why use of the visitor, singleton & facade patterns is controversial},
  year={2012},
  issn={978-1-4503-1056-7},
  pages={79-88},
  author={Zhang, Cheng and Budgen, David and Drummond, Sarah},
  url={https://doi.org/10.1145/2372251.2372264},
  publisher={Association for Computing Machinery},
  series={ESEM '12},
  keywords={survey, design pattern, empirical},
  abstract={Context: A previous study has shown that software developers who are experienced with using design patterns hold some conflicting opinions about three of the more popular design patterns: Facade, Singleton and Visitor. Aim: To identify the characteristics of these three patterns that have caused them to generate such differing views.Method: We employed a qualitative follow-on survey of those developers who had taken part in the earlier survey about design patterns. Results: We received 46 usable responses from a possible total of 188, with nearly 85% of respondents having six or more years of experience with design patterns. Of these, 27 also provided comments and descriptions of experiences about the patterns, which we categorised. Conclusions: All three patterns can easily be misused and in each case, the consequences of misuse are regarded as being particularly significant.}
}

@article{rayyan-727967632,
  title={An empirical investigation of personality traits of software testers},
  year={2015},
  pages={1-7},
  author={Kanij, Tanjila and Merkel, Robert and Grundy, John},
  publisher={IEEE Press},
  series={CHASE '15},
  keywords={⛔ No DOI found, Software},
  abstract={Software testing is the process of an execution-based investigation of some aspects of the software's quality. The efficiency of the process depends on the methods and technologies used, but crucially also on the human testers. Software testers typically attempt to anticipate and expose ways software may be defective, a fundamentally different task set to those of other software development practitioners. This raises the question of whether the personality of software testers may be different to other people involved in software development. To test this hypothesis, we collected personality profiles using the big five factor model of around 200 software development practitioners. Analysis of this data indicates that software testers are significantly higher on the conscientiousness factor than other software development practitioners, while other factors remain broadly consistent.}
}

@article{rayyan-727967633,
  title={Are computer science and engineering graduates ready for the software industry? Experiences from an industrial student training program},
  year={2018},
  issn={978-1-4503-5660-2},
  pages={68-77},
  author={Tuzun, Eray and Erdogmus, Hakan and Ozbilgin, Izzet Gokhan},
  url={https://doi.org/10.1145/3183377.3185754},
  publisher={Association for Computing Machinery},
  series={ICSE-SEET '18},
  keywords={hiring practices for software professionals, software engineering education, software engineering summer school, software engineering training, Software},
  abstract={It has been 50 years since the term "software engineering" was coined in 1968 at a NATO conference. The field should be relatively mature by now, with most established universities covering core software engineering topics in their Computer Science programs and others offering specialized degrees. However, still many practitioners lament a lack of skills in new software engineering hires. With the growing demand for software engineers from the industry, this apparent gap becomes more and more pronounced. One corporate strategy to address this gap is for the industry to develop supplementary training programs before the hiring process, which could also help companies screen viable candidates. In this paper, we report on our experiences and lessons learned in conducting a summer school program aimed at screening new graduates, introducing them to core skills relevant to the organization and industry, and assessing their attitudes toward mastering those skills before the hiring process begins. Our experience suggests that such initiatives can be mutually beneficial for new hires and companies alike. We support this insight with pre- and post-training data collected from the participants during the first edition of the summer school and a follow-up questionnaire conducted after a year with the participants, 50% of whom were hired by the company shortly after the summer school.}
}

@article{rayyan-727967634,
  title={Exploring security issues in telehealth systems},
  year={2019},
  pages={65-72},
  author={Márquez, Gastón and Astudillo, Hernán and Taramasco, Carla},
  url={https://doi.org/10.1109/SEH.2019.00019},
  publisher={IEEE Press},
  series={SEH '19},
  keywords={software engineering, telehealth, telehealth systems},
  abstract={Telehealth systems (TS's) provide remote health-based services to improve the quality of service of patient treatment. Most healthcare professionals have access to standard telecommunications technology (such as Wireless Body Area Network (WBAN), biosensors, remote medical robots, and others) to offer remote care of elderly and physically less able patients as well as remote surgeries, treatments, and diagnoses. In order to ensure the functionality of TS's, several systemic properties must be satisfied, including security. Although there are studies that discuss different security approaches in TS's, it is difficult to have a clear view of existing security issues and solutions for these systems. In this article, a systematic mapping study was performed to detect, organize and characterize security issues in TS's. We identified 41 studies which were classified according to their research strategy, target problem, security issue addressed, and proposals. Results reveal that (i) 4 security issues were identified; (ii) 3 strategies were distinguished to handle security issues; (iii) patient and wireless medical data are the most affected medical supplies. Security in TS's reveals diverse challenges that concern Software Engineering. Areas such as requirements, software architecture, and security patterns play an important role in order to handle security issues.}
}

@article{rayyan-727967635,
  title={Comprehending studies on program comprehension},
  year={2017},
  issn={978-1-5386-0535-6},
  pages={308-311},
  author={Schröter, Ivonne and Krüger, Jacob and Siegmund, Janet and Leich, Thomas},
  url={https://doi.org/10.1109/ICPC.2017.9},
  publisher={IEEE Press},
  series={ICPC '17},
  keywords={systematic review, empirical research, study comprehension},
  abstract={Program comprehension is an important aspect of developing and maintaining software, as programmers spend most of their time comprehending source code. Thus, it is the focus of many studies and experiments to evaluate approaches and techniques that aim to improve program comprehension. As the amount of corresponding work increases, the question arises how researchers address program comprehension. To answer this question, we conducted a literature review of papers published at the International Conference on Program Comprehension, the major venue for research on program comprehension. In this article, we i) present preliminary results of the literature review and ii) derive further research directions. The results indicate the necessity for a more detailed analysis of program comprehension and empirical research.}
}

@article{rayyan-727967636,
  title={What scope is there for adopting evidence-informed teaching in SE?},
  year={2012},
  issn={978-1-4673-1067-3},
  pages={1205-1214},
  author={Budgen, David and Drummond, Sarah and Brereton, Pearl and Holland, Nikki},
  publisher={IEEE Press},
  series={ICSE '12},
  abstract={Context: In teaching about software engineering we currently make little use of any empirical knowledge. Aim: To examine the outcomes available from the use of Evidence-Based Software Engineering (EBSE) practices, so as to identify where these can provide support for, and inform, teaching activities. Method: We have examined all known secondary studies published up to the end of 2009, together with those published in major journals to mid-2011, and identified where these provide practical results that are relevant to student needs. Results: Starting with 145 candidate systematic literature reviews (SLRs), we were able to identify and classify potentially useful teaching material from 43 of them. Conclusions: EBSE can potentially lend authority to our teaching, although the coverage of key topics is uneven. Additionally, mapping studies can provide support for research-led teaching.}
}

@article{rayyan-727967637,
  title={Social aspects and how they influence MSECO developers},
  year={2019},
  pages={99-106},
  author={Steglich, Caio and Marczak, Sabrina and de Souza, Cleidson R B and Guerra, Luiz Pedro and Mosmann, Luiz Henrique and Filho, Fernando Figueira and Perin, Marcelo},
  url={https://doi.org/10.1109/CHASE.2019.00032},
  publisher={IEEE Press},
  series={CHASE '19},
  keywords={developer's collaboration, mobile software ecosystem, social aspects},
  abstract={Mobile software ecosystem (MSECO) is a new software development paradigm for mobile technologies, having three main dimensions, namely: Technical, Business and Social. The literature has a considerable number of studies on technical and business dimensions, but only a few studies focus on the social aspects of MSECOs. However, the literature has enough to provide evidence that the actors involved, such as developers, are crucial to an MSECO. This study aims to complement earlies studies by describing new social factors that influence developers to work in a MSECO. We conducted a systematic literature review in order to identify these new factors, and a field study in which 20 developers were interviewed to understand how these factors can influence them to join or keep participating in a MSECO. We found that developer become more rigorous to continue participating then to adopt a MSECO.}
}

@article{rayyan-727967638,
  title={Reference framework for digital twins within cyber-physical systems},
  year={2019},
  pages={25-31},
  author={Josifovska, Klementina and Yigitbas, Enes and Engels, Gregor},
  url={https://doi.org/10.1109/SEsCPS.2019.00012},
  publisher={IEEE Press},
  series={SEsCPS '19},
  keywords={cyber-physical system, digital twin, reference framework, Twins},
  abstract={Cyber-Physical Systems (CPSs) represent systems which integrate physical units and processes with computational entities over Internet and allow ubiquitous access of information and services. Although the application of CPSs promise to positively transform many application fields, there are still many open questions and challenges on how to design and realize a CPS. As indicated in the third level of the 5-level CPS architecture, the so-called cyber level, one of the challenges addresses the need for digital twins as high-fidelity mirroring images of CPSs entities. This is a prerequisite to realize the upper levels of the 5-level CPS architecture - the cognition and configuration level. In the scientific literature, the concept of a Digital Twin is introduced as a concrete realization for mirroring physical entities in the virtual world. However, a reference framework for the main building blocks of a Digital Twin framework is missing. This hinders a reuse of best practices and proven solutions for concrete realizations of a Digital Twin. In order to tackle this problem, we have established a reference framework for Digital Twins within a CPS. Our framework specifies the main building blocks of a Digital Twin in terms of structure and interrelations. To achieve this goal, we performed a systematic literature review, where we evaluated existing Digital Twin realizations used in different application domains of CPSs and we applied Grounded Theory and Framework Analysis as underlying methodologies. This reference framework serves a blueprint for developing Digital Twins of physical entities which are part of a CPS.}
}

@article{rayyan-727967639,
  title={Continuous experimentation and A/B testing: A mapping study},
  year={2018},
  issn={978-1-4503-5745-6},
  pages={35-41},
  author={Ros, Rasmus and Runeson, Per},
  url={https://doi.org/10.1145/3194760.3194766},
  publisher={Association for Computing Machinery},
  series={RCoSE '18},
  keywords={mapping study, A/B testing, continuous experimentation},
  abstract={Background. Continuous experimentation (CE) has recently emerged as an established industry practice and as a research subject. Our aim is to study the application of CE and A/B testing in various industrial contexts. Objective. We wanted to investigate whether CE is used in different sectors of industry, by how it is reported in academic studies. We also wanted to explore the main topics researched to give an overview of the subject and discuss future research directions. Method. We performed a systematic mapping study of the published literature and included 62 papers, using a combination of database search and snowballing. Results. Most reported software experiments are done online and with software delivered as a service, although varied exemptions exist for e.g., financial software and games. The most frequently researched topics are challenges to conduct experiments and statistical methods for software experiments. Conclusions. The software engineering research on CE is still in its infancy. There are future research opportunities in evaluation research of technical topics and investigations of ethical experimentation. We conclude that the included studies show that A/B testing is applicable to a diversity of software and organisations.}
}

@article{rayyan-727967640,
  title={(automated) literature analysis: Threats and experiences},
  year={2018},
  issn={978-1-4503-5748-7},
  pages={20-27},
  author={Shakeel, Yusra and Krüger, Jacob and von Nostitz-Wallwitz, Ivonne and Lausberger, Christian and Durand, Gabriel Campero and Saake, Gunter and Leich, Thomas},
  url={https://doi.org/10.1145/3194747.3194748},
  publisher={Association for Computing Machinery},
  series={SE4Science '18},
  keywords={systematic literature review, software engineering, lessons learned, literature analysis, threats to validity},
  abstract={The number of scientific publications is increasing each year, specifically in the field of computer science. In order to condense existing knowledge, evidence-based software engineering is concerned with systematic literature reviews, surveys, and other kinds of literature analysis. These methods are used to summarize the evidence on empirical studies - or approaches in general - and to identify gaps for new research opportunities. However, executing systematic review processes requires a considerable amount of time and effort. Consequently, researchers have proposed several semi-automated approaches to support and facilitate different steps of such methods. With our current research, we aim to assist researchers to efficiently and effectively execute different steps, namely the search for and selection of primary studies. In this paper, we report several issues we identified during our research that threaten any kind of literature analysis and hamper suitable tool support. We further recommend solutions to mitigate these threats. Overall, our goal is to raise researchers' and publishers' awareness regarding several potential threats on literature analysis, to support software engineers in designing suitable tools for research, and to encourage the research community to solve these threats.}
}

@article{rayyan-727967641,
  title={Analyzing forty years of software maintenance models},
  year={2017},
  issn={978-1-5386-1589-8},
  pages={146-148},
  author={Lenarduzzi, Valentina and Sillitti, Alberto and Taibi, Davide},
  url={https://doi.org/10.1109/ICSE-C.2017.122},
  publisher={IEEE Press},
  series={ICSE-C '17},
  keywords={systematic literature review, component, software maintenance, Software},
  abstract={Software maintenance has dramatically evolved in the last four decades, to cope with the continuously changing software development models and programming languages and adopting increasingly advanced prediction models. In this work, we present the initial results of a Systematic Literature Review (SLR), highlighting the evolution of the metrics and models adopted in the last forty years.}
}

@article{rayyan-727967642,
  title={Systematic literature studies: Database searches vs. Backward snowballing},
  year={2012},
  issn={978-1-4503-1056-7},
  pages={29-38},
  author={Jalali, Samireh and Wohlin, Claes},
  url={https://doi.org/10.1145/2372251.2372257},
  publisher={Association for Computing Machinery},
  series={ESEM '12},
  keywords={snowballing, systematic literature review, agile practices, global software engineering},
  abstract={Systematic studies of the literature can be done in different ways. In particular, different guidelines propose different first steps in their recommendations, e.g. start with search strings in different databases or start with the reference lists of a starting set of papers.In software engineering, the main recommended first step is using search strings in a number of databases, while in information systems, snowballing has been recommended as the first step. This paper compares the two different search approaches for conducting literature review studies.The comparison is conducted by searching for articles addressing "Agile practices in global software engineering". The focus of the paper is on evaluating the two different search approaches.Despite the differences in the included papers, the conclusions and the patterns found in both studies are quite similar. The strengths and weaknesses of each first step are discussed separately and in comparison with each other.It is concluded that none of the first steps is outperforming the other, and the choice of guideline to follow, and hence the first step, may be context-specific, i.e. depending on the area of study.}
}

@article{rayyan-727967643,
  title={Challenges for static analysis of java reflection: Literature review and empirical study},
  year={2017},
  issn={978-1-5386-3868-2},
  pages={507-518},
  author={Landman, Davy and Serebrenik, Alexander and Vinju, Jurgen J},
  url={https://doi.org/10.1109/ICSE.2017.53},
  publisher={IEEE Press},
  series={ICSE '17},
  keywords={systematic literature review, empirical study, Java, reflection, static analysis},
  abstract={The behavior of software that uses the Java Reflection API is fundamentally hard to predict by analyzing code. Only recent static analysis approaches can resolve reflection under unsound yet pragmatic assumptions. We survey what approaches exist and what their limitations are. We then analyze how real-world Java code uses the Reflection API, and how many Java projects contain code challenging state-of-the-art static analysis.Using a systematic literature review we collected and categorized all known methods of statically approximating reflective Java code. Next to this we constructed a representative corpus of Java systems and collected descriptive statistics of the usage of the Reflection API. We then applied an analysis on the abstract syntax trees of all source code to count code idioms which go beyond the limitation boundaries of static analysis approaches. The resulting data answers the research questions. The corpus, the tool and the results are openly available.We conclude that the need for unsound assumptions to resolve reflection is widely supported. In our corpus, reflection can not be ignored for 78% of the projects. Common challenges for analysis tools such as non-exceptional exceptions, programmatic filtering meta objects, semantics of collections, and dynamic proxies, widely occur in the corpus. For Java software engineers prioritizing on robustness, we list tactics to obtain more easy to analyze reflection code, and for static analysis tool builders we provide a list of opportunities to have significant impact on real Java code.}
}

@article{rayyan-727967644,
  title={Investigating the use of a hybrid search strategy for systematic reviews},
  year={2017},
  issn={978-1-5090-4039-1},
  pages={193-198},
  author={Mourão, Erica and Kalinowski, Marcos and Murta, Leonardo and Mendes, Emilia and Wohlin, Claes},
  url={https://doi.org/10.1109/ESEM.2017.30},
  publisher={IEEE Press},
  series={ESEM '17},
  keywords={systematic review, snowballing, hybrid strategy, search strategy},
  abstract={[Background] Systematic Literature Reviews (SLRs) are one of the important pillars when employing an evidence-based paradigm in Software Engineering. To date most SLRs have been conducted using a search strategy involving several digital libraries. However, significant issues have been reported for digital libraries and applying such search strategy requires substantial effort. On the other hand, snowballing has recently arisen as a potentially more efficient alternative or complementary solution. Nevertheless, it requires a relevant seed set of papers. [Aims] This paper proposes and evaluates a hybrid search strategy combining searching in a specific digital library (Scopus) with backward and forward snowballing. [Method] The proposed hybrid strategy was applied to two previously published SLRs that adopted database searches. We investigate whether it is able to retrieve the same included papers with lower effort in terms of the number of analysed papers. The two selected SLRs relate respectively to elicitation techniques (not confined to Software Engineering (SE)) and to a specific SE topic on cost estimation. [Results] Our results provide preliminary support for the proposed hybrid search strategy as being suitable for SLRs investigating a specific research topic within the SE domain. Furthermore, it helps overcoming existing issues with using digital libraries in SE. [Conclusions] The hybrid search strategy provides competitive results, similar to using several digital libraries. However, further investigation is needed to evaluate the hybrid search strategy.}
}

@article{rayyan-727967645,
  title={International workshop on realising evidence-based software engineering},
  year={2005},
  issn={1-58113-963-2},
  pages={687},
  author={Budgen, David and Brereton, Pearl and Kitchenham, Barbara and Linkman, Stephen},
  url={https://doi.org/10.1145/1062455.1062613},
  publisher={Association for Computing Machinery},
  series={ICSE '05},
  keywords={evidence, empirical studies, methodological issues, Software},
  abstract={This workshop is concerned with defining the procedures that are needed to establish a sound empirical foundation for the practices of Software Engineering. Our goal is to begin building a community that will review, analyse, codify and promulgate software engineering experiences as well as to identify the processes and infrastructure that are needed to support these activities.}
}

@article{rayyan-727967646,
  title={Work design and job rotation in software engineering: Results from an industrial study},
  year={2019},
  pages={139-146},
  author={Santos, Ronnie E S and Baldassarre, Maria Teresa and da Silva, Fabio Q B and Magalhães, Cleyton V C and Capretz, Luiz Fernando and Correia-Neto, Jorge S},
  url={https://doi.org/10.1109/CHASE.2019.00040},
  publisher={IEEE Press},
  series={CHASE '19},
  keywords={software engineering, job rotation, work design, Software, Rotation},
  abstract={Context: Job rotation is a managerial practice to be applied in the organizational environment to reduce job monotony, boredom, and exhaustion resulting from job simplification, specialization, and repetition. Previous studies have identified and discussed the use of project-to-project rotations in software practice, gathering empirical evidence from qualitative and field studies and pointing out set of work-related factors that can be positively or negatively affected by this practice. Goal: We aim to collect and discuss the use of job rotation in software organizations in order to identify the potential benefits and limitations of this practice supported by the statement of existing theories of work design. Method: Using a survey-based research design, we collected and analyzed quantitative data from software engineers about how software development work is designed and organized, as well as the potential effects of job rotations on this work design. We investigated 21 work design constructs, along with job burnout, role conflict, role ambiguity, and two constructs related to job rotation. Results: We identified one new benefit and six new limitations of job rotation, not observed in previous studies and added new discussions to the existing body of knowledge concerning the use of job rotation in software engineering practice. Conclusion: We believe that these results represent another important step towards the construction of a consistent and comprehensive body of evidence that can guide future research and also inform practice about the potential positive and negative effects of job rotation in software development companies.}
}

@article{rayyan-727967647,
  title={Influencing the adoption of software engineering methods using social software},
  year={2012},
  issn={978-1-4673-1067-3},
  pages={1325-1328},
  author={Singer, Leif and Schneider, Kurt},
  publisher={IEEE Press},
  series={ICSE '12},
  keywords={Software},
  abstract={Software engineering research and practice provide a wealth of methods that improve the quality of software and lower the costs of producing it. Even though processes mandate their use, methods are not employed consequently. Software developers and development organizations thus cannot fully benefit from these methods. We propose a method that, for a given software engineering method, provides instructions on how to improve its adoption using social software. This employs the intrinsic motivation of software developers rather than prescribing behavior. As a result, we believe that software engineering methods will be applied better and more frequently.}
}

@article{rayyan-727967648,
  title={2nd InternationalWorkshop on realising evidence-based software engineering (REBSE-2): Overview and introduction},
  year={2007},
  issn={0-7695-2962-3},
  pages={1},
  author={Kitchenham, Barbara and Budgen, David and Brereton, Pearl and Turner, Mark},
  url={https://doi.org/10.1109/REBSE.2007.1},
  publisher={IEEE Computer Society},
  series={REBSE '07},
  keywords={Software},
  abstract={The REBSE international workshops are concerned with exploring the adaptation and use of the evidence-based paradigm in software engineering research and practice, through a mix of presentations and discussion. Here, we provide some background about evidence-based software engineering and its current state.}
}

@article{rayyan-727967649,
  title={2nd international workshop on realising evidence-based software engineering (REBSE-2)},
  year={2007},
  issn={0-7695-2892-9},
  pages={137-138},
  author={Budgen, David and Kitchenham, Barbara and Brereton, Pearl and Turner, Mark},
  url={https://doi.org/10.1109/ICSECOMPANION.2007.6},
  publisher={IEEE Computer Society},
  series={ICSE COMPANION '07},
  keywords={Software},
  abstract={The REBSE international workshops are concerned with exploring the adaptation and use of the evidence-based paradigm in software engineering research and practice. The workshops address this goal through a mix of presentations and discussion, drawing upon ideas and experiences from other disciplines where appropriate.}
}

@article{rayyan-727967650,
  title={Is scrum fit for global software engineering?},
  year={2017},
  issn={978-1-5386-1587-4},
  pages={1-10},
  author={Lous, Pernille and Kuhrmann, Marco and Tell, Paolo},
  url={https://doi.org/10.1109/ICGSE.2017.13},
  publisher={IEEE Press},
  series={ICGSE '17},
  keywords={systematic literature review, systematic mapping study, global software engineering, agile software development, Software},
  abstract={Distributed software engineering and agility are strongly pushing on today's software industry. Due to inherent incompatibilities, for years, studying Scrum and its application in distributed setups has been subject to theoretical and applied research, and an increasing body of knowledge reports insights into this combination. Through a systematic literature review, this paper contributes a collection of experiences on the application of Scrum to global software engineering (GSE). In total, we identified 40 challenges in 19 categories practitioners face when using Scrum in GSE. Among the challenges, scaling Scrum to GSE and adopting practices accordingly are the most frequently named. Our findings also show that most solution proposals aim at modifying elements of the Scrum core processes. We thus conclude that, even though Scrum allows for extensive modification, Scrum itself represents a barrier for global software engineering, and development teams have to customize Scrum properly to benefit from agile software development in GSE.}
}

@article{rayyan-727967651,
  title={Software analytics to software practice: A systematic literature review},
  year={2015},
  pages={30-36},
  author={Abdellatif, Tamer Mohamed and Capretz, Luiz Fernando and Ho, Danny},
  publisher={IEEE Press},
  series={BIGDSE '15},
  keywords={⛔ No DOI found, systematic literature review, big data analytics, software analytics, software development analytics, Software},
  abstract={Software Analytics (SA) is a new branch of big data analytics that has recently emerged (2011). What distinguishes SA from direct software analysis is that it links data mined from many different software artifacts to obtain valuable insights. These insights are useful for the decision-making process throughout the different phases of the software lifecycle. Since SA is currently a hot and promising topic, we have conducted a systematic literature review, presented in this paper, to identify gaps in knowledge and open research areas in SA. Because many researchers are still confused about the true potential of SA, we had to filter out available research papers to obtain the most SA-relevant work for our review. This filtration yielded 19 studies out of 135. We have based our systematic review on four main factors: which software practitioners SA targets, which domains are covered by SA, which artifacts are extracted by SA, and whether these artifacts are linked or not. The results of our review have shown that much of the available SA research only serves the needs of developers. Also, much of the available research uses only one artifact which, in turn, means fewer links between artifacts and fewer insights. This shows that the available SA research work is still embryonic leaving plenty of room for future research in the SA field.}
}

@article{rayyan-727967652,
  title={Synthesizing qualitative research in software engineering: A critical review},
  year={2018},
  issn={978-1-4503-5638-1},
  pages={1207-1218},
  author={Huang, Xin and Zhang, He and Zhou, Xin and Babar, Muhammad Ali and Yang, Song},
  url={https://doi.org/10.1145/3180155.3180235},
  publisher={Association for Computing Machinery},
  series={ICSE '18},
  keywords={evidence-based software engineering, research synthesis, qualitative (synthesis) methods, systematic (literature) review, Software},
  abstract={Synthesizing data extracted from primary studies is an integral component of the methodologies in support of Evidence Based Software Engineering (EBSE) such as System Literature Review (SLR). Since a large and increasing number of studies in Software Engineering (SE) incorporate qualitative data, it is important to systematically review and understand different aspects of the Qualitative Research Synthesis (QRS) being used in SE. We have reviewed the use of QRS methods in 328 SLRs published between 2005 and 2015. We also inquired the authors of 274 SLRs to confirm whether or not any QRS methods were used in their respective reviews. 116 of them provided the responses, which were included in our analysis. We found eight QRS methods applied in SE research, two of which, narrative synthesis and thematic synthesis, have been predominantly adopted by SE researchers for synthesizing qualitative data. Our study determines that a significant amount of missing knowledge and incomplete understanding of the defined QRS methods in the community. Our effort also identifies an initial set factors that may influence the selection and use of appropriate QRS methods in SE.}
}

@article{rayyan-727967653,
  title={The educational value of mapping studies of software engineering literature},
  year={2010},
  issn={978-1-60558-719-6},
  pages={589-598},
  author={Kitchenham, Barbara and Brereton, Pearl and Budgen, David},
  url={https://doi.org/10.1145/1806799.1806887},
  publisher={Association for Computing Machinery},
  series={ICSE '10},
  keywords={systematic literature review, evidence-based software engineering, education, mapping studies, Software},
  abstract={We identify three challenges related to the provenance of the material we use in teaching software engineering. We suggest that these challenges can be addressed by using evidence-based software engineering (EBSE) and its primary tool of systematic literature reviews (SLRs). This paper aims to assess the educational and scientific value of undergraduate and postgraduate students undertaking a specific form of SLR called a mapping study. Using a case study methodology, we asked three postgraduate students and three undergraduates and their supervisor to complete a questionnaire concerning the educational value of mapping studies and any problems they experienced. Students found undertaking a mapping study to be a valuable experience providing both reusable research skills and a good overview of a research topic. Postgraduates found it useful as a starting point for their studies. Undergraduates reported problems undertaking the study in the required timescales. Searching and classifying the literature was difficult.}
}

@article{rayyan-727967654,
  title={Evidence-based software engineering},
  year={2004},
  issn={0-7695-2163-0},
  pages={273-281},
  author={Kitchenham, Barbara A and Dyba, Tore and Jorgensen, Magne},
  publisher={IEEE Computer Society},
  series={ICSE '04},
  keywords={Software},
  abstract={Objective: Our objective is to describe how softwareengineering might benefit from an evidence-basedapproach and to identify the potential difficultiesassociated with the approach.Method: We compared the organisation and technicalinfrastructure supporting evidence-based medicine (EBM)with the situation in software engineering. We consideredthe impact that factors peculiar to software engineering(i.e. the skill factor and the lifecycle factor) would haveon our ability to practice evidence-based softwareengineering (EBSE).Results: EBSE promises a number of benefits byencouraging integration of research results with a view tosupporting the needs of many different stakeholdergroups. However, we do not currently have theinfrastructure needed for widespread adoption of EBSE.The skill factor means software engineering experimentsare vulnerable to subject and experimenter bias. Thelifecycle factor means it is difficult to determine howtechnologies will behave once deployed.Conclusions: Software engineering would benefit fromadopting what it can of the evidence approach providedthat it deals with the specific problems that arise from thenature of software engineering.}
}

@article{rayyan-727967655,
  title={The impacts of software process improvement on developers: A systematic review},
  year={2012},
  issn={978-1-4673-1067-3},
  pages={113-122},
  author={Lavallée, Mathieu and Robillard, Pierre N},
  publisher={IEEE Press},
  series={ICSE '12},
  keywords={Software},
  abstract={This paper presents the results of a systematic review on the impacts of Software Process Improvement (SPI) on developers. This review selected 26 studies from the highest quality journals, conferences, and workshop in the field. The results were compiled and organized following the grounded theory approach. Results from the grounded theory were further categorized using the Ishikawa (or fishbone) diagram. The Ishikawa Diagram models all the factors potentially impacting software developers, and shows both the positive and negative impacts. Positive impacts include a reduction in the number of crises, and an increase in team communications and morale, as well as better requirements and documentation. Negative impacts include increased overhead on developers through the need to collect data and compile documentation, an undue focus on technical approaches, and the fact that SPI is oriented toward management and process quality, and not towards developers and product quality. This systematic review should support future practice through the identification of important obstacles and opportunities for achieving SPI success. Future research should also benefit from the problems and advantages of SPI identified by developers.}
}

@article{rayyan-727967656,
  title={Software industry experiments: A systematic literature review},
  year={2013},
  issn={978-1-4673-6286-3},
  pages={2-8},
  author={Dieste, Oscar and Juristo, Natalia and Martínez, Mauro Danilo},
  publisher={IEEE Press},
  series={CESI '14},
  keywords={industry, experiment, scoping study, Software},
  abstract={Background: There is no specialized survey of experiments conducted in the software industry. Goal: Identify the major features of software industry experiments, such as time distribution, independent and dependent variables, subject types, design types and challenges. Method: Systematic literature review, taking the form of a scoping study. Results: We have identified 10 experiments and five quasi-experiments up to July 2012. Most were run as of 2003. The main features of these studies are that they test technologies related to quality and management and analyse outcomes related to effectiveness and effort. Most experiments have a factorial design. The major challenges faced by experimenters are to minimize the cost of running the experiment for the company and to schedule the experiment so as not to interfere with production processes. Conclusion: Companies appear to be disinclined to run experiments because they are not perceived to have direct benefits. We believe that researchers staging a field experiment in a company should adopt a business-aligned stance and plan an experiment that clearly benefits managers and professionals.}
}

@article{rayyan-727967657,
  title={Scalability, elasticity, and efficiency in cloud computing: A systematic literature review of definitions and metrics},
  year={2015},
  issn={978-1-4503-3470-9},
  pages={83-92},
  author={Lehrig, Sebastian and Eikerling, Hendrik and Becker, Steffen},
  url={https://doi.org/10.1145/2737182.2737185},
  publisher={Association for Computing Machinery},
  series={QoSA '15},
  keywords={systematic literature review, cloud computing, cloud, definitions, efficiency, elasticity, metrics, scalability, Metronidazole, Elasticity},
  abstract={Context: In cloud computing, there is a multitude of definitions and metrics for scalability, elasticity, and efficiency. However, stakeholders have little guidance for choosing fitting definitions and metrics for these quality properties, thus leading to potential misunderstandings. For example, cloud consumers and providers cannot negotiate reliable and quantitative service level objectives directly understood by each stakeholder. Objectives: Therefore, we examine existing definitions and metrics for these quality properties from the viewpoint of cloud consumers, cloud providers, and software architects with regard to commonly used concepts. Methods: We execute a systematic literature review (SLR), reproducibly collecting common concepts in definitions and metrics for scalability, elasticity, and efficiency. As quality selection criteria, we assess whether existing literature differentiates the three properties, exemplifies metrics, and considers typical cloud characteristics and cloud roles. Results: Our SLR yields 418 initial results from which we select 20 for in-depth evaluation based on our quality selection criteria. In our evaluation, we recommend concepts, definitions, and metrics for each property. Conclusions: Software architects can use our recommendations to analyze the quality of cloud computing applications. Cloud providers and cloud consumers can specify service level objectives based on our metric suggestions.}
}

@article{rayyan-727967658,
  title={Trade-off decisions across time in technical debt management: A systematic literature review},
  year={2018},
  issn={978-1-4503-5713-5},
  pages={85-94},
  author={Becker, Christoph and Chitchyan, Ruzanna and Betz, Stefanie and McCord, Curtis},
  url={https://doi.org/10.1145/3194164.3194171},
  publisher={Association for Computing Machinery},
  series={TechDebt '18},
  keywords={behavioral software engineering, decision making, intertemporal choice, naturalistic, rationalistic, technical debt, time},
  abstract={Technical Debt arises from decisions that favour short-term outcomes at the cost of longer-term disadvantages. They may be taken knowingly or based on missing or incomplete awareness of the costs; they are taken in different roles, situations, stages and ways. Whatever technical or business factor motivate such decisions, they always imply a trade-off in time, a 'now vs. later'. How exactly are such decisions made, and how have they been studied?This paper analyzes how decisions on technical debt are studied in software engineering via a systematic literature review. It examines the presently published Software Engineering research on Technical Debt, with a particular focus on decisions involving time. The findings reveal surprising gaps in published work on empirical research in decision making. We observe that research has rarely studied how decisions are made, even in papers that focus on the decision process. Instead, most attention is focused on engineering measures and feeding them into an idealized decision making process. These findings lead to a set of recommendations for future empirical research on Technical Debt.}
}

@article{rayyan-727967659,
  title={Dispersion, coordination and performance in global software teams: A systematic review},
  year={2012},
  issn={978-1-4503-1056-7},
  pages={129-138},
  author={Anh, Nguyen-Duc and Cruzes, Daniela S and Conradi, Reidar},
  url={https://doi.org/10.1145/2372251.2372274},
  publisher={Association for Computing Machinery},
  series={ESEM '12},
  keywords={systematic literature review, global software development, communication, distribution, performance, team coordination, Software},
  abstract={Effective team coordination is crucial for successful global software projects. Although considerable research effort has been made in this area, no agreement has been reached on the influence of dispersion on team coordination and performance. The objective of this paper is to summarize the evidence on the relationship among context dispersion, team coordination and performance in global software projects. We have performed a Systematic literature review (SLR) to collect relevant studies and a thematic analysis to synthesize the extracted data. We found 28 primary studies reporting the impact of five dispersion dimensions on team performance. Previously, only two primary studies considered and distinguished all of these dispersion dimensions in studying dispersed team performance. The dispersion dimensions affect team outcomes indirectly through influencing organic and mechanistic coordination processes. Empirical evidence show that geographical dispersion impacts negatively and temporal dispersion has a mixed effect on team performance. While studies with teams working across different time zones shows a tendency that the team performance is pessimistically perceived, studies that use direct measure on task performance shows a positive association to temporal dispersion. The paper provides implications for future research and practitioners in establishing effective distributed team coordination.}
}

@article{rayyan-727967660,
  title={Software evolution visualization techniques and methods - a systematic review},
  year={2016},
  pages={1-6},
  author={Salameh, Hani Bani and Ahmad, Ayat and Aljammal, Ashraf},
  keywords={Software, Systematics, Visualization, Tools, Evolution, Unified modeling language, Data visualization, Systematic Literature Review (SLR), CVS, History, Object oriented modeling, Repository, SEV},
  abstract={Background: Software is an important asset for organizations and development teams. It must evolve over time in order to meet different changes in its environment, satisfy the developers' needs, and adapt to new requirements. Software developers and team members face difficulties tracking the changes others made to their software. Software visualization is one of the effective techniques that help stakeholders to better understand how software evolves, and which parts of the software are most affected by the change. Many visualization tools and techniques have been introduced by researchers and organizations to facilitate such understanding. Method: This article presents a systematic literature review (SLR) on software evolution visualization (SEV) tools. The SLR's main focus is to: (1) explore the main target of SEV, (2) analyze the classifications and taxonomies that are used to represent SEV tools, and (3) find out what are the main sources of information used to visualize software's evolution. Result: 29 papers were analyzed out of 55 papers. The result showed that SEV tools can be classified into five different groups: graph-based, notation-based, matrix-based, and metaphor-based and others. Graph-based are most popular while Notation-based are the least. SEVs focus can be either Artifact-centric visualization, Metric-centric visualization, Feature-centric visualization, or Architecture-centric visualization. The main source of information used to ex-tract information are the software repositories. Conclusion: This work can help developer, maintainer, researcher to get good knowledge about the state of software evolution and visualization as a whole.}
}

@article{rayyan-727967661,
  title={Search strategy to update systematic literature reviews in software engineering},
  year={2019},
  pages={355-362},
  author={Mendes, Emilia and Felizardo, Katia and Wohlin, Claes and Kalinowski, Marcos},
  keywords={Software Engineering, Software engineering, Systematics, Bibliographies, Search problems, Databases, Searching for evidence, Snowballing, Google, Optical wavelength conversion, Systematic Literature Review Update, Systematic Literature Reviews, Software},
  abstract={[Context] Systematic Literature Reviews (SLRs) have been adopted within the Software Engineering (SE) domain for more than a decade to provide meaningful summaries of evidence on several topics. Many of these SLRs are now outdated, and there are no standard proposals on how to update SLRs in SE. [Objective] The goal of this paper is to provide recommendations on how to best to search for evidence when updating SLRs in SE. [Method] To achieve our goal, we compare and discuss outcomes from applying different search strategies to identifying primary studies in a previously published SLR update on effort estimation. [Results] The use of a single iteration forward snowballing with Google Scholar, and employing the original SLR and its primary studies as a seed set seems to be the most cost-effective way to search for new evidence when updating SLRs. [Conclusions] The recommendations can be used to support decisions on how to update SLRs in SE.}
}

@article{rayyan-727967662,
  title={Requirements management techniques and tools in small and medium enterprises (SMEs): a systematic review},
  year={2019},
  pages={1-7},
  author={García, Yolanda-Meredith and Montes, Ángel and Lira, Jairo and Martínez, Juan},
  keywords={Software, Software engineering, Systematics, Visualization, Tools, Unified modeling language, requirements management, Requirements management, SME's, software engineering systematic review, techniques, tools},
  abstract={Currently, software is an important element used in different areas, both in the personal or professional. Therefore, it is required that the software be of quality, that is, that it fulfills the purpose for which it was created and satisfies the client. That's why to develop software, you must follow an engineering approach as well as a discipline, being of special importance the Requirements Management (REQM). In such a way that this work considers the REQM as one of the basic areas of project management, it starts with the requirements to define the "what to build" for the product and the project. This paper presents a software engineering systematic review process, the phases followed, the description of each one of them, as well as its implementation applied to the subject of the techniques and tools of the REQM area in software development Small and Medium Enterprises (SME's).}
}

@article{rayyan-727967663,
  title={Non-functional requirements prioritization: A systematic literature review},
  year={2019},
  pages={379-386},
  author={Ijaz, Khush Bakht and Inayat, Irum and Allah Bukhsh, Faiza},
  keywords={systematic literature review, Software engineering, Systematics, Bibliographies, Data mining, Search problems, Quality assessment, non-functional requirements, quality attributes, quality requirements},
  abstract={Continuous delivery and rapidly changing requirements in agile environments force the developers to put non-functional requirements (NFRs) on halt till maintenance phase. However, neglecting NFRs during prioritization phase may lead to inaccurate estimations for software projects resulting in high maintenance cost and failures. The subjective and uncertain nature of non-functional requirements makes them unfit to be prioritized using conventional prioritization methods. Although the existing literature reports on inadequate consideration given to NFRs prioritization, still no comprehensive systematic effort has been done to report the limitations and evaluation mechanisms of existing NFRs prioritization approaches. Requirements engineering society lacks a broad understanding of NFRs prioritization approaches and the challenges which need to be overcome. Therefore, we aim to investigate (i) the existing NFR prioritization techniques and their validation mechanisms, (ii) the role of Artificial Intelligence (AI) in NFRs prioritization, and (iii) the limitations of existing NFRs prioritization techniques. For this, we reviewed the literature published from 2008 till present and extracted 30 studies. The results reveal twenty-five NFRs prioritization techniques out of which only three are AI based. The major limitations we have come across are that most of the NFRs prioritization techniques are not scalable to large datasets, inter-dependencies between functional requirements (FRs) and NFRs are ignored, and the uncertainties associated with NFRs are not considered at all. However, the literature suggests that AI-based techniques and Fuzzy logic may be used to solve issues such as uncertainties i.e. ambiguities, vagueness, and subjective opinions of stakeholders. This review adds to the existing body of knowledge on NFRs and motivates the practitioners to focus on the NFR prioritization by highlighting the limitations of the existing methods.}
}

@article{rayyan-727967664,
  title={Multi-objective optimization techniques for software refactoring: A systematic literature review},
  year={2019},
  pages={1-7},
  author={Rafique, Muhammad Zaid and Alam, Khubaib Amjab and Iqbal, Umer},
  keywords={Software, Software Engineering, Systematics, Systematic Literature Review, Bibliographies, Tools, Optimization, Databases, Automated Software Refactoring, Complexity theory, Multi-Objective Software Refactoring, Search-Based Software Engineering, Search-Based Software Refactoring, Software refactoring},
  abstract={Software Refactoring is an essential activity of software maintenance. It aims at improving the internal structure of the program without affecting its external functionalities which not only aids in improving maintainability and readability but also helps in reducing overall software complexity. Many different manuals and automated software refactoring tools are available but most of these tools focus single objective refactoring i.e. improving the quality or reducing the code lines. Software refactoring involves many factors so different authors have proposed different multi-objective software refactoring approaches. We have performed systematic literature to classify and analyzed the studies published in the field of multi-objective software refactoring. The main objectives of our research are to categorize the studies on multi-objective software refactoring according to 4 criteria. We have considered studies from electronics databases from 2014 to 2019. A total of 19 studies were finalized based on our inclusion-exclusion and quality assessment criteria. The results of our research show that NSGA-II is a widely popular technique in the domain of multi-objective software refactoring whereas NSGA-III is popular when many objectives were considered. Furthermore, 11 most widely uses open source and industrial projects are identified which are used to evaluate the multi-objective software refactoring approaches. It was also observed that Precision, Recall and Inverse Generation Distance are commonly used evaluation metrics. The chronological distribution of studies shows that 2016 was the most productive research year in this field. Our results show that 76% of studies are ranked high based on our predefined quality assessment criteria. Based on our results we have concluded that multiobjective software refactoring is still an emerging field and there is a need to apply the latest state-of-the-art multi-objective approaches to get better results.}
}

@article{rayyan-727967665,
  title={Uncovering theories in software engineering},
  year={2013},
  pages={5-14},
  author={Stol, Klaas-Jan and Fitzgerald, Brian},
  keywords={Software, Software engineering, Systematics, Guidelines, Context, Software engineering research, empirical research, Buildings, middle-range theory, Physics, theory building, theory fragment},
  abstract={There has been a growing interest in the role of theory within Software Engineering (SE) research. For several decades, researchers within the SE research community have argued that, to become a real engineering science, SE needs to develop stronger theoretical foundations. A few authors have proposed guidelines for constructing theories, building on insights from other disciplines. However, so far, much SE research is not guided by explicit theory, nor does it produce explicit theory. In this paper we argue that SE research does, in fact, show traces of theory, which we call theory fragments. We have adapted an analytical framework from the social sciences, named the Validity Network Schema (VNS), that we use to illustrate the role of theorizing in SE research. We illustrate the use of this framework by dissecting three well known research papers, each of which has had significant impact on their respective subdisciplines. We conclude this paper by outlining a number of implications for future SE research, and show how by increasing awareness and training, development of SE theories can be improved.}
}

@article{rayyan-727967666,
  title={Overcoming the equivalent mutant problem: A systematic literature review and a comparative experiment of second order mutation},
  year={2014},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={40},
  number={1},
  pages={23-42},
  author={Madeyski, Lech and Orzeszyna, Wojciech and Torkar, Richard and Józala, Mariusz},
  keywords={Systematics, Testing, Databases, Mutation testing, Java, Educational institutions, equivalent mutant problem, higher order mutation, Informatics, Libraries, second order mutation, Mutation},
  abstract={Context. The equivalent mutant problem (EMP) is one of the crucial problems in mutation testing widely studied over decades. Objectives. The objectives are: to present a systematic literature review (SLR) in the field of EMP; to identify, classify and improve the existing, or implement new, methods which try to overcome EMP and evaluate them. Method. We performed SLR based on the search of digital libraries. We implemented four second order mutation (SOM) strategies, in addition to first order mutation (FOM), and compared them from different perspectives. Results. Our SLR identified 17 relevant techniques (in 22 articles) and three categories of techniques: detecting (DEM); suggesting (SEM); and avoiding equivalent mutant generation (AEMG). The experiment indicated that SOM in general and JudyDiffOp strategy in particular provide the best results in the following areas: total number of mutants generated; the association between the type of mutation strategy and whether the generated mutants were equivalent or not; the number of not killed mutants; mutation testing time; time needed for manual classification. Conclusions . The results in the DEM category are still far from perfect. Thus, the SEM and AEMG categories have been developed. The JudyDiffOp algorithm achieved good results in many areas.}
}

@article{rayyan-727967667,
  title={Software configuration engineering in practice interviews, survey, and systematic literature review},
  year={2020},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={46},
  number={6},
  pages={646-673},
  author={SAYAGH, Mohammed and Kerzazi, Noureddine and Adams, Bram and Petrillo, Fabio},
  keywords={Systematic literature review, Systematics, survey, Bibliographies, Empirical study, Interviews, Software systems, configuration, configuration engineering, Facebook, interviews, Software algorithms, Software},
  abstract={Modern software applications are adapted to different situations (e.g., memory limits, enabling/disabling features, database credentials) by changing the values of configuration options, without any source code modifications. According to several studies, this flexibility is expensive as configuration failures represent one of the most common types of software failures. They are also hard to debug and resolve as they require a lot of effort to detect which options are misconfigured among a large number of configuration options and values, while comprehension of the code also is hampered by sprinkling conditional checks of the values of configuration options. Although researchers have proposed various approaches to help debug or prevent configuration failures, especially from the end users' perspective, this paper takes a step back to understand the process required by practitioners to engineer the run-time configuration options in their source code, the challenges they experience as well as best practices that they have or could adopt. By interviewing 14 software engineering experts, followed by a large survey on 229 Java software engineers, we identified 9 major activities related to configuration engineering, 22 challenges faced by developers, and 24 expert recommendations to improve software configuration quality. We complemented this study by a systematic literature review to enrich the experts' recommendations, and to identify possible solutions discussed and evaluated by the research community for the developers' problems and challenges. We find that developers face a variety of challenges for all nine configuration engineering activities, starting from the creation of options, which generally is not planned beforehand and increases the complexity of a software system, to the non-trivial comprehension and debugging of configurations, and ending with the risky maintenance of configuration options, since developers avoid touching and changing configuration options in a mature system. We also find that researchers thus far focus primarily on testing and debugging configuration failures, leaving a large range of opportunities for future work.}
}

@article{rayyan-727967668,
  title={Historical, conceptual, and methodological aspects of the publications of the brazilian symposium on software engineering: A systematic mapping study},
  year={2011},
  pages={14-23},
  author={Cavalcanti, Thiago Rodrigues and da Silva, Fabio Q B},
  keywords={Software, Software engineering, Systematics, Guidelines, Conferences, engenharia de software, estudo secundÃ¡rio, Glass, Knowledge engineering, revisÃ£o de literatura.},
  abstract={The goal of this article is to provide a comprehensive and systematic analysis of the scientific work published in the Brazilian Symposium on Software Engineering (SBES). We used a systematic literature review methodology to extract, catalog, analyze, and synthesize data from all articles published in each of the 24 editions of SBES, with respect to historical, conceptual, and methodological aspects. The results of our review showed that 509 articles have been published, which were authored and co-authored by 818 researchers from 151 organizations, demonstrating the relevance of the SBES to bring together a significant portion of the Brazilian Software Engineering research community. Consistent with other studies, our results show that research published at SBES is diversified on the topics of software engineering addressed, but narrow on research approach and methods used. Besides, there is a strong concentration on technical aspects and much less studies addressing human or social aspects. We discuss the implications of these results for research and practice of software engineering in Brazil.}
}

@article{rayyan-727967669,
  title={Kanban in software development: A systematic literature review},
  year={2013},
  pages={9-16},
  author={Ahmad, Muhammad Ovais and Markkula, Jouni and Oivo, Markku},
  keywords={systematic literature review, Systematics, Visualization, Bibliographies, Context, kanban, lean approach, Manufacturing industries, software development, Software development management, Software},
  abstract={Using of Kanban in software development is an emerging topic. This systematic literature review was conducted in order to analyze the current trend of Kanban usage in software development and to identify the obtained benefits and involved challenges. The search strategy resulted in 492 papers, of which 19 were identified as primary studies relevant to our research. The main reported benefits of using the Kanban method were improved lead time to deliver software, improved quality of software, improved communication and coordination, increased consistency of delivery, and decreased customer reported defects. The reported challenges included lack of knowledge and specialized training as well as various organizational issues. Additionally, suggested practices were extracted from the primary studies and summarized for guiding the practitioners interested in adopting Kanban. The findings of this literature review are intended for helping researchers and practitioners to gain a better understanding of the current state of Kanban usage in software development.}
}

@article{rayyan-727967670,
  title={Tacit knowledge in software testing: A systematic review},
  year={2019},
  pages={1-6},
  author={Idrus, Hariaty Mohd and Ali, Nor'ashikin},
  keywords={Software, Software testing, systematic literature review, Systematics, Bibliographies, Knowledge management, software testing, Knowledge engineering, competency, knowledge management, software testers, tacit knowledge},
  abstract={This paper presents a systematic review of tacit knowledge in software testing. The main objectives of conducting this systematic review is to identify and synthesize issues and challenges regarding tacit knowledge in software testing, its influential factors that affect the tacit knowledge creation, sharing, use and transfer among software testers, its impacts on software testing, and the research methods being undertaken by the selected reviewed studies. Systematic Literature Review (SLR) is used for reviewing 14 primary studies that focus specifically on tacit knowledge in software testing. This paper provides significant evidences on the importance of tacit knowledge in software testing that will direct further directions for knowledge management (KM) and software testing practitioners and researchers.}
}

@article{rayyan-727967671,
  title={Change Impact analysis and propagation in service based business process management systems preliminary results from a systematic review},
  year={2014},
  pages={7-12},
  author={Amjad Alam, Khubaib and Ahmad, Rodina Binti and Akhtar, Maria},
  keywords={Systematics, SOA, Service-oriented architecture, Context, systematic literature review (SLR), BPM, Organizations, Analytical models, Change Propagation, CIA, Dependency Analysis, Web Services},
  abstract={Change Impact analysis and propagation have widely been studied in Software engineering research, but most studies are related to monolithic software applications, and very few studies have focused on distributed environments. Newer technologies like SOA, BPM and Cloud demand different perspectives and newer tools and techniques to support impact analysis and propagation in distributed environments. SOA adoption is fairly recent and major concern is now shifting towards maintenance and evolution of the service based business Process management systems. Change impact analysis and propagation have been identified as a potential research area in this context. This study is part of a larger study to systematically review all available research on impact analysis and propagation in context of Business process management (BPM) and Service Oriented Architecture (SOA). Preliminary results have been reported by answering 2 selected research Questions. 43 studies were selected out of initial set of 182 research articles. Studies answering selected research questions have been included in this report. BPM is considered at Business Level for Business operations and Process Models, while SOA is considered as a Deployment Architecture at service level. We have extended the scope of our study to Inter-Process and inter-service change analysis in addition to Top-Down, Bottom-Up analysis. This study revealed that although evolution of service based systems is getting significant attention, very few approaches and tools exist to support impact analysis and propagation activities.}
}

@article{rayyan-727967672,
  title={Using scrum in global software development: A systematic literature review},
  year={2009},
  pages={175-184},
  author={Hossain, Emam and Babar, Muhammad Ali and Paik, Hye-young},
  keywords={Software engineering, systematic literature reviews, Data mining, Project management, Scrum, Programming, Global software development, Australia, agile approaches, Computer architecture, Computer industry, Meeting planning, Time to market, Variable speed drives, Software},
  abstract={There is a growing interest in applying agile practices in global software development (GSD) projects. The literature on using Scrum, one of the most popular agile approaches, in distributed development projects has steadily been growing. However, there has not been any effort to systematically select, review, and synthesize the literature on this topic. We have conducted a systematic literature review of the primary studies that report using Scrum practices in GSD projects. Our search strategy identified 366 papers, of which 20 were identified as primary papers relevant to our research. We extracted data from these papers to identify various challenges of using Scrum in GSD. Current strategies to deal with the identified challenges have also been extracted. This paper presents the reviewpsilas findings that are expected to help researchers and practitioners to understand the challenges involved in using Scrum for GSD projects and the strategies available to deal with them.}
}

@article{rayyan-727967673,
  title={A systematic literature review of best practices and challenges in follow-the-sun software development},
  year={2013},
  pages={18-23},
  author={Kroll, Josiane and Hashmi, Sajid Ibrahim and Richardson, Ita and Audy, Jorge L N},
  keywords={Software, Software engineering, Testing, Global software development, challenges, Best practices, best practice, Cultural differences, Electronic mail, Follow-the-sun (FTS), Global communication},
  abstract={Follow-the-sun (FTS) software development is a strategy used to reduce the length of software projects that are developed across globally distributed locations. However, due to communication and collaboration challenges, software companies find it difficult to adopt this development strategy during task allocation and daily project handovers. In this study, we present results from a Systematic Literature Review (SLR) performed on papers published between 1990 and 2012. Our goal was to identify best practices and challenges for FTS implementation. We found 36 best practices and 17 challenges for FTS. These results are discussed in this paper in order to indicate opportunities for future research and make our results useful for the project managers.}
}

@article{rayyan-727967674,
  title={Requirements engineering visualization: A systematic literature review},
  year={2016},
  pages={6-15},
  author={Abad, Zahra Shakeri Hossein and Noaeen, Mohammad and Ruhe, Guenther},
  keywords={Software, Visualization, Manuals, Requirements engineering, Unified modeling language, Data visualization, Stakeholders},
  abstract={Requirements Engineering (RE) is a decision-centric activity which is highly data-intensive. The results of this process are known to have key impact on the results of the project. As known from the experience in other fields and disciplines, visualization can potentially provide more insights into data, information and knowledge studied. While research in the area of information visualization and its application to software engineering has rapidly increased over the last decade, there is only a limited amount of studies addressing the usage and impact of visualization techniques for RE activities. In this paper, we report on the results of a Systematic Literature Review (SLR) related to RE visualization. Extending the established SLR process by the usage of grounded theory for the encoding of papers, we synthesize 18 usage patterns. Even though there are punctual applications, there is a clear deficit on a holistic perspective across the different RE activities. As another conclusion, we derive the clear need for more research on visualization support in particular for tackling requirements uncertainty, requirements verification, and modeling, as well as non-functional requirements (NFRs).}
}

@article{rayyan-727967675,
  title={Evidence relating to object-oriented software design: A survey},
  year={2007},
  pages={482-484},
  author={Bailey, John and Budgen, David and Turner, Mark and Kitchenham, Barbara and Brereton, Pearl and Linkman, Stephen},
  keywords={Software engineering, Software measurement, Data mining, Quality assessment, Software design, Computer science, Computer languages, Performance analysis, Protocols, Software Design, Software},
  abstract={There is little empirical knowledge of the effectiveness of the object-oriented paradigm. To conduct a systematic review of the literature describing empirical studies of this paradigm. We undertook a Mapping Study of the literature. 138 papers have been identified and classified by topic, form of study involved, and source. The majority of empirical studies of OO (object oriented software) concentrate on metrics, relatively few consider effectiveness.}
}

@article{rayyan-727967676,
  title={Continuous practices and technical debt: a systematic literature review},
  year={2020},
  pages={40-44},
  author={Lunde, Bj⊘rn Arild and Colomo-Palacios, Ricardo},
  keywords={Software, Software engineering, Systematics, Bibliographies, Tools, Technical debt, Databases, continuous delivery, continuous deployment, Complexity theory, continuous integration, continuous refactoring, continuous release},
  abstract={Technical debt in software development is a common problem that is overlooked by many development teams. This debt can be generated from a variety of reasons, including time pressure and complexity in software. Technical debt in simple terms is when a simple and less optimized solution is carried out in order to gain short term benefits, which leads to refactoring and reworking code later on, costing both time and money. The issue is present in both big, established companies and small startups, and is the reason why many of these small startups never get enough economic grip before debt catch up and they go bankrupt. This paper aims to address this problem by exploring how continuous practices including DevOps could help resolve this issue by adopting the right approaches into the software development cycle and workflow. So as to collect information about these topics, a systematic literature review has been conducted, covering both positive and negative impacts these practices can have on technical debt. The findings will present the current practices used to manage and reduce the accumulation of technical debt, if and how these approaches can be used to reduce already existing technical debt and which of these practices that have the biggest impact on technical debt. The paper concludes that there's potential for continuous practices including DevOps to possibly reduce technical debt if applied appropriately.}
}

@article{rayyan-727967677,
  title={Multivocal literature reviews in software engineering: Preliminary findings from a tertiary study},
  year={2019},
  pages={1-6},
  author={Neto, Geraldo Torres G and Santos, Wylliams B and Endo, Patricia Takako and Fagundes, Roberta A A},
  keywords={Software, Software engineering, Systematics, Bibliographies, Guidelines, Libraries, Industries},
  abstract={Background: In recent years, studies involving Grey Literature (GL) have been growing and attracting the attention of researchers in software engineering (SE). One of the sources of GL refers to content produced by professionals based on their practical experiences? Recent researches in the SE states that GL can complement areas of research that are not yet clearly defined in the scientific literature. In this context, the Multivocal Literature Review (MLR), a form of Systematic Literature Review (SLR) with the inclusion of GL, emerges. Goal: Provide preliminary work about the current research involving MLR studies? First, we investigate the motivation of the researchers to include GL in review studies; and second, we examine how GL was included in the studies. Method: A tertiary study was conducted to search MLR studies published between 2009 to April of 2019. Results: The main motivations for including GL in review studies are: lack of academic research on the topic, emerging research on this topic, and complementary evidence in the GL? Internet articles and white papers were the main sources of GL data used. Conclusions: The conducting of MLR studies is still in its early stages; we have identified only 12 secondary studies. The MLR studies were conducted using guidelines for performing SLRs. What we consider to be a threat to the validity of these studies, since guidelines to conduct SLR studies do not provide recommendations for quality analysis and synthesis of primary studies, including GL.}
}

@article{rayyan-727967678,
  title={HTTPS contribution in web application security: A systematic literature review},
  year={2020},
  pages={347-356},
  author={Wijitrisnanto, Fajar and Suhardi and Yustianto, Purnomo},
  keywords={systematic literature review, Systematics, Data mining, Search problems, Databases, Protocols, Application security, cookie, csp, hpkp, hsts, https, security, Servers, tls, web application},
  abstract={A Web application is one of the most used technology nowadays due to its flexibility in delivering services to society. It also plays a good portion in enhancing our daily life since it could provide almost any kind of services through an application served from the internet. Thus, many users' private information runs the risk of being exposed to an unauthorized party. Standard browser connection uses HTTPS protocol, while both TLS over HTTP and Web application are known for several of vulnerabilities. This paper presents the results of an SLR study on web application security of HTTPS implementation. The study selects 45 qualified papers related to the topic and analyzed 24 of the documents. The findings are categorized into three labels: threats, threats impact, and defense mechanisms. This work also classifies the attack and threats based on the impact produced. In this study, the lack of understanding about security-related mechanism in TLS, session management, and web application still become the culprit of most attack and vulnerability. Based on this work, a researcher could better prioritize and prepare security mechanism to overcome the threats.}
}

@article{rayyan-727967679,
  title={Software architecture optimization methods: A systematic literature review},
  year={2013},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={39},
  number={5},
  pages={658-683},
  author={Aleti, Aldeida and Buhnova, Barbora and Grunske, Lars and Koziolek, Anne and Meedeniya, Indika},
  keywords={Software, systematic literature review, Systematics, Software architecture, Taxonomy, Computer architecture, optimization methods, Optimization methods, problem overview, Software architecture optimization},
  abstract={Due to significant industrial demands toward software systems with increasing complexity and challenging quality requirements, software architecture design has become an important development activity and the research domain is rapidly evolving. In the last decades, software architecture optimization methods, which aim to automate the search for an optimal architecture design with respect to a (set of) quality attribute(s), have proliferated. However, the reported results are fragmented over different research communities, multiple system domains, and multiple quality attributes. To integrate the existing research results, we have performed a systematic literature review and analyzed the results of 188 research papers from the different research communities. Based on this survey, a taxonomy has been created which is used to classify the existing research. Furthermore, the systematic analysis of the research literature provided in this review aims to help the research community in consolidating the existing research efforts and deriving a research agenda for future developments.}
}

@article{rayyan-727967680,
  title={Evaluation and measurement of software process Improvement—A systematic literature review},
  year={2012},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={38},
  number={2},
  pages={398-424},
  author={Unterkalmsteiner, Michael and Gorschek, Tony and Islam, A K M Moinul and Cheng, Chow Kian and Permadi, Rahadian Bayu and Feldt, Robert},
  keywords={Software, Software measurement, Systematics, Data mining, Organizations, Current measurement, metrics/measurement, Process implementation and change, process measurement, systematic literature review.},
  abstract={BACKGROUND-Software Process Improvement (SPI) is a systematic approach to increase the efficiency and effectiveness of a software development organization and to enhance software products. OBJECTIVE-This paper aims to identify and characterize evaluation strategies and measurements used to assess the impact of different SPI initiatives. METHOD-The systematic literature review includes 148 papers published between 1991 and 2008. The selected papers were classified according to SPI initiative, applied evaluation strategies, and measurement perspectives. Potential confounding factors interfering with the evaluation of the improvement effort were assessed. RESULTS-Seven distinct evaluation strategies were identified, wherein the most common one, “Pre-Post Comparison,” was applied in 49 percent of the inspected papers. Quality was the most measured attribute (62 percent), followed by Cost (41 percent), and Schedule (18 percent). Looking at measurement perspectives, “Project” represents the majority with 66 percent. CONCLUSION-The evaluation validity of SPI initiatives is challenged by the scarce consideration of potential confounding factors, particularly given that “Pre-Post Comparison” was identified as the most common evaluation strategy, and the inaccurate descriptions of the evaluation context. Measurements to assess the short and mid-term impact of SPI initiatives prevail, whereas long-term measurements in terms of customer satisfaction and return on investment tend to be less used.}
}

@article{rayyan-727967681,
  title={Using bio-inspired features selection algorithms in software effort estimation: A systematic literature review},
  year={2019},
  pages={220-227},
  author={Ali, Asad and Gravino, Carmine},
  keywords={Software, Systematic literature review, Effort estimation, Quality assessment, Estimation, Software algorithms, Bio inspired algorithms, Feature extraction, Feature selection algorithms, Genetic algorithms, Prediction algorithms, Algorithms, Tocopherols},
  abstract={Feature selection algorithms select the best and relevant set of features of the datasets which leads to an increase in the accuracy of predictions when employed with the machine learning techniques. Different feature selection algorithms are used in the domain of Software Development Effort Estimations (SDEE) and recently the use of bio-inspired feature selection algorithms got the attention of the researchers, which provided the best results in terms of the accuracy measures. In this paper, we manage to systematically evaluate and assess different bio-inspired feature selection algorithms which have been employed and investigated in the studies related to SDEE with the aim of increasing the accuracy of estimations. To the best of our knowledge, there is no Systematic Literature Review (SLR) which investigated the use of bio-inspired algorithms in SDEE. Since, the use of bio-inspired algorithms in the area of SDEE started in the late 2000, we have considered the studies published between 2007-2018. We have selected about 30 different studies from five digital libraries, i.e., IEEE explore, Springer, ScienceDirect, ACM digital library, and Google Scholar, after the filtering of inclusion/exclusion and quality assessment criteria. The main findings of our SLR are that Genetic Algorithms (GA) and Particle Swarm Optimizations (PSO) are widely used bio-inspired algorithms. Moreover, GA and PSO are the algorithms which outperform baseline estimation techniques (estimation techniques employed without any feature selection algorithms) in more number of experiments, in terms of prediction accuracy.}
}

@article{rayyan-727967682,
  title={Quantitative determination of the relationship between internal validity and bias in software engineering experiments: Consequences for systematic literature reviews},
  year={2011},
  pages={285-294},
  author={Dieste, Oscar and Grim´n, Anna and Juristo, Natalia and Saxena, Himanshu},
  keywords={Software engineering, Systematics, Checklist, Quality assessment, Inspection, Context, Systematic Literature Review (SLR), Correlation, Instruments, Quality Assessment (QA) of experiments, Quality Scale, Software, Bias (Epidemiology)},
  abstract={Quality assessment is one of the activities performed as part of systematic literature reviews. It is commonly accepted that a good quality experiment is bias free. Bias is considered to be related to internal validity (e.g., how adequately the experiment is planned, executed and analysed). Quality assessment is usually conducted using checklists and quality scales. It has not yet been proven, however, that quality is related to experimental bias. Aim: Identify whether there is a relationship between internal validity and bias in software engineering experiments. Method: We built a quality scale to determine the quality of the studies, which we applied to 28 experiments included in two systematic literature reviews. We proposed an objective indicator of experimental bias, which we applied to the same 28 experiments. Finally, we analysed the correlations between the quality scores and the proposed measure of bias. Results: We failed to find a relationship between the global quality score (resulting from the quality scale) and bias, however, we did identify interesting correlations between bias and some particular aspects of internal validity measured by the instrument. Conclusions: There is an empirically provable relationship between internal validity and bias. It is feasible to apply quality assessment in systematic literature reviews, subject to limits on the internal validity aspects for consideration.}
}

@article{rayyan-727967683,
  title={Programmer eXperience: A systematic literature review},
  year={2019},
  journal={IEEE Access},
  issn={2169-3536},
  volume={7},
  pages={71079-71094},
  author={Morales, Jenny and Rusu, Cristian and Botella, Federico and Quiñones, Daniela},
  keywords={systematic literature review, Systematics, Bibliographies, Programming, Usability, Heuristic evaluation, Programmer eXperience, Programming environments, usability, User experience, User eXperience},
  abstract={Programmers use various software development artifacts in their work, such as programming environments, design documents, and programming codes. These software artifacts can be studied and improved based on usability and User eXperience (UX) factors. In this paper, we consider programmers to be a specific case of users and analyze different elements that influence their experience in this specific context. We conducted a systematic literature review of papers published over the last ten years related to 1) the definition of the Programmer eXperience (PX); 2) the PX, UX, and usability factors regarding the programming environments, design documents, and programming codes; and 3) sets of heuristics to evaluate the software development artifacts mentioned before. We analyzed 73 articles, and the results obtained show that: 1) the important elements that influence the PX are the motivation of programmers and the choice of tools they use in their work, such as programming environments; 2) most of the identified studies (59%) aimed to evaluate the influence of the PX, UX, and usability on programming environments; 3) the majority of the studies (70%) used methods such as usability tests and/or heuristic evaluation methods; and 4) four sets of heuristics are used to evaluate software development artifacts in relation to programming environments, programming languages, and application programming interfaces. The results suggest that further research in this area is necessary to better understand and evaluate the concept of the PX.}
}

@article{rayyan-727967684,
  title={Software stability: A systematic literature review},
  year={2018},
  pages={109-115},
  author={Ramirez, Saul Melchor and Cortes, Karen and Ocharan-Hernandez, Jorge Octavio and Sanchez Garcia, Angel Juan},
  keywords={Software, Measurement, Software architecture, Computer architecture, design stability, evolvability, Proposals, software architecture design, software eolution, software stability, Stability criteria, stability metric},
  abstract={Evolvability is the capability of a software product to be evolved to continue to serve its customers in a cost effective way. The term software evolution is closely related to maintenance, and evolvability is often used to mean maintainability or modifiability. When developing software architectures, evolvability is a desired quality attribute. It must be remembered that software architecture design is related to the proper consideration of quality attributes. Architectural decisions are made in order to fulfill, not only functional requirements, but also quality attributes. In order to design a software architecture that properly considers evolvability, Maccari and Galal have proposed an evolvability view which considers components stability. However, there is no guidance on how to obtain components stability. In order to propose a stability metric for architectural components, a systematic literature review (SLR) was performed. The intention of such a systematic literature review was to identify and evaluate available research about software stability. The results of the SRL are presented along with some proposals for further research.}
}

@article{rayyan-727967685,
  title={Supporting evidence-based Software Engineering with collaborative information retrieval},
  year={2010},
  pages={1-5},
  author={Ramampiaro, Heri and Cruzes, Daniela and Conradi, Reidar and Mendona, Manoel},
  keywords={Information retrieval, Systematics, Data mining, Systematic Review, Empirical Software Engineering, Collaborative Information Retrieval, Software, Information Storage and Retrieval},
  abstract={The number of scientific publications is constantly increasing, and the results published on Empirical Software Engineering are growing even faster. Some software engineering publishers have began to collaborate with research groups to make available repositories of software engineering empirical data. However, these initiatives are limited due to issues related to the available search tools. As a result, many researchers in the area have adopted a semi-automated approach for performing searches for systematic reviews as a mean to extract empirical evidence from published material. This makes this activity labor intensive and error prone. In this paper, we argue that the use of techniques from information retrieval, as well as text mining, can support systematic reviews and improve the creation of repositories of SE empirical evidence.}
}

@article{rayyan-727967686,
  title={What do affect customers to use mobile payment continually? A systematic literature review},
  year={2020},
  pages={1-6},
  author={Putri, Mutia Fadhila and Purwandari, Betty and Hidayanto, Achmad Nizar},
  keywords={Systematics, Bibliographies, Databases, Business, Informatics, Customer Relationship Management, intention to use, Mobile Payment, Online banking, Reliability theory},
  abstract={The intention to use mobile payment (M-Payment) has been growing, in line with digital economy growth. This phenomenon contributes to the emergence of M-Payment research lately. Several theories can be applied to those researches. In general, the research tends to adopt any particular theory to explore the factors that affect the use of M-Payment. A comprehensive and succinct review is needed to continually help scholars and practitioners understand what affects customers' use of M-Payment, the marketing strategy applied in the M - Payment business, and the theories implemented in current M-Payment research. This study aims to categorize the findings and assess M-Payment research's state of the art to facilitate future research. Data retrieved from IEEE, ACM, Science Direct, Scopus, EBSCOhost, and SpringerLink databases were published between 2017 and 2020. The data is analyzed by following the Kitchenham systematic literature review approach. As a result, this study establishes a table of theoretical used in M-Payment research, the marketing strategy applied in the M - Payment business, and factors affecting M-Payment's use classified into three main groups: technological factors, behavioral factors, and personal factors.}
}

@article{rayyan-727967687,
  title={Usability heuristics: A systematic review},
  year={2016},
  pages={1-8},
  author={Jimenez, Cristhy and Lozada, Pablo and Rosas, Pablo},
  keywords={Systematics, Systematic Literature Review, Bibliographies, Data mining, Usability, Software systems, Proposals, Heuristic Evaluation, Software Evaluation, Usability Heuristics},
  abstract={Heuristic evaluation is one of the most commonly used usability inspection methods. The set of usability heuristics plays a key role in the performance of this method. Traditionally, the ten Nielsen's usability heuristics have been widely applied. However, nowadays, new sets of specific usability heuristics are been developed in order to improve the results of usability evaluations. This paper presents a preliminar systematic literature review (SLR) conducted in order to analyze the progress in the development and use of usability heuristics. The main goal was to identify new sets of usability heuristics but also justify the need to formalize a process for developing specific usability heuristics.}
}

@article{rayyan-727967688,
  title={The role of project manager in agile software teams: A systematic literature review},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={117109-117121},
  author={Gandomani, Taghi Javdani and Tavakoli, Zeinab and Zulzalil, Hazura and Farsani, Hadi Khosravi},
  keywords={Software, systematic literature review, Systematics, Bibliographies, Agile software development, Project management, Scrum, Databases, Agile project management, Agile project manager, Companies, Planning, project manager},
  abstract={The use of Agile methodologies in software development has grown steadily over recent years. One of the main emphases of these methods is employing cross-functional and self-organized teams and highly skilled developers in software projects. In such a condition, project management would be a serious concern. Indeed, it would be confusing whether Agile teams are really in need of the role of the project manager. While Agile methodologies do not explicitly define the role of the project manager, many reports mention the existence of this role in Agile projects in real environments. So, it seems that the existence of this role is debated. Conducting a Systematic Literature Review, this study tried to find out answers to the ambiguities and questions regarding the role of Agile project management, the role of the project manager, and related issues. Focusing on the primary studies, the results show that there is no independent job called project manager in Agile methodologies. However, there is a need for it. Moreover, in the absence of this role in Agile methodologies and the need for it, it seems that this role would be structurally different from the traditional role of the project manager in terms of responsibilities and duties. Finally, the results show that pre-defined roles in Agile methodologies are often responsible for the project manager duties in software teams with no project manager.}
}

@article{rayyan-727967689,
  title={Global software engineering: Evolution and trends},
  year={2016},
  pages={144-153},
  author={Ebert, Christof and Kuhrmann, Marco and Prikladnicki, Rafael},
  keywords={Software, Software engineering, Collaboration, Organizations, global software engineering, Industries, GSE, ICGSE, Indexes, longitudinal study, Market research, near-shoring, offshoring, outsourcing},
  abstract={Professional software products and IT systems and services today are developed mostly by globally distributed teams, projects, and companies. Successfully orchestrating Global Software Engineering (GSE) has become the major success factor both for organizations and practitioners. Yet, more than a half of all distributed projects does not achieve the intended objectives and is canceled. This paper summarizes experiences from academia and industry in a way to facilitate knowledge and technology transfer. It is based on an evaluation of 10 years of research, and industry collaboration and experience reported at the IEEE International Conference on Software Engineering (ICGSE) series. The outcomes of our analysis show GSE as a field highly attached to industry and, thus, a considerable share of ICGSE papers address the transfer of Software Engineering concepts and solutions to the global stage. We found collaboration and teams, processes and organization, sourcing and supplier management, and success factors to be the topics gaining the most interest of researchers and practitioners. Beyond the analysis of the past conferences, we also look at current trends in GSE to motivate further research and industrial collaboration.}
}

@article{rayyan-727967690,
  title={The impact of limited search procedures for systematic literature reviews — A participant-observer case study},
  year={2009},
  pages={336-345},
  author={Kitchenham, Barbara and Brereton, Pearl and Turner, Mark and Niazi, Mahmood and Linkman, Stephen and Pretorius, Rialette and Budgen, David},
  keywords={Software engineering, Software measurement, Guidelines, Computer science, Aggregates, Councils, Mathematics, Particle measurements, Physics computing, Software libraries},
  abstract={This study aims to compare the use of targeted manual searches with broad automated searches, and to assess the importance of grey literature and breadth of search on the outcomes of SLRs. We used a participant-observer multi-case embedded case study. Our two cases were a tertiary study of systematic literature reviews published between January 2004 and June 2007 based on a manual search of selected journals and conferences and a replication of that study based on a broad automated search. Broad searches find more papers than restricted searches, but the papers may be of poor quality. Researchers undertaking SLRs may be justified in using targeted manual searches if they intend to omit low quality papers; if publication bias is not an issue; or if they are assessing research trends in research methodologies.}
}

@article{rayyan-727967691,
  title={A systematic literature review of interoperable architecture for e-government portals},
  year={2011},
  pages={82-87},
  author={Sedek, Khairul Anwar and Sulaiman, Shahida and Omar, Mohd Adib},
  keywords={Systematic literature review, Security, Reliability, Interoperability, Computer architecture, E-government Portal, Electronic government, Portals, Service oriented architecture, Software Architecture},
  abstract={One of the roles of e-government portals is to provide a one-stop service to users. In order to fulfill this role, it requires collaboration with other government agencies and businesses to provide an effective one-stop center for users to access and perform various services. Current e-government portals are mostly lack of interoperability whereby users still need to access government services from various portals or websites. Interoperability is a technical requirement to achieve government services collaboration and integration. There are many challenges and approaches to achieve better interoperability in e-government portals. Architecture-based and model-based approaches are essential research areas that can improve interoperability starting from the planning stages. Architecture provides overall overview of e-government components and relationship between components. This paper systematically reviews current architecture-based approaches to find a suitable approach and its requirements to produce a better architecture for e-government portal based on the lessons learned from the previous works.}
}

@article{rayyan-727967692,
  title={Reporting guidelines for simulation-based studies in software engineering},
  year={2012},
  pages={156-160},
  author={de França, Breno Bernard Nicolau and Travassos, Guilherme Horta},
  keywords={Software Engineering, Systematic Review, Computer Simulation, Guideline, Simulation Studies, Software},
  abstract={Background: Some scientific fields, such as automobile, drugs discovery or engineer have used simulation-based studies (SBS) to faster the observation of phenomena and evolve knowledge. All of them organize their working structure to perform computerized experiments based on explicit research protocols and evidence. The benefits have been many and great advancements are continuously obtained for the society. However, could the same approach be observed in Software Engineering (SE)? Are there research protocols and evidence based models available in SE for supporting SBS? Are the studies reports good enough to support their understanding and replication? AIM: To characterize SBS in SE and organize a set of reporting guidelines aiming at improving SBS' understandability, replicability, generalization and validity. METHOD: To undertake a secondary study to characterize SBS. Besides, to assess the quality of reports to understand the usually reported information regarding SBS. RESULTS: From 108 selected papers, it has been observed several relevant initiatives regarding SBS in software engineering. However, most of the reports lack information concerned with the research protocol, simulation model building and evaluation, used data, among others. SBS results are usually specific, making their generalization and comparison hard. No reporting standard has been observed. CONCLUSIONS: Advancements can be observed in SBS in Software Engineering. However, the lack of reporting consistency can reduce understandability, replicability, generalization and compromise their validity. Therefore, an initial set of guidelines is proposed aiming at improving SBS report quality. Further evaluation must be accomplished to assess the guidelines feasibility when used to report SBS in Software Engineering.}
}

@article{rayyan-727967693,
  title={Taxonomy for complexity estimation in agile methodologies: A systematic literature review},
  year={2019},
  pages={87-96},
  author={Durán, Mayra and Juárez-Ramírez, Reyes and Jiménez, Samantha and Tona, Claudia},
  keywords={Scrum, complexity, estimation, User Story},
  abstract={Currently, software development teams use different methods to estimate their projects, in Scrum, there are different methods to estimate the value and complexity of a user story. However, these methods are not accurate and most of them depend on many attributes that sometimes are not contemplated or are unknown by the development team. Although Planning Poker has many benefits, it has been found that this method is efficient mostly for experience teams, since the estimation will always depend on the observation of an expert and his experience. On the other hand, without a standard set of attributes for describing complexity, subjectivity problems remain when estimating user stories. This study presents a systematic literature review in which identifies the attributes that influence the complexity estimation in user stories.}
}

@article{rayyan-727967694,
  title={Usability of mobile applications: A systematic literature study},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={55563-55577},
  author={Weichbroth, Paweł},
  keywords={systematic literature review, Systematics, Bibliographies, Mobile applications, Usability, usability, attributes, ISO Standards, measures, Smart phones, usability evaluation methods},
  abstract={Since the release of the first mobile devices, the usability of on-board applications has been the concern not only of software vendors but hardware manufacturers as well. The academia community later willingly joined the discussion on usability in terms of theory and empirical measurement, having experience and knowledge in desktop settings. At first sight, such a background should guarantee a solid foundation to conduct research on software usability in a new setting. However, a preliminary study on the subject matter revealed methodological disorder in contemporary literature. As a matter of fact, a need emerged to review existing usability definitions, attributes and measures to recognize all associated aspects. In order to fill this void, we conducted a systematic literature review on usability studies indexed by the Scopus database and devoted to mobile applications. The input volume covers 790 documents from 2001 to 2018. The data analysis shows that the ISO 9241-11 usability definition has been adopted in an unchanged form and popularized as the standard by the HCI community. Secondly, in total, 75 attributes were identified and analysed. The most frequent are efficiency (70%), satisfaction (66%) and effectiveness (58%), which directly originate from the above definition. Subsequently, the less frequent are learnability (45%), memorability (23%), cognitive load (19%) and errors (17%). The last two concern simplicity (13%) and ease of use (9%). Thirdly, in the evaluation of usability, controlled observation and surveys are two major research methods applied, while eye-tracking, thinking aloud and interview are hardly used and serve as complementary to collect additional data. Moreover, usability evaluations are often confused with user experience dimensions, covering not only application quality characteristics, but also user beliefs, emotions and preferences. All these results indicate the need for further research on the usability of mobile applications, aiming to establish a consensus in the theory and practice among all interested parties.}
}

@article{rayyan-727967695,
  title={A simplified systematic literature review: Improving Software Requirements Specification quality with boilerplates},
  year={2015},
  pages={99-105},
  author={Anuar, Umairah and Ahmad, Sabrina and Emran, Nurul A},
  keywords={Software, Systematics, Bibliographies, Data mining, Requirements engineering, Stakeholders, Protocols, boilerplates, software requirements specification, SRS quality},
  abstract={The quality of Software Requirements Specification (SRS) is crucial in order to ensure successful project completion. SRS of poor quality usually lacks of quality attributes such as completeness, accuracy and disambiguity. Boilerplate is a technique used to deal with problems in SRS. However, study on the coverage of boilerplate contribution especially in improving SRS quality is limited. This paper presents Systematic Literature Review (SLR) on problems in SRS and boilerplates. The review that covers literature from 1997 to 2015 reveals that 1) poor quality SRS is the most popular problem among the other five SRS problems discovered, 2) Boilerplate technique has been applied to cope with SRS of poor quality, where disambiguity has been found the most popular quality attribute.}
}

@article{rayyan-727967696,
  title={Evolution of the web of social machines: A systematic review and research challenges},
  year={2020},
  journal={IEEE Transactions on Computational Social Systems},
  issn={2329-924X},
  volume={7},
  number={2},
  pages={373-388},
  author={Brito, Kellyton dos Santos and de Lima, Alysson Alves and Ferreira, Sérgio Endrigo and de Arruda Burégio, Vanilson and Garcia, Vinicius Cardoso and de Lemos Meira, Silvio Romero},
  keywords={systematic review, Software, Software engineering, Systematics, Bibliographies, Quality assessment, Taxonomy, Conferences, Human???machine interaction, social computing, social machines (SMs), web 2.0},
  abstract={Social machines (SMs) are the term used to define processes in which the people do the creative work and the machine does the administration. The concept was scarcely studied until 2013, when the series of workshops on SMs was created, and the topic began to receive more attention. However, it is not clear how research has evolved since then. This article aims to investigate and summarize how the research field of SM has evolved since 2013, to outline the state of the art and practice, and identify research opportunities within this field. We performed a systematic literature review analyzing the quantity and quality of publications, the main topics addressed, the current classifications of SMs, the context in which the concepts are used, and the main perceived challenges. We identified and analyzed 56 relevant studies addressing 12 topics, representing the current practical landscape of research regarding SM. Our findings suggest that: 1) research interest in SM is increasing, but is still concentrated into two research clusters; 2) topics are grouped under two main headings: a) human behavior and b) software development; 3) there is still a need for a common taxonomy to define and classify SM; 4) the main contexts are crowdsourcing and social networks, and the majority of studies are small-scale studies in an academic setup; and 5) more empirical rigor and evidence is needed regarding their use, benefits and challenges, despite some evidence regarding challenges related to user engagement, trust, scalability, and a better human-machine collaboration. Finally, a vision of the future of SMs, with the integration of web of people, artificial intelligence, and things, is also presented and discussed.}
}

@article{rayyan-727967697,
  title={The relationship between requirements engineering and virtual reality systems: A systematic literature review},
  year={2013},
  pages={53-62},
  author={dos Santos, Alinne C Corrêa and Delamaro, Márcio Eduardo and Nunes, Fátima L S},
  keywords={Software, Systematics, Data mining, Manuals, Databases, Conferences, requirements engineering, systematic literature revie, virtual reality, Virtual reality},
  abstract={The development of Virtual Reality (VR) is a difficult task, requiring knowledge and understanding of different areas. Furthermore, the Requirements Engineering (RE) has a key role in the development of VR systems, because the requirements of these systems are imposed directly by human senses and abstracted by the ability of the developers to represent the physical and kinematic models of these environments. The aim of this paper is to present the results of a Systematic Literature Review to consolidate existing evidence regarding the use of RE for VR systems and VR contributions to the RE process. We have analyzed 12 primary studies published between 1998 and 2011. The results indicate mainly the deficiency of studies that show the use of the RE process for VR systems, because it is necessary that this process occur in accordance with the technological peculiarities of VR systems.}
}

@article{rayyan-727967698,
  title={Using visual text mining to support the study selection activity in systematic literature reviews},
  year={2011},
  pages={77-86},
  author={Felizardo, Katia R and Salleh, Norsaremah and Martins, Rafael M and Mendes, Emilia and MacDonell, Stephen G and Maldonado, Jose C},
  keywords={Systematics, Text mining, Visualization, systematic literature review (SLR), Data visualization, Evidence-based software engineering (EBSE), Educational institutions, Color, study selection activity, visual text mining (VTM)},
  abstract={Background: A systematic literature review (SLR) is a methodology used to aggregate all relevant existing evidence to answer a research question of interest. Although crucial, the process used to select primary studies can be arduous, time consuming, and must often be conducted manually. Objective: We propose a novel approach, known as 'Systematic Literature Review based on Visual Text Mining' or simply SLR-VTM, to support the primary study selection activity using visual text mining (VTM) techniques. Method: We conducted a case study to compare the performance and effectiveness of four doctoral students in selecting primary studies manually and using the SLR-VTM approach. To enable the comparison, we also developed a VTM tool that implemented our approach. We hypothesized that students using SLR-VTM would present improved selection performance and effectiveness. Results: Our results show that incorporating VTM in the SLR study selection activity reduced the time spent in this activity and also increased the number of studies correctly included. Conclusions: Our pilot case study presents promising results suggesting that the use of VTM may indeed be beneficial during the study selection activity when performing an SLR.}
}

@article{rayyan-727967699,
  title={Systematic literature review: Success, failure, risks, benefits and barriers factors in the adoption of open source software},
  year={2018},
  pages={328-336},
  author={Carvallo Vega, Juan Pablo and Crespo Martinez, Paul Esteban and Carvajal Vargas, Fabian Marcelo and Vintimilla Guzman, Rosalva Natali},
  keywords={Systematics, SLR, Systematic Literature Review, Bibliographies, Open source software, OSS, Open Source Software, Information systems, Information and communication technology, IT, Silicon compounds, Software},
  abstract={The paradigm of Open Source Software (OSS) has revolutionized the way in which the software is used, marketed and distributed. Due to its strategic importance, in recent years, public administrations have defined plans for the promotion and strengthening of Information and Communication Technologies (ICT) based on the use of OSS. These strategies have been recognized benefits and a wide social repercussion, given that the open and collaborative paradigm of the OSS phenomenon allows the use and diffusion of ICTs at all social levels. However, it limits the exploitation of the benefits of adopting OSS in the public, private industry and in the Ecuadorian society in general, due to shortcomings in the identification, assessment and risk management, in addition to good practices and adoption, the motive this project is to make a systematic literature review of the OSS adoption, based on Kitchenham and Charters methodological guide; this guide consists in a technique based on empirical research, which requires following a protocol to collect the literature on existing research, related to the free software adoption by organizations, for obtaining relevant references of success, failure, risk, benefits and barriers factors of adoption, in order to determinate the current situation of the OSS use in Ecuador.}
}

@article{rayyan-727967700,
  title={Predicting infections using computational intelligence – a systematic review},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={31083-31102},
  author={Baldominos, Alejandro and Puello, Adráan and Oğul, Hasan and Aşuroğlu, Tunç and Colomo-Palacios, Ricardo},
  keywords={machine learning, systematic literature review, Systematics, Bibliographies, Machine learning, Computational intelligence, Diseases, expert systems, infection prediction, Medical diagnostic imaging, physiological signals, Physiology, Intelligence},
  abstract={Infections encompass a set of medical conditions of very diverse kinds that can pose a significant risk to health, and even death. As with many other diseases, early diagnosis can help to provide patients with proper care to minimize the damage produced by the disease, or to isolate them to avoid the risk of spread. In this context, computational intelligence can be useful to predict the risk of infection in patients, raising early alarms that can aid medical teams to respond as quick as possible. In this paper, we survey the state of the art on infection prediction using computer science by means of a systematic literature review. The objective is to find papers where computational intelligence is used to predict infections in patients using physiological data as features. We have posed one major research question along with nine specific subquestions. The whole review process is thoroughly described, and eight databases are considered which index most of the literature published in different scholarly formats. A total of 101 relevant documents have been found in the period comprised between 2003 and 2019, and a detailed study of these documents is carried out to classify the works and answer the research questions posed, resulting to our best knowledge in the most comprehensive study of its kind. We conclude that the most widely addressed infection is by far sepsis, followed by Clostridium difficile infection and surgical site infections. Most works use machine learning techniques, from which logistic regression, support vector machines, random forest and naive Bayes are the most common. Some machine learning works provide some ideas on the problems of small data and class imbalance, which can be of interest. The current systematic literature review shows that automatic diagnosis of infectious diseases using computational intelligence is well documented in the medical literature.}
}

@article{rayyan-727967701,
  title={An evaluation framework for communication and coordination processes in offshore software development outsourcing relationship: Using fuzzy methods},
  year={2019},
  journal={IEEE Access},
  issn={2169-3536},
  volume={7},
  pages={112879-112906},
  author={Khan, Rafiq Ahmad and Idris, Mohd Yazid and Khan, Siffat Ullah and Ilyas, Muhammad and Ali, Sikandar and Ud Din, Aziz and Murtaza, Ghulam and Khan, Abdul Wahid and Jan, Saeed Ullah},
  keywords={Software, systematic literature review, Collaboration, survey, Outsourcing, Computer science, challenges, Cultural differences, Companies, case study, fuzzy multi-attribute decision-making, practices and mitigation levels, Software outsourcing},
  abstract={Offshore software development outsourcing (OSDO) is a modern business strategy for producing high-quality software at a low cost. The OSDO refers to the practice of contracting to an offshore (extrinsic) organization to perform some or all software development work of a product. For the benefit of the OSDO vendors, this paper aims to develop a “communication and coordination challenges mitigation model” (CCCMM) that provides solutions for unambiguously defined communication and coordination processes in global software development (GSD) environment. Our proposed model is based on the fuzzy multi-attribute decision-making (FMADM) approach incorporating the capability of group decision-making. The FMADM approach is used both in the ranking of survey and assessment of case studies. First, the authors undertook a systematic literature review (SLR) that identified all cited challenges from a set of 101 articles. We identified 18 problem areas faced by the GSD vendors in OSDO relationships. Of these, six were ranked as critical. For the purpose of identifying corrective interventions, a second SLR was conducted that revealed 75 remedial measures extracted from 63 chosen articles. To validate our SLR findings, we surveyed 42 outsourcing experts from six countries. We also categorized six critical challenges and 75 corrective practices into four mitigation levels based on CMMI, SOVRM, and SOPM. In addition, two case studies were conducted to evaluate CCCMM outcomes in OSDO companies. The assessment results of the first case study do not recommend Company-A for the successful implementation of level-2 of the CCCMM, so Company-A stands at level-1. We have observed from the second case study that Company-B has implemented all the critical challenges of the level-2 only; therefore, Company-B is at level-2 “success” of the proposed assessment model.}
}

@article{rayyan-727967702,
  title={Towards a unified checklist for empirical research in software engineering: first proposal},
  year={2012},
  pages={161-165},
  author={Wieringa, Roel},
  keywords={Empirical research methodology, experimental research, observational research, unified checklist, Software},
  abstract={Background: Current checklists for empirical software engineering cover either experimental research or case study research but ignore the many commonalities that exist across all kinds of empirical research. Identifying these commonalities, and explaining why they exist, would enhance our understanding of empirical research in general and of the differences between experimental and case study research in particular. Aim: In this short paper we design a unified checklist for empirical research, that identify commonalities and differences between experimental and case study research. Method: We design the unified checklist as a specialization of the general engineering cycle, which itself is a special case of the rational choice cycle. The unified checklist is based on an analysis and integration of a number of existing checklists. Results: The current version of the checklist exhibits a shared structure of experimental and case study research. Conclusions: Although the checklist exhibits a shared underlying structure of empirical research, its limitations are that it ignores other research methods such as meta-research or surveys and that very little empirical validation of usability and utility of the checklist has been performed so far. We are currently performing one validation, and are planning additional ones. These validations will likely lead to improvements of the current proposal.}
}

@article{rayyan-727967703,
  title={How does software process improvement address global software engineering?},
  year={2016},
  pages={89-98},
  author={Kuhrmann, Marco and Diebold, Philipp and Münch, Jürgen and Tell, Paolo},
  keywords={Software, systematic literature review, Software engineering, Systematics, systematic mapping study, Context, global software development, Companies, Market research, Data analysis, software process improvement},
  abstract={For decades, Software Process Improvement (SPI) programs have been implemented, inter alia, to improve quality and speed of software development. To set up, guide, and carry out SPI projects, and to measure SPI state, impact, and success, a multitude of different SPI approaches and considerable experience are available. SPI addresses many aspects ranging from individual developer skills to entire organizations. It comprises for instance the optimization of specific activities in the software lifecycle as well as the creation of organization awareness and project culture. In the course of conducting a systematic mapping study on the state-of-the-art in SPI from a general perspective, we observed Global Software Engineering (GSE) becoming a topic of interest in recent years. Therefore, in this paper, we provide a detailed investigation of those papers from the overall systematic mapping study that were classified as addressing SPI in the context of GSE. From the main study's result set, a set of 30 papers dealing with GSE was selected for an in-depth analysis using the systematic review instrument to study the contributions and to develop an initial picture of how GSE is considered from the perspective of SPI. Our findings show the analyzed papers delivering a substantial discussion of cultural models and how such models can be used to better address and align SPI programs with multi-national environments. Furthermore, experience is shared discussing how agile approaches can be implemented in companies working at the global scale. Finally, success factors and barriers are studied to help companies implementing SPI in a GSE context.}
}

@article{rayyan-727967704,
  title={The role of software process simulation modeling in software risk management: A systematic review},
  year={2009},
  pages={302-311},
  author={Liu, Dapeng and Wang, Qing and Xiao, Junchao},
  keywords={Software engineering, Project management, Risk management, Software development management, Analytical models, Application software, Discrete event simulation, Process planning, Risk analysis, Software tools, Risk Management, Software},
  abstract={Nowadays software projects are still suffering from many problems due to various kinds of software risks. Software risk management is a crucial part of successful project management, but it is often not well implemented in real-world software projects. One reason is that project managers lack effective and practical tools to manage software risks. Software process simulation modeling (SPSM) has been emerging as a promising approach to address a variety of issues in software engineering area, including risk management. However, the current state of how SPSM supports software risk management is not yet clear. This paper presents a systematic literature review which purpose is to obtain the state of the art of the applications of SPSM in software risk management. We drew the following conclusions from the review results: (1) The number of SPSM studies on software risk management is relatively small, but increasing gradually in recent years. (2) SPSM is mainly applied in risk analysis and risk management planning activities. (3) Software risks related to requirements, development process and management process are the ones most studied by SPSM. (4) Discrete-event simulation and system dynamics are two most popular simulation paradigms, while Hybrid simulation methods are more and more widely used. (5) Extend, iThink and Vensim are the most popular simulation tools in SPSM. (6) Most of SPSM approaches and models have not been well applied into real-world risk management practices.}
}

@article{rayyan-727967705,
  title={On using grey literature and google scholar in systematic literature reviews in software engineering},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={36226-36243},
  author={Yasin, Affan and Fatima, Rubia and Wen, Lijie and Afzal, Wasif and Azhar, Muhammad and Torkar, Richard},
  keywords={Internet, software engineering, tertiary study, Software engineering, Systematics, Bibliographies, Guidelines, systematic mapping, Databases, Grey literature, Google, empirical evaluation, Google scholar, gray, quality checklist, Software},
  abstract={Context: The inclusion of grey literature (GL) is important to remove publication bias while gathering available evidence regarding a certain topic. The number of systematic literature reviews (SLRs) in Software Engineering (SE) is increasing but we do not know about the extent of GL usage in these SLRs. Moreover, Google Scholar is rapidly becoming a search engine of choice for many researchers but the extent to which it can find the primary studies is not known. Objective: This tertiary study is an attempt to i) measure the usage of GL in SLRs in SE. Furthermore this study proposes strategies for categorizing GL and a quality checklist to use for GL in future SLRs; ii) explore if it is feasible to use only Google Scholar for finding scholarly articles for academic research. Method: We have conducted a systematic mapping study to measure the extent of GL usage in SE SLRs as well as to measure the feasibility of finding primary studies using Google Scholar. Results and conclusions: a) Grey Literature: 76.09% SLRs (105 out of 138) in SE have included one or more GL studies as primary studies. Among total primary studies across all SLRs (6307), 582 are classified as GL, making the frequency of GL citing as 9.23%. The intensity of GL use indicate that each SLR contains 5 primary studies on average (total intensity of GL use being 5.54). The ranking of GL tells us that conference papers are the most used form 43.3% followed by technical reports 28.52%. Universities, research institutes, labs and scientific societies together make up 67.7% of GL used, indicating that these are useful sources for searching GL. We additionally propose strategies for categorizing GL and criteria for evaluating GL quality, which can become a basis for more detailed guidelines for including GL in future SLRs. b) Google Scholar Results: The results show that Google Scholar was able to retrieve 96% of primary studies of these SLRs. Most of the primary studies that were not found using Google Scholar were from grey sources.}
}

@article{rayyan-727967706,
  title={Handwritten optical character recognition (OCR): A comprehensive systematic literature review (SLR)},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={142642-142668},
  author={Memon, Jamshed and Sami, Maira and Khan, Rizwan Ahmed and Uddin, Mueen},
  keywords={Systematics, Bibliographies, Databases, Protocols, Character recognition, classification, deep learning, feature extraction, languages, Optical character recognition, Optical character recognition software, Optical imaging},
  abstract={Given the ubiquity of handwritten documents in human transactions, Optical Character Recognition (OCR) of documents have invaluable practical worth. Optical character recognition is a science that enables to translate various types of documents or images into analyzable, editable and searchable data. During last decade, researchers have used artificial intelligence/machine learning tools to automatically analyze handwritten and printed documents in order to convert them into electronic format. The objective of this review paper is to summarize research that has been conducted on character recognition of handwritten documents and to provide research directions. In this Systematic Literature Review (SLR) we collected, synthesized and analyzed research articles on the topic of handwritten OCR (and closely related topics) which were published between year 2000 to 2019. We followed widely used electronic databases by following pre-defined review protocol. Articles were searched using keywords, forward reference searching and backward reference searching in order to search all the articles related to the topic. After carefully following study selection process 176 articles were selected for this SLR. This review article serves the purpose of presenting state of the art results and techniques on OCR and also provide research directions by highlighting research gaps.}
}

@article{rayyan-727967707,
  title={Process mining of duplicate tasks: A systematic literature review},
  year={2020},
  pages={778-784},
  author={Duan, Chenchen and Wei, Qingjie},
  keywords={Systematics, Bibliographies, Data mining, Tools, Task analysis, Measurement, Databases, conformance checking, duplicate tasks, process discovery, process mining, systematic literature reviewe},
  abstract={Process mining improves and provides insights for business processes, which are information related to process execution. In general, process mining can be separated into three classes: process discovery, conformance checking and process enhancement. In order to simplify the process model, we make an assumption that both events in the log and tasks in the model have an injective relation in process mining, i.e., do not allow two tasks to share the same label (thus duplicates task). In addition, Duplicate tasks have some issues concerning the quality of process model discovered and the potential indeterminism in conformance checking. In this paper, we perform a systematic literature review of process discovery and conformance checking metrics for duplicate tasks. This review can: (1) provide a comprehensive review of the current work of duplicate tasks in process discovery and conformance checking; (2) help researchers choose proper process mining approach, tools, and metrics; (3) identify research opportunities in duplicate tasks.}
}

@article{rayyan-727967708,
  title={Bio-inspired algorithms in software fault prediction: A systematic literature review},
  year={2020},
  pages={1-8},
  author={Ali, Asad and Gravino, Carmine},
  keywords={Systematic Literature Review, Bibliographies, Optimization, Estimation, Fault prediction, Software algorithms, Feature extraction, Genetic algorithms, Prediction algorithms, Bio-inspired algorithms, Feature selection, Parameter tuning, Algorithms, Software, Tocopherols},
  abstract={Bio-inspired (and meta-heuristic) algorithms are successfully employed in different domains and the research is going on to accommodate them in all the contexts where optimization is required. In software engineering, and especially Software Fault Prediction (SFP), they are investigated in various forms, e.g., to extract the most relevant features in a dataset or to select the most appropriate set of parameter values in the application of estimation techniques. In SFP, feature selection and optimization/tuning of estimation technique's parameters are an active research area, where recently various bio-inspired algorithms have been employed for both strategies. In this work, we present a Systematic Literature Review (SLR) about the use of bio-inspired algorithms for feature selection and parameter optimization aiming at increasing fault prediction accuracy of the models built with various estimation techniques. To the best of our knowledge, there is no SLR in SFP which covers the use of bioinspired algorithms, both for feature selection and parameter optimization. Since, the use of bio-inspired algorithms in the area of SFP started to be investigated in the late 2000, we have considered studies published between 2007 and 2019. As result, we have selected about 19 studies related to parameter optimization and 15 dealing with feature selection (in total 34 studies), extracted from five well-known digital libraries (ACM digital library, IEEE explore, Springer, ScienceDirect, and Scopus). Genetic Algorithms (GA) and Particle Swarm Optimization (PSO) are the widely used bio-inspired algorithms, both for parameter optimization and feature selection. Among them, GA is the better performed algorithm when evaluating its performance against the baseline (i.e., estimation techniques without any algorithm for feature selection or parameter optimization and trained with their default values). The SLR results also suggests that bio-inspired algorithms seem to provide more accurate predictions for feature selection than for parameter optimization.}
}

@article{rayyan-727967709,
  title={A systematic literature review on autonomous agile teams},
  year={2019},
  pages={146-151},
  author={Acharya, Bibek and Colomo-Palacios, Ricardo},
  keywords={Software, Software engineering, Systematics, Bibliographies, Task analysis, Databases, Organizations, agile teams, autonomous, autonomy, self-managed, self-organizing},
  abstract={Organizations are needed of flexible structures to achieve their aims. In the software arena, Self-organization is the one of the principles of the agile methodologies as defined in the agile manifesto. Autonomous agile teams are a way to conform self-organization. The main objective of this paper is to know about the how autonomous agile teams organize themselves and what are the benefits and the challenges faced by these teams. A systematic literature review was conducted to find answers to these questions. 23 relevant papers were identified as primary sources to define and study autonomous and self-organized agile team. From the review, it was found that there are many benefits and challenges of autonomous agile teams and the various strategies to overcome the challenges.}
}

@article{rayyan-727967710,
  title={Lessons from conducting a distributed quasi-experiment},
  year={2013},
  pages={143-152},
  author={Budgen, David and Kitchenham, Barbara and Charters, Stuart and Gibbs, Shirley and Pohthong, Amnart and Keung, Jacky and Brereton, Pearl},
  keywords={Software, Software Engineering, Software engineering, Educational institutions, Electronic mail, Data collection, Abstracts, Documentation, Materials, Review and Evaluation},
  abstract={Context: Due to the lack of suitably skilled participants, software engineering experiments often lack the statistical power needed to detect the levels of effect that may be encountered. Aim: To investigate whether this can be remedied by running an experiment across multiple sites, organised as a single study rather than as a set of replications. Method: We performed a `trial' of the idea using a topic (structured abstracts) that some of us had studied previously and which required no participant training. We used five sites, each with 16 participants. Results: We were able to demonstrate the benefits of increased statistical power (and of structured abstracts). We report on our experiences with designing and conducting the study and identify some key lessons about how future studies of this form might be organised. Conclusions: The distributed model offers a flexible, robust form that is capable of delivering better statistical power than would be achieved by running a set of parallel replicated studies.}
}

@article{rayyan-727967711,
  title={Towards a taxonomy of replications in empirical software engineering research: A research proposal},
  year={2013},
  pages={50-55},
  author={de Magalhães, Cleyton V C and da Silva, Fabio Q B},
  keywords={Software engineering, Context, Taxonomy, empirical software engineering, Conferences, Instruments, Proposals, Communities, replication types, survey research, taxonomy of replications, Research Design, Software},
  abstract={Goal: We present a research proposal that aims to collect, analyze, and synthesize data towards the construction of a taxonomy of replications in empirical software engineering research. Method: We propose a cross-sectional survey with researchers that performed replications of empirical studies in software engineering. The population of participants is comprised of all researchers that published replications in software engineering and that were identified in a recently published mapping study. Expected Results: We expect to collect data from researchers that have performed different types of replications in order to support the definition of types or categories of replication using a grounded approach. Conclusion: We expect that the study proposed in this article will motivate a discussion in the empirical software engineering community about the need for a clear cut classification of types of replications among other definitions that will be investigated.}
}

@article{rayyan-727967712,
  title={Software process simulation modeling: Facts, trends and directions},
  year={2008},
  pages={59-66},
  author={Zhang, He and Kitchenham, Barbara and Pfahl, Dietmar},
  keywords={systematic literature review, Software engineering, Information systems, Computer science, Mathematics, Software tools, Computational modeling, Computer simulation, Helium, Mathematical model, software process simulation modeling, Software reusability, Software},
  abstract={Software process simulation modeling (SPSM) research has increased since the first ProSim workshop held in 1998 and Kellner, Madachy and Raffo (KMR) discussed the "why, what and how" of process simulation. This paper aims to assess how SPSM has evolved during the past 10 years in particular whether the reasons for SPSM, the simulation paradigms, tools, problem domains, and model scopes have changed. We performed a systematic literature review of software process simulation papers from the ProSim series publications in the last decade. We identified 96 studies from the sources and included them in this review. The papers were categorized into four major types and data needed to address each research question was extracted. We found a need for refining the reasons and the classification scheme for SPSM introduced by KMR. More emerging SPSM paradigms and model scopes were added to enhance KMR's discussion. Trends over time showed that interest in continuous modeling was decreasing and interest in micro-processes was increasing. Hybrid models were based primarily on system dynamics and discrete event simulation and were all implemented by vertical integration. We recommend SPSM research concentrate more on recent software processes and on making SPSM more reusable and thus easier to build.}
}

@article{rayyan-727967713,
  title={A systematic review of feature selection techniques in software quality prediction},
  year={2019},
  pages={1-5},
  author={Alsolai, Hadeel and Roper, Marc},
  keywords={Systematic literature review, feature selection, prediction, software defect, software maintainability, Software},
  abstract={Background: Feature selection techniques are important factors for improving machine learning models because they increase prediction accuracy and decrease the time to create a model. Recently, feature selection techniques have been employed on software quality prediction problems with different results and no clear indication of which techniques are frequently used.Objective: This study aims to conduct a systematic review of the application of feature selection techniques in software quality prediction and answers eight research questions.Method: The review evaluates 15 papers in 9 journals and 6 conference proceedings from 2007 to 2017 using the standard systematic literature review method.Results: The results obtained from this study reveal that the filter feature selection method was the most commonly used in the studies (60%) and RELIEF was the most employed among this method, and a limited number of studies employed an ensemble method. Several studies used public datasets available in the PROMISE software project repository (60%). Most studies focused on software defect prediction (classification problem) using area under curve (AUC) as a primary evaluation measure, whereas only two studies focused on software maintainability prediction (regression problem) using mean magnitude of relative error (MMRE) as a primary evaluation measure. All selected studies performed k-fold cross-validation to evaluate model accuracy. Individual prediction models were mostly employed and ensemble models appeared only in three studies. Naive Bayes was the most investigated among individual models, whereas Random forest was the most investigated among ensemble models.Conclusion: Feature selection techniques used by selected primary studies have a positive impact on the performance of the prediction models. Further, both ensemble feature selection method and ensemble models have the ability for increasing prediction accuracy over single methods or individual models and have reported improvement in the prediction accuracy; however, the application of these techniques in software quality prediction is still limited.}
}

@article{rayyan-727967714,
  title={Identifying strategies for study selection in systematic reviews and maps},
  year={2011},
  pages={351-354},
  author={Petersen, Kai and Ali, Nauman Bin},
  keywords={Software, systematic reviews, Evidence based software engineering, Software engineering, Systematics, Guidelines, Uncertainty, Computer science, Search engines, inclusion and exclusion, paper selection},
  abstract={Study selection in systematic reviews is prone to bias and there exist no commonly defined strategies of how to reduce the bias and resolve disagreement between researchers. This study aims at identifying strategies for bias reduction and disagreement resolution. A review of existing systematic reviews is conducted for study selection strategy identification. In total 13 different strategies have been identified.}
}

@article{rayyan-727967715,
  title={Tools and practices to software quality assurance: A systematic literature review},
  year={2018},
  pages={1-6},
  author={Munoz, Mirna and Mejia, Jezreel and Ibarra, Saúl},
  keywords={Systematics, Tools, Quality assurance, Software quality, software, tools, ISO Standards, development, IEC Standards, practices, quality assurance, quality management, Software},
  abstract={In the last few years the software industry has become relevant, 4 of the 5 most valuable companies in the world are developing software, and in the same way, the budget dedicated to Information Technology (IT) in companies have grown in a 49%. Therefore, currently doesn't exist a market without being related to software. In parallel, there has been developed a set of models and standards for quality assurance. This is the reason why in the software industry the resources focused on software quality assurance have been increased, this is reflected in the large number of quality models such as: CMMI, ISO/IEC standards among others, which contain practices that help organizations to achieve better levels in quality assurance. Besides, this has generated the development of tools to support the quality assurance. This paper presents a systematic review to identify the tools and practices that are implemented to ensure quality in the development of software products and services. Moreover, an analysis of defect types with higher value in the software products is performed.}
}

@article{rayyan-727967716,
  title={A systematic review on code clone detection},
  year={2019},
  journal={IEEE Access},
  issn={2169-3536},
  volume={7},
  pages={86121-86144},
  author={Ain, Qurat Ul and Butt, Wasi Haider and Anwar, Muhammad Waseem and Azam, Farooque and Maqbool, Bilal},
  keywords={Software, SLR, Tools, Databases, CCD, CCD tools, Charge coupled devices, Cloning, code clone detection, code clone types, Computer bugs, Semantics},
  abstract={Code cloning refers to the duplication of source code. It is the most common way of reusing source code in software development. If a bug is identified in one segment of code, all the similar segments need to be checked for the same bug. Consequently, this cloning process may lead to bug propagation that significantly affects the maintenance cost. By considering this problem, code clone detection (CCD) appears as an active area of research. Consequently, there is a strong need to investigate the latest techniques, trends, and tools in the domain of CCD. Therefore, in this paper, we comprehensively inspect the latest tools and techniques utilized for the detection of code clones. Particularly, a systematic literature review (SLR) is performed to select and investigate 54 studies pertaining to CCD. Consequently, six categories are defined to incorporate the selected studies as per relevance, i.e., textual approaches (12), lexical approaches (8), tree-based approaches (3), metric-based approaches (7), semantic approaches (7), and hybrid approaches (17). We identified and analyzed 26 CCD tools, i.e., 13 existing and 13 proposed/developed. Moreover, 62 open-source subject systems whose source code is utilized for the CCD are presented. It is concluded that there exist several studies to detect type1, type2, type3, and type4 clones individually. However, there is a need to develop novel approaches with complete tool support in order to detect all four types of clones collectively. Furthermore, it is also required to introduce more approaches to simplify the development of a program dependency graph (PDG) while dealing with the detection of the type4 clones.}
}

@article{rayyan-727967717,
  title={The mHealth applications usability evaluation review},
  year={2020},
  pages={70-73},
  author={Ansaar, Muhammad Zaki and Hussain, Jamil and Bang, Jaehun and Lee, Sungyoung and Shin, Kim Young and Young Woo, Kim},
  keywords={Systematic literature review, Systematics, Bibliographies, Reliability, Usability, Computer science, usability, applications, Medical services, mHealth, Operating systems, smartphones},
  abstract={Smartphones are contributing to the improvement of healthcare information and services with the help of mHealth apps. Commercially available mHealth apps have drawn prominent public attention by providing improved medication adherence and efficient results, but some of the studies exist to support their use. Usability has become the main factor for the success or adoption of smartphone apps since it helps to organize the consistency for the users to achieve their goal in an easy and efficient way. This study aims to investigate the usability evaluation process of mHealth applications with the help of a Systematic Literature Review (SLR). Our findings show that the usability evaluation process can be more reliable and satisfactory by applying the mix-method approach. This study will encourage developers and researchers to design more effortless and usable applications for users, especially for older adults and novice users.}
}

@article{rayyan-727967718,
  title={Formalizing a systematic review updating process},
  year={2008},
  pages={143-150},
  author={Dieste, Oscar and López, Marta and Ramos, Felicidad},
  keywords={Data mining, Systematic review, Terminology, Interviews, Protocols, Proposals, Book reviews, process, Strontium},
  abstract={The objective of a systematic review is to obtain empirical evidence about the topic under review and to allow moving forward the body of knowledge of a discipline. Therefore, systematic reviewing is a tool we can apply in Software Engineering to develop well founded guidelines with the final goal of improving the quality of the software systems. However, we still do not have as much experience in performing systematic reviews as in other disciplines like medicine, and therefore we need detailed guidance. This paper presents a proposal of a improved process to perform systematic reviews in software engineering. This process is the result of the tasks carried out in a first review and a subsequent update concerning the effectiveness of elicitation techniques.}
}

@article{rayyan-727967719,
  title={A systematic literature review on attractiveness and learnability factors in Web applications},
  year={2015},
  pages={22-27},
  author={Ngadiman, Norzila and Sulaiman, Shahida and Mohd Nasir Wan Kadir, Wan},
  keywords={Systematic literature review, Systematics, Bibliographies, Guidelines, Web applications, Quality, Usability, Navigation, Conferences, Attractiveness, Learnability, Operability},
  abstract={User interface design is an essential element for consideration in Web application development. Usability (ISO/IEC 9126-1:2001) or operability (ISO/IEC 25010:2011) is one of non-functional requirements which must be taken into consideration when designing a user interface. Attractiveness and learnability are among the sub criteria under the operability factor. Analysis of these criteria may be used to evaluate user interface design. This study has adopted a systematic literature review method to study existing works concerning the two factors of operability. The outcome of the review provides insight into existing works on attractiveness and learnability factors in software applications generally and Web applications specifically.}
}

@article{rayyan-727967720,
  title={A systematic literature review on knowledge representation approaches for systems-of-systems},
  year={2015},
  pages={70-79},
  author={Abdalla, Gabriel and Damasceno, Carlos Diego N and Guessi, Milena and Oquendo, Flavio and Nakagawa, Elisa Yumi},
  keywords={Systematic Literature Review, Ontology, Ontologies, Modeling, Taxonomy, Terminology, Interoperability, Knowledge Representation, System of systems, Systems-of-Systems},
  abstract={Systems-of-Systems are a class of systems composed of diverse, independent constituent systems. Together, these constituents can accomplish missions that otherwise could not be performed by any of them separately. In another perspective, knowledge representation approaches can assist in the establishment of a common understanding in this field by formalizing and standardizing the main terms and concepts adopted. In spite of the relevance of SoS, a consolidated terminology which could support the community working with such systems is still missing. Furthermore, the multiplicity of stakeholders, technologies, and expertise involved in an SoS makes the need of a common understanding even more imperative. In this study, we report on the main findings of a systematic literature review covering knowledge representation approaches in the SoS field. With this study, we are able to present a comprehensive panorama of the knowledge representation approaches that are currently adopted. Even though a consolidated terminology is not available yet, such panorama can be helpful for devising a common, comprehensive terminology for the SoS field. Therefore, we conclude this paper with directions for future work.}
}

@article{rayyan-727967721,
  title={Does the adoption of ERP systems help improving decision-making? A systematic literature review},
  year={2018},
  pages={61-66},
  author={OUIDDAD, Ahmed and OKAR, Chafik and CHROQUI, Razane and HASSANI, Imane BEQQALI},
  keywords={Systematics, Systematic Literature Review, Bibliographies, Data mining, Decision making, Planning, Art, Decision-Making, Enterprise resource planning, ERP, Decision Making},
  abstract={Enterprise Resource Planning (ERP) systems have significantly grown and evolved in the last decades. As a result, they have become highly important for operational data analysis and subsequently to create decision support and analytical applications. The majority of studies on ERP systems have aimed to evaluate their transactional and operational impacts, without taking into consideration their importance in the decision-making aspects. Based on a systematic literature review methodology, this article intended to analyze previous research works on the role of ERP in improving the decision-making. The research was conducted based on four scientific search engines: Emerald Insight, IEEE Xplore, ProQuest and ScienceDirect. After screening more than 386 articles, 27 of them were selected, categorized and then synthesized. The study provides useful information on the way these systems contribute to improving the decisional aspect in the enterprise.}
}

@article{rayyan-727967722,
  title={Challenges of migrating legacies web to mobile: A systematic literature review},
  year={2020},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={18},
  number={5},
  pages={861-873},
  author={Cajas, Viviana and Urbieta, Matías and Rossi, Gustavo and Domínguez Mayo, Francisco},
  keywords={systematic literature review, Systematics, Bibliographies, Legacy application, Web 2.0, Business, Adaptation models, evolution, IEEE transactions, migration, mobile applications, Mobile handsets},
  abstract={The multi-device era comes thanks to mobile computing which gives remote access to resources from anywhere changing the people's life and opening new business opportunities. However, the legacy systems do not render the content properly in mobile devices because they were thought to be only compliant with Web browsers. Economic availability is often the reason why these have not been modernized. This work proposes a systematic literature review about the approaches used for the portabilization or modernization of web 1.0 business applications to mobile devices in the period 2006-2017, from SCOPUS, IEEE, and ACM. The search obtained 824 articles, 44 were selected and classified with respect to focus, scope, type of research and type of contribution. The results obtained allowed us to reach conclusions about the state of the subject and to determine the research gaps, such as the need for better use of the mobile characteristics because the adaptations are mostly basic. In addition, an approach is proposed and compared with the aforementioned them.}
}

@article{rayyan-727967723,
  title={Identification of key success factors and challenges for ERP systems — A systematic literature review},
  year={2017},
  pages={1-6},
  author={Wijaya, Santo F and Prabowo, Harjanto and Meyliana and Kosala, Raymond},
  keywords={Systematic literature review, Challenges, Critical success factors, ERP systems},
  abstract={Organizations need information system as a tool for decision making. Enterprise Resource Planning (ERP) systems are a tool of information systems to achieve work efficiency. ERP systems are the best solution to maintain a competitive advantage for most organizations. Successful implementations of ERP systems have a lot of benefits for the organizations. But the fact that many organizations have failed in implementing ERP systems, but also many organizations have succeeded in implementation of ERP systems. It is the challenge for the organizations to implementation of ERP systems success. The purpose of this study is to propose identify key success factors and challenges for ERP systems. This study employs Systematic. Literature Review approach for review and summary about the key success factors of ERP implementation.}
}

@article{rayyan-727967724,
  title={Critical success factors for big data: A systematic literature review},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={118940-118956},
  author={Al-Sai, Zaher Ali and Abdullah, Rosni and Husin, Mohd Heikal},
  keywords={Systematics, Bibliographies, critical success factors (CSFs), systematic literature review (SLR), Big Data, Organizations, Planning, Big data (BD), big data analytics (BDA), big data challenges, big data implementation, information system success, Investment, maturity, readiness},
  abstract={During the last few decades, many organizations have started recognizing the benefits of Big Data (BD) to drive their digital transformation and to gain faster insights from faster data. Making smart data-driven decisions will help the organizations to ride the waves toward invaluable investments. The successful implementation of Big Data projects depends on their alignment with the current organizational, technological, and analytical aspects. Identifying the Critical Success Factors (CSFs) for Big Data is fundamental to overcome the challenges surrounding Big Data Analytics (BDA) and implementation. In recent years, the investigations related to identifying the CSFs of Big Data and Big Data Analytics expanded on a large scale trying to address the limitations in existing publications and contribute to the body of knowledge. This paper aims to provide more understanding about the existing CSFs for Big Data Analytics and implementation and contributes to the body of knowledge by answering three research questions: 1) How many studies have investigated on Big Data CSFs for analytics and implementation?, 2) What are the existing CSFs for Big Data Analytics, and 3) What are the categories of Big Data Analytics CSFs?. By conducting a Systematic Literature Review (SLR) for the available studies related to Big Data CSFs in the last twelve years (2007-2019), a final list of sixteen (16) related articles was extracted and analyzed to identify the Big Data Analytics CSFs and their categories. Based on the descriptive qualitative content analysis method for the selected literature, this SLR paper identifies 74 CSFs for Big Data and proposes a classification schema and framework in terms of 5 categories, namely Organization, Technology, People, Data Management, and Governance. The findings of this paper could be used as a referential framework for a successful strategy and implementation of Big Data by formulating more effective data-driven decisions. Future work will investigate the priority of the Big Data CSFs and their categories toward developing a conceptual framework for assessing the success of Big Data projects.}
}

@article{rayyan-727967725,
  title={Open source software development process: A systematic review},
  year={2020},
  pages={135-144},
  author={Napoleão, Bianca M and Petrillo, Fabio and Hallé, Sylvain},
  keywords={Software, systematic literature review, Software engineering, Systematics, Bibliographies, Data mining, Libraries, Conferences, open source, software development process},
  abstract={Open Source Software (OSS) has been recognized by the software development community as an effective way to deliver software. Unlike traditional software development, OSS development is driven by collaboration among developers spread geographically and motivated by common goals and interests. Besides this fact, it is recognized by the OSS community the need to understand OSS development process and its activities. Our goal is to investigate the state-of-art about OSS process through conducting a systematic literature review providing an overview of how the OSS community has been investigating OSS process over past years. We identified and summarized OSS process activities and their characteristics and translated them into an OSS macro process using BPMN notation. As a result, we systematically analyzed 33 studies presenting an overview of the OSS process research and a generalized OSS development macro process represented by BPMN notation with a detailed description of each OSS process activity and roles in OSS environment. We conclude that OSS process can be in practice further investigated by researchers. In addition, the presented OSS process can be used as a guide for OSS projects and be adapted according to each OSS project reality. It provides insights to managers and developers who want to improve their development process even in OSS and traditional environments. Finally, recommendations for OSS community regarding OSS process activities are provided.}
}

@article{rayyan-727967726,
  title={A systematic review on creativity techniques for requirements engineering},
  year={2012},
  pages={34-39},
  author={Saha, Shishir Kumar and Selvi, Mehmet and Büyükcan, Güral and Mohymen, Mirza},
  keywords={Requirements Engineering, Databases, Creativity Techniques, IEEE Xplore, Innovation Candidate, RE},
  abstract={Now-a-days Creativity is very much important for requirements analysis. The practice of systematic creativity technique is needed to generate inventive ideas and to achieve better outcome in requirements engineering. Creativity techniques are used to discover creative and innovative requirements and to understand client's expectation. In the paper we performed systematic review on articles those are related to creativity, creativity techniques, requirements engineering and empirical evaluation on creativity techniques for requirements engineering. In the systematic literature review, different sources are used (e.g. IEEE Xplore, ACM, Compendex, Inspec, Springerlink, Science Direct). Then preformed search with relevant search terms. We minimized search result combining creativity and requirements engineering and then go through the title and abstract and selected articles for review using inclusion and exclusion criteria. From the systematic review we determined the roles of creativity and identified the impact of creativity techniques in requirements engineering area. Also, we explored ideas on future impact and trend of creativity in requirements engineering.}
}

@article{rayyan-727967727,
  title={A systematic review of Web engineering research},
  year={2005},
  pages={10-pp.–},
  author={Mendes, E},
  keywords={Software engineering, Software measurement, Testing, Computer science, Physics, Engineering management, Maintenance engineering, Manufacturing processes, Quality management, Reliability engineering},
  abstract={This paper uses a systematic literature review as means of investigating the rigor of claims arising from Web engineering research. Rigor is measured using criteria combined from software engineering research. We reviewed 173 papers and results have shown that only 5% would be considered rigorous methodologically. In addition to presenting our results, we also provide suggestions for improvement of Web engineering research based on lessons learnt by the software engineering community.}
}

@article{rayyan-727967728,
  title={What do we know about the effectiveness of software design patterns?},
  year={2012},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={38},
  number={5},
  pages={1213-1231},
  author={Zhang, Cheng and Budgen, David},
  keywords={systematic literature review, Software engineering, Systematics, Software design, Design patterns, Terminology, empirical software engineering, Search engines, Maintenance engineering, Software Design, Software},
  abstract={Context. Although research in software engineering largely seeks to improve the practices and products of software development, many practices are based upon codification of expert knowledge, often with little or no underpinning from objective empirical evidence. Software design patterns seek to codify expert knowledge to share experience about successful design structures. Objectives. To investigate how extensively the use of software design patterns has been subjected to empirical study and what evidence is available about how and when their use can provide an effective mechanism for knowledge transfer about design. Method. We conducted a systematic literature review in the form of a mapping study, searching the literature up to the end of 2009 to identify relevant primary studies about the use of the 23 patterns catalogued in the widely referenced book by the “Gang of Four.” These studies were then categorized according to the forms of study employed, the patterns that were studied, as well as the context within which the study took place. Results. Our searches identified 611 candidate papers. Applying our inclusion/exclusion criteria resulted in a final set of 10 papers that described 11 instances of “formal” experimental studies of object-oriented design patterns. We augmented our analysis by including seven “experience” reports that described application of patterns using less rigorous observational forms. We report and review the profiles of the empirical evidence for those patterns for which multiple studies exist. Conclusions. We could not identify firm support for any of the claims made for patterns in general, although there was some support for the usefulness of patterns in providing a framework for maintenance, and some qualitative indication that they do not help novices learn about design. For future studies we recommend that researchers use case studies that focus upon some key patterns, and seek to identify the impact that their use can have upon maintenance.}
}

@article{rayyan-727967729,
  title={Towards evidence in software engineering},
  year={2004},
  pages={261-270},
  author={Jedlitschka, A and Ciolkowski, M},
  keywords={Software engineering, Guidelines, Programming, Performance analysis, Costs, Injuries, Law, Packaging, Scattering, Software packages, Software},
  abstract={The aggregation of studies is of growing interest for the empirical software engineering community, since the numbers of studies steadily grow. We discuss challenges with the aggregation of studies into a common body of knowledge, based on a quantitative and qualitative evaluation of experience from the Experimental Software Engineering Network, ESERNET. Challenges are that the number of studies available is usually low, and the studies that exist are often too scattered and diverse to allow systematic aggregation as a means for generating evidence. ESERNET therefore attempted to coordinate studies and thus create research synergies to achieve a sufficiently large number of comparable studies to allow for aggregation; however, the coordination approach of ESERNET proved to be insufficient. Based on some lessons learned from ESERNET, a four-step procedure for evolving Empirical Software Engineering towards the generation of evidence is proposed. This consists of (1) developing a methodology for aggregating different kinds of empirical results, (2) establishing guidelines for performing, analyzing, and reporting studies as well as for aggregating the results for every kind of empirical study, (3) extract evidence, that is, apply the methodology to different areas of software engineering, and (4) package the extracted evidence into guidelines for practice.}
}

@article{rayyan-727967730,
  title={Network survivability analysis modeling approach for MANETS: A systematic review},
  year={2014},
  pages={74-79},
  author={Azni, A H and Ahmad, Rabiah and Mohamad Noh, Zul Azri and Hazwani, Farida and Hayaati, Najwa},
  keywords={Systematic Literature Review, Measurement, Reliability, Mobile computing, Analytical models, Mathematical model, Ad hoc networks, MANETs, Network survivability, Network topology, Survival Analysis},
  abstract={Network survivability analysis modeling in MANETs was hardly an issue in the early years of wireless technology because there was no critical network system that depended on wireless technology yet. Today, network survivability analysis is an essential aspect of reliable communication especially in MANETs. Although various methods have been proposed to model network survivability analysis in MANETs, no related review has been published as to date for this topic. Thus, a comprehensive review of this body of work would be beneficial to researchers to have an overview of the current state of research trend in this area. This paper provides a systematic literature review (SLR) of the state of the art approach in modeling network survivability analysis in MANETs. We used studies from a number of relevant article sources, and our results showed the existence of twenty six articles. From this SLR we found that the existing of modeling method is focusing on individual node in which the node is treated as independent event. Furthermore, the analysis also reveals the less popular methods in modeling network survivability analysis are with statistical methods such as regression analysis and survival analysis. The implication of this study is to give a clear direction to future researchers in this area for a better and accurate analysis in measuring network survivability in MANETs.}
}

@article{rayyan-727967731,
  title={Systematic literature reviews in distributed software development: A tertiary study},
  year={2012},
  pages={134-143},
  author={Marques, Anna Beatriz and Rodrigues, Rosiane and Conte, Tayana},
  keywords={Software, Software engineering, Systematics, Collaboration, Distributed Software Development, Systematic Review, Context, Organizations, Empirical Evidence, Global Software Engineering, Research and development management, Tertiary Study},
  abstract={Distributed Software Development (DSD) emerged from the need to achieve geographically distant customers and currently, allows organizations have global customers and other benefits. This scenario has given rise to new Software Engineering challenges resulting from DSD particularities. Several Systematic Reviews were conducted to address these new challenges. The objective of this paper is to categorize systematic reviews conducted in DSD context. We used the systematic review method to identify SLRs (Systematic Literature Reviews) that address DSD aspects. This study is categorized as a tertiary review. Of fourteen SLRs, seven address aspects of managing distributed development. Four SLRs addressed topics of engineering process. The three remaining are related to Requirements, Design and Software Engineering Education in DSD. The topic areas covered by SLRs are limited, where the majority are focused on summarize the current knowledge concerning a research question. Despite the number of SLRs, the amount of empirical studies is relatively small.}
}

@article{rayyan-727967732,
  title={Computing curricula software engineering: position paper},
  year={2004},
  pages={174-175 vol.1},
  author={Budgen, D},
  keywords={Software engineering, Software design, Glass, Mathematics, Books, Concurrent computing, North America, Solids, Time factors, Software},
  abstract={No curriculum, however good, will solve our problems on its own. We need to take and use this effectively (and hence need measures of what is effective too). My position is therefore that the CCSE is a useful and quite important stepping-stone, but it is not a watershed in the education process nfortunately, I think there are many more tasks ahead of us. The SEI's programme to develop curriculum modules back in the late 1980s represented a determined attempt to bootstrap software engineering education, using expert judgement with some degree of consultation. The process used to develop the CCSE has moved on a step from this, but was still heavily dependent upon expert judgement-even if this has drawn upon a larger community and longer experience. For the next step, we therefore need to begin to replace the use of expert judgement with evidence that has been carefully assembled and systematically reviewed}
}

@article{rayyan-727967733,
  title={Automated software testing on mobile applications: A review with special focus on android platform},
  year={2020},
  pages={292-293},
  author={Musthafa, Fathima Naja and Mansur, Syeda and Wibawanto, Adika},
  keywords={Software, Software testing, Software engineering, Tools, Automation, Software Testing, Mobile applications, Automated testing techniques, Graphical user interfaces, Mobile application testing tools, Quality Assurance, Methyltestosterone},
  abstract={Development of mobile applications and hence testing them with automated tools are rapidly achieving grounds on the technology market with the explosive evolution, demand and use of mobile technology. Although automated software testing has been a topic of research for many years, automation testing on mobile application is being a key topic in the software engineering industry and hence the development and introduction of new automated tools have gained attention in the recent years. This research aims at identifying, analyzing, and synthesizing the current state-of-the-art of automated testing on mobile application with special focus on to the popular mobile application platform "Android". A systematic literature review was conducted on studies retrieved from electronic search space. Various literature reviewed and referred there in, as a conclusion, its obvious that none of the techniques used in automated testing is alone inadequate and a combination of them works well as an effective testing strategy. Also, the challenges thus faced by the practitioners of automated testing could be still mitigated with an efficient selection of testing tool.}
}

@article{rayyan-727967734,
  title={A systematic review of the effects of team climate on software team productivity},
  year={2014},
  pages={1-7},
  author={Soomro, Arjumand Bano and Salleh, Norsaremah},
  keywords={systematic review, Software, Software engineering, Software measurement, Systematics, Data mining, Productivity, Meteorology, personality, team climate, team productivity},
  abstract={The term team-work has been a significant topic in software engineering over the past 50 years. The team climate is the exchange of ideas and perceptions among team members in favor to promote the innovation in work processes. In this paper, we presented our work on a systematic review on the effect of team climate on the software productivity or performance. The summarized results from this research would be useful for achieving effectiveness in software engineering work teams.}
}

@article{rayyan-727967735,
  title={A systematic literature review of methodology of learning evaluation based on item response theory in the context of programming teaching},
  year={2020},
  pages={1-9},
  author={Santos, Jucelio S and Andrade, Wilkerson L and Brunet, João and Araujo Melo, Monilly Ramos},
  keywords={Search problems, Tools, Programming, Education, Libraries, Instruments, Real-time systems},
  abstract={This Research Full Paper presents a Systematic Literature Review (SLR) that investigates state-of-the-art methods, processes, approaches, and instruments based on Item Response Theory (IRT) in the programming teaching context. Various studies support professors and students to improve the evaluation process in teaching programming. Among the different techniques and theories associated with these studies, the IRT gained prominence because of its effectiveness. Due to the lack of an overview in the area, we present a SLR with the objective of identity studies that use IRT as a methodology of evaluation of learning in the context of Programming Teaching; studies which present real-time feedback; and experimental studies that have been carried out to validate them. To achieve these goals, we planned an SLR following the guidelines proposed by Kitchenham (2004). Our main findings are a) There is a limited number of studies that explore IRT in evaluation methodologies in the teaching programming context. Among these studies, the main focus was the development on of instruments to measure programming skills; b) Most instruments are multiple-choice, adopt the 3PL model, and do not show evidence of real-time feedback except SIETTE and CodeWorkout and are also the only instruments that evaluate the coding capacity of individuals; c) The studies showed scientific evidence on the use of IRT in the evaluation methodology in the programming teaching context. The studies presented experiments that indicate a significant gain in the measurement of programming abilities when compared to the traditional evaluation method.}
}

@article{rayyan-727967736,
  title={Agile practices in global software engineering - a systematic map},
  year={2010},
  pages={45-54},
  author={Jalali, Samireh and Wohlin, Claes},
  keywords={Software engineering, Systematics, Data mining, Guidelines, Programming, Context, global software development, global software engineering, Book reviews, open source, agile, distributed team, extreme programming, lean software development, offshore, outsource, pair programming, scrum, systematic map, virtual team, Software},
  abstract={This paper presents the results of systematically reviewing the current research literature on the use of agile practices and lean software development in global software engineering (GSE). The primary purpose is to highlight under which circumstances they have been applied efficiently. Some common terms related to agile practices (e.g. scrum, extreme programming) were considered in formulating the search strings, along with a number of alternatives for GSE such as offshoring, outsourcing, and virtual teams. The results were limited to peer-reviewed conference papers/journal articles, published between 1999 and 2009. The synthesis was made through classifying the papers into different categories (e.g. research type, distribution). The analysis revealed that in most cases agile practices were modified with respect to the context and situational requirements. This indicates the need for future research on how to integrate all experiences and practices in a way to assist practitioners when setting up non-collocated agile projects.}
}

@article{rayyan-727967737,
  title={An empirical investigation of systematic reviews in software engineering},
  year={2011},
  pages={87-96},
  author={Zhang, He and Babar, Muhammad Ali},
  keywords={Software engineering, Systematics, evidence-based software engineering, Data mining, Guidelines, Interviews, Electronic mail, Communities, methodology adoption, systematic (literature) reviews, Software},
  abstract={BACKGROUND: Systematic Literature Reviews (SLRs) have gained significant popularity among software engineering (SE) researchers since 2004. Several researchers have also been working on improving the scientific and technological support for SLRs in SE. We argue that there is also an essential need for evidence-based body of knowledge about different aspects of the adoption of SLRs in SE. OBJECTIVE: The main objective of this research is to empirically investigate the adoption and use of SLRs in SE research from various perspectives. METHOD: We used multi-method approach as it is based on a combination of complementary research methods which are expected to compensate each others' limitations. RESULTS: A large majority of the participants are convinced of the value of using a rigorous and systematic methodology for literature reviews. However, there are concerns about the required time and resources for SLRs. One of the most important motivators for performing SLRs is new findings and inception of innovative ideas for further research. The reported SLRs are more influential compared to the traditional literature reviews in terms of number of citations. One of the main challenges of conducting SLRs is drawing a balance between rigor and required effort. CONCLUSIONS: SLR has become a popular research methodology for conducting literature review and evidence aggregation in SE. There is an overall positive perception about this methodology. The findings provide interesting insights into different aspects of SLRs. We expect that the findings can provide valuable information to readers on what can be expected from conducting SLRs and the potential impact of such reviews.}
}

@article{rayyan-727967738,
  title={Software engineering research community viewpoints on rapid reviews},
  year={2019},
  pages={1-12},
  author={Cartaxo, Bruno and Pinto, Gustavo and Fonseca, Baldoíno and Ribeiro, Márcio and Pinheiro, Pedro and Baldassarre, Maria Teresa and Soares, Sérgio},
  keywords={Software engineering, Systematics, Collaboration, Decision making, Evidence Based Software Engineering, Statistics, Q-Methodology, Rapid Reviews, Sociology, Standards, Systematic Reviews, Software},
  abstract={Background: One of the most important current challenges of Software Engineering (SE) research is to provide relevant evidence to practice. In health related fields, Rapid Reviews (RRs) have shown to be an effective method to achieve that goal. However, little is known about how the SE research community perceives the potential applicability of RRs. Aims: The goal of this study is to understand the SE research community viewpoints towards the use of RRs as a means to provide evidence to practitioners. Method: To understand their viewpoints, we invited 37 researchers to analyze 50 opinion statements about RRs, and rate them according to what extent they agree with each statement. Q-Methodology was employed to identify the most salient viewpoints, represented by the so called factors. Results: Four factors were identified: Factor A groups undecided researchers that need more evidence before using RRs; Researchers grouped in Factor B are generally positive about RRs, but highlight the need to define minimum standards; Factor C researchers are more skeptical and reinforce the importance of high quality evidence; Researchers aligned to Factor D have a pragmatic point of view, considering RRs can be applied based on the context and constraints faced by practitioners. Conclusions: In conclusion, although there are opposing viewpoints, there are also some common grounds. For example, all viewpoints agree that both RRs and Systematic Reviews can be poorly or well conducted.}
}

@article{rayyan-727967739,
  title={A preliminary analysis of various testing techniques in Agile development - a systematic literature review},
  year={2016},
  pages={600-605},
  author={Thangiah, Murugan and Basri, Shuib},
  keywords={Software, Software testing, Software engineering, Systematics, Bibliographies, Agile development, SME's, Computers, Exploratory software Testing},
  abstract={There are numerous software development and testing methods, tools and techniques have emerged over the past few decades with the main objective are to enhance the software quality. In Small and Medium size Enterprises (SME's), Agile methods have been gaining acceptance but quality of the product it produced is remains as the major concern because the tesing methods and standards are not improved. In Agile development process, though there are many testing techniques are exists, SME's cannot afford to follow the traditional methods of testing process, because of high cost and it is a time consuming process. In this research paper, only the issues surfaced in Regression Testing and Automated Testing are analysed and proposed that exploratory testing (ET) methods either can be used as an alternative or can be integrate along with the existing testing methods. However, there is not enough research work is conducted on Exploratory testing and emphasized the importance to focus more research work to be carried out in ET.}
}

@article{rayyan-727967740,
  title={Personality requirements in requirement engineering of web development: A systematic literature review},
  year={2016},
  pages={183-188},
  author={Askarinejadamiri, Zahra},
  keywords={Software, Software engineering, Systematics, Bibliographies, Requirements engineering, Software development, Human factors, Education, Personality, requirement engineering, web development},
  abstract={Personality requirement is a key factor for the success of software development. In order to doing requirement engineering for web development, some human personality should be considered. In fact, Personality represents some human factors such as human, ability human behavior and human capital which will be discussed in this paper. Therefore, these factors were distinguished and presented as an effective element of software development and subsequently web development in this research. Knowing these components assist the developer to have a better view on the project. This paper review some selected paper in research are of requirement engineering and human personality. A major result of this paper is a categorization of Personality of human that usually ignored in the project and they cause the problem in project to improving the success of web development.}
}

@article{rayyan-727967741,
  title={Maintainability prediction of relational database-driven applications: A systematic review},
  year={2012},
  pages={263-272},
  author={Riaz, Mehwish},
  keywords={Measures, Systematic Review, Prediction, Relational Database-Driven Sofwtare Applications, Software Maintainability},
  abstract={Background: Maintainability is an important quality attribute. Its prediction for relational database-driven applications can help organizations improve the maintainability of these applications. Aims: The aim of this paper is to present the results of a Systematic Literature Review - also known as Systematic Review (SR) - with an up-to-date account of the state of art on maintainability prediction and measures for relational database-driven applications, to compare these results with those from a previously conducted SR [19], and to provide a baseline for conducting further research in this area. Method: A SR on maintainability prediction for relational databasedriven applications was carried out using Kitchenham's guidelines for conducting SRs in Software Engineering. Results: The results show little evidence on maintainability prediction for relational database-driven applications with expert judgment as the most common prediction technique, coupling related measures as the most common predictors, and subjective assessment as most common maintainability measure. Conclusions: The presented results suggest a strong need for further investigating the area of maintainability prediction for relational database-driven applications.}
}

@article{rayyan-727967742,
  title={Systematic literature review: Model refactoring},
  year={2017},
  pages={1-5},
  author={Dharmawan, Tio and Rochimah, Siti},
  keywords={Software, Systematics, Systematic Literature Review, Unified modeling language, Organizations, Object oriented modeling, Filtering, Model Driven Refactoring, Observers, Pattern Based Model Refactoring, Refactoring Model, Software Evolution},
  abstract={Refactoring is the method to detecting and fixing bad smells in software. Refactoring techniques that have developed is a refactoring technique that is done on the source code. Along with the development of model driven software engineering (MDSE), it also developed the method of refactoring on the model. Refactoring method in the model is considered more effective and efficient because the detection and repair of bad smell is done at the design phase. The method of refactoring on the model evolves into a variety of techniques. Due to this, the systematic literature review is done to get the development of refactoring method on the developing model.}
}

@article{rayyan-727967743,
  title={Situational requirement engineering: A systematic literature review protocol},
  year={2013},
  pages={123-126},
  author={Khan, Huma Hayat and bin Mahrin, Mohd. Naz'ri and bt Chuprat, Suriayati},
  keywords={Software, systematic literature review, Software engineering, Systematics, Data mining, Quality assessment, Conferences, Protocols, situational requirement engineering},
  abstract={Requirements Engineering (RE) is known to be one of the critical phases in software development. Lots of work related to RE is already published. Field of RE is maturing day by day, leading to exploration at its deeper level. It is argued that RE is subject to situational characteristics. This exposure becomes even more when RE is performed in global software development environment. There is a need to identify these situational characteristics based on RE literature. We plan to systematically explore situational RE based studies to distinguish and account state of the art in situational RE based reported research. This paper objective is to provide the systematic literature review (SLR) protocol to illustrate a process for combining the situational RE work that will ultimately present a state of the art of the field in global software development environment. This SLR aims to not only summarize the data related to situational RE in form of situational characteristics but will also be useful for RE practitioners specifically working in global software development environment by providing a check list base upon situational characteristics. It will also assist RE researchers to discover knowledge gaps to distinguish needs and probability for future research directions in the field of situational RE in global software development environment.}
}

@article{rayyan-727967744,
  title={Variability in software Systems—A systematic literature review},
  year={2014},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={40},
  number={3},
  pages={282-306},
  author={Galster, Matthias and Weyns, Danny and Tofan, Dan and Michalik, Bartosz and Avgeriou, Paris},
  keywords={systematic review, software engineering, Software engineering, Systematics, Manuals, Variability, Context, Software systems, Data collection, Decision support systems, Software},
  abstract={Context: Variability (i.e., the ability of software systems or artifacts to be adjusted for different contexts) became a key property of many systems. Objective: We analyze existing research on variability in software systems. We investigate variability handling in major software engineering phases (e.g., requirements engineering, architecting). Method: We performed a systematic literature review. A manual search covered 13 premium software engineering journals and 18 premium conferences, resulting in 15,430 papers searched and 196 papers considered for analysis. To improve reliability and to increase reproducibility, we complemented the manual search with a targeted automated search. Results: Software quality attributes have not received much attention in the context of variability. Variability is studied in all software engineering phases, but testing is underrepresented. Data to motivate the applicability of current approaches are often insufficient; research designs are vaguely described. Conclusions: Based on our findings we propose dimensions of variability in software engineering. This empirically grounded classification provides a step towards a unifying, integrated perspective of variability in software systems, spanning across disparate or loosely coupled research themes in the software engineering community. Finally, we provide recommendations to bridge the gap between research and practice and point to opportunities for future research.}
}

@article{rayyan-727967745,
  title={Requirements prioritization techniques and different aspects for prioritization a systematic literature review protocol},
  year={2014},
  pages={31-36},
  author={Sher, Falak and Jawawi, Dayang N A and Mohamad, Radziah and Babar, Muhammad Imran},
  keywords={Software, Software engineering, Systematics, Bibliographies, Decision making, Requirements prioritization, Protocols, business aspects, client aspects, systematic review protocol, technical aspects},
  abstract={Requirements prioritization (RP) is considered as an important part of software requirements engineering in which requirements are ranked to develop high-quality software. Success of quality software depends on the selection of well-prioritized requirements. Different techniques are proposed and used to prioritize the software requirements. Requirements are assigned weights or ranked according to their importance and are placed in a priority list for implementation in successive releases. These techniques depend on many aspects that need to be addressed while prioritizing requirements. Requirements prioritization aspects are categorized into three major groups' technical aspects, business aspects and client aspects. Most of the existing techniques are unable to support these aspects, and it affects the quality of decision-making in the requirements prioritization process. Hence, there is a need to explore the different techniques and their support for different aspects. A comparison of the existing techniques is performed. The requirements prioritization aspects are selected to determine the current trends in the software requirements prioritization process. The issue of scalability and the business/client related aspects are the key focal points of this research paper. This paper describes the review protocol, as per guidelines of the Barbara Kitchenham, in order to conduct a systematic literature review.}
}

@article{rayyan-727967746,
  title={Gamification in software engineering teamworks: A systematic literature review},
  year={2016},
  pages={1-8},
  author={Hernández, Luis and Muñoz, Mirna and Mejia, Jezreel and Peña, Adriana},
  keywords={Software, systematic literature review, software engineering, Software engineering, Systematics, Bibliographies, Teamwork, Collaborative work, Silicon compounds, collaborative work, gamification, gamification elements, teamwork},
  abstract={Nowadays, software development is done by teams where there are several factors involved in their performance. One of the most important factors is the collaborative work, fundamental skill that every professional should have, especially in the area of software engineering. Based on the above mentioned, the adequate integration of a teamwork influences its performance. Therefore, the skills, knowledge and interactive styles for each member it should be complemented, in order to get a high effective teams. In this context, one of the techniques currently being used for achieving activities related to collaborative work is the gamification, which aims to guide the improvement of the collaborative work. This study presents a comparison among the different gamification elements that can be applied to create a teamwork, reducing its integration time, and therefore, improve its performance.}
}

@article{rayyan-727967747,
  title={Trends in software engineering processes using deep learning: A systematic literature review},
  year={2020},
  pages={445-454},
  author={Del Carpio, Alvaro Fernández and Angarita, Leonardo Bermón},
  keywords={Software, Software engineering, Systematics, Machine Learning, Systematic Review, Deep learning, Maintenance engineering, Biological system modeling, Deep Learning, Predictive models, Software Processes},
  abstract={In recent years, several researchers have applied machine learning techniques to several knowledge areas achieving acceptable results. Thus, a considerable number of deep learning models are focused on a wide range of software processes. This systematic review investigates the software processes supported by deep learning models, determining relevant results for the software community. This research identified that the most extensively investigated sub-processes are software testing and maintenance. In such sub-processes, deep learning models such as CNN, RNN, and LSTM are widely used to process bug reports, malware classification, libraries and commits recommendations generation. Some solutions are oriented to effort estimation, classify software requirements, identify GUI visual elements, identification of code authors, the similarity of source codes, predict and classify defects, and analyze bug reports in testing and maintenance processes.}
}

@article{rayyan-727967748,
  title={Ontologies supporting the distributed software development: A systematic literature review},
  year={2012},
  pages={55-59},
  author={Junior, Alex Nery B and de Azevedo, Ryan R and da Silva, Fabio Q B and Rocha, Rodrigo G C and Costa, Catarina},
  keywords={systematic literature review, Software engineering, ontology, Conferences, distributed software development, Software},
  abstract={In the past decade, there was a notably significant raise in the adoption of the Distributed Software Development approach. This so-called approach has brought a lot of competitive advantages, as well as new challenges, such as communication and sharing of information among distributed teams. In this scenario, the use of the ontology concept simplifies and normalizes the teams' understanding of shared information and eases the communication between them. This research's purpose is to make a systematic literature review in order to identify which tools, models, techniques and satisfactory practices that utilize ontology as an aid to DSD.}
}

@article{rayyan-727967749,
  title={Knowledge sharing management in offshore software development outsourcing relationships from vendors' perspective: A systematic literature review protocol},
  year={2011},
  pages={469-474},
  author={Alam, Asad Ullah and Khan, Siffat Ullah},
  keywords={Software, Systematic Literature Review, Programming, Outsourcing, Knowledge transfer, Organizations, Client-Vendor Relationships, Knowledge Sharing Management, Software Development Outsourcing},
  abstract={Offshore software development outsourcing (OSDO) is a well known business strategy adopted by many organizations in developed countries by outsourcing their software development work to low-wages countries. Developing software at offshore locations offers many benefits including access to skilled human resource and high quality software development at low cost. However outsourcing is not a risk free business for both vendor and client organizations. Vendor organizations need to address a number of factors for successful outcomes of the outsourcing relationships. Likewise knowledge sharing management (KSM) plays an important role in OSDO relationships. This research seeks to explore KSM in the context of OSDO relationships from vendors' perspective. The objective is to identify the factors that can play a positive or negative role in KSM by reviewing the literature in a systematic way. Systematic Literature Review (SLR) is based on a structured protocol, and is therefore, different from ordinary literature review. SLR provides in-depth, more thorough and comparatively unbiased results than ordinary literature review. We have developed a SLR protocol for the KSM, and are in the process of implementing the protocol. The expected outcomes of this review will be the identification of critical success factors and critical barriers to be addressed by vendor organizations for enhancing KSM in the context of OSDO relationships.}
}

@article{rayyan-727967750,
  title={Intercultural challenges in offshore software development outsourcing relationships: A systematic literature review protocol},
  year={2011},
  pages={475-480},
  author={Azeem, Muhammad Ilyas and Khan, Siffat Ullah},
  keywords={Software, systematic literature review, Systematics, Programming, Outsourcing, Cultural differences, Global communication, Protocols, Intercultural challenges, outsourcing relationships, SLR protocol},
  abstract={Offshore software development outsourcing (OSDO) is an emerging business approach adopted by many software development organizations in developed countries. Developing software at offshore locations has many benefits including access to large labor pool, low development cost and round-the-clock development. However, inspite of these benefits OSDO presents a variety of challenges to the software development organizations including temporal, geographical and intercultural differences. Intercultural differences cause many problems to vendors in OSDO relationships. This research seeks to identify the potential intercultural challenges faced by vendors in OSDO relationships by reviewing the literature in a systematic way. We have developed a systematic literature review (SLR) protocol, and are in the process of implementing the protocol. SLR is based on a structured protocol, and is therefore, different from ordinary literature review. SLR provides in-depth and more thorough results than ordinary literature review. The expected outcomes of this review will be the identification of intercultural challenges or barriers faced by vendor organisations in the establishment and maintenance of OSDO relationships.}
}

@article{rayyan-727967751,
  title={Definitions, features, and technologies on classroom response systems: A systematic literature review},
  year={2020},
  pages={221-225},
  author={Natanael, Yosua and Rosmansyah, Yusep},
  keywords={Software, systematic literature review, Systematics, SLR, Bibliographies, Education, audience response systems, classroom response systems, clickers, Communications technology, CRS, Input devices, Receivers},
  abstract={Classroom Response System (CRS) is a learning technology that supports face-to-face learning sessions by gathering responses to questions given by teachers in classroom which provides feedback to teacher during learning process using electronic devices. This paper described the definition of CRS and also its features and technologies with Systematic Literature Review (SLR) method. SLR was conducted to trace related studies trough three reputable journal database; IEEE Xplore, ACM Digital Library, and Sciencedirect. From 17,809 articles gathered, only 20 articles are eligible for synthesizing. The results are: CRS can be defined as a system used to support interaction and engagement between teachers and students in the classroom. CRS adopts an electronic voting system where the teacher displays questions via projector screen and students respond to them by answering through input device; the types of questions that can be accommodated by CRS can be grouped into three features, namely simple type interaction, text based interaction, and freeform based interaction; CRS takes the advantages of existing hardware and no longer used proprietary hardware.}
}

@article{rayyan-727967752,
  title={Identifying barriers to the systematic literature review process},
  year={2013},
  pages={203-212},
  author={Carver, Jeffrey C and Hassler, Edgar and Hernandes, Elis and Kraft, Nicholas A},
  keywords={systematic literature review, survey, Data mining, Search problems, Quality assessment, Databases, empirical software engineering, Protocols, Planning, Encoding},
  abstract={Conducting a systematic literature review (SLR) is difficult and time-consuming for an experienced researcher, and even more so for a novice graduate student. With a better understanding of the most common difficulties in the SLR process, mentors will be better prepared to guide novices through the process. This understanding will help researchers have more realistic expectations of the SLR process and will help mentors guide novices through its planning, execution, and documentation phases. Consequently, the objectives of this work are to identify the most difficult and time-consuming phases of the SLR process. Using data from two sources - 52 responses to an online survey sent to all authors of SLRs published in software engineering venues and qualitative experience reports from 8 PhD students who conducted SLRs as part of a course - we identified specific difficulties related to each phase of the SLR process. Our findings highlight the importance of planning, teamwork, and mentoring by an experienced researcher throughout the process. The paper also identifies implications for the teaching of the SLR process.}
}

@article{rayyan-727967753,
  title={A study of computing undergraduates undertaking a systematic literature review},
  year={2011},
  journal={IEEE Transactions on Education},
  issn={1557-9638},
  volume={54},
  number={4},
  pages={558-563},
  author={Brereton, Pearl},
  keywords={Software engineering, Systematics, Data mining, Case study, systematic literature review (SLR), Education, Protocols, Planning, Book reviews, computing education, evidence-based software engineering (EBSE), group projects},
  abstract={Teaching computing students about the importance of evidence and about the use of empirical methods for evaluating computing technologies can be difficult, especially within dual honors undergraduate degree programs. The aims of this study were to explore the effectiveness of second-year undergraduate computing students in carrying out a systematic literature review and to identify the elements of the process that the students found most difficult. A multicase case study of students carrying out an assignment to perform a systematic literature review (SLR) was undertaken. Students worked in groups and were studying across a range of computing programs. Data was collected from three sources: student grades, the comments made by the teaching staff on the submitted reports, and a debriefing questionnaire. All of the groups successfully completed the assignment. Results on which parts of the process were the most difficult were mixed, although much of the evidence suggests that the students found the conduct phase more problematic than the planning phase. It can be concluded that undergraduates can do SLRs, but the task is clearly quite challenging and time-consuming. SLRs are well suited to being undertaken by groups.}
}

@article{rayyan-727967754,
  title={Poster: A systematic literature review to support the selection of user acceptance testing techniques},
  year={2018},
  pages={418-419},
  author={Dos Santos, Ernani César and Vilain, Patrícia and Hiura Longo, Douglas},
  keywords={Software, Software engineering, Systematics, Bibliographies, Tools, Testing, techniques, classification, features, Natural languages, User acceptance testing},
  abstract={User Acceptance Testing (UAT) aims to determine whether or not a software satisfies users acceptance criteria. Although some studies have used acceptance tests as software requirements, no previous study has collected information about available UAT techniques and established a comparison of them, to support an organization in the selection of one over another. This work presents a Systematic Literature Review on UAT to find out available techniques and compare their main features. We selected 80 studies and found out 21 UAT techniques. As result, we created a comparative table summarizing these techniques and their features.}
}

@article{rayyan-727967755,
  title={Towards evidence-based ontology for supporting Systematic Literature Review},
  year={2012},
  pages={171-175},
  author={Sun, Yueming and Yang, Ye and Zhang, He and Zhang, Wen and Wang, Qing},
  keywords={systematic literature review, ontology, software cost estimation, structured abstract},
  abstract={Systematic Literature Review (SLR) has become an important software engineering research method but costs tremendous efforts. This paper proposes an approach to leverage on empirically evolved ontology to support automating key SLR activities. [Method]: First, we propose an ontology, SLRONT, built on SLR experiences and best practices as a groundwork to capture common terminologies and their relationships during SLR processes; second, we present an extended version of SLRONT, the COSONT and instantiate it with the knowledge and concepts extracted from structured abstracts. Case studies illustrate the details of applying it for supporting SLR steps. Results show that through using COSONT, we acquire the same conclusion compared with sheer manual works, but the efforts involved is significantly reduced. The approach of using ontology could effectively and efficiently support the conducting of systematic literature review.}
}

@article{rayyan-727967756,
  title={A systematic literature review on fault prediction performance in software engineering},
  year={2012},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={38},
  number={6},
  pages={1276-1304},
  author={Hall, Tracy and Beecham, Sarah and Bowes, David and Gray, David and Counsell, Steve},
  keywords={Software testing, Systematic literature review, Systematics, Context modeling, Analytical models, Predictive models, Data models, Fault diagnosis, software fault prediction, Software},
  abstract={Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.}
}

@article{rayyan-727967757,
  title={Model-based software design and testing in blockchain smart contracts: A systematic literature review},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={164556-164569},
  author={Sánchez-Gómez, Nicolás and Torres-Valderrama, Jesus and García-García, J A and Gutiérrez, Javier J and Escalona, M J},
  keywords={Software, systematic literature review, Software engineering, Systematics, Testing, software testing, Proposals, blockchain, Contracts, model-based software engineering, smart contract, software development life cycle, Software Design},
  abstract={Blockchain technology promises to spark a real revolution. One of most important concepts associated with this technology is smart contracts, which enable the automatic execution of agreements and augur a world without intermediaries. The conditions and rules of “contracts” are established in a computer codes and trust is enforced by consensus among the participants. One relevant feature associated with smart contract is the immutability property, which establishes the non-alteration of blockchain network data after the clauses of the contract are been approved by all parties or entities involved. For this reason, smart contract development requires more effort and care than the development of other common programs. They require systematic mechanisms to collect requirements and functional specifications. In addition, it is necessary to verify and validate the agreed functionality and the implemented code before they are deployed in the blockchain platform. This article presents a systematic literature review of primary studies in the field of Software Development Life Cycle, focusing on model-based software design and testing in the blockchain domain of smart contracts. This research aims to identify gaps and/or opportunities for further research. After carried out this review, it was observed that no clear methodology exists for evaluating and validating the quality either of this software or the overall development process. This means that software developers may implement smart contract code in which bugs and serious security vulnerabilities appear when the software is delivered to their customers.}
}

@article{rayyan-727967758,
  title={Causes of requirement change - A systematic literature review},
  year={2012},
  pages={22-31},
  author={Bano, Muneera and Imtiaz, Salma and Ikram, Naveed and Niazi, Mahmood and Usman, Muhammad},
  keywords={systematic literature review, requirements engineering, causes, requirements change},
  abstract={Context: Research shows that one of the main reasons of project failure is changing requirements. The success or failure of software projects largely depends upon how we respond to changing requirements. The knowledge about the causes of requirements change can improve our ability to make better decisions and manage changing requirements effectively. Objective: In this paper we present findings from an empirical study that was aimed at identifying the causes of requirement change and the frequency of these causes in different software development phases. Methods: We performed a systematic literature review and went through all the stages required by the process. Although our search strings yielded a large amount of papers but after careful filtration we were left with only five papers (six studies) which reported empirical knowledge about the causes of requirement change. Results: We have identified different causes and their frequency in software development phases. We have classified the extracted causes of requirements change into two major types i.e., essential and accidental causes. Conclusions: It is surprising to find little empirical evidence on the causes of requirements change as requirements change has been widely quoted as one of the major challenges faced by requirements engineers. With this small number of evidences, it is hard to generalize the research results. There is a need for further empirical research to identify and fully understand the causes of requirement change.}
}

@article{rayyan-727967759,
  title={Using the 5W+1H model in reporting systematic literature review: A case study on software testing for cloud computing},
  year={2013},
  pages={222-229},
  author={Jia, Changjiang and Yu, Yuen Tak},
  keywords={Software testing, systematic literature review, Systematics, Cloud computing, software testing, Computational modeling, 5W+1H, cloud-based application, Software as a service, Software},
  abstract={This paper documents a case study of using the 5W+1H model for reporting systematic literature review on software testing for cloud computing. To our knowledge, this is the first systematic literature review that applies the 5W+1H model, which is widely used in the journalism domain, to report the full picture of the research area in both software engineering and services computing. Existing guidelines on systematic literature review heavily rely on the researcher to pose the right research questions, and the review results are tightly focused on these research questions. For researchers new to a field, defining the right research questions that are effective in revealing the critical issues in the field can be challenging. Our case study demonstrates that the 5W+1H model provides an easy aid for the researcher to get over such initial challenges. As the researcher becomes more familiar with the field, he/she may then refine the research questions by adding more topic-specific contexts. In this way, the 5W+1H model serves to provide an exploratory framework to shape a systematic literature review. Applying to software testing for cloud computing, we are able to synthesize a comprehensive picture of recent researches on the field, including publication pattern, article citation immediacy, research topic diversity, research ideas for addressing testing challenges at different cloud service architectural layers. Based on the case study, we summarize the lessons learned on using the 5W+1H model in reporting systematic literature review.}
}

@article{rayyan-727967760,
  title={Software requirements prioritisation: A systematic literature review on significance, stakeholders, techniques and challenges},
  year={2018},
  journal={IEEE Access},
  issn={2169-3536},
  volume={6},
  pages={71497-71523},
  author={Hujainah, Fadhl and Bakar, Rohani Binti Abu and Abdulgabber, Mansoor Abdullateef and Zamli, Kamal Z},
  keywords={systematic literature review, Software engineering, Systematics, Bibliographies, Requirements prioritization, challenges, Software systems, techniques, Complexity theory, Stakeholders, requirements prioritization criteria, stakeholders, Software},
  abstract={As one of the gatekeepers of quality software systems, requirements' prioritization (RP) is often used to select the most important requirements as perceived by system stakeholders. To date, many RP techniques that adopt various approaches have been proposed in the literature. To identify the strengths, opportunities, and limitations of these existing approaches, this paper studied and analyzed the RP field in terms of its significance in the software development process based on the standard review guidelines by Kitchenham. By a rigorous study selection strategy, 122 relevant studies were selected to address the defined research questions. Findings indicated that RP plays a vital role in ensuring the development of a quality system with defined constraints. The stakeholders involved in RP were reported, and new categories of the participating stakeholders were proposed. Additionally, 108 RP techniques were identified and analyzed with respect to their benefits, prioritization criteria, size of requirements, types in terms of automation level, and their limitations; 84 prioritization criteria were disclosed with their frequency usages in prioritizing the requirements. The study revealed that the existing techniques suffer from serious limitations in terms of scalability, the lack of quantification, and the prioritization of the participating stakeholders, time consumption, requirement interdependences, and the need for highly professional human intervention. These findings are useful for researchers and practitioners in improving the current state of the art and state of practices.}
}

@article{rayyan-727967761,
  title={A systematic review of logging practice in software engineering},
  year={2017},
  pages={534-539},
  author={Rong, Guoping and Zhang, Qiuping and Liu, Xinbei and Gu, Shenghiu},
  keywords={Software Engineering, Software engineering, Systematics, Systematic Literature Review, Tools, Software systems, Fault tolerance, Fault tolerant systems, Logging Practice, Software},
  abstract={Background: Logging practice is a critical activity in software development, which aims to offer significant information to understand the runtime behavior of software systems and support better software maintenance. There have been many relevant studies dedicated to logging practice in software engineering recently, yet it lacks a systematic understanding to the adoption state of logging practice in industry and research progress in academia. Objective: This study aims to synthesize relevant studies on the logging practice and portray a big picture of logging practice in software engineering so as to understand current adoption status and identify research opportunities. Method: We carried out a systematic review on the relevant studies on logging practice in software engineering. Results: Our study identified 41 primary studies relevant to logging practice. Typical findings are: (1) Logging practice attracts broad interests among researchers in many concrete research areas. (2) Logging practice occurred in many development types, among which the development of fault tolerance systems is the most adopted type. (3) Many challenges exist in current logging practice in software engineering, e.g., tradeoff between logging overhead and analysis cost, where and what to log, balance between enough logging and system performance, etc. Conclusion: Results show that logging practice plays a vital role in various applications for diverse purposes. However, there are many challenges and problems to be solved. Therefore, various novel techniques are necessary to guide developers conducting logging practice and improve the performance and efficiency of logging practice.}
}

@article{rayyan-727967762,
  title={Sustainability in software engineering: A systematic literature review},
  year={2012},
  pages={32-41},
  author={Penzenstadler, Birgit and Bauer, Veronika and Calero, Coral and Franch, Xavier},
  keywords={Software},
  abstract={Background: Supporting sustainability in software engineering is becoming an active area of research. We want to contribute the first Systematic Literature Review(SLR) in this field to aid researchers who are motivated to contribute to that topic by providing a body of knowledge as starting point, because we know from own experience, this search can be tedious and time consuming. Aim: We aim to provide an overview of different aspects of sustainability in software engineering research with regard to research activity, investigated topics, identified limitations, proposed approaches, used methods, available studies, and considered domains. Method: The applied method is a SLR in five reliable and commonly-used databases according to the (quasi-standard) protocol by Kitchenham et al. [1]. We assessed the 100 first results of each database ordered by relevance with respect to the search query. Results: Of 500 classified publications, we regard 96 as relevant for our research questions. We sketch a taxonomy of their topics and domains, and provide lists of used methods and proposed approaches. Most of the excluded publications were ruled out because of an unfitting usage of terms within the search query. Conclusions: Currently, there is little research coverage on the different aspects of sustainability in software engineering while other disciplines are already more active. Future work includes extending the study by reviewing a higher number of publications, including dedicated journal and workshop searches, and snowballing.}
}

@article{rayyan-727967763,
  title={Empirical research in software process modeling: A systematic literature review},
  year={2011},
  pages={339-342},
  author={Bai, Xu and Zhang, He and Huang, LiGuo},
  keywords={Software, systematic literature review, Software engineering, Systematics, Data mining, Guidelines, empirical research, Educational institutions, Data analysis, Software process modeling and simulation},
  abstract={Recognized as one of the powerful technologies in software process engineering, Software Process Modeling (SPM) has received significant attention over the last three decades. Although empirical research plays a critical role in software engineering, the state-of-the-practice of empirical research in SPM has not been systematically reviewed. This paper serves as a status report of the assessment of empirical research in SPM by analyzing all refereed studies that were published in relevant venues from 1987 to 2008 using systematic review methodology. The primary findings indicate that in current SPM-related empirical studies, (1) software process management and improvement (SPI) was not yet the most popular primary research objectives, (2) exploratory empirical research methods, e.g., case study and action research, were dominantly used, (3) there were common issues in empirical research reports in terms of following rigorous reporting guidelines. Based on the review results, we also suggest the future needs for empirical research in SPM, in terms of research topics, SPM techniques, the strengths of research methodology and the rigors of empirical studies.}
}

@article{rayyan-727967764,
  title={Improvements to the function point analysis method: A systematic literature review},
  year={2015},
  journal={IEEE Transactions on Engineering Management},
  issn={1558-0040},
  volume={62},
  number={4},
  pages={495-506},
  author={de Freitas Junior, Marcos and Fantinato, Marcelo and Sun, Violeta},
  keywords={Software, Artificial intelligence, Software measurement, systematic literature review (SLR), Complexity theory, ISO Standards, Accuracy, Accuracy improvement, function point analysis (FPA)},
  abstract={Function point analysis (FPA) is a standardized method to systematically measure the functional size of software. This method is proposed by an international organization and it is currently recommended by governments and organizations as a standard method to be adopted for this type of measurement. This paper presents a compilation of improvements, focused on increasing the accuracy of the FPA method, which have been proposed over the past 13 years. The methodology used was a systematic literature review (SLR), which was conducted with four research questions aligned with the objectives of this study. As a result of the SLR, of the 1600 results returned by the search engines, 454 primary studies were preselected according to the criteria established for the SLR. Among these studies, only 18 specifically referred to accuracy improvements for FPA, which was the goal of this study. The low number of studies that propose FPA improvements might demonstrate the maturity of the method in the current scenario of software metrics. Specifically in terms of found issues, it was found that the step for calculating the functional size exhibited the highest number of problems, indicating the need to revise FPA in order to encompass the possible improvements suggested by the researchers.}
}

@article{rayyan-727967765,
  title={Quality factors enhancement of requirement engineering: A systematic literature review},
  year={2019},
  pages={13-135},
  author={Abbas, Syed Manzar and Alam, Khubaib Amjad and Iqbal, Umer and Ajmal, Sahar},
  keywords={Systematic literature review, Quality Enhancement, RE Phases, Requirement Engineering},
  abstract={Software requirement engineering is among the most important issues for starting any software project. The most-reported problem in requirement engineering (RE) is the difficulty to identify quality requirements. Sometime analysts may face incorrect and incomplete requirements, which may become the reason for project failures from the perspective of dissatisfaction of stakeholders. Hence, the quality in each phase of RE is important. The main focus of this article is to categorizing, Identifying, and synthesizing the existing researches on quality enhancement of the RE process. The systematically reviewing the relevant studies based on the Kitchenham systematic review methodology is the main objective of this study. This review identifies the methods which are used for enhancing the quality of RE by improving requirement elicitation, requirement analysis and specification, and requirement validation process. In this research, we are not specific to any phase of RE. Hence, the techniques obtained for quality enhancement purpose can target different phases as we are going to deal with the overall quality of requirements to enhance different quality factors as defined by IEEE standards. In general, 3 defined research questions have been invested for the sake of discussion on explored results. Likewise, 44 articles were selected from an initial set of 101 research papers. In particular, research articles answering formulated question have been included in this SLR.}
}

@article{rayyan-727967766,
  title={Requirements prioritization techniques in the last decade: A systematic literature review},
  year={2020},
  pages={11-20},
  author={Somohano-Murrieta, Juan Carlos B and Ocharán-Hernández, Jorge Octavio and Sánchez-García, Angel J and de los Ángeles Arenas-Valdés, Maria},
  keywords={Software, Systematics, Systematic Literature Review, Bibliographies, Data mining, Guidelines, Requirements engineering, Quality assessment, Requirements Prioritization, Requirements Analysis, Requirements Prioritization Techniques},
  abstract={Requirements Prioritization is an activity, part of Requirements Engineering, whose purpose is to determine the relevance that each requirement has within a software system to be developed. In order to conduct this activity, several requirements prioritization techniques have been proposed. However, companies and software developers do not know which techniques can give the best results possible or when to apply them. Choosing the wrong technique might lead the prioritization process to an inappropriate approach. This systematic literature review aims to know the state of the art of the prioritization techniques which have been used in software projects developed during the last decade, as well as knowing the benefits provided by their implementation. Our results show that AHP, Cumulative Voting, Cost-Value Approach, and Numerical Assignment were the most documented techniques. According to the studies reviewed, these techniques have specific advantages, for example, accuracy and effort reduction benefits. They also have disadvantages such as scalability issues and time consumption problems. However, the use of Requirements Prioritization Techniques, in general terms, generate cost and effort benefits as well as a considerable reduction in the time needed to conduct the Prioritization Process.}
}

@article{rayyan-727967767,
  title={Systematic literature review on effort estimation for Open Sources (OSS) web application development},
  year={2016},
  pages={1158-1167},
  author={Lee, Tseu Kwan and Wei, Koh Tieng and Abd Ghani, Abdul Azim},
  keywords={systematic literature review, Systematics, Bibliographies, Effort estimation, Measurement, Estimation, Databases, Complexity theory, Libraries, open sources, web application development},
  abstract={The development of Web applications has a crucial role as most organizations have their own corporate Web applications to meet the needs of their respective businesses. Different needs create different complexities which represent a new challenge to Web application development. In order to ensure the timely delivery of a project, software providers offering this service choose to use Open Sources (OSS) as an alternative. Since OSS consist of an existing framework that can be implemented directly into the application, how far does this affect the complexity of the effort estimation? A number of research papers have outlined the efforts made to refine the complexity of this field. However, to our best knowledge a systematic overview of the research done on Web application development that involves OSS usage does not appear to exist. Hence, the aim of this paper is to conduct a systematic literature review (SLR) of OSS Web application development. For this purpose, 34 papers from a total of 67 papers were identified and studied. The findings of this study indicate that (a) no research has been carried out on the field mentioned; (b) there is no early effort estimation model for Web projects that involve the usage of OSS. Therefore, this work provides an overview of the field besides identifying future research possibilities.}
}

@article{rayyan-727967768,
  title={The scalability-efficiency/maintainability-portability trade-off in simulation software engineering: Examples and a preliminary systematic literature review},
  year={2016},
  pages={26-34},
  author={Pflüger, Dirk and Mehl, Miriam and Valentin, Julian and Lindner, Florian and Pfander, David and Wagner, Stefan and Graziotin, Daniel and Wang, Yang},
  keywords={Software, Software engineering, Scalability, Computational modeling, Mathematical model, Hardware, Numerical models},
  abstract={Large-scale simulations play a central role in science and the industry. Several challenges occur when building simulation software, because simulations require complex software developed in a dynamic construction process. That is why simulation software engineering (SSE) is emerging lately as a research focus. The dichotomous trade-off between scalability and efficiency (SE) on the one hand and maintainability and portability (MP) on the other hand is one of the core challenges. We report on the SE/MP trade-off in the context of an ongoing systematic literature review (SLR). After characterizing the issue of the SE/MP trade-off using two examples from our own research, we (1) review the 33 identified articles that assess the trade-off, (2) summarize the proposed solutions for the tradeoff, and (3) discuss the findings for SSE and future work. Overall, we see evidence for the SE/MP trade-off and first solution approaches. However, a strong empirical foundation has yet to be established; general quantitative metrics and methods supporting software developers in addressing the trade-off have to be developed. We foresee considerable future work in SSE across scientific communities.}
}

@article{rayyan-727967769,
  title={Understanding institutional repository in higher learning institutions: A systematic literature review and directions for future research},
  year={2019},
  journal={IEEE Access},
  issn={2169-3536},
  volume={7},
  pages={35242-35263},
  author={Asadi, Shahla and Abdullah, Rusli and Yah, Yusmadi and Nazir, Shah},
  keywords={Software, systematic literature review, Systematics, Bibliographies, Data mining, Education, Protocols, Institutional repositories, IRs, Open Access, university},
  abstract={Institutional repositories (IRs) have received considerable attention from researchers across disciplines and around the globe. They have potentially increased the public value, ranking, prestige, and visibility of researchers, and relevant universities. However, despite the important and rapid growth of research in this area, few efforts have been made to systematically review and integrate the findings from previous research studies or to examine the current state of study regarding IRs. The primary goal of this paper is to provide a better understanding and an in-depth review of the current state of study regarding IRs. This research uses a systematic literature review (SLR) and followed a protocol to properly organize the work related to institutional repositories. The data were collected from primary studies published from 2007 to 2018 from the six major databases (ScienceDirect, IEEE Explorer, Springer, ACM, Taylor and Francis, and Emerald insight). Several papers regarding IRs were reviewed, applying inclusion and exclusion criteria, and a total of 115 studies were included as the main part of this research. The results obtained from these studies indicated that the absence of knowledge of open access IRs among scholars and institutions and inadequate information and communication technology infrastructure were significant challenges behind the development of open access IRs. Meanwhile, enhanced visibility of the academic institution, increased local and global rankings, increased prestige and public value, and improved teaching, learning, and research development by the scholars of the institution were found to be the main benefits of institutional repositories. This paper also highlighted that most of the studies in this research area were focused on the ”deployment, implementation, and adoption” and ”benefits and challenges” of institutional repositories. The outcomes of this paper can assist future researchers by providing a roadmap of institutional repositories and highlighting guidelines for successful implementation of IRs in higher learning institutions.}
}

@article{rayyan-727967770,
  title={A systematic literature review and a unified model of ATD},
  year={2016},
  pages={189-197},
  author={Besker, Terese and Martini, Antonio and Bosch, Jan},
  keywords={Systematic literature review, Software engineering, Software Architecture, Architectural Technical Debt, Software Maintenance},
  abstract={Fast software deliveries are hindered by high maintenance efforts due to internal quality issues and Technical Debt (TD) and specifically, Architectural Technical Debt (ATD) has received increased attention in the last few years. ATD has a significant influence and impact on system success and, left unchecked, it can cause expensive repercussions, it is, therefore, of maintenance and evolutionary importance to understand the basic underlying factors of ATD. Thus, with this as background, there is a need for a descriptive model to illustrate and explain the different ATD issues. The aim of this study is to synthesize and compile research efforts with the goal of creating new knowledge with a specific interest in the ATD field. The contribution of this paper is the presentation of a novel descriptive model, providing a comprehensive interpretation of the ATD phenomenon. This model categorizes the main characteristics of ATD and reveals their corresponding relations. The model is based on a systematic literature review (SLR) of currently recognized knowledge concerning ATD.}
}

@article{rayyan-727967771,
  title={A ranking-based approach for supporting the initial selection of primary studies in a Systematic Literature Review},
  year={2019},
  pages={1-10},
  author={González-Toral, Santiago and Freire, Renán and Gualán, Ronald and Saquicela, Víctor},
  keywords={automation, text mining, Systematic literature review, Systematics, Text mining, Bibliographies, data mining, Tools, Ontologies, Semantics, knowledge graphs, machinelearning, NLP, PCA, ranking indexing},
  abstract={Traditionally most of the steps involved in a Systematic Literature Review (SLR) process are manually executed, causing inconvenience of time and effort, given the massive amount of primary studies available online. This has motivated a lot of research focused on automating the process. Current state-of-the-art methods combine active learning methods and manual selection of primary studies from a smaller set so they can maximize the finding of relevant papers while at the same time minimizing the number of manually reviewed papers. In this work, we propose a novel strategy to further improve these methods whose early success heavily depends on an effective selection of initial papers to be read by researchers using a PCAbased method which combines different document representation and similarity metric approaches to cluster and rank the content within the corpus related to an enriched representation of research questions within the SLR protocol. Validation was carried out over four publicly available data sets corresponding to SLR studies from the Software Engineering domain. The proposed model proved to be more efficient than a BM25 baseline model as a mechanism to select the initial set of relevant primary studies within the top 100 rank, which makes it a promising method to bootstrap an active learning cycle.}
}

@article{rayyan-727967772,
  title={A systematic literature review on machine learning for automated requirements classification},
  year={2020},
  pages={21-28},
  author={Pérez-Verdejo, J Manuel and Sánchez-García, Angel J and Ocharán-Hernández, Jorge Octavio},
  keywords={Software, Software engineering, Systematics, Systematic Literature Review, Machine learning, Machine Learning, Requirements Engineering, Classification, Databases, Machine learning algorithms, Software algorithms, Requirements Classification},
  abstract={The development of quality software begins with the correct identification of the system needs. These requirements represent the basis of the subsequent activities in the software life cycle. The correct identification of these requirements in their different categories impacts on the actions taken to meet them. However, this classification can be often time-consuming or error-prone when it comes to large-scale systems, so different proposals have been made to assist in this process automatically. This systematic literature review identifies those applications of Machine Learning techniques in the classification of software requirements. In this regard, 13 articles were identified, from which relevant information on the applied algorithms, their training process, and their evaluation metrics are analyzed. From the results obtained, it is identified that the most recurrent classification algorithms featured on the identified studies are Naive Bayes, Decision Trees, and Natural Language Processing algorithms. The most frequent training datasets are academic databases and collected user reviews.}
}

@article{rayyan-727967773,
  title={Methodology for systematic literature review applied to engineering and education},
  year={2018},
  pages={1364-1373},
  author={Torres-Carrión, Pablo Vicente and González-González, Carina Soledad and Aciar, Silvana and Rodríguez-Morales, Germania},
  keywords={systematic literature review, Systematics, Bibliographies, Databases, Computer science, Proposals, Planning, educational enginering, methodology, Thesauri},
  abstract={A systematic review of the scientific literature in a specific area is important for identifying research questions, as well as for justifying future research in said area. This process is complex for beginners in scientific research, especially if you have not developed skills for searching and filtering information, and do not know which high-level databases are relevant in their field of study. The method proposed leads the researcher from "My" to "The" current state of the problem; we propose an adaptation of the method by Kitchenham and Bacca, which divides the process into three sub-parts: planning, conducting and reporting results. From the approach of the research problem in the preliminary phase research questions (recommended between 3 to 5) and "mentefacto conceptual" is drawn; this last one gives originality to the method and facilitates the development of the thesaurus for searches and inclusion and exclusion criteria. Early research requires doing a basic systematic study to identify work done to review the literature in the area and, if any is found, to verify if those results yield an answer to our research questions. As part of planning the search process, general and specific inclusion and exclusion criteria were defined, along with some complementary inclusion and exclusion parameters. The method followed with rigor, returns to the researcher a list of impact journals in the study area, and a detail of articles that are related to each category of the research questions. A study case has been considered as a guide to expose each of the phases of the methodology in a practical way, with results that support the proposal.}
}

@article{rayyan-727967774,
  title={What software engineering “Best practices” are we teaching students - a systematic literature review},
  year={2018},
  pages={1-8},
  author={Marques, Maíra and Robledo, Javier},
  keywords={Software, Software engineering, Bibliographies, Guidelines, Education, Best practices, Libraries},
  abstract={This research presents that teaching software engineering can be a demanding challenge for instructors, considering that the software industry grows rapidly and there are new development technologies being released constantly. Some of the methodologies being used in industry with time are converted in “best practices”. This systematic literature review (SLR) focuses on finding what are the software engineering best practices being taught to students in academia. It is expected to show with this SLR what are the best practices being taught and how, so other instructors and the teaching staff can evaluate what to choose and level up their courses to the extent of what is being used and is already tested. To perform this SLR, a well-known protocol was used with the search string: “software engineering education” and “best practices” (variations and synonyms of the words were also used), the search was performed in six well-known databases. It is surprising that the amount of primary studies found in this SLR was not what was expected, seventeen primary studies were identified. These studies mentioned a total of seventy best practices that are being used in academia to teach software engineering, some of them are mentioned in more than one paper. But the granularity of the primary studies was quite different some of the best practices are really software engineering best practices and others are instructional best practices. It was also evaluated how the use of these practices was validated and reported, and the results are very diverse, some of them did not have a validation, others have qualitative data, and a few qualitative and quantitative data.}
}

@article{rayyan-727967775,
  title={How reliable are systematic reviews in empirical software engineering?},
  year={2010},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={36},
  number={5},
  pages={676-687},
  author={MacDonell, Stephen and Shepperd, Martin and Kitchenham, Barbara and Mendes, Emilia},
  keywords={meta-analysis, systematic review, Software engineering, Empirical software engineering, Robustness, Computer science, Best practices, Instruments, Mathematics, Costs, cost estimation., Stability, Software},
  abstract={BACKGROUND-The systematic review is becoming a more commonly employed research instrument in empirical software engineering. Before undue reliance is placed on the outcomes of such reviews it would seem useful to consider the robustness of the approach in this particular research context. OBJECTIVE-The aim of this study is to assess the reliability of systematic reviews as a research instrument. In particular, we wish to investigate the consistency of process and the stability of outcomes. METHOD-We compare the results of two independent reviews undertaken with a common research question. RESULTS-The two reviews find similar answers to the research question, although the means of arriving at those answers vary. CONCLUSIONS-In addressing a well-bounded research question, groups of researchers with similar domain experience can arrive at the same review outcomes, even though they may do so in different ways. This provides evidence that, in this context at least, the systematic review is a robust research method.}
}

@article{rayyan-727967776,
  title={Systematic literature review on global software development risks in agile methodology},
  year={2020},
  pages={231-236},
  author={Podari, Zuriyaninatasa and Arbain, Adila Firdaus and Ibrahim, Noraini and Abang Jawawi, Dayang Norhayati and Nasir Wan Kadir, Wan Mohd and Fahmi, Azim Muhammad},
  keywords={Software, Software engineering, Systematics, Bibliographies, Risk management, Agile methodology, challenges, global software development, Standards, Focusing, risks},
  abstract={Background: The word “Global Software Development” can be described as the development of software, with development teams spread across different geographical locations. Problem statement: The issues arise when there are gaps in information, workflows or processes, policies and others in the world. Objective: This paper aims to build an understanding of the risk in Global Software Development. Then, to identify category risk in Global Software Development and how Agile can reduce or mitigate their challenges. Method: This review paper using the standard systematic literature review method by Kitchehamm by reviewing and analyzing the relevant state-of -art techniques and approaches in the journal libraries based on the research questions. Results: The findings show that communication in Agile Global Software development is the main risk challenge. Contribution: The contributions of this paper may support the academician to propose and validate an enhanced approach to solve the issues if risk management in Agile GSD environment and assist the practitioners in choosing the most suitable and relevant method based on the requirement of Agile GSD project. This review paper is also focusing on the contribution to the Software Engineering Management and Software Engineering Models and Methods Knowledge Area.}
}

@article{rayyan-727967777,
  title={Software architecture and requirements: A systematic literature review},
  year={2015},
  pages={1-5},
  author={Batool, Dur-e-Benish and Molta, Yasir Hafeez and Sarwar, Amber and Abbasi, Mateen Ahmed and Jabeen, Javeria},
  keywords={Systematic literature review, Systematics, Bibliographies, Data mining, Ontology, Service-oriented architecture, Computer architecture, Architecture description languages, software architecture, Software},
  abstract={Research in software architecture (SA) can be seen in two perspectives: a traditional and modern one. Software architecture is represented using graphical diagrams (Models, Frameworks) of the system, Architecture Description Languages, Ontology. The main focus of this paper is to know about the software architecture and to find out the strength of evidence in empirical work reported within literature. The outcome of this systematic literature review will be useful for researchers and practitioners and this SLR also includes widely used tools, models, and techniques used in SA; software architecture challenges/Issues widely reported; the SA areas which are under major consideration; the SA areas that require sufficient attention. In the end we also provide information model regarding requirements and architecture, which handles portability and contextual issues regarding platform dependency. Main purpose of information model is to facilitate practitioners for achieving traceability between requirements and architecture.}
}

@article{rayyan-727967778,
  title={A map of threats to validity of systematic literature reviews in software engineering},
  year={2016},
  pages={153-160},
  author={Zhou, Xin and Jin, Yuqin and Zhang, He and Li, Shanshan and Huang, Xin},
  keywords={Software, Software engineering, Systematics, Bibliographies, Data mining, Search problems, Manuals, Threats to Validity, Evidence-Based Software Engineering, Systematic (Literature) Review},
  abstract={Context: The assessment of Threats to Validity (TTVs) is critical to secure the quality of empirical studies in Software Engineering (SE). In the recent decade, Systematic Literature Review (SLR) was becoming an increasingly important empirical research method in SE. One of the mechanisms of insuring the level of scientific value in the findings of an SLR is to rigorously assess its validity. Hence, it is necessary to realize the status quo and issues of TTVs of SLRs in SE. Objective: This study aims to investigate the-state-of-the-practice of TTVs of the SLRs published in SE, and further support SE researchers to improve the assessment and strategies against TTVs in order to increase the quality of SLRs in SE. Method: We conducted a tertiary study by reviewing the SLRs in SE that report the assessment of TTVs. Results: We identified 316 SLRs published from 2004 to the first half of 2015, in which TTVs are discussed. The issues associated to TTVs were also summarized and categorized. Conclusion: The common TTVs related to SLR research, such as internal validity and reliability, were thoroughly discussed in most SLRs. The threats to construct validity and external validity drew less attention. Moreover, there are few strategies and tactics being reported to cope with the various TTVs.}
}

@article{rayyan-727967779,
  title={Software requirements modeling: A systematic literature review},
  year={2020},
  pages={194-200},
  author={Arif, Mohd. and Mohammad, Chaudhary Wali and Sadiq, Mohd.},
  keywords={systematic literature review, Requirements engineering, notations, requirements modeling, Software},
  abstract={Software requirements modeling (SRM) is a subprocess of requirements engineering (RE) which is used to elicit and represent the need of the stakeholders. Different systematic literature reviews (SLR) have been performed in different areas of RE like requirements elicitation, stakeholder identification, requirements prioritization, use case models, etc. Despite the availability of different SRM techniques, less attention is given to synthesize the existing SRM techniques in the context of the unified modeling language (UML) and goal oriented techniques like “Knowledge Acquisition for Automated Specifications” (KAOS), I* framework, non-functional requirements (NFR) framework, and Tropos, etc. Therefore, to address this issue, in this paper we present the SLR by analysing the existing SRM techniques based on the following formulated research questions (RQs): (a) how UML and goal oriented techniques were evolved? (b) which modeling techniques are appropriate for modeling the NFRs? (c) what are the tools available for modeling the different types of the software requirements, i.e., functional and nonfunctional requirements? Search items were extracted from the RQs to identify the primary studies from the Journals, Conferences, Workshops, and Symposium. Our SLR has identified 56 distinct studies which have been published from 2008 to 2019. Selected studies were assessed according to the formulated RQs for their quality and coverage to specific SRM technique thus identifying some gaps in the literature. We observed that there is need to develop the SRM techniques for representing the different types of the NFRs; and also to strengthen the UML by integrating the NFRs and multi-criteria decision making techniques.}
}

@article{rayyan-727967780,
  title={Objectivity in research: Challenges from the evidence-based paradigm},
  year={2009},
  pages={73-80},
  author={Charters, Stuart and Budgen, David and Turner, Mark and Kitchenham, Barbara and Brereton, O Pearl and Linkman, Stephen},
  keywords={Humans, systematic literature review, Software engineering, Decision making, Computer science, Knowledge engineering, Mathematics, Biological system modeling, Predictive models, evidence based software engineering, Muscles, Software standards},
  abstract={For other domains that have adopted the evidence-based paradigm, the impact has included research outcomes having greater influence in terms of informing and influencing practitioners and policy-makers. We examine how evidence-based practices are being adapted for use in software engineering and discuss how decision-making in our own discipline can be liberated from over-reliance on expert judgement. To support our arguments we discuss some outcomes from recent studies and present an example in which performing a systematic literature review demonstrates the unreliability of depending only upon the outcomes of individual studies. Finally, we identify six challenges that need to be addressed in order to provide software engineers with standards and practices that are underpinned by evidence.}
}

@article{rayyan-727967781,
  title={Understanding quality attributes in microservice architecture},
  year={2017},
  pages={9-10},
  author={Li, Shanshan},
  keywords={Software, systematic literature review, Software engineering, Systematics, Bibliographies, quality attributes, Conferences, Computer architecture, IEEE Press, microservices, monolith},
  abstract={As a prevalent architectural style, microservice architecture overcomes the challenges of monolithic architecture and achieves better quality by implementing small-scale microservice, rather than binding all functions into one monolith. Welldesigned microservice architecture with better quality relies on clear understanding of related quality attributes. However, current understanding of quality attributes in microservice architecture is deficient and not comprehensive. In this study, we aim to construct knowledge of quality attributes in architecture through a Systematic Literature Review (SLR), the exploratory case study and the explanatory survey. By analyzing the influential factors and the corresponding tactics of related quality attributes, our research is aimed at providing a comprehensive guide on quality improvement in microservice architecture.}
}

@article{rayyan-727967782,
  title={Systematic literature reviews in global software development: A tertiary study},
  year={2012},
  pages={2-11},
  author={Verner, J M and Brereton, O P and Kitchenham, B A and Turner, M and Niazi, M},
  keywords={systematic review, mapping study, global software development, distributed development, tertiary review, Software},
  abstract={Context: There has been an increase in research into global software development (GSD) and in the number of systematic literature reviews (SLRs) addressing this topic. Objective: The aim of this research is to catalogue GSD SLRs in order to identify the topics covered, the active researchers, the publication vehicles, and to assess the quality of the SLRs identified. Method: We performed a broad automated search to find SLRs dealing with GSD. We differentiate between SLR studies and papers reporting those studies. Data relating to each of the following was extracted and synthesized from each study: authors and their affiliation at the time of publication, the journal or conference in which the paper was published, the quality of each study and the main GSD study topic. Results: Twenty-four GSD SLR studies and 37 papers reporting those studies were identified. Major GSD topics covered include: (1) organizational environment, (2) project execution, and (3) project planning and control. The main research groups are based in Brazil (17), Ireland (8), and Sweden (7). Conclusions: GSD SLR studies are most frequently reported in the International Conference on Global Software Engineering and IEEE Software; the two most popular topics for research are risk factors due to the organizational environment and the development process. The most active researchers are based in Brazil. The quality of the SLR studies has not changed over time.}
}

@article{rayyan-727967783,
  title={Empirical evaluation of the impact of object-oriented code refactoring on quality attributes: A systematic literature review},
  year={2018},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={44},
  number={1},
  pages={44-69},
  author={Al Dallal, Jehad and Abdin, Anas},
  keywords={systematic literature review, Systematics, Bibliographies, Software quality, Unified modeling language, Object oriented modeling, Libraries, quality attribute, quality measure, refactoring scenario},
  abstract={Software refactoring is a maintenance task that addresses code restructuring to improve its quality. Many studies have addressed the impact of different refactoring scenarios on software quality. This study presents a systematic literature review that aggregates, summarizes, and discusses the results of 76 relevant primary studies (PSs) concerning the impact of refactoring on several internal and external quality attributes. The included PSs were selected using inclusion and exclusion criteria applied to relevant articles published before the end of 2015. We analyzed the PSs based on a set of classification criteria, including software quality attributes and measures, refactoring scenarios, evaluation approaches, datasets, and impact results. We followed the vote-counting approach to determine the level of consistency among the PS reported results concerning the relationship between refactoring and software quality. The results indicated that different refactoring scenarios sometimes have opposite impacts on different quality attributes. Therefore, it is false that refactoring always improves all software quality aspects. The vote-counting study provided a clear view of the impacts of some individual refactoring scenarios on some internal quality attributes such as cohesion, coupling, complexity, inheritance, and size, but failed to identify their impacts on external and other internal quality attributes due to insufficient findings.}
}

@article{rayyan-727967784,
  title={A systematic literature review of applications of the physics of notations},
  year={2019},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={45},
  number={8},
  pages={736-759},
  author={van der Linden, Dirk and Hadar, Irit},
  keywords={Systematic literature review, Visualization, Unified modeling language, Complexity theory, Physics, Semantics, cognitive effectiveness, design rationale, physics of notations, visual notations},
  abstract={INTRODUCTION: The Physics of Notations (PoN) is a theory for the design of cognitively effective visual notations, emphasizing the need for design grounded in objective and verifiable rationale. Although increasingly applied, no systematic analysis of PoN applications has yet been performed to assess the theory's efficacy in practice. OBJECTIVES: Our primary objective was to assess the scope and verifiability of PoN applications. METHOD: We performed a systematic literature review (SLR) of peer-reviewed PoN applications. We analyzed what visual notations have been evaluated and designed using the PoN, for what reasons, to what degree applications consider requirements of their notation's users, and how verifiable these applications are. RESULTS: Seventy PoN applications were analyzed. We found major differences between applications evaluating existing notations and applications designing new notations. Particularly, in the case of new notations, we found that most applications adopted the PoN with little critical thought towards it, rarely considered its suitability for a particular context, and typically treated and discussed the PoN with few, if any, verifiable details and data. CONCLUSION: The results warrant consideration for those applying the PoN to do so carefully, and show the need for additional means to guide designers in systematically applying the PoN.}
}

@article{rayyan-727967785,
  title={A systematic literature review and meta-analysis on cross project defect prediction},
  year={2019},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={45},
  number={2},
  pages={111-147},
  author={Hosseini, Seyedrebvar and Turhan, Burak and Gunarathna, Dimuthu},
  keywords={meta-analysis, systematic literature review, Systematics, Bibliographies, Measurement, Context modeling, Defect prediction, Object oriented modeling, Predictive models, Data models, cross project, fault prediction, within project},
  abstract={Background: Cross project defect prediction (CPDP) recently gained considerable attention, yet there are no systematic efforts to analyse existing empirical evidence. Objective: To synthesise literature to understand the state-of-the-art in CPDP with respect to metrics, models, data approaches, datasets and associated performances. Further, we aim to assess the performance of CPDP versus within project DP models. Method: We conducted a systematic literature review. Results from primary studies are synthesised (thematic, meta-analysis) to answer research questions. Results: We identified 30 primary studies passing quality assessment. Performance measures, except precision, vary with the choice of metrics. Recall, precision, f-measure, and AUC are the most common measures. Models based on Nearest-Neighbour and Decision Tree tend to perform well in CPDP, whereas the popular naïve Bayes yields average performance. Performance of ensembles varies greatly across f-measure and AUC. Data approaches address CPDP challenges using row/column processing, which improve CPDP in terms of recall at the cost of precision. This is observed in multiple occasions including the meta-analysis of CPDP versus WPDP. NASA and Jureczko datasets seem to favour CPDP over WPDP more frequently. Conclusion: CPDP is still a challenge and requires more research before trustworthy applications can take place. We provide guidelines for further research.}
}

@article{rayyan-727967786,
  title={Problems in the adoption of agile-scrum methodologies: A systematic literature review},
  year={2016},
  pages={141-148},
  author={López-Martínez, Janeth and Juárez-Ramírez, Reyes and Huertas, Carlos and Jiménez, Samantha and Guerra-García, Cesar},
  keywords={Software, systematic literature review, Systematics, Bibliographies, Scrum, Companies, adoption problems, agile methodologies, Scrum (Software development)},
  abstract={Agile methodologies are focused on the people and functional product delivery in short periods of time. There are methodologies that change considerably the work habits of software developers. Scrum is an agile methodology that involves an iterative, incremental, and empiric process. Besides it is designed to add value, focus, clarity and transparency to the activities and products of a project. Nowadays, most companies are interested in the adoption of agile methodologies. Although Scrum is a light process and easy to understand, its adoption sometimes is difficult. Agile methodologies are not obvious by themselves, so they are difficult to introduce in the culture of a company. In order to identify the problems presented during the adoption, a Systematic Literature Review is performed focusing in Scrum. We found several problems, these are categorized in four groups: people, process, project, and company (organization). The results represent a basis to propose a framework to support the agile adoption.}
}

@article{rayyan-727967787,
  title={Testability and software robustness: A systematic literature review},
  year={2015},
  pages={341-348},
  author={Hassan, Mohammad Mahdi and Afzal, Wasif and Blom, Martin and Lindström, Birgitta and Andler, Sten F and Eldh, Sigrid},
  keywords={Software, Systematic literature review, Testing, Robustness, Software robustness, Contracts, Fault tolerance, Fault tolerant systems, Observability, Software testability},
  abstract={The concept of software testability has been researched in several different dimensions, however the relation of this important concept with other quality attributes is a grey area where existing evidence is scattered. The objective of this study is to present a state-of-the-art with respect to issues of importance concerning software testability and an important quality attribute: software robustness. The objective is achieved by conducting a systematic literature review (SLR) on the topic. Our results show that a variety of testability issues are in focus with observability and controllability issues being most researched. Fault tolerance, exception handling and handling external influence are prominent robustness issues in focus.}
}

@article{rayyan-727967788,
  title={Systematic literature review: Teaching novices programming using robots},
  year={2011},
  pages={21-30},
  author={Major, L and Kyriacou, T and Brereton, O P},
  keywords={systematic literature review, SLR, innovative, learning, novices, programming, robotics, robots, teaching, Robotics},
  abstract={Background: Teaching programming to novices is a difficult task due to the complex nature of the subject, as negative stereotypes are associated with programming and because introductory programming courses often fail to encourage student understanding. Aim: This study investigates the effectiveness of using robots as tools to aid the process of teaching programming and to determine whether such technology can help to overcome the current barriers for learners in this context. Method: The Systematic Literature Review (SLR) methodology has been selected to discover how effective the use of robotics has been in the teaching of introductory programming concepts. Nine electronic databases, the proceedings from six conferences and two journals have been searched for literature relevant to the study. Results: After applying inclusion and exclusion criteria 34 articles have been accepted in the SLR. 74% of included literature report robots to be an effective teaching tool and one that can help novice programmers in their studies. Conclusion: Robots can be a powerful and effective tool when used in an introductory programming course but the potential remains to further investigate methods for their implementation. Thoughts on the use of the SLR methodology from the perspective of a PhD student are also given.}
}

@article{rayyan-727967789,
  title={Reporting usability defects: A systematic literature review},
  year={2017},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={43},
  number={9},
  pages={848-867},
  author={Yusop, Nor Shahida Mohamad and Grundy, John and Vasa, Rajesh},
  keywords={Software engineering, Systematics, Bibliographies, Systematic review, Testing, Usability, Human computer interaction, test management, usability defect reporting, usability testing, user interface},
  abstract={Usability defects can be found either by formal usability evaluation methods or indirectly during system testing or usage. No matter how they are discovered, these defects must be tracked and reported. However, empirical studies indicate that usability defects are often not clearly and fully described. This study aims to identify the state of the art in reporting of usability defects in the software engineering and usability engineering literature. We conducted a systematic literature review of usability defect reporting drawing from both the usability and software engineering literature from January 2000 until March 2016. As a result, a total of 57 studies were identified, in which we classified the studies into three categories: reporting usability defect information, analysing usability defect data and key challenges. Out of these, 20 were software engineering studies and 37 were usability studies. The results of this systematic literature review show that usability defect reporting processes suffer from a number of limitations, including: mixed data, inconsistency of terms and values of usability defect data, and insufficient attributes to classify usability defects. We make a number of recommendations to improve usability defect reporting and management in software engineering.}
}

@article{rayyan-727967790,
  title={Systematic literature reviews in software engineering: Preliminary results from interviews with researchers},
  year={2009},
  pages={346-355},
  author={Babar, Muhammad Ali and Zhang, He},
  keywords={Software engineering, Software measurement, Guidelines, Logistics, Best practices, Data analysis, Application software, Helium, Gain measurement, Resource management, Software},
  abstract={Systematic Literature Reviews (SLRs) have been gaining significant attention from software engineering researchers since 2004. Several researchers have reported their experiences of and lessons learned from applying systematic reviews to different subject matters in software engineering. However, there has been no attempt at independently exploring experiences and perceptions of the practitioners of systematic reviews in order to gain an in-depth understanding of various aspects of systemic reviews as a new research methodology in software engineering. We assert that there is a need of evidence based body of knowledge about the application of systematic reviews in software engineering. To address this need, we have started an empirical research program that aims to contribute to the growing body of knowledge about systematic reviews in software engineering. This paper reports the design, logistics, and results of the first phase empirical study carried out in this program. The results provide interesting insights into different aspects of systematic reviews based on the analysis of the data gathered from 17 interviewees with varying levels of knowledge of and experiences in systematic reviews. The findings from this study are expected to contribute to the existing knowledge about using systematic reviews and help further improve the state-of-the-practice of this research methodology in software engineering.}
}

@article{rayyan-727967791,
  title={Towards a set of factors to identify the success in scrum project delivery: A systematic literature review},
  year={2019},
  pages={97-106},
  author={Tona, Claudia and Juárez-Ramírez, Reyes and Jiménez, Samantha and Durán, Mayra and Guerra-García, César},
  keywords={systematic literature review, Scrum, component, software development, Scrum management, Sprint, Sprint management, success},
  abstract={Agile-based software development is increasingly being adopted by software professionals, as it guarantees the early development of high-quality software and software products. One of the most popular agile methods is Scrum, which is a methodology that involves an iterative, incremental and empirical process. In addition, it is designed to add value, focus, clarity, and transparency to the activities and products of a project. Agile methods have been criticized and defended, and research has shown that accommodating change can be a factor in both success and failure. Although Scrum is a light and easy to understand process, its adoption is sometimes difficult. To gather a set of factors to identify success during a Sprint implementation, a systematic review of the literature is carried out in which we find several factors that help to achieve the success of a Sprint; these are classified into four groups: personnel, project, product, and organization. The results represent a basis for companies seeking to improve the implementation of Scrum.}
}

@article{rayyan-727967792,
  title={Software architecture degradation in open source software: A systematic literature review},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={173681-173709},
  author={Baabad, Ahmed and Zulzalil, Hazura Binti and Hassan, Sa'adah and Baharom, Salmi Binti},
  keywords={systematic literature review, Systematics, Bibliographies, Software architecture, Open source software, OSS, Computer architecture, architectural degradation, architectural erosion, Degradation, open-source, Software},
  abstract={Software architecture (SA) has a prominent role in all stages of system development. Given the persistent evolution of software systems over time, SA tends to be eroded or degraded. Such phenomenon is called architectural degradation. In light of this phenomenon, the current study focuses on problems of architectural erosion in the open-source software (OSS). There has been a significant research activity on the OSS over the last decade. Nonetheless, the architectural degradation problems in the OSS are still scattered and disorganized. In addition, there has been no systematic attempt made on existing studies to provide evidence, insight and better understanding for researchers and practitioners. The main objective of the present study is to provide a profound understanding and to review the existing studies on the architectural erosion of the OSS. In this study, we conduct a systematic literature review (SLR) to gather, organize, classify, and analyze the architectural degradation of previous papers published until the year 2020. The data for this study were collected from eight major online databases (ACM, Springer, ScienceDirect, Taylor, IEEE Explorer, Scopus, Web of Science, and Wiley). A total of 74 primary studies were identified as the final samples of this research. The results indicated that rapid software evolution, frequent changes, and the lack of developers' awareness are the most common causes occurred in architecture degradation. Meanwhile, the prominent key indicators of architectural erosion symptoms are code smells and architectural smells. Additionally, the results indicated the most commonly used of the proposed solution for addressing architectural erosion is the metrics-based strategy. Acknowledging the limitations of the current study, more studies are needed that focus on determining other causes that are still ambiguous and improving the other solutions to provide better results in the precision and effectiveness of addressing architectural erosion.}
}

@article{rayyan-727967793,
  title={Information dashboards and tailoring capabilities - a systematic literature review},
  year={2019},
  journal={IEEE Access},
  issn={2169-3536},
  volume={7},
  pages={109673-109688},
  author={Vázquez-Ingelmo, Andrea and Garcia-Peñalvo, Francísco J and Therón, Roberto},
  keywords={Artificial intelligence, systematic literature review, Systematics, Visualization, SLR, Bibliographies, Tools, User experience, Planning, adaptive, custom, information dashboards, personalized, tailoring},
  abstract={The design and development of information dashboards are not trivial. Several factors must be accounted; from the data to be displayed to the audience that will use the dashboard. However, the increase in popularity of these tools has extended their use in several and very different contexts among very different user profiles. This popularization has increased the necessity of building tailored displays focused on specific requirements, goals, user roles, situations, domains, etc. Requirements are more sophisticated and varying; thus, dashboards need to match them to enhance knowledge generation and support more complex decision-making processes. This sophistication has led to the proposal of new approaches to address personal requirements and foster individualization regarding dashboards without involving high quantities of resources and long development processes. The goal of this work is to present a systematic review of the literature to analyze and classify the existing dashboard solutions that support tailoring capabilities and the methodologies used to achieve them. The methodology follows the guidelines proposed by Kitchenham and other authors in the field of software engineering. As results, 23 papers about tailored dashboards were retrieved. Three main approaches were identified regarding tailored solutions: customization, personalization, and adaptation. However, there is a wide variety of employed paradigms and features to develop tailored dashboards. The present systematic literature review analyzes challenges and issues regarding the existing solutions. It also identifies new research paths to enhance tailoring capabilities and thus, to improve user experience and insight delivery when it comes to visual analysis.}
}

@article{rayyan-727967794,
  title={A systematic literature review and quality analysis of javascript malware detection},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={190539-190552},
  author={Sohan, Md. Fahimuzzman and Basalamah, Anas},
  keywords={systematic literature review, Systematics, Bibliographies, Data mining, Quality assessment, Databases, Cybersecurity, javascript attacks, javascript malware detection, malicious code detection, Malware},
  abstract={Context: JavaScript (JS) is an often-used programming language by millions of web pages and is also affected by thousands of malicious attacks. Objective: In this investigation, we provided a general view and a quick understanding of JavaScript Malware Detection (JSMD) research reported in the scientific literature from several perspectives. Method: We performed a Systematic Literature Review (SLR) and quality analysis of published research articles on the topic. We investigated 32 articles published between the year 2009 to the year 2019. Results: Selected 32 papers explained in this article reflect the outline of what was published so far. One of our key findings is the performance of Machine Learning (ML) based detection models were relatively higher than others. We also found that only a few papers were able to achieve high scores according to the quality assessment criteria. Conclusion: In this SLR, we summarized and synthesized the existing JSMD studies to identify the previous research practices and also to shed light on future guidelines in the malware detection space. This study will guide and help future researchers to investigate the previous literature efficiently and effectively.}
}

@article{rayyan-727967795,
  title={Analysing the use of graphs to represent the results of systematic reviews in software engineering},
  year={2011},
  pages={174-183},
  author={Felizardo, Katia Romero and Riaz, Mehwish and Sulayman, Muhammad and Mendes, Emilia and MacDonell, Stephen G and Maldonado, Jose Carlos},
  keywords={Software engineering, Systematics, Visualization, Systematic Literature Review, Context, Data visualization, Computer science, Educational institutions, Visual Text Mining, Software},
  abstract={The presentation of results from Systematic Literature Reviews (SLRs) is generally done using tables. Prior research suggests that results summarized in tables are often difficult for readers to understand. One alternative to improve results' comprehensibility is to use graphical representations. The aim of this work is twofold: first, to investigate whether graph representations result is better comprehensibility than tables when presenting SLR results; second, to investigate whether interpretation using graphs impacts on performance, as measured by the time consumed to analyse and understand the data. We selected an SLR published in the literature and used two different formats to represent its results - tables and graphs, in three different combinations: (i) table format only; (ii) graph format only; and (iii) a mixture of tables and graphs. We conducted an experiment that compared the performance and capability of experts in SLR, as well as doctoral and masters students, in analysing and understanding the results of the SLR, as presented in one of the three different forms. We were interested in examining whether there is difference between the performance of participants using tables and graphs. The graphical representation of SLR data led to a reduction in the time taken for its analysis, without any loss in data comprehensibility. For our sample the analysis of graphical data proved to be faster than the analysis of tabular data. However, we found no evidence of a difference in comprehensibility whether using tables, graphical format or a combination. Overall we argue that graphs are a suitable alternative to tables when it comes to representing the results of an SLR.}
}

@article{rayyan-727967796,
  title={An investigation into software product innovation: A systematic literature review},
  year={2016},
  pages={1-9},
  author={Edison, Henry and Duc, Anh Nguyen and Jabangwe, Ronald and Wang, Xiaofeng and Abrahamsson, Pekka},
  keywords={Software, systematic literature review, Software engineering, Systematics, Bibliographies, Data mining, Databases, empirical evidence, software product innovation, Technological innovation},
  abstract={New products enable firms not only to accrue high profits but also to leapfrog competition. Different key activities and processes have been proposed to increase the likelihood of successful product innovation. Software products have different characteristics, which need different treatment than products in other domains. Current research on software innovation is still scattered among different areas. This study accumulates the current knowledge and empirical evidences of software product innovation using a systematic literature review approach, and builds the path for future research in this area. Among others, our findings highlight research gaps, as well as best practices and determinants for software product innovation from the reviewed empirical studies.}
}

@article{rayyan-727967797,
  title={Knowledge mapping system implementation in knowledge management: A systematic literature review},
  year={2018},
  pages={131-136},
  author={Hakim, Shidiq Al and Sensuse, Dana Indra},
  keywords={systematic literature review, Systematics, Bibliographies, Search problems, Knowledge management, Databases, Conferences, Protocols, knowledge map system, knowledge mapping, knowledge mapping system},
  abstract={Knowledge mapping has been studied widely in knowledge management context, but the terminology of knowledge map system (KMSs) is not been much studied and it needs to identify state the art KMSs topic from a literature review to propose the future research. The method refers to the systematic literature review as guidelines from Kitchenham, this research gathers, synthesizes, and analyses some paper based on keyword “knowledge map system” or “knowledge mapping system” and “knowledge management”, where it published from 2008 until 2017 on four international electronic databases and using predefined review protocol. We obtained 9 articles used in this study and find that most design systems use customized with some computational technics. The scope is still a lot of focus on enterprise and virtual community, accordingly it needs to be developed further system for the country as public services and also validation method that combines elements of content and software.}
}

@article{rayyan-727967798,
  title={The case for knowledge translation},
  year={2013},
  pages={263-266},
  author={Budgen, David and Kitchenham, Barbara and Brereton, Pearl},
  keywords={Software, Software engineering, Systematics, Guidelines, Context, Educational institutions, D2.18.e Software process models},
  abstract={Context: For the outcomes of systematic literature reviews to be of use for practitioners, we need to develop models for addressing the needs of Knowledge Translation (KT). Aim: To identify some of the key issues that need to be addressed by a KT process for software engineering (SE) and possible routes for achieving these. Method: We have examined some of the models used in other disciplines, and suggested a possible interpretation for software engineering. Results: We propose a model for achieving KT. Conclusions: Research with industry and commerce is needed to explore how this can be realised.}
}

@article{rayyan-727967799,
  title={Ludic practices to support the development of software engineering educational games: A systematic review},
  year={2018},
  pages={794-802},
  author={Zambon, Cláudia and Thiry, Marcello},
  keywords={Software, Software Engineering, Software engineering, Systematics, Information services, Games, catalog, Earth, educational games, Electronic publishing, learning objectives, play activity},
  abstract={In this paper we presente a systematic review of the literature with the objective of identifying playful practices used in educational games. From this result, began the construction of a catalog of playful practices to support the development of educational games. Our research is focused initially on the area of Software Engineering. However, the structure of the systematic review and the catalog under development can be extended to other areas. With a traceable structure among the key elements of the catalog (play practices, levels of knowledge and contents), we try to present a practical reference material for developers of educational games or even for teachers who want to apply recreational activities in the classroom. The catalog under construction is available on a wiki in order to enable it to grow collaboratively. In addition to the results of the systematic review, this article also presents an initial version of the catalog of playful practices.}
}

@article{rayyan-727967800,
  title={Quality metrics in software design: A systematic review},
  year={2019},
  pages={80-86},
  author={Hernandez-Gonzalez, Esmeralda Yamileth and Sanchez-Garcia, Angel Juan and Cortes-Verdin, Maria Karen and Perez-Arriaga, Juan Carlos},
  keywords={Systematic Literature Review, Metrics, software quality, object-oriented software design, Metronidazole, Software Design, Software},
  abstract={This paper presents the results of a systematic literature review, which aimed to identify metrics for the quality of software that are applied in the design stage. Fifteen papers from different electronic databases were selected to answer three questions. The analysis allowed us to provide an overview of the metrics used to design artifacts. These metrics will serve as the basis for generating models based on artificial intelligence techniques (Neuronal Networks, Regression, or some other), that help to estimate quality in the early stages of the software development process. It is concluded that most of the design metrics are object oriented. In addition, the design metrics are applied to class diagrams, package diagrams and sequence diagrams.}
}

@article{rayyan-727967801,
  title={Trends and perceptions of evidence-based software engineering research in Malaysia},
  year={2014},
  pages={1-6},
  author={Salleh, Norsaremah and Nordin, Azlin},
  keywords={systematic literature review, survey, Decision support systems, Evidence-based approach, Software, Malaysia},
  abstract={Evidence-based research has been matured and established in many other disciplines such as in Medicine and Psychology. One of the methods that has been widely used to support evidence-based practices is the Systematic Literature Review (SLR) method. The SLR is a review method that aims to provide unbiased or fair evaluation to existing research evidence. The aim of this study is to gather the trends of evidence-based software engineering (SE) research in Malaysia in particular to identify the usage of SLR method among researchers, academics or practitioners. Based on our tertiary study, we found only 19 published work utilizing evidence-based practices in Malaysia within SE and Computer Science related domains. We have also conducted a survey during SLR workshops for the purpose of gathering perceptions on using SLR. The survey was participated by 78 academics and researchers from five universities in Malaysia. Our findings show that researchers in this country are still at preliminary stage in practicing evidence-based approach. We believe that knowledge and skill on using SLR should be promoted to encourage more researchers to apply it in their research.}
}

@article{rayyan-727967802,
  title={Preparing students and engineers for global software development: A systematic review},
  year={2010},
  pages={177-186},
  author={Monasor, Miguel J and Vizcaíno, Aurora and Piattini, Mario and Caballero, Ismael},
  keywords={Software, systematic literature review, Programming, global software development, education, Proposals, Companies, Book reviews, distributed software development, learning, teaching, training, Training},
  abstract={In recent years, the evolution of Global Software Development (GSD) has grown both rapidly and significantly, and although the efficiency of this new type of development has been proven, some challenging issues must still be confronted. Of all these, our research line is focused on designing the specific training that members of virtual teams must receive. Universities and companies therefore need to design training schemas to deal with the specifics of GSD, which are principally related to communication difficulties and time and cultural differences. In this work we present the findings of a Systematic Literature Review in the field of GSD training and teaching. Our intention is twofold: on the one hand we wish to discover the existing strategies and proposals available up to the present day, and on the other hand we wish to identify the open challenges, that will be helpful for practitioners and researchers in the future.}
}

@article{rayyan-727967803,
  title={Software process improvement: A systematic literature review},
  year={2012},
  pages={459-464},
  author={Zil-e-Huma and Bano, Muneera and Ikram, Naveed},
  keywords={Systematic Literature Review, Empirical, Software Process Improvement, Software},
  abstract={CONTEXT - Software Process Improvement (SPI) initiatives create new and improve existing processes to increase productivity, customer satisfaction, quality of product while reducing cost, and time to market thus maximizing Return on investments. OBJECTIVE - The main focus of this paper is to know about the state of art in SPI and to find out the strength of evidence in empirical work reported within SPI literature. METHOD - Methodology of systematic literature review (SLR) is used. A protocol has been developed and executed. Search strings developed and mentioned in the protocol were applied to the databases to extract relevant papers. A set of papers were identified after reading abstracts of papers extracted after application of search string. A quality criterion was applied on this set to finally select the studies for data extraction. Currently, we are at the data extraction phase of SLR. EXPECTED OUTCOME - The anticipated outcome of this systematic review will be state of art in SPI including widely used tools, models, and techniques; reasons to initiate SPI; SPI challenges/Issues widely reported; the SPI areas which are under more consideration; the SPI areas that lack of attention; frequencies of empirical studies in each of the SPI sub-areas.}
}

@article{rayyan-727967804,
  title={Motivation to perform systematic reviews and their impact on software engineering practice},
  year={2013},
  pages={292-295},
  author={Santos, Ronnie E S and da Silva, Fábio Q B},
  keywords={Software, Software engineering, Systematics, systematic literature reviews, Decision making, Context, empirical software engineering, Educational institutions, Industries, survey research, evidence based software engineering, Motivation},
  abstract={Context: Systematic literature reviews (SLRs) are a particular type of secondary study used as the main research method in evidence based research and practice. The starting point of a SLR should be a problem from the practice and the results should, somehow, have an impact on improving practice. Objective: To investigate the motivation of software engineering researchers to conduct a SLR and how the results of their reviews are potentially impacting the software engineering practice. Method: We conducted a cross-sectional survey with authors of 120 SLRs published between 2004 and 2010 identified by three tertiary studies previously published. Results: Forty-four authors of SLR participate in the survey. The motivation behind the vast majority of the SLRs was to gather knowledge about a particular field of study. However, only six participants affirmed that their reviews actually had a direct impact in industrial practice. Conclusions: The use of SLR has increased over the years but they were mostly focused on academic problems and had very little impact on industrial practice. Therefore, the full potential benefits of evidence-based software engineering are not being achieved.}
}

@article{rayyan-727967805,
  title={Comparison of software process models. A systematic literature review},
  year={2015},
  pages={1-6},
  author={Cano, Christian and Melgar, Andrés and Dávila, Abraham and Pessoa, Marcelo},
  keywords={Software, Systematics, Systematic Literature Review, Comparison, Analytical models, ISO Standards, Computational modeling, IEC Standards, Software Process Model},
  abstract={Nowadays, there are several software process models, which fulfill different purposes, approaches and requirements. However, this proliferation causes some confusion in the industry about the benefits or advantages of each proposal. In this context, studies have been conducted to determine the existing equivalence or the extent of coverage between these models having used different approaches to the comparisons. This work aims to present a study of techniques and experiences on comparison of software process models. For this study, a systematic literature review was conducted in relevant databases and available documents finding that there are few works or experiences in this area and it represents an aspect in software engineering the requires a higher level of research and development. Five different methods to compare process models were found and it was identified that the CCT - Comparison Composition Tree method is the unique that have a graphic representation.}
}

@article{rayyan-727967806,
  title={Closing the gap between software engineering education and industrial needs},
  year={2020},
  journal={IEEE Software},
  issn={1937-4194},
  volume={37},
  number={2},
  pages={68-77},
  author={Garousi, Vahid and Giray, Gorkem and Tuzun, Eray and Catal, Cagatay and Felderer, Michael},
  keywords={Software engineering, Computer science education, Information systems, Computer science, Software engineering education, Knowledge engineering, Education courses, important skills, industry needs, knowledge gap, software engineering curriculum, Software},
  abstract={Many recent software engineering graduates often face difficulties when beginning their professional careers, due to misalignment of the skills learned in their university education with what is needed in industry. In this article, we report a literature review of the studies that have been done to make improvements on this issue.}
}

@article{rayyan-727967807,
  title={Empirical studies of pair programming for CS/SE teaching in higher education: A systematic literature review},
  year={2011},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={37},
  number={4},
  pages={509-525},
  author={Salleh, Norsaremah and Mendes, Emilia and Grundy, John},
  keywords={Testing, Software design, Empirical studies, Education, systematic review., Computer science, Collaborative work, pair programming, Algorithm design and analysis, Educational programs, Performance evaluation, Programming profession, Time measurement},
  abstract={The objective of this paper is to present the current evidence relative to the effectiveness of pair programming (PP) as a pedagogical tool in higher education CS/SE courses. We performed a systematic literature review (SLR) of empirical studies that investigated factors affecting the effectiveness of PP for CS/SE students and studies that measured the effectiveness of PP for CS/SE students. Seventy-four papers were used in our synthesis of evidence, and 14 compatibility factors that can potentially affect PP's effectiveness as a pedagogical tool were identified. Results showed that students' skill level was the factor that affected PP's effectiveness the most. The most common measure used to gauge PP's effectiveness was time spent on programming. In addition, students' satisfaction when using PP was overall higher than when working solo. Our meta-analyses showed that PP was effective in improving students' grades on assignments. Finally, in the studies that used quality as a measure of effectiveness, the number of test cases succeeded, academic performance, and expert opinion were the quality measures mostly applied. The results of this SLR show two clear gaps in this research field: 1) a lack of studies focusing on pair compatibility factors aimed at making PP an effective pedagogical tool and 2) a lack of studies investigating PP for software design/modeling tasks in conjunction with programming tasks.}
}

@article{rayyan-727967808,
  title={Systematic review of success factors for scaling agile methods in global software development environment: A client-vendor perspective},
  year={2017},
  pages={17-24},
  author={Shameem, Mohammad and Kumar, Chiranjeev and Chandra, Bibhas and Khan, Arif Ali},
  keywords={Software, systematic literature review, Software engineering, Systematics, Data mining, Quality assessment, Global software development, Organizations, Standards organizations, distributed agile process},
  abstract={Presently, global software development(GSD) is gaining a much attention from the softwaredevelopment organizations. In GSD, the agile developmentbecomes more challenging due to the geographically,socio-cultural and temporal boundaries.Several frameworks have been developed to managethe agile development programs in large scale softwaredevelopment organizations e.g. scaled agile framework(SAFe), large-scale scrum (LeSS) and disciplined agiledelivery (DAD). However, these frameworks arelacking to provide the detail information about theagile development programs in GSD environment. Inthis study, the mentioned research gap has been triedto fill by conducting a systematic literature review(SLR) study to identify the key factors that couldpositively impact the agile development activities inGSD environment. Using SLR approach, a total of 15success factors were identified that could positivelyaffect the agile development in GSD environment.The identified success factors were further categorizedinto two broad categories of client and vendor GSDorganizations. Client-vendor classification was used toprovide a broad picture of agile programs, and theirrespective success factors. Moreover, the identifiedfactors were also mapped into six different categoriesbased on a model of process improvement.}
}

@article{rayyan-727967809,
  title={A systematic literature review of the pain management mobile applications: Toward building a conceptual model},
  year={2019},
  journal={IEEE Access},
  issn={2169-3536},
  volume={7},
  pages={131512-131526},
  author={Shah, Umm E Mariya and Chiew, Thiam Kian},
  keywords={systematic literature review, Systematics, Bibliographies, Mobile applications, Databases, Usability, Conceptual model, mobile applications, m-health, pain, Pain, self-management, Pain Management},
  abstract={In healthcare, mobile-based interventions support the improvement of clinical process and result in a positive behavioral change and improve the patients' health condition. This study aims at reviewing mobile applications documented for pain management in the scientific databases, to identify the key factors that are vital for pain management. In this research, a systematic literature review was conducted on the selected studies collected from five scientific databases: Medline, PubMed, EMBASE, Web of Science and Scopus. After applying the inclusion and exclusion criteria and performing the quality assessment, twenty-five studies were finalized. It has been observed that the apps were not all-inclusive in features to provide an effective pain self-management solution. As found from the review, the general features of the pain management mobile applications are pain information, pain coping strategy, social support, sub-goals and achievements, self-reporting, feedback, and patient report. Some apps involved psychological interventions. A prominent technique found was cognitive behavior therapy. This study has contributed to the body of knowledge by proposing a conceptual model in guiding the development of pain management mobile applications. The conceptual model was evaluated by a panel of experts to evaluate comprehensiveness, accuracy, and dependencies among the elements of the model, and the appropriateness of the proposed model. Experts recognized the importance of pain management and provided positive feedback to the proposed model.}
}

@article{rayyan-727967810,
  title={Developers' coordination issues and its impact on software quality: A systematic review},
  year={2017},
  pages={659-663},
  author={Suali, A J and Fauzi, S S M and Sobri, W A W M and Nasir, M.H.N.M.},
  keywords={Software engineering, Knowledge management, Software quality, Organizations, Information technology, Coordination, Software Quality, Systematic Literature Review (STC), Software},
  abstract={Complex projects require involvement from many experts to fulfill the requirements. Complexity arises as working in distributed projects indeed requires expertise to overcome the gaps. Coordination among developers increases the likelihood of delivering high quality software. However, working in isolation impedes coordination and causes issues during development. Coordination issues impact the software quality in software engineering projects. This paper aims to investigate issues relating to coordination and its impact on software quality in software engineering projects. Systematic Literature Review (SLR) is applied to perform this study. Among the coordination issues uncovered are language barriers, intercultural, inefficient communication, lack of trust, lack of project flow understanding, different time zones, dependency issues, strategic issues, knowledge management, geographical distance, awareness, and organizational boundaries. As for the finding of this study, all these obstacles significantly impact software quality.}
}

@article{rayyan-727967811,
  title={Arabic text classification methods: Systematic literature review of primary studies},
  year={2016},
  pages={361-367},
  author={Alabbas, Waleed and Al-Khateeb, Haider M and Mansour, Ali},
  keywords={systematic literature review, Systematics, Bibliographies, data mining, Text categorization, Databases, big data, Analytical models, Data models, Arabic text classification, Text corpus, Arabs},
  abstract={Recent research on Big Data proposed and evaluated a number of advanced techniques to gain meaningful information from the complex and large volume of data available on the World Wide Web. To achieve accurate text analysis, a process is usually initiated with a Text Classification (TC) method. Reviewing the very recent literature in this area shows that most studies are focused on English (and other scripts) while attempts on classifying Arabic texts remain relatively very limited. Hence, we intend to contribute the first Systematic Literature Review (SLR) utilizing a search protocol strictly to summarize key characteristics of the different TC techniques and methods used to classify Arabic text, this work also aims to identify and share a scientific evidence of the gap in current literature to help suggesting areas for further research. Our SLR explicitly investigates empirical evidence as a decision factor to include studies, then conclude which classifier produced more accurate results. Further, our findings identify the lack of standardized corpuses for Arabic text; authors compile their own, and most of the work is focused on Modern Arabic with very little done on Colloquial Arabic despite its wide use in Social Media Networks such as Twitter. In total, 1464 papers were surveyed from which 48 primary studies were included and analyzed.}
}

@article{rayyan-727967812,
  title={An update on effort estimation in agile software development: A systematic literature review},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={166768-166800},
  author={Fernández-Diego, Marta and Méndez, Erwin R and González-Ladrón-De-Guevara, Fernando and Abrahão, Silvia and Insfran, Emilio},
  keywords={Software, Systematics, Bibliographies, Effort estimation, Agile software development, Estimation, systematic literature review (SLR), Variable speed drives, Planning, agile methods, agile software development (ASD)},
  abstract={Software developers require effective effort estimation models to facilitate project planning. Although Usman et al. systematically reviewed and synthesized the effort estimation models and practices for Agile Software Development (ASD) in 2014, new evidence may provide new perspectives for researchers and practitioners. This article presents a systematic literature review that updates the Usman et al. study from 2014 to 2020 by analyzing the data extracted from 73 new papers. This analysis allowed us to identify six agile methods: Scrum, Xtreme Programming and four others, in all of which expert-based estimation methods continue to play an important role. This is particularly the case of Planning Poker, which is very closely related to the most frequently used size metric (story points) and the way in which software requirements are specified in ASD. There is also a remarkable trend toward studying techniques based on the intensive use of data. In this respect, although most of the data originate from single-company datasets, there is a significant increase in the use of cross-company data. With regard to cost factors, we applied the thematic analysis method. The use of team and project factors appears to be more frequent than the consideration of more technical factors, in accordance with agile principles. Finally, although accuracy is still a challenge, we identified that improvements have been made. On the one hand, an increasing number of papers showed acceptable accuracy values, although many continued to report inadequate results. On the other, almost 29% of the papers that reported the accuracy metric used reflected aspects concerning the validation of the models and 18% reported the effect size when comparing models.}
}

@article{rayyan-727967813,
  title={A systematic literature review to identify human related challenges in globally distributed agile software development: towards a hypothetical model for scaling agile methodologies},
  year={2018},
  pages={1-7},
  author={Shameem, Mohammad and Chandra, Bibhas and Kumar, Rakesh Ranjan and Kumar, Chiranjeev},
  keywords={Software, Systematic literature review, Systematics, Bibliographies, Quality assessment, Databases, Agile development, Global software development, Organizations, Standards organizations, Human related challenges, Scaling, Humanities, Humanism, Humans},
  abstract={Currently, software organizations are implementing agile methodologies in global software development (GSD) because of low development cost, schedule and high quality product. However, GSD project is complex undertaken because of it distributed dimensions especially when the agile methodologies are concerned. The objective of this study is to identify the human related factors that can negatively influence agile practices in GSD organizations, and proposed a hypothetical model of the identified challenges related to the scaling agile methodologies. A Systematic Literature Review (SLR) method was used to identify the challenges. In the findings, a total of eleven challenges were identified using SLR. This study also reported the Critical Challenges (CChs) for scaling agile methodologies using a criterion of the challenges having a frequency ≥ 50 %. Findings reported the six out of eleven challenges as critical challenges in scaling agile methods. Based on the identified challenges, a hypothetical model was presented that is highlighted a relationship between identified challenges and the implementation of agile methodologies in GSD environment.}
}

@article{rayyan-727967814,
  title={Comparison of mobile interaction management products using systematic literature review method and a new product suggestion},
  year={2017},
  pages={183-188},
  author={Öztürk, Selin and Elmas, Can and Bozyiğbit, Fatma and Kılınç, Deniz},
  keywords={machine learning, Systematics, Systematic Literature Review, Bibliographies, Mobile applications, Androids, Humanoid robots, mobile application, Mobile communication, mobile-user interaction, Reactive power, user loyalty},
  abstract={Because of innovations and improvements in technology, the use of smartphones that make it easier for users to work has become widespread. At this point, companies can reach their customers more easily and can communicate continuously. Once mobile applications are created, the system infrastructure needs to be improved in response to changing needs and demands to actively retain registered users and continually capture their insights. In this case, a dynamic framework that will create user profiles in a mobile application and provide services according to different user needs. In this study, the main features of the mobile interaction management applications on the market and other features they provide to create a loyal user base have been evaluated using the Systematic Literature Review (SLI) method and the necessary gaps have been discussed. In order to acquire loyal mobile-app user, Machine Learning support system is proposed as solution.}
}

@article{rayyan-727967815,
  title={Model management tools for models of different domains: A systematic literature review},
  year={2019},
  pages={1-8},
  author={Torres, Weslley and van den Brand, Mark and Serebrenik, Alexander},
  keywords={Systematics, Systematic Literature Review, Bibliographies, Tools, Modeling, Product lifecycle management, Google, Model Management, Model-Based Systems Engineering, Systems Engineering},
  abstract={Objective: The goal of this study is to present an overview of industrial and academic approaches to cross-domain model management. We aim at identifying industrial and academic tools for cross-domain model management and describing the inconsistency types addressed by them as well as strategies the users of the tools employ to keep consistency between models of different domains. Method: We conducted a systematic literature review. Using the keyword-based search on Google Scholar we analyzed 515 potentially relevant studies; after applying inclusion and exclusion criteria 88 papers were selected for further analysis. Results: The main findings/contributions are: (i) a list of available tools used to support model management; (ii) approximately 31% of the tools can provide consistency model checking on models of different domains and approximately 24% on the same domain; (iii) available strategies to keep the consistency between models of different domains are not mature enough; (iv) explicit modeling dependencies between models is not common in the industry. However, it is considered as a requirement by academia if one wishes to manage inconsistency between models of different domains. Conclusion: This study presents an overview of industrial practices and academic approaches about the cross-domain model management. The results presented in this study can be used as a starting point for future research on model management topics, and also for further improvement of actual model management tools.}
}

@article{rayyan-727967816,
  title={Using blockchain to improve collaborative business process management: Systematic literature review},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={142312-142336},
  author={Garcia-Garcia, Julian Alberto and Sánchez-Gómez, Nicolás and Lizcano, David and Escalona, M J and Wojdyński, Tomás},
  keywords={Systematic literature review, Systematics, Collaboration, BPM, Business process management, Computer architecture, Companies, blockchain technology, business process management, collaborative business processes, inter-organizational process management},
  abstract={BlockChain Technology (BCT) has appeared with strength and promises an authentic revolution on business, management, and organizational strategies related to utilization of advanced software systems. In fact, BCT promotes a decentralized architecture to process management and the collaborative work between entities when these ones are working together in a business process. This paper aims to know what proposals exist to improve any stage of business process management using BCT because this technology could provide benefits in this management. For this purpose, this paper presents a systematic literature review in area of Collaborative Business Processes (CBP) in BCT domain to identify opportunities and gaps for further research. This paper concludes there is a rapid and growing interest of public bodies, scientific community and software industries to know opportunities that BCT offers to improve CBP management in a decentralized manner. However, although the topic is in early stages, there are very promising lines of research and relevant open issues, but there also is lack of scientific rigor in validation process into the different studies.}
}

@article{rayyan-727967817,
  title={A systematic literature review of machine learning applications for port's operations},
  year={2020},
  pages={1-5},
  author={Mekkaoui, Sara E and Benabbou, Loubna and Berrado, Abdelaziz},
  keywords={machine learning, systematic literature review, Data mining, Machine learning, Optimization, analytics, Libraries, Planning, Biological system modeling, ports, shipping, Transportation, Learning},
  abstract={In transportation systems, there is a need of Machine Learning (ML) to create intelligent solutions for many issues. In port's studies, there has been a long focus on optimization and simulation modeling, but still, ML served in building more complex decision support systems for better management of port operations. To evaluate the contributions in the application of ML for port operations, we conducted this Systematic Literature Review (SLR) in order to select and characterize the most relevant papers. This paper, reports the applied research protocol and its results, and highlights the gaps that could be addressed in future works.}
}

@article{rayyan-727967818,
  title={Reference architectures for self-managed software systems: A systematic literature review},
  year={2014},
  pages={21-31},
  author={Affonso, Frank J and Scannavino, Katia R F and Oliveira, Lucas B R and Nakagawa, Elisa Y},
  keywords={Systematic literature review, Software architecture, Databases, Reference architecture, Software systems, Computer architecture, Computational modeling, Self-managed software systems, Software},
  abstract={Self-Managed Software Systems (SMSS) have emerged as an important type of software systems. However, the development of such systems is not a trivial task, as they constantly deal with adaptations at runtime so as to fulfill new needs of both users and execution environment. From another perspective, Reference Architectures (RAs) have been used for the aggregation of knowledge on specific domains, promoting the reuse of design expertise and facilitating the development, standardization, and evolution of software systems. Considering the relevance of such architectures, RAs for SMSS (RA4SMSS) have also been proposed. On the other hand, to the best of our knowledge, no study on a panorama or comparison on RA4SMSS has been published. This paper reports the results of a systematic literature review on RA4SMSS. They show that although relevant initiatives have been found, the SMSS area needs a broader contribution to boost the development of such systems. Moreover, research lines that must further investigated were also identified.}
}

@article{rayyan-727967819,
  title={Research on proposals and trends in the architectures of semantic search engines: A systematic literature review},
  year={2017},
  pages={271-280},
  author={Morales, Jorge and Melgar, Andrés},
  keywords={systematic literature review, Systematics, Bibliographies, Quality assessment, Semantic web, Computer architecture, Protocols, software architecture, Engines, knowledge, knowledge representation, ontologies, semantic search engines, Research Design, Semantics},
  abstract={Semantic web technologies have gained some spot-light in recent years, mostly explained by the spread of mobile devices and broadband Internet access. As once envisioned by Tim Berners-Lee, semantic web technologies have fostered the development of standards that enable, in turn, the emergence of semantic search engines that give users the information they are looking for. This paper presents the results of a systematic literature review that focuses on understanding the proposals on the semantic search engines from an architectural point of view. From the results it is possible to say that most of the studies propose an integral solution for their users where their requirements, the context and the modules that comprise the search engine have a great role to play. Ontologies and knowledge also play an important role in these architectures as they evolve, enabling a great myriad of solutions that respond in a better way to the users' expectations.}
}

@article{rayyan-727967820,
  title={Defining protocols of systematic literature reviews in software engineering: A survey},
  year={2017},
  pages={202-209},
  author={Felizardo, Katia Romero and de Souza, Érica Ferreira and Falbo, Ricardo Almeida and Vijaykumar, Nandamudi Lankalapalli and Mendes, Emilia and Nakagawa, Elisa Yumi},
  keywords={Software engineering, Systematics, Systematic Literature Review, Bibliographies, Data mining, Systematic Mapping Study, Survey, Secondary Study, Electronic mail, Protocols, Planning, Protocol Definition, Software},
  abstract={Context: Despite being defined during the first phase of the Systematic Literature Review (SLR) process, the protocol is usually refined when other phases are performed. Several researchers have reported their experiences in applying SLRs in Software Engineering (SE) however, there is still a lack of studies discussing the iterative nature of the protocol definition, especially how it should be perceived by researchers conducting SLRs. Objective: The main goal of this study is to perform a survey aiming to identify: (i) the perception of SE researchers related to protocol definition; (ii) the activities of the review process that typically lead to protocol refinements; and (iii) which protocol items are refined in those activities. Method: A survey was performed with 53 SE researchers. Results: Our results show that: (i) protocol definition and pilot test are the two activities that most lead to further protocol refinements; (ii) data extraction form is the most modified item. Besides that, this study confirmed the iterative nature of the protocol definition. Conclusions: An iterative pilot testcan facilitate refinements in the protocol.}
}

@article{rayyan-727967821,
  title={Google scholar vs. Dblp vs. Microsoft academic search: An indexing comparison for software engineering literature},
  year={2020},
  pages={1097-1098},
  author={Fatima, Rubia and Yasin, Affan and Liu, Lin and Wang, Jianmin},
  keywords={Software, Software Engineering, Google Scholar, Software engineering, Systematics, Bibliographies, Mapping Study, Google, Search engines, Tertiary Study, dblp, Indexing, Microsoft Academic Search, Search Engines},
  abstract={Background: One of the necessary conditions for any substantial research work is to synthesis the depth and the breath of the existing published literature on that topic. It is, thus, of extreme importance for a researcher to understand and look for both credible and exhaustive information sources. This first (important) step can be made significantly easier if the researcher can employ a more systematic way to extract the maximum of the literature on the topic. Objective: Essentially, the objective of this preliminary study is to rank three freely available academic search engines (Google Scholar, DBLP, Microsoft Academic Search) on the basis of the indexed Software Engineering academic literature they contain. Method: We have used a systematic mapping to conduct the study. Results: After extracting and analyzing 1067 secondary studies (from 18 tertiary studies), we have concluded that Google Scholar has indexed 98.96%, DBLP has indexed 93.43%, and Microsoft Academic Search engine has indexed 97.46% of the secondary studies. Thus, this implies that Google Scholar and Microsoft Academic Search might be a better-suited option for searching for secondary studies.}
}

@article{rayyan-727967822,
  title={Global software development and capability maturity model integration: A systematic literature review},
  year={2018},
  pages={1-6},
  author={Hidayati, Anita and Purwandari, Betty and Budiardjo, Eko K and Solichah, Iis},
  keywords={Software, Outsourcing, Systematic Literature Review (SLR), Companies, Standards organizations, software process improvement, Capability maturity model, Capability Maturity Model Integration (CMMI), Global Software Development (GSD), Process Area (PA)},
  abstract={Global Software Development (GSD) offers many benefits. On the other hand, it also has several challenges such as its impact on software quality. The implementation of software processes improvement, such as the Capability Maturity Model Integration (CMMI), can improve quality of software products. However, CMMI as a process improvement model has been rarely discussed in GSD context. To fill this gap a systematic literature study was conducted by using Kitchenham's method. It investigated GSD research from several databases between 2013-2018. At the end there are 12 selected papers. The findings indicate that CMMI is essential for the success of GSD projects. However, it requires multiple customizations and combinations with other standards. Concept Process Areas (PAs) and maturity level from CMMI are used in new standards Software Process Improvement (SPI) with different approaches. The results of this study are potential as a reference to develop customized CMMI to increase success rate in GSD projects.}
}

@article{rayyan-727967823,
  title={A systematic literature review in multi-agent systems: Patterns and trends},
  year={2019},
  pages={1-10},
  author={Falco, Mariana and Robiolo, Gabriela},
  keywords={systematic literature review, Systematics, Bibliographies, Guidelines, Multi-agent systems, Analytical models, Market research, AOSE, Integrated circuit modeling, multi-agent systems},
  abstract={Multi-Agent Systems became a powerful solution to model and solve problems in complex and dynamic environments. While research in this area grew exponentially before 2009, there is a need to understand the status quo of the field from 2009 to June 2017 in order to comprehend the general evolution. The results of a SLR related to Multi-Agent Systems, its applications and research gaps, following Kitchenham and Wholin guidelines are presented in this paper. From the analysis of279 papers (out of3522 candidates), our findings suggest that: a) there is a general decreasing trend of publications (but it is increasing for specific domains), b) only 15% of the papers portrayed a real case study, c) the top 20 were formed by 67 authors, d) the papers were mostly published in journals and conferences, e) there is no unified methodology or framework, f) the top 3 application domains were transport/traffic, healthcare/biology, and logistics/manufacturing, g) MAS interact with different disciplines like machine learning. Finally, the MAS community should work together to close the gaps and unify the field, bridging with other disciplines and industry.}
}

@article{rayyan-727967824,
  title={On the security aspects of Internet of Things: A systematic literature review},
  year={2019},
  journal={Journal of Communications and Networks},
  issn={1976-5541},
  volume={21},
  number={5},
  pages={444-457},
  author={Macedo, Evandro L C and de Oliveira, Egberto A R and Silva, Fabio H and Mello, Rui R and França, Felipe M G and Delicato, Flavia C and de Rezende, José F and de Moraes, Luís F M},
  keywords={trust, Internet of Things, IoT, techniques, Protocols, security, Market research, Access control, architecture, authentication, Authentication, data protection, Data protection, internet of things, Internet},
  abstract={Internet of Things (IoT) has gained increasing visibility among emerging technologies and undoubtedly changing our daily life. Its adoption is strengthened by the growth of connected devices (things) as shown in recent statistics. However, as the number of connected things grows, responsibility related to security aspects also needs to increase. For instance, cyberattacks might happen if simple authentication mechanisms are not implemented on IoT applications, or if access control mechanisms are weakly defined. Considering the relevance of the subject, we performed a systematic literature review (SLR) to identify and synthesize security issues in IoT discussed in scientific papers published within a period of 8 years. Our literature review focused on four main security aspects, namely authentication, access control, data protection, and trust. We believe that a study considering these topics has the potential to reveal important opportunities and trends related to IoT security. In particular, we aim to identify open issues and technological trends that might guide future studies in this field, thus providing useful material both to researchers and to managers and developers of IoT systems. In this paper, we describe the protocol adopted to perform the SLR and present the state-of-the-art on the field by describing the main techniques reported in the retrieved studies. To the best of our knowledge, ours is the first study to compile information on a comprehensive set of security aspects in IoT. Moreover, we discuss the placement, in terms of architectural tiers, for deploying security techniques, in an attempt to provide guidelines to help design decisions of security solution developers. We summarize our results showing security trends and research gaps that can be explored in future studies.}
}

@article{rayyan-727967825,
  title={Using a systematic literature review to strengthen the evidence supporting a simulation model of distributed software projects},
  year={2019},
  pages={371-378},
  author={Lima, Adailton M and Quites Reis, Rodrigo and de Souza, Cleidson R B},
  keywords={Software engineering, Distributed Software Development, Literature Review, In Silico Studies, Process Simulation, Software},
  abstract={In silico studies allow the use of computer models to simulate the environment, object, and subject behavior of an experiment. They are valuable tools to support the quantitative analysis of the behavior of different types of projects because they allow the use of different techniques such as stochastic and analytical methods to evaluate the impact of project decisions without the real execution costs. Recently, in silico studies have been used to study distributed software development projects. In this paper, we present a simulation model defined by a set of variables that describe the dynamics of Distributed Software Development (DSD) projects. The assessment of our simulation model is based on a Systematic Literature Review (SLR) where we seek evidence, from primary studies published until October 2018, that the variables and relationships in our model do exist in actual DSD projects. We use a SLR as a validation mechanism. Our results are twofold. First, there is a gap of studies on some variables including knowledge expertise and task allocation strategy in distributed software development projects. And, second, there still is a lack of data to support results related to productivity, especially testing activities, on DSD projects. Despite the problem of lack of common terminology for reporting the results on DSD studies, we argue that we need more studies to be able to clearly infer the dynamics of the relationship between variables on distributed software development. Our study brings one step closer to fulfill this gap.}
}

@article{rayyan-727967826,
  title={Parallelization, modeling, and performance prediction in the multi-/many core area: A systematic literature review},
  year={2017},
  pages={48-55},
  author={Frank, Markus and Hilbrich, Marcus and Lehrig, Sebastian and Becker, Steffen},
  keywords={Software, Unified modeling language, Computational modeling, Predictive models, Hardware, Multicore processing, Parallel programming, Software Performance Engineering Performance Prediction Modelling Multicore Many Core Parallel Programming},
  abstract={Context: Software developers face complex, connected, and large software projects. The development of such systems involves design decisions that directly impact the quality of the software. For an early decision making, software developers can use model-based prediction approaches for (non-)functional quality properties. Unfortunately, the accuracy of these approaches is challenged by newly introduced hardware features like multiple cores within a single CPU (multicores) and their dependence on shared memory and other shared resources. Objectives: Our goal is to understand whether and how existing model-based performance prediction approaches face this challenge. We plan to use gained insights as foundation for enriching existing prediction approaches with capabilities to predict systems running on multicores. Methods: We perform a Systematic Literature Review (SLR) to identify current model-based prediction approaches in the context of multicores. Results: Our SLR covers the software engineering, embedded systems, High Performance Computing, and Software Performance Engineering domains for which we examined 34 sources in detail. We found various performance prediction approaches which tries to increase prediction accuracy for multicore systems by including shared memory designs to the prediction models. Conclusion: However, our results show that the memory designs models are only in an initial phase. Further research has to be done to improve cache, memory, and memory bandwidth model as well as to include auto tuner support.}
}

@article{rayyan-727967827,
  title={Cloud computing security requirements: A systematic review},
  year={2012},
  pages={1-7},
  author={Iankoulova, Iliana and Daneva, Maya},
  keywords={systematic literature review, Cloud computing, Data privacy, Privacy, cloud computing, empirical study, Access control, Fires, sequirity requirements engineering, Software-as-a-Service},
  abstract={Many publications have dealt with various types of security requirements in cloud computing but not all types have been explored in sufficient depth. It is also hard to understand which types of requirements have been under-researched and which are most investigated. This paper's goal is to provide a comprehensive and structured overview of cloud computing security requirements and solutions. We carried out a systematic review and identified security requirements from previous publications that we classified in nine sub-areas: Access Control, Attack/Harm Detection, Non-repudiation, Integrity, Security Auditing, Physical Protection, Privacy, Recovery, and Prosecution. We found that (i) the least researched sub-areas are non-repudiation, physical protection, recovery and prosecution, and that (ii) access control, integrity and auditability are the most researched sub-areas.}
}

@article{rayyan-727967828,
  title={SPIIMM: Toward a model for software process improvement implementation and management in global software development},
  year={2017},
  journal={IEEE Access},
  issn={2169-3536},
  volume={5},
  pages={13720-13741},
  author={Khan, Arif Ali and Keung, Jacky W and Fazal-E-Amin and Abdullah-Al-Wadud, M},
  keywords={Software, systematic literature review, Software process improvement, barriers, Organizations, global software development, Industries, Standards organizations, ISO Standards, practices, Capability maturity model, success factors},
  abstract={Software development organizations are globalizing their activities by adopting the phenomenon of global software development (GSD), mainly due to the significant return on investment it offers. Various challenges are associated with the software process improvement (SPI). The aim of this paper is to develop a software process improvement implementation and management model (SPIIMM) that can assist GSD organizations in assessing and improving their SPI activities. A thorough systematic literature review (SLR) study was performed to identify the critical success factors (CSFs), critical barriers (CBs), and the relevant practices of SPI. An empirical study of the industry was conducted with 111 SPI experts using a survey questionnaire to verify the outcomes of the SLR. The final CSFs and CBs were categorized into five maturity levels based on the implementation maturity model, the software outsourcing vendor readiness model, and capability maturity model integration. Each maturity level consisted of different CSFs and CBs to assess and improve the SPI-related maturity level of an organization. Three case studies were conducted to evaluate the effectiveness of the proposed model. The results revealed that SPIIMM can provide a robust framework to assess and improve SPI activities in GSD organizations.}
}

@article{rayyan-727967829,
  title={A systematic review on the use of ontologies in requirements engineering},
  year={2014},
  pages={1-10},
  author={Dermeval, Diego and Vilela, Jéssyka and Bittencourt, Ig Ibert and Castro, Jaelson and Isotani, Seiji and Brito, Patrick},
  keywords={Software, Software engineering, Systematics, Data mining, Quality assessment, Ontologies, Industries},
  abstract={Requirements Engineering (RE) discipline deals with elicitation, analysis, specification, validation and management of requirements. Several ontology-driven approaches have been proposed to improve these RE activities. However, the requirements engineering community still lacks a comprehensive understanding on how ontologies are used in RE process. The objective of this work is to explore how ontologies are employed in requirements engineering, aiming to identify the main phases addressed, the languages that have been used, the types of existing contributions, as well as the requirements modeling styles have been used and the benefits of using ontology in RE. We conducted a systematic literature review to identify the primary studies on the use of ontologies in RE, following a pre-defined review protocol. Sixty-six papers were selected, covering the five main RE process phases. Moreover, we have identified thirteen ontology-related languages. Furthermore, twenty-six empirical studies have been identified which provided evidence of five group of benefits. The main findings of this review are: (1) there are empirical evidences to state that ontologies benefit RE activities in both academy and industry settings, helping to reduce ambiguity, inconsistency and incompleteness of requirements; (2) the vast majority of papers do not meet all RE phases; (3) nearly half of the papers use W3C recommended languages; (4) the majority of contributions are supported by a tool; and (5) there is a great diversity of requirements modeling styles supported by ontologies.}
}

@article{rayyan-727967830,
  title={Cultural factors influencing international collaborative software engineering education in china},
  year={2017},
  pages={31-40},
  author={Wang, Yuqing and Markkula, Jouni and Jiang, Jing},
  keywords={Software, Systematics, Collaboration, Guidelines, Education, Interviews, software engineering education, Cultural differences, Chinese culture, cross-cultural education, international collaborative education},
  abstract={Software engineering (SE) is a rapidly developing international discipline that requires up-to-date knowledge and skills. The need for well-educated professional software engineers is increasing globally. In China, universities are opening opportunities for collaboration and building cooperative relationships with Western universities in technology fields, including SE, to offer Chinese students possibilities for international education in China instead of studying abroad. Designing high-quality SE education in international collaborative programs faces challenges introduced by cultural factors that affect learning practices. In this study, we addressed these challenges in the context of international collaborative SE education in China. In the first step, we synthesized existing knowledge of Chinese cultural factors affecting learning by conducting a systematic literature review (SLR). In the second step, we conducted interviews with SE students and teachers in a Chinese university that is preparing an international collaborative SE program, in order to see whether the identified cultural factors are valid in the current learning contexts of SE education. The results revealed that many of the identified factors are still valid, but some of them present differently in the current context because of the novelty of the SE discipline and the changing educational environment in China.}
}

@article{rayyan-727967831,
  title={Agile to lean software development transformation: A systematic literature review},
  year={2018},
  pages={969-973},
  author={Kišš, Filip and Rossi, Bruno},
  keywords={Software, Systematics, Bibliographies, Measurement, Quality assessment, Companies},
  abstract={Context: Lean development has been often proposed as an adaptation to agile for scaling-up to larger contexts. Goals: we wanted to better understand the “agile-to-lean” transformation, in terms of: i) reported benefits, ii) challenges faced, iii) metrics used. Method: we performed a Systematic Literature Review (SLR) about “agile-to-lean” transformations. Results: reduced lead time, improved flow, continuous improvement, and improved defect fix rate were the main reported benefits. Adaptation to lean thinking, teaching the lean mindset, identification of the concept of waste, and scaling flexibility were the main challenges. Lead time was the most reported metric.}
}

@article{rayyan-727967832,
  title={The dynamic aspects of product derivation in DSPL: A systematic literature review},
  year={2013},
  pages={466-473},
  author={da Silva, Jackson Raniel F and da Silva, Francisco Airton P and do Nascimento, Leandro M and Martins, Dhiego A O and Garcia, Vinicius C},
  keywords={Software, Quality assessment, Context, Computer architecture, Adaptation models, Heuristic algorithms, Runtime},
  abstract={Dynamic Software Product Lines (DSPL) have gained significant attention in academic community by involving aspects of product lines and runtime adaptable systems development. Managing dynamic variations demands is a challenge addressed by DSPL paradigm. In this context, this paper introduces the results of a systematic literature review that involved 2,084 studies, with the objective of understanding how the dynamic derivation in DSPL is made. The contributions of this study are: a) an embracing analysis and classification of scientific literature in DSPL area, b) the definition of inputs that are needed to perform the dynamic derivation, c) the description of what composes these inputs, and d) the understanding of the process to perform the dynamic derivation. We conclude that due to the lack of maturity in the dynamic derivation field, there are many open research opportunities still available.}
}

@article{rayyan-727967833,
  title={Blockchain technology and implementation : A systematic literature review},
  year={2018},
  pages={370-374},
  author={Andrian, Henry Rossi and Kurniawan, Novianto Budi and Suhardi},
  keywords={SLR, Blockchain, Smart contracts, Cryptography, Digital forensics, Distributed databases, Peer-to-peer computing, technology},
  abstract={As research on blockchain continues to grow, blockchain technology was adopted to develop several information systems. There are many opportunities to examine the utilization of blockchain technology to be used in developing systems as needed. This paper summarizes the conditions of blockchain research in terms of technology and its implementation. This paper was written using a systematic literature review (SLR) as one of the methodologies used to solve problems by tracing the results of previous studies. The problem that you want to study in SLR is usually referred to as a research question (RQ). The defined RQ is related to the topic and clarifies each question by tracing previous research papers indexed in reputable journal databases such as IEEE Xplore, Springerlink, Scopus, and ScienceDirect. After synthesizing 41 articles, the result is: blockchain can be used in many applications, some applications that adopt blockchain technology are banking applications, e-voting applications and digital forensic applications. Often, applications that use blockchain technology only focus on developing one of the blockchain technologies that suits their needs. Some developers maximize the components of the contract on the blockchain, some further develop the data structure of the blockchain itself. The use of blockchain technology is still wide open for the implementation of other information systems. The expected contribution of this paper is to provide a general overview for researchers who want to build applications that use blockchain as a basis for researching so they can conduct further studies to better understand whether the applications they will develop are suitable for using blockchain technology as a basis.}
}

@article{rayyan-727967834,
  title={Process mining for cloud-based applications: A systematic literature review},
  year={2019},
  pages={34-43},
  author={El-Gharib, Najah Mary and Amyot, Daniel},
  keywords={Data mining, Cloud computing, Monitoring, Process mining, Web services, Big Data, Requirements elicitation, Computational modeling, Heuristic algorithms, Cloud applications, User behavior},
  abstract={Process mining uses event log data to discover processes, hence enabling multiple requirements elicitation activities. As the number of applications deployed on a cloud infrastructure is increasing, it becomes important to understand their processes and the ways these existing systems are actually used. However, the cloud brings new challenges to process mining that deserve special attention. This paper reports on a systematic literature review based on a selection of 27 papers. The aim is to assess the applicability of process mining techniques to cloud-based applications, to document the processes of these existing systems. We observe there is a growing interest in applying process mining to these areas, and we report on algorithms, tools, and validation approaches taken. We also report on many cloud-specific challenges for process mining, which require further attention from the research community.}
}

@article{rayyan-727967835,
  title={Business process ambidexterity and its impact on business-it alignment. A systematic literature review},
  year={2019},
  pages={1-12},
  author={Helbin, Tomek and Van Looy, Amy},
  keywords={Systematics, SLR, Bibliographies, Databases, Business process management, Organizations, Process control, Business Process Ambidexterity, Business Process Management, Business-IT alignment, Organizational Ambidexterity, Paradoxical Thinking, Functional Laterality},
  abstract={In the age of digital disruption, organizations face the growing need to both optimize existing processes, and to radically innovate and disrupt them. Business Process Management (BPM) has proven great value for process control and process optimization, however not for process disruption. Our Systematic Literature Review of Business Process Ambidexterity provides an overview of this nascent domain. In our research, we have divided all articles into two main strands of research, focusing on: (1) organizational capabilities enabling Business Process Ambidexterity, and (2) the actual dynamic balance between exploration and exploitation in BPM, which is enabled by the modelling and optimization stages in the business process lifecycle. As there is no widely accepted definition of Business Process Ambidexterity, we propose one for future use, based on the concept of paradoxical thinking. Business Process Ambidexterity may both enable, and be facilitated by digitalization; however, there is limited evidence that Business Process Ambidexterity enables Business-IT alignment. The main research gap is the lack of conceptual and practical guidelines on Business Process Ambidexterity implementation. Hence, this article encourages future research and provides preliminary guidance for practitioners.}
}

@article{rayyan-727967836,
  title={Computer-supported collaborative learning in programming education: A systematic literature review},
  year={2020},
  pages={1086-1095},
  author={Silva, Leonardo and Mendes, António José and Gomes, Anabela},
  keywords={computer-supported collaborative learning, introductory programming, programming learning},
  abstract={Social dimension plays a crucial role in the learning process. The use of technological resources to stimulate and mediate the interaction between students is known as Computer-supported collaborative learning (CSCL). Despite being widely used in programming education, the summarization of the academic literature about this topic is scarce. To create that body of knowledge, a systematic literature review was performed. The findings provide an understanding of how collaboration is explored in introductory programming, resources used to stimulate them and challenges in the process. Opportunities for future research are discussed, especially related to motivation, self-efficacy and engagement in CSCL, and the exploitation of learning analytics.}
}

@article{rayyan-727967837,
  title={Systematic literature review of knowledge sharing barriers and facilitators in global software development organizations using concept maps},
  year={2019},
  journal={IEEE Access},
  issn={2169-3536},
  volume={7},
  pages={24231-24247},
  author={Anwar, Rayhab and Rehman, Mobashar and Wang, Khor Siak and Hashmani, Manzoor Ahmed},
  keywords={Software, Systematics, Bibliographies, Data mining, Task analysis, Organizations, Industries, cultural barriers, geographical barriers, global software development organizations, Knowledge sharing barriers, knowledge sharing facilitators},
  abstract={Knowledge is the most important resource in software development. The success of software development relies on knowledge sharing between software developers working across the globe. Global software development has brought many benefits to the software industry; however, at the same, time knowledge sharing across diverse team members is one of the main concerns of global software development organizations. This paper provides a systematic literature review of 42 studies on knowledge sharing barriers and facilitators from 2010 to 2017 and classifies them into five main categories: Individual, Organizational, Technological, Cultural, and Geographical. In order to synthesize and represent the complexity of the knowledge sharing factors in a more manageable and visual manner, this paper proposes concept maps for each category. The identified factors can be strategically used as the guidelines in the global software development organizations to boost the culture of knowledge sharing.}
}

@article{rayyan-727967838,
  title={Product roadmap – from vision to reality: A systematic literature review},
  year={2019},
  pages={1-8},
  author={Münch, Jürgen and Trieflinger, Stefan and Lang, Dominic},
  keywords={Systematics, Bibliographies, Organizations, Computer science, Conferences, Technological innovation, agile development, product discovery, product management, product roadmap, product strategy, product vision, roadmapping},
  abstract={Context: Companies in highly dynamic markets increasingly struggle with their ability to plan product development and to create reliable roadmaps. A main reason is the decreasing lack of predictability of markets, technologies, and customer behaviors. New approaches for product roadmapping seem to be necessary in order to cope with today's highly dynamic conditions. Little research is available with respect to such new approaches. Objective: In order to better understand the state of the art and to identify research gaps, this article presents a review of the scientific literature with respect to product roadmapping. Method: We performed a systematic literature review (SLR) with respect to identify papers in the field of computer science. Results: After filtering, the search resulted in a set of 23 relevant papers. The identified papers focus on different aspects such as roadmap types, processes for creating and updating roadmaps, problems and challenges with roadmapping, approaches to visualize roadmaps, generic frameworks and specific aspects such as the combination of roadmaps with business modeling. Overall, the scientific literature covers many important aspects of roadmapping but does provide only little knowledge on how to create product roadmaps under highly dynamic conditions. Research gaps address, for instance, the inclusion of goals or outcomes into product roadmaps, the alignment of a roadmap with a product vision, and the inclusion of product discovery activities in product roadmaps. In addition, the transformation from traditional roadmapping processes to new ways of roadmapping is not sufficiently addressed in the scientific literature.}
}

@article{rayyan-727967839,
  title={Designing engaging games for education: A systematic literature review on game motivators and design principles},
  year={2020},
  journal={IEEE Transactions on Learning Technologies},
  issn={1939-1382},
  volume={13},
  number={4},
  pages={804-821},
  author={Laine, Teemu H and Lindberg, Renny S N},
  keywords={Systematics, Bibliographies, Taxonomy, Education, Business, taxonomy, Games, gamification, educational games, Design principles (DPs), Entertainment industry, game design, motivation, serious games},
  abstract={Effective educational interventions require sufficient learner engagement, which can be difficult to achieve if the learner is inadequately motivated. Games have been shown to possess powerful motivators that fuel a person's desire to engage in unattractive activities, such as learning theoretical material. However, to design an educational game that is capable of providing motivated engagement is a challenging task. Previous research has proposed various game motivators and game design principles to alleviate this, but a comprehensive synthesis has yet to appear. In this article, we conducted a systematic literature review that yielded two major contributions: 1) a taxonomy of 56 game motivators in 14 classes; and 2) a taxonomy of 54 educational game design principles in 13 classes, with linkages to the identified game motivators. As a minor contribution, we have also presented a classification of gamification-related terms and proposed different strategies for applying gamification. The results of this article are available for educational game designers and researchers to use as a practical toolkit for the creation and evaluation of motivating educational games that keep players engaged. Moreover, this article is the first step toward the creation of a unified gamification framework.}
}

@article{rayyan-727967840,
  title={Smart campus features, technologies, and applications: A systematic literature review},
  year={2017},
  pages={384-391},
  author={Muhamad, Wardani and Kurniawan, Novianto Budi and Suhardi and Yazid, Setiadi},
  keywords={SLR, research challenge, smart campus, summary},
  abstract={Research in the smart campus area is still growing, where every researcher defines the concept of smart campus with a less thorough perspective that has not been conical in the same conception of the concept. In this paper, we summarize the existing condition of smart campus development in term of features, supported technologies, and applications were built using systematic literature review (SLR) as the standard methodology used to solve any problems by tracing the results of previous research. The problems declared in SLR are commonly called as research question (RQ). To achieve that goal, we define some RQs related to that scope and clarify each question by tracing previous research papers which are indexed in reputable journal databases such as IEEE Xplore, Scopus, Springerlink, and ScienceDirect. After synthesizing 29 articles, the results are: contactless technology provides an easier way to enter data when accessing a particular room or equipment than using a keyboard; IoT supports an easier way to report real-time environment status; cloud computing is used to organize various information effectively and provide data services; iCampus becomes a popular smart campus model and if we map the applications that have been built into iCampus; there is no applications as part of iHealth domain. The main contribution of smart campus development based on previous research is to make easier in all campus aspect life. The contribution expected through this paper is to provide an overview for researchers who want to build applications in a campus as research challenge so that they can use appropriate technology and meet the characteristics of smart campus.}
}

@article{rayyan-727967841,
  title={Security and privacy for big data: A systematic literature review},
  year={2016},
  pages={3693-3702},
  author={Nelson, Boel and Olovsson, Tomas},
  keywords={Security, Data privacy, Big data, Privacy, Data visualization, Conferences, Distributed databases},
  abstract={Big data is currently a hot research topic, with four million hits on Google scholar in October 2016. One reason for the popularity of big data research is the knowledge that can be extracted from analyzing these large data sets. However, data can contain sensitive information, and data must therefore be sufficiently protected as it is stored and processed. Furthermore, it might also be required to provide meaningful, proven, privacy guarantees if the data can be linked to individuals. To the best of our knowledge, there exists no systematic overview of the overlap between big data and the area of security and privacy. Consequently, this review aims to explore security and privacy research within big data, by outlining and providing structure to what research currently exists. Moreover, we investigate which papers connect security and privacy with big data, and which categories these papers cover. Ultimately, is security and privacy research for big data different from the rest of the research within the security and privacy domain? To answer these questions, we perform a systematic literature review (SLR), where we collect recent papers from top conferences, and categorize them in order to provide an overview of the security and privacy topics present within the context of big data. Within each category we also present a qualitative analysis of papers representative for that specific area. Furthermore, we explore and visualize the relationship between the categories. Thus, the objective of this review is to provide a snapshot of the current state of security and privacy research for big data, and to discover where further research is required.}
}

@article{rayyan-727967842,
  title={Open source software evaluation, selection, and adoption: a systematic literature review},
  year={2020},
  pages={437-444},
  author={Lenarduzzi, Valentina and Taibi, Davide and Tosi, Davide and Lavazza, Luigi and Morasca, Sandro},
  keywords={Software, Systematics, Data mining, Tools, Open source software, Libraries, Analytical models, Open-source software, soft-ware adoption, software quality models, software selection},
  abstract={Background. Open Source Software (OSS) is experiencing an increasing popularity both in industry and in academia. Aim. We investigated models for the selection, evaluation, and adoption of OSS, focusing on factors that affect most the evaluation of OSS. Method. We conducted a Systematic Literature Review of 262 studies published until the end of 2019, to understand whether OSS selection is still an interesting topic for researchers, and which factors are considered by stakeholders and are assessed by the available models. Result. We selected 60 primary studies: 20 surveys and 5 lessons learned studies elicited the motivations for OSS adoption; 35 papers proposed several OSS evaluation models focusing on different technical aspects. This Systematic Literature Review provides an overview of the available OSS evaluation methods, highlighting their limits and strengths, based on the wide range of technicalities and aspects explored by the selected primary studies. Conclusion. OSS producers can benefit from our results by checking if they are providing all the information commonly required by potential adopters. Users can learn how models work and which models cover the relevant characteristics of OSS they are most interested in.}
}

@article{rayyan-727967843,
  title={Replication of empirical studies in software engineering: Preliminary findings from a systematic mapping study},
  year={2011},
  pages={61-70},
  author={da Silva, Fabio Q B and Suassuna, Marcos and Lopes, Rodrigo. F and Gouveia, Tatiana B and Franca, A Cesar A and de Oliveira, Joao Paulo N and de Oliveira, Leonardo F M and Santos, Andre L M},
  keywords={Software, software engineering, mapping study, Software engineering, Systematics, Data mining, Manuals, literature review, experiments, empirical studies, Knowledge engineering, replications, Sections},
  abstract={Our goal in this study is to review the research related to the replication of empirical studies in software engineering in terms of replications of empirical studies and conceptual or theoretical work about replications. In this article we present the preliminary findings of this review, concentrating on the studies reporting replications and the related original studies. We applied the systematic review method to perform a mapping study about the current state of the replication work of empirical studies performed in software engineering research. We analyzed 16,126 articles, from which we extracted 93 articles reporting 125 replications performed between 1994 and 2010, of 76 original studies. Over 60% of the replications were performed in the last six years and 71% percent of the studies were internal replications. The topics of software construction, testing, and maintenance concentrate nearly 50% of the replication work, while software design, configuration management and software tools and methods are the topics with least replications. The number of replications grew in the last few years, but the absolute number of replications is still very small, in particular considering the breadth of topics in software engineering. Incentive to perform external replications and better standards to report empirical studies and their replications are still needed.}
}

@article{rayyan-727967844,
  title={Analysis of non-functional properties in software product lines: A systematic review},
  year={2014},
  pages={328-335},
  author={Soares, Larissa Rocha and Potena, Pasqualina and Do Carmo Machado, Ivan and Crnkovic, Ivica and de Almeida, Eduardo},
  keywords={Software, Systematic Literature Review, Data mining, Unified modeling language, Reliability, Predictive models, Runtime, Non-functional Properties, Product Derivation, Software Product Lines},
  abstract={Software Product Lines (SPL) approach has been widely developed in academia and successfully applied in industry. Based on the selection of features, stakeholders can efficiently derive tailor-made programs satisfying different requirements. While SPL was very successful at building products based on identified features, achievements and preservation of many nonfunctional properties (NFPs) remain challenging. A knowledge how to deal with NFPs is still not fully obtained. In this paper, we present a systematic literature review of NFPs analysis for SPL products, focusing on runtime NFPs. The goal of the paper is twofold: (i) to present an holistic overview of SPL approaches that have been reported regarding the analysis of runtime NFPs, and (ii) to categorize NFPs treated in the scientific literature regarding development of SPLs. We analyzed 36 research papers, and identified that system performance attributes are typically the most considered. The results also aid future research studies in NFPs analysis by providing an unbiased view of the body of empirical evidence and by guiding future research directions.}
}

@article{rayyan-727967845,
  title={Effort estimation for ERP projects — A systematic review},
  year={2017},
  pages={96-103},
  author={Ömüral, Neslihan Küçükateş and Demirörs, Onur},
  keywords={systematic literature review, Systematics, Estimation, Databases, Business, Analytical models, Predictive models, Data models, effort estimation, enterprise resource planning},
  abstract={Enterprise Resource Planning (ERP) systems are large scale integrated systems covering most of the business processes of an enterprise. ERP projects differ from software projects with customization, modification, integration and data conversion phases. Most of the time effort and time estimations are performed in an ad-hoc fashion in ERP projects and as a result they frequently suffer from time and budget overruns. Although there is no consensus on a methodology to estimate size, effort and cost of ERP projects there are various research studies in the field. The purpose of this paper is to review the literature on effort estimation methods for ERP projects, their validations and limitations. The systematic literature review used online journal indexes between January 2000 and December 2016. Studies focusing on effort estimation for ERP projects were selected. Two reviewers assessed all studies and 41 were shortlisted. In most of the studies, cost factors for ERP projects were investigated and validated. Our findings showed that effort estimation methods have mostly used function points as an input. Validations of these methods were mostly done by using history-based validation approaches.}
}

@article{rayyan-727967846,
  title={Towards greener software engineering using software analytics: A systematic mapping},
  year={2017},
  pages={157-166},
  author={Anwar, Hina and Pfahl, Dietmar},
  keywords={Software, Software engineering, Systematics, systematic mapping, Measurement, software analytics, Energy efficiency, Green products, green software engineering, sustainability, Sustainable development},
  abstract={Sustainability in software engineering is a relatively new and fast growing field of research. Green software engineering aims to produce sustainable software products with minimum negative impact on the environment. In order to make greener software products, software practitioners need actionable timely information, to make useful trade-offs between energy efficiency and other quality attributes, like performance, during development. Software analytics could be used to provide this support, as it combines information from different software artifacts and converts it into useful information. The objective of this paper is to provide an overview of the sub-domains, contribution types, research types, research methods, future research potentials and the role of software analytics in the field of green software engineering in 2015-16. We applied the systematic mapping method and conducted a search for studies in six online databases. Screening of papers was done according to inclusion/exclusion criteria and 50 selected studies were classified after analysis and data extraction. We found that there are many validation studies but hardly any evaluation and experience papers in the domain of green software engineering. Only 11 out of 50 papers in the green software engineering domain used software analytics techniques to foster green software engineering. Our results indicate the need to develop new/improved automated software analytics tools for software practitioners along with metrics explaining the correlation between energy usage and other quality attributes.}
}

@article{rayyan-727967847,
  title={Systematic reviews},
  year={2004},
  pages={xii-xii},
  author={Kitchenham, B A},
  keywords={Software engineering, Data mining, Databases, Information systems, Australia, Computer science, Protocols, Medical services, Software standards, Writing},
  abstract={During the past few years, I have become interested in evidence-based software engineering [5]. The evidence-based paradigm began in medicine and has totally revolutionised medical research [7]. It is now being adopted by a number of humancentered disciplines such as nursing, social policy, education etc. Although I have some reservations about whether software engineering can adopt fully the evidencebased paradigm, I am convinced that we should adopt certain aspects of the approach immediately. In particular, I believe we should adopt systematic review in place of ad hoc literature reviews, and recognise that a systematic review is a research method in its own right. Currently, all PhD students need to conduct a literature review as a part of their research but such reviews are seldom performed with the rigour now being required in other disciplines (including Information Systems Research, [6]). A systematic review is rigorous method of synthesising existing research related to a topic of interest. Quantitative meta-analysis is one form of systematic review, but it is not mandatory to provide a quantitative summary. Systematic reviews aim to provide a synthesis that is complete (with respect to defined criteria) and unbiased. Systematic reviews must be rigorous i.e. be conducted according to a well-defined procedure and open i.e. the research process must be reported in a manner is understandable to other researchers.}
}

@article{rayyan-727967848,
  title={Implementation of the ISO/IEC 29110 standard in agile environments: A systematic literature review},
  year={2018},
  pages={1-6},
  author={Munoz, Mirna and Mejia, Jezreel and Lagunas, Arturo},
  keywords={Software, Organizations, Standards organizations, ISO Standards, Silicon compounds, IEC Standards, practices, agile, ISO/IEC 29110},
  abstract={Nowadays, around 76% of software is developed by very small entities or small entities in México. Thus, there have been emerging new models and standards focused in this type of organizations, such as ISO/IEC 29110 standard. Nevertheless, the lack of process culture, guides, support tools and practices make difficult its implementation in these organizations. This study presents a literature review to identify how the ISO/IEC 29110 standard is being implemented focusing on support tools, practices from other models, standards or agile methodologies which help in its implementation.}
}

@article{rayyan-727967849,
  title={DsdK: An ontology-based system to explore distributed software development experiments},
  year={2012},
  pages={73-75},
  author={Rocha, Rodrigo G C and Meira, Silvio},
  keywords={Software Engineering, Software engineering, Distributed Software Development, Ontologies, Conferences, Software},
  abstract={In the past decade, there was a notably significant raise in the adoption of the Distributed Software Development approach. This so-called approach has brought a lot of competitive advantages, as well as new challenges, such as communication and sharing of information among distributed teams. In this scenario, the use of the ontology concept simplifies and normalizes the teams' understanding of shared information and eases the communication between them. The purpose of this research is to develop a tool based in a specific ontology for DSD environments. This tool assists distributed teams with distributed software development problems, proposing the techniques and best practices to solve these problems.}
}

@article{rayyan-727967850,
  title={Gamification in requirements engineering: A systematic review},
  year={2018},
  pages={119-125},
  author={Cursino, Rodrigo and Ferreira, Daniel and Lencastre, Maria and Fagundes, Roberta and Pimentel, João},
  keywords={Software, Systematics, Search problems, Task analysis, Requirements engineering, Requirements Engineering, Systematic Review, Gamification, Games, Libraries},
  abstract={Gamification has been applied to a large diversity of settings in recent years, with encouraging and promising results. In the Software Engineering context, there are some research works that focused on the application of gamification in Requirements Engineering that need to be better analyzed and understood. Does gamification impact RE? What are the gamification elements that have been used? What sub-processes and RE tasks have been the subject of gamification? The objective of this paper is to carry out a systematic review to characterize the state of the art of gamification in RE, answering those and other questions, identifying gaps and opportunities for further research. It was conducted a systematic review to find the primary studies in the existent literature which objectives is applying gamification to RE and its sub-processes. As a result of the systematic review, 8 primary studies were analyzed. They were published between 2012 and 2017 and most of them focus on the application of gamification onto elicitation, negotiation and prioritization.}
}

@article{rayyan-727967851,
  title={Multilingual source code analysis: A systematic literature review},
  year={2017},
  journal={IEEE Access},
  issn={2169-3536},
  volume={5},
  pages={11307-11336},
  author={Mushtaq, Zaigham and Rasool, Ghulam and Shehzad, Balawal},
  keywords={Software, Software engineering, Systematics, Bibliographies, Data mining, Manuals, software maintenance, Analytical models, software architecture, reverse engineering, Reverse engineering, software design},
  abstract={Contemporary software applications are developed using cross-language artifacts, which are interdependent with each other. The source code analysis of these applications requires the extraction and examination of artifacts, which are build using multiple programming languages along with their dependencies. A large number of studies presented on multilingual source code analysis and its applications in the last one and half decade. The objective of this systematic literature review (SLR) is to summarize state of the art and prominent areas for future research. This SLR is based on different techniques, tools, and methodologies to analyze multilingual source code applications. We finalized 56 multi-discipline published papers relevant to multilingual source code analysis and its applications out of 3820 papers, filtered through multi-stage search criterion. Based on our findings, we highlight research gaps and challenges in the field of multilingual applications. The research findings are presented in the form of research problems, research contributions, challenges, and future prospects. We identified 46 research issues and requirements for analyzing multilingual applications and grouped them in 13 different software engineering domains. We examined the research contributions and mapped them with individual research problems. We presented the research contributions in the form of tools techniques and approaches that are presented in the form of research models, platforms, frameworks, prototype models, and case studies. Every research has its limitations or prospects for future research. We highlighted the limitations and future perspectives and grouped them in various software engineering domains. Most of the research trends and potential research areas are identified in static source code analysis, program comprehension, refactoring, reverse engineering, detection, and traceability of cross-language links, code coverage, security analysis, cross-language parsing, and abstraction of source code models.}
}

@article{rayyan-727967852,
  title={A systematic review of goal-oriented requirements management frameworks for business process compliance},
  year={2011},
  pages={25-34},
  author={Ghanavati, Sepideh and Amyot, Daniel and Peyton, Liam},
  keywords={Systematic literature review, Databases, Unified modeling language, Organizations, Analytical models, requirements engineering, Law, business process, goal modeling, legal compliance},
  abstract={Legal compliance has been an active topic in Software Engineering and Information Systems for many years. However, business analysts and others recently started exploiting Requirements Engineering techniques, and in particular goal-oriented approaches, to model and reason about legal documents in system design and business process management. Many contributions involve extracting legal requirements, providing law-compliant business processes, as well as managing and maintaining compliance. In this paper, we report on a systematic literature review focusing on goal-oriented legal compliance of business processes. 88 papers were selected out of nearly 800 unique papers extracted from five search engines, with manual additions from the Requirements Engineering Journal and four relevant conferences. We grouped these papers in eight categories based on a set of criteria and then highlight their main contributions. We found that the main areas for contributions have been in extracting legal requirements, modeling them with goal modeling languages, and integrating them with business processes. We identify gaps and opportunities for future work in areas related to prioritization to improve compliance, templates for generating law-compliant processes, general links between legal requirements, goal models, and business processes, and semi-automation of legal compliance and analysis.}
}

@article{rayyan-727967853,
  title={What works for whom, where, when, and why? On the role of context in empirical software engineering},
  year={2012},
  pages={19-28},
  author={Dybå, Tore and Sjøberg, Dag I K and Cruzes, Daniela S},
  keywords={theory, Software, Software engineering, evidence-based software engineering, Productivity, Programming, Context, Organizations, Complexity theory, Evidence-Based Software Engineering, Empirical Methods, Generalization, Sociotechnical System, Theory, empirical methods, generalization, sociotechnical system},
  abstract={Context is a central concept in empirical software engineering. It is one of the distinctive features of the discipline and it is an indispensable part of software practice. It is likely responsible for one of the most challenging methodological and theoretical problems: study-to-study variation in research findings. Still, empirical software engineering research is mostly concerned with attempts to identify universal relationships that are independent of how work settings and other contexts interact with the processes important to software practice. The aim of this paper is to provide an overview of how context affects empirical research and how empirical software engineering research can be better `contextualized' in order to provide a better understanding of what works for whom, where, when, and why. We exemplify the importance of context with examples from recent systematic reviews and offer recommendations on the way forward.}
}

@article{rayyan-727967854,
  title={Literature review of empirical research studies within the domain of acceptance testing},
  year={2016},
  pages={181-188},
  author={Weiss, Johannes and Schill, Alexander and Richter, Ingo and Mandl, Peter},
  keywords={Software, systematic literature review, Software engineering, Systematics, Bibliographies, Data mining, Testing, Context, empirical software engineering, acceptance testing},
  abstract={Acceptance test driven development is a promising method to support agile software development. It is aiming to improve development efficiency by automation of test execution and detailed communication of domain knowledge. In this paper a systematic literature review in the field of acceptance test driven development is reported, with the aim of studying existing knowledge and observations in this area. Our search strategy resulted in finding 26 research studies, with 14 experiment papers and 12 use cases papers. We investigated the studies according to a predefined data extraction form. The review investigates the findings of identified empirical research studies. The results are categorized and discussed within the context of previous related reviews.}
}

@article{rayyan-727967855,
  title={Current trends in usability evaluation methods: A systematic review},
  year={2014},
  pages={11-15},
  author={Paz, Freddy and Pow-Sang, José Antonio},
  keywords={systematic review, Systematics, evaluation methods, Testing, Inspection, Usability, usability, human-computer interaction, ISO standards, User interfaces, user-centered design},
  abstract={Since usability is considered as a critical success factor for any software application, several evaluation methods have been developed. Nowadays, it is possible to find many proposals in the literature that address to evaluate usability issues. However, there is still discussion about what usability evaluation method is the most widely accepted by the scientific community. In this research, a systematic review was performed to identify the evaluation methods that have been more employed over the last three years in order to assess the level of usability of a software application. From these results, it has been possible to establish clear evidence about the current trends in this field. A total of 274 usability studies have allowed to reach useful information for scholars in this area.}
}

@article{rayyan-727967856,
  title={Software requirements triage and selection: State-of-the-art and state-of-practice},
  year={2012},
  volume={1},
  pages={416-421},
  author={Khurum, Mahvish and Uppalapati, Niroopa and Veeramachaneni, Ramya Chowdary},
  keywords={systematic review, Software, Systematics, survey, Data mining, challenges, Industries, Companies, software requirements triage and selection, Triage},
  abstract={Software requirement triage and selection in market-driven requirements engineering is a crucial activity for the success of a project, product and company. This paper presents state-of-the-art and state-of-practice in requirements triage and selection through systematic literature review and an industrial survey. Industry practitioner can read solutions that have been proposed in literature and most applicable challenges and factors considered today and factors to be considered ideally to address the challenges, which have been collected through the survey. For researchers, the results show which challenges still needs to be addressed.}
}

@article{rayyan-727967857,
  title={What about misconceptions in software engineering? A research proposal},
  year={2017},
  pages={709-713},
  author={Gold-Veerkamp, Carolin and Abke, Jorg and Diethelm, Ira},
  keywords={Software, Software Engineering, Software engineering, Tools, Conferences, 1/f noise, Constructivism, Engineering education, Misconceptions, Research Design},
  abstract={In Software Engineering education didactical foundation is still at its beginnings. In recent years, this topic is put into stronger focus, as Software Engineering becomes more and more important in our everyday life. However, it is hard to teach and learn, because of diverse factors, e.g. the immateriality of the product. Therefore it is significant to improve the learning process on the one hand and to minimise obstacles to learning on the other hand. Other disciplines - with a long historical background in their “Fachdidaktik”1 (e.g., biology or chemistry) - have already dealt with one sort of learning barriers; so-called misconceptions. The hurdle that misconceptions create can and will be explained by the constructivist learning theory. This paper covers a research approach to identify, analyse, filter and cluster misconceptions in Software Engineering.}
}

@article{rayyan-727967858,
  title={A systematic literature review for development, implementation and deployment of MOOCs focused on older people},
  year={2017},
  pages={287-294},
  author={Beltran, Paola and Rodriguez-Ch, Paul and Cedillo, Priscila},
  keywords={Systematics, Manuals, Quality assessment, Education, MOOCs, techniques, Buildings, Libraries, Planning, learning, andragogy, older people, sistematic review, strategies},
  abstract={According to Administration of Aging (AoA), by the year 2060, the older people will represent among the 98 million of people of the total population. That is, much of this priority group will be involved with technology. Hence, the Massive Open Online Courses (MOOCs) are a good alternative for lifelong learning. Therefore, it should be taken into account the needs of older people according to their learning styles by using andragogical techniques, strategies and accessibility criteria. However, nowadays many courses are offered in different platforms, but they are not oriented to older people with special characteristics according to the age. Moreover, the methodologies reviewed do not take into account accessibility criteria and andragogical techniques and strategies for building MOOCs.}
}

@article{rayyan-727967859,
  title={Challenges and benefits of modern code review-systematic literature review protocol},
  year={2018},
  pages={1-5},
  author={Fatima, Nargis and Chuprat, Suriayati and Nazir, Sumaira},
  keywords={Software, Systematics, Bibliographies, Data mining, benefits, Inspection, challenges, software quality, Conferences, Protocols, modern code review, reviewer},
  abstract={Software quality is major concern of all the stakeholders either acquirer of the software or supplier of the software. Software inspection is the static verification technique that can contribute to the quality development of the software. As the development trend is focused towards open source software development the smart and light weight techniques are emerged. One of them is modern code review, a light weight software inspection technique. The modern code review is informal, static code analysis performed by small team using collaborative tools. It is argued that modern code review provides benefits other than defect detection and is also subject to various challenges such as lack of trust, communication issues, effective knowledge sharing, personal conflicts and varying situations etc. There has been little research with respect to systematizing and identifying the challenges faced by, and benefits gained by Modern Code Review (MCR) team. There is dire need to recognize and organize these challenges and benefits based on modern code review literature. The aim is to highlight and organize the challenges and benefits regarding modern code review context. It will be beneficial for software quality assurance practitioners and researchers by providing classified list of challenges and benefits.}
}

@article{rayyan-727967860,
  title={Signs of agile trends in global software engineering research: A tertiary study},
  year={2011},
  pages={17-23},
  author={Hanssen, Geir K and Šmite, Darja and Moe, Nils Brede},
  keywords={Software, tertiary study, Software engineering, Systematics, Programming, Teamwork, global software engineering, Conferences},
  abstract={In this paper we present preliminary findings from a tertiary study on global software engineering. In particular, we observe current trends in the software engineering research and perform an investigation of the role of agile topics in the GSE research literature. Our findings indicate that agility is one of the topics attracting attention in the research agenda for global software companies. In contrast to recent beliefs that agile and distributed are two incompatibilities Global Agile development becomes more and more accepted, a trend which we also see from the growing amount of research on GSE and agile. Finally we conclude that there are indications that both globalization and "agilization" of software companies are stable trends for the future but that there is a strong need for further studies on the particular challenges that distribution of work imposes on the principles of agile development.}
}

@article{rayyan-727967861,
  title={A systematic literature review of assessment tools for programming assignments},
  year={2016},
  pages={147-156},
  author={Souza, Draylson M and Felizardo, Katia R and Barbosa, Ellen F},
  keywords={Systematics, Bibliographies, Data mining, Mapping study, Databases, Education, Programming profession, Assessment tools, Programming assignments},
  abstract={The benefits of using assessment tools for programming assignments have been widely discussed in computing education. However, as both researchers and instructors are unaware of the characteristics of existing tools, they are either not used or are reimplemented. This paper presents the results of a study conducted to collect and evaluate evidence about tools that assist in the assessment of programming assignments. To achieve our goal, we performed a systematic literature review since it provides an objective procedure for identifying the quantity of existing research related to a research question. The results identified subjects in the development of new assessment tools that researchers could better investigate and characteristics of assessment tools that could help instructors make selections for their programming courses.}
}

@article{rayyan-727967862,
  title={Effect analysis of the introduction of AUTOSAR: A systematic literature review},
  year={2011},
  pages={239-246},
  author={Dersten, Sara and Axelsson, Jakob and Froberg, Joakim},
  keywords={Software, Systematics, Automotive engineering, Complexity theory, Computer architecture, Companies, Hardware, software architecture, automotive systems, AUTOSAR, distributed systems, effect analysis, embedded systems, system evolution, system refactoring},
  abstract={Many complex software-intensive systems have a long life time, and undergo substantial evolution. These evolutions are either additions of functionality or system refactoring, i.e., updating the architecture to improve quality attributes without changing functionality. However, the return of investment for such a system refactoring is not easily measured due to a lack of understanding of its effects. In order to improve our understanding of these effects, we have conducted a systematic literature review of the reported effects of one such refactoring: the introduction of AUTOSAR, an open automotive software architecture standard. The effects include both benefits, like lower complexity and more efficient system development, and costs, like performance risks. We have investigated how the effects depend on different elements in AUTOSAR, and how the reports correspond to the stated objectives of the standard. It is also discussed to what extent these effects can be generalized to other types of refactoring.}
}

@article{rayyan-727967863,
  title={Comparative analysis of requirement change management challenges between in-House and global software development: Findings of literature and industry survey},
  year={2019},
  journal={IEEE Access},
  issn={2169-3536},
  volume={7},
  pages={116585-116611},
  author={Anwer, Sajid and Wen, Lian and Wang, Zhe and Mahmood, Sajjad},
  keywords={Software, Systematics, Project management, systematic literature review (SLR), Unified modeling language, Organizations, challenges, Requirements management, Industries, global project structures, global software development (GSD), Requirement change management (RCM)},
  abstract={Requirement changes are inevitable, and Requirement Change Management (RCM) is a complex process in software development. In-house software development and Global Software Development (GSD) are two widely used development approaches and there is a need to explore the RCM commonalities and differences in the two development approaches. The primary objective of this study is to identify the challenges that influence RCM in both approaches. First, we have implemented Systematic Literature Review (SLR) and identified 9 challenges that impact the general RCM process and 3 more challenges related to RCM with GSD. Second, we have conducted a questionnaire survey based on SLR results and collected feedback from 69 industry practitioners. The survey result indicates that there are four out of nine challenges, namely impact analysis, requirement traceability, requirement dependency, and system instability having the same impact in both in-house and GSD approaches. On the other hand, cost/time estimation, artifacts documents management, user involvement, requirement consistency, and requirement prioritization need more attention while implemented in GSD paradigm. Furthermore, regarding two important project management structures in GSD, centralized project structure and distributed project structure, the survey results reveal that all challenges have same impact except user involvement and change control board management, which are more important in centralized project structure. Lastly, the result from t-test indicates that both data sets retrieved from SLR and survey are close to each other. This study distinguishes RCM challenges in in-house and GSD approaches and in the context of two prominent project management structures followed in GSD projects. It would assist researchers by providing potential research directions and industry professionals to understand and implement RCM in different context more efficiently.}
}

@article{rayyan-727967864,
  title={Security in telehealth systems from a software engineering viewpoint: A systematic mapping study},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={10933-10950},
  author={Márquez, Gastón and Astudillo, Hernán and Taramasco, Carla},
  keywords={Software, software engineering, Software engineering, Systematics, Security, systematic mapping study, Databases, security, Medical services, Telehealth systems, Telemedicine},
  abstract={Telehealth systems deliver remote care of elderly and physically less able patients as well as remote surgeries, treatments, and diagnoses. In this regard, several systemic properties must be satisfied (such as security) in order to ensure the functionality of Telehealth systems. Although existing studies discuss different security episodes that involve Telehealth systems, it is difficult to have a clear standpoint about which are the most reported security issues and which solutions have been proposed. Furthermore, since Telehealth systems are composed of several software systems, it is not clear which critical areas of Software Engineering are relevant to develop secure Telehealth systems. This article reports a systematic mapping study (SMS) whose purpose is to detect, organize, and characterize security issues in Telehealth systems. Based on the SMS results, we examine how Software Engineering may help to develop secure Telehealth systems. From over a thousand studies, we distinguished and classified 41 primary studies. Results show that (i) four security classifications (attacks, vulnerabilities, weaknesses, and threats) concentrate the most reported security issues; (ii) three security strategies (detect attacks, stop or mitigate attacks and react to attacks) characterize security issues, and (iii) the most relevant research themes are related to insecure data transmission and privacy. The SMS's findings suggest that software design, requirements, and models are key areas to develop secure Telehealth systems.}
}

@article{rayyan-727967865,
  title={Development of critical embedded systems using model-driven and product lines techniques: A systematic review},
  year={2014},
  pages={74-83},
  author={Gadelha Queiroz, Paulo Gabriel and Vaccare Braga, Rosana Teresinha},
  keywords={Systematics, Embedded software, Systematic Review, Embedded systems, Libraries, Computer architecture, Computational modeling, Real-time systems, Model-driven Engineering, Product Lines Engineering, Safety-Critical Embedded Systems},
  abstract={Several methodologies have been proposed in the last decades to improve the quality of Safety-Critical Embedded Systems (SCES) and, at the same time, keep costs and schedule compatible with project plans. In particular, approaches such as Product Line Engineering (PLE) and Model-Driven Engineering (MDE) offer an interesting solution to reduce development complexity and time to market due to their synergy and common goals. However, the current state of how MDE and PLE can be combined to enhance productivity in the domain of SCES is not clear yet. This paper presents a systematic literature review, with the purpose of obtaining the state of the art of the aproaches, methods and methodologies whose goal is the combination of PLE and MDE for the development of SCES, and to verify the existence of empirical studies that demonstrate the application of these techniques in this type of development. We drew the following conclusions from the review results: (1) The number of studies using PLE with MDE to build SCES is relatively small, but has increased gradually in recent years. (2) The approaches diverge about what is needed to build Model-driven Product Lines. (3) Most of the approaches do not consider to differentiate between hardware and software variabilities. (4) Most of the studies propose the use of UML and feature diagrams. (5) The studies present case studies implemented in different tools and most of them are free. (6) The approaches do not cover the entire development lifecycle.}
}

@article{rayyan-727967866,
  title={Comparing local and global software effort estimation models – reflections on a systematic review},
  year={2007},
  pages={401-409},
  author={MacDonell, Stephen G and Shepperd, Martin J},
  keywords={systematic review, Software engineering, Software measurement, Protocols, Mathematics, Mathematical model, prediction, Costs, Predictive models, D.2.9.b Cost estimation, empirical analysis., Management information systems, project effort, Reflection, Technology management, Software},
  abstract={The availability of multi-organisation data sets has made it possible for individual organisations to build and apply management models, even if they do not have data of their own. In the absence of any data this may be a sensible option, driven by necessity. However, if both cross-company (or global) and within-company (or local) data are available, which should be used in preference? Several research papers have addressed this question but without any apparent convergence of results. We conduct a systematic review of empirical studies comparing global and local effort prediction systems. We located 10 relevant studies: 3 supported global models, 2 were equivocal and 5 supported local models. The studies do not have converging results. A contributing factor is that they have utilised different local and global data sets and different experimental designs thus there is substantial heterogeneity. We identify the need for common response variables and for common experimental and reporting protocols.}
}

@article{rayyan-727967867,
  title={The future of empirical methods in software engineering research},
  year={2007},
  pages={358-378},
  author={Sjoberg, Dag I K and Dyba, Tore and Jorgensen, Magne},
  keywords={Software engineering, Project management, Computer science, Software systems, Informatics, Software development management, Computer industry, Computer Society, Information science, Laboratories, Software},
  abstract={We present the vision that for all fields of software engineering (SE), empirical research methods should enable the development of scientific knowledge about how useful different SE technologies are for different kinds of actors, performing different kinds of activities, on different kinds of systems. It is part of the vision that such scientific knowledge will guide the development of new SE technology and is a major input to important SE decisions in industry. Major challenges to the pursuit of this vision are: more SE research should be based on the use of empirical methods; the quality, including relevance, of the studies using such methods should be increased; there should be more and better synthesis of empirical evidence; and more theories should be built and tested. Means to meet these challenges include (1) increased competence regarding how to apply and combine alternative empirical methods, (2) tighter links between academia and industry, (3) the development of common research agendas with a focus on empirical methods, and (4) more resources for empirical research.}
}

@article{rayyan-727967868,
  title={Automated testing of android apps: A systematic literature review},
  year={2019},
  journal={IEEE Transactions on Reliability},
  issn={1558-1721},
  volume={68},
  number={1},
  pages={45-66},
  author={Kong, Pingfan and Li, Li and Gao, Jun and Liu, Kui and Bissyandé, Tegawendé F and Klein, Jacques},
  keywords={Systematics, survey, Bibliographies, Ecosystems, Testing, literature review, Android, Java, Androids, Humanoid robots, automated testing, Methyltestosterone},
  abstract={Automated testing of Android apps is essential for app users, app developers, and market maintainer communities alike. Given the widespread adoption of Android and the specificities of its development model, the literature has proposed various testing approaches for ensuring that not only functional requirements but also nonfunctional requirements are satisfied. In this paper, we aim at providing a clear overview of the state-of-the-art works around the topic of Android app testing, in an attempt to highlight the main trends, pinpoint the main methodologies applied, and enumerate the challenges faced by the Android testing approaches as well as the directions where the community effort is still needed. To this end, we conduct a systematic literature review during which we eventually identified 103 relevant research papers published in leading conferences and journals until 2016. Our thorough examination of the relevant literature has led to several findings and highlighted the challenges that Android testing researchers should strive to address in the future. After that, we further propose a few concrete research directions where testing approaches are needed to solve recurrent issues in app updates, continuous increases of app sizes, as well as the Android ecosystem fragmentation.}
}

@article{rayyan-727967869,
  title={Modeling the experimental software engineering process},
  year={2007},
  pages={77-90},
  author={Goulao, Miguel and Abreu, Fernando Brito e},
  keywords={Software engineering, Programming, Computer science, Best practices, Physics, Proposals, Costs, Communications technology, Software standards, Communication standards, Software},
  abstract={Reviews on software engineering literature have shown an insufficient experimental validation of claims, when compared to the standard practice in other well-established sciences. Poor validation of software engineering claims increases the risks of introducing changes in the software process of an organization, as the potential benefits assessment is based on hype, rather than on facts. The community lacks highly disseminated experimental best practices. We contribute with a model of the experimental software engineering process that is aligned with recent proposals for best practices in experimental data dissemination. The model can be used in the definition of software engineering experiments and in comparisons among experimental results.}
}

@article{rayyan-727967870,
  title={Performance analysis for object-oriented software: A systematic mapping},
  year={2015},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={41},
  number={7},
  pages={691-710},
  author={Maplesden, David and Tempero, Ewan and Hosking, John and Grundy, John C},
  keywords={Systematics, survey, Software performance, Systematic review, Databases, performance, Performance analysis, Mathematical model, Runtime, object-oriented, Software},
  abstract={Performance is a crucial attribute for most software, making performance analysis an important software engineering task. The difficulty is that modern applications are challenging to analyse for performance. Many profiling techniques used in real-world software development struggle to provide useful results when applied to large-scale object-oriented applications. There is a substantial body of research into software performance generally but currently there exists no survey of this research that would help identify approaches useful for object-oriented software. To provide such a review we performed a systematic mapping study of empirical performance analysis approaches that are applicable to object-oriented software. Using keyword searches against leading software engineering research databases and manual searches of relevant venues we identified over 5,000 related articles published since January 2000. From these we systematically selected 253 applicable articles and categorised them according to ten facets that capture the intent, implementation and evaluation of the approaches. Our mapping study results allow us to highlight the main contributions of the existing literature and identify areas where there are interesting opportunities. We also find that, despite the research including approaches specifically aimed at object-oriented software, there are significant challenges in providing actionable feedback on the performance of large-scale object-oriented applications.}
}

@article{rayyan-727967871,
  title={Systematic mapping on quality in web application testing},
  year={2019},
  pages={1-5},
  author={Akbulut, Sinan and Gebreyesus, Yasmin Tesfaldet and Mishra, Alok and Yazici, Ali},
  keywords={Software, Software engineering, Systematics, Tools, systematic mapping, Testing, Conferences, web application, Market research, quality, testing},
  abstract={Similar to all other fields of study, assuring the quality of web applications is very important. With the aim of increasing web applications quality in testing, our task in this paper is to reflect on the results of the literature searches made in this area. The literature survey carried out in this study searches for studies in the area of quality in application testing and retrieves 1972 papers, published within the timeframe of 2006 and 2016. The publications are systematically identified, analyzed, and classified and an overview of the trends in this specialized field is presented. Systematic mapping is applied to review and structure the body of knowledge related to quality in web application testing. To conduct our study, we selected a set of 27 papers published in the area of quality in web application testing between 2006 and 2016. The results of this work can help other researchers to obtain an overview of the existing web application testing quality trends and approaches as well as to identify areas in the field that require more attention from research point of view.}
}

@article{rayyan-727967872,
  title={Users' involvement in requirements engineering and system success},
  year={2013},
  pages={24-31},
  author={Bano, Muneera and Zowghi, Didar},
  keywords={Software, systematic literature review, Systematics, Guidelines, Uncertainty, Organizations, Psychology, Cultural differences, requirements engineering, software development lifecycle, users' involvement},
  abstract={Involving users in software development in general, and in Requirements Engineering (RE) in particular, has been considered for over three decades. It is axiomatically believed to contribute significantly to a successful system. However, not much attention has been paid to ascertain in which phases of software development life cycle involvement or participation of users is most beneficial. In this paper we present an investigation into the concept of users' involvement during RE activities and explore its relationship with system success. We have conducted a systematic literature review (SLR) using guidelines of Evidence Based Software Engineering. Our SLR identified 87 empirical studies from the period of 1980 to 2012. Only 13 studies focused specifically on investigating users' involvement in RE and 9 of these confirmed benefits of involving users in requirements analysis and 4 remain inconclusive. Effective involvement of users in RE may reduce the need for their more active involvement in the rest of software development. This paper also offers a checklist we have created from the identified factors of all 87 empirical studies that should be utilised for effective users' involvement in RE.}
}

@article{rayyan-727967873,
  title={Barriers to software outsourcing partnership formation: An exploratory analysis},
  year={2019},
  journal={IEEE Access},
  issn={2169-3536},
  volume={7},
  pages={164556-164594},
  author={Ali, Sikandar and Ullah, Niamat and Abrar, Muhammad Faisal and Majeed, Muhammad Faran and Umar, Muhammad Atif and Huang, Jiwei},
  keywords={Software, Systematic literature review, Outsourcing, Companies, Contracts, client-vendor relationship, Continents, empirical survey, software outsourcing partnership},
  abstract={Software Outsourcing Partnership (SOP) is a type of cooperative client-vendor relationship. SOP is an emerging strategy and is different from ordinary software development outsourcing (SDO). Usually, a fruitful outsourcing association might be converted to an outsourcing partnership. Conversely, SOP is not a risk-free business, numerous barriers associated with SOP. The overarching target of this exploratory paper is to find and analyze a list of barriers that are considered obstacles for vendors in the conversion of their surviving contractual outsourcing relationship to a partnership. Firstly, twenty-six barriers to SOP formation were identified through systematic literature review (SLR) from a sample of 106 papers and then an empirical survey was conducted with fifty experts to analyze the significance and applicability of these barriers in the SOP context. The identified barriers were further analyzed based on five variables such as decades, company size, continents, location of analysis, and perspective of the study. Ten barriers were considered as critical barriers (CBs) via SLR. Industrial experts indicate they extremely agree with five CBs. Eight CBs were equally reported on all continents. We found ten CBs common in all types of organizations. Further, twelve CBs were shared in both decades while ten CBs were found common in both academia and industry. Furthermore, four CBs were specific to clients; five were specific to vendors while ten were common to both. The association of various barriers with SOP formation is found statistically significant for twenty-five barriers with effect size (0.41 ¡; Ø ¡; 0.90, p ¡; 0.05). Stakeholders in SOP should address all the listed barriers especially the critical ones to attain a partner position.}
}

@article{rayyan-727967874,
  title={A systematic review and comparison of security ontologies},
  year={2008},
  pages={813-820},
  author={Blanco, Carlos and Lasheras, Joaquin and Valencia-García, Rafael and Fernández-Medina, Eduardo and Toval, Ambrosio and Piattini, Mario},
  keywords={systematic review, comparison, Software engineering, Data mining, Ontologies, Privacy, ontology, Information security, security, Proposals, Software libraries, Abstracts, Availability, Data security},
  abstract={The use of ontologies for representing knowledge provides us with organization, communication and reusability. Information security is a serious requirement which must be carefully considered. Concepts and relations managed by any scientific community need to be formally defined and ontological engineering supports their definition. In this paper, the method of systematic review is applied with the purpose of identifying, extracting and analyzing the main proposals for security ontologies. The main identified proposals are compared using a formal framework and we conclude by stating their early state of development and the need of additional research efforts.}
}

@article{rayyan-727967875,
  title={An environment to support large scale experimentation in software engineering},
  year={2008},
  pages={193-202},
  author={Travassos, Guilherme H and dos Santos, Paulo Sérgio Medeiros and Mian, Paula Gomes and Neto, Paula Gomes Mian and Biolchini, Jorge},
  keywords={Software engineering, Software measurement, Knowledge management, Software quality, Large-scale systems, Software systems, Knowledge engineering, Engineering management, Area measurement, case tools, Distributed computing, e-science, experimental software engineering, experimentation environments, Software},
  abstract={Experimental studies have been used as a mechanism to acquire knowledge through a scientific approach based on measurement of phenomena in different areas. However it is hard to run such studies when they require models (simulation), produce amount of information, and explore science in scale. In this case, a computerized infrastructure is necessary and constitutes a complex system to be built. In this paper we discuss an experimentation environment that has being built to support large scale experimentation and scientific knowledge management in software engineering.}
}

@article{rayyan-727967876,
  title={The impact of human factors on agile projects},
  year={2015},
  pages={87-91},
  author={Chagas, Aline and Santos, Melquizedequi and Santana, Célio and Vasconcelos, Alexandre},
  keywords={Software, Software engineering, Systematics, Collaboration, Decision making, Agile software development, Human factors, Agile Software Development, Human Factors, Systematic Literature Review., Humanities, Humanism, Humans},
  abstract={This research aims to provide more evidences about the impact of human factors in agile software projects. In this light we have conducted a systematic literature review (SLR) to investigate which human factors impact agile projects and conducted a survey in software companies in order to verify their perceptions. Considering the SLR, we found 48 resulting studies where most cited human factors were: Communication (23 papers), Collaboration (6 papers) and Trust (8 papers). The survey was answered for 186 companies that consider communication the most important factor. So we conclude that Communication, Trust and Collaboration are important factors in projects using agile methods but others factors need to be more investigated due the perception of its value to the industry.}
}

@article{rayyan-727967877,
  title={AAL platforms challenges in IoT era: A tertiary study},
  year={2018},
  pages={106-113},
  author={Duarte, Paulo A S and Barreto, Felipe M and Aguilar, Paulo A C and Boudy, Jérôme and Andrade, Rossana M C and Viana, Windson},
  keywords={Systematic literature review, Systematics, Internet of Things, Statistics, Protocols, Medical services, AAL, AAL platform, Ambient Assisted Living, Population aging, Senior citizens},
  abstract={Ambient Assisted Living (AAL) has become an emerging and increasingly essential area of research, given the prospects of increasing the elderly world population. The establishment of the Internet of Things (IoT) paradigm provided several advances in the field of healthcare for older adults. However, the development of AAL platforms focused on older adults is still presenting several challenges. Many of these challenges are similar to those existing for IoT architectures (i.e., interoperability) and others are derived from the healthcare domain (e.g., patients and caregivers as end-users). In this paper, we present a survey of secondary studies on AAL platforms published between 2013 to 2017. Our focus resided in investigating the state-of-the-art of the adaptive AAL platforms for older adults assistance. We have established a set of research questions with which we seek to gain an overview of AAL platforms, development patterns, and the main challenges in this domain. We describe our search protocol, the execution, the selection of the papers, and the results obtained. Among the conclusions of our overview, we can mention the absence of a reference architecture for AAL platforms and some open questions (e.g., data privacy, usability, accessibility) present in this domain.}
}

@article{rayyan-727967878,
  title={Applying systematic reviews to diverse study types: An experience report},
  year={2007},
  pages={225-234},
  author={Dyba, Tore and Dingsoyr, Torgeir and Hanssen, Geir K},
  keywords={Software engineering, Software measurement, Guidelines, Decision making, Software maintenance, Databases, Programming, Medical services, Strontium, Appraisal},
  abstract={Systematic reviews are one of the key building blocks of evidence-based software engineering. Current guidelines for such reviews are, for a large part, based on standard meta-analytic techniques. However, such quantitative techniques have only limited applicability to software engineering research. In this paper, therefore, we describe our experience with an approach to combine diverse study types in a systematic review of empirical research of agile software development.}
}

@article{rayyan-727967879,
  title={Reporting guidelines for controlled experiments in software engineering},
  year={2005},
  pages={10-pp.–},
  author={Jedlitschka, A and Pfahl, D},
  keywords={Software engineering, Data mining, Guidelines, Programming, Taxonomy, Psychology, Proposals, Costs, Feedback, Standards publication, Software},
  abstract={One major problem for integrating study results into a common body of knowledge is the heterogeneity of reporting styles: (1) it is difficult to locate relevant information and (2) important information is often missing. Reporting guidelines are expected to support a systematic, standardized presentation of empirical research, thus improving reporting in order to support readers in (1) finding the information they are looking for, (2) understanding how an experiment is conducted, and (3) assessing the validity of its results. The objective of this paper is to survey the most prominent published proposals for reporting guidelines, and to derive a unified standard that which can serve as a starting point for further discussion. We provide detailed guidance on the expected content of the sections and subsections for reporting a specific type of empirical studies, i.e., controlled experiments. Before the guidelines can be evaluated, feedback from the research community is required. For this purpose, we propose to adapt guideline development processes from other disciplines.}
}

@article{rayyan-727967880,
  title={Software security in open source development: A systematic literature review},
  year={2017},
  pages={364-373},
  author={Wen, Shao-Fang},
  keywords={Systematics, Security, Knowledge management, Open source software, Buildings, Focusing, Software},
  abstract={Despite the security community's emphasis on the importance of building secure open source software (OSS), the number of new vulnerabilities found in OSS is increasing. In addition, software security is about the people that develop and use those applications and how their vulnerable behaviors can lead to exploitation. This leads to a need for reiteration of software security studies for OSS developments to understand the existing security practices and the security weakness among them. In this paper, a systematic review method with a sociotechnical analysis approach is applied to identify, extract and analyze the security studies conducted in the context of open source development. The findings include: (1) System verification is the most cited security area in OSS research; (2) The socio-technical perspective has not gained much attention in this research area; and (3) No research has been conducted focusing on the aspects of security knowledge management in OSS development.}
}

@article{rayyan-727967881,
  title={Open source adoption Factors—A systematic literature review},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={94594-94609},
  author={Sánchez, Víctor Rea and Ayuso, Pablo Neira and Galindo, José A and Benavides, David},
  keywords={Software, Systematics, Data mining, Guidelines, Databases, Organizations, Economics, Free software adoption, libre software adoption, open source adoption},
  abstract={Nowadays, Free/Libre/OpenSource Software (FLOSS) is becoming a strategic option for many organizations in the public and the private sector. The lack of well defined guidelines for IT managers may jeopardize the FLOSS adoption process. FLOSS adoption procedures are developed ad-hoc in every organization, hence, leading to potential wheel reinvention situations. Identifying factors that influence and determine adoption is crucial. In this article, we survey existing literature through systematic review methodologies to make visible the technical, organizational and economic factors that must be evaluated in the adoption process. We also provide hints for researchers on publications and the type of research that already covered this topic in the past. We studied almost 500 papers from which we selected a final set of 54 primary studies directly related to FLOSS adoption. We found twenty-two different adoption factors categorized as technical (nine), organizational (nine) and economic (four). This article aims to provide the basic building blocks to step into the creation of a guide for the FLOSS adoption. All the data we used in this study is available at this online repository: https://github.com/jagalindo/rea.victor.19-foss and doi: https://doi.org/10.5281/zenodo.2632543.}
}

@article{rayyan-727967882,
  title={What makes a great manager of software engineers?},
  year={2019},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={45},
  number={1},
  pages={87-106},
  author={Kalliamvakou, Eirini and Bird, Christian and Zimmermann, Thomas and Begel, Andrew and DeLine, Robert and German, Daniel M},
  keywords={Software, Software engineering, Productivity, Interviews, Organizations, Psychology, empirical studies, Knowledge engineering, software companies, software engineering management},
  abstract={Having great managers is as critical to success as having a good team or organization. In general, a great manager is seen as fuelling the team they manage, enabling it to use its full potential. Though software engineering research studies factors that may affect the performance and productivity of software engineers and teams (like tools and skills), it has overlooked the software engineering manager. The software industry's growth and change in the last decades is creating a need for a domain-specific view of management. On the one hand, experts are questioning how the abundant work in management applies to software engineering. On the other hand, practitioners are looking to researchers for evidence-based guidance on how to manage software teams. We conducted a mixed methods empirical study of software engineering management at Microsoft to investigate what manager attributes developers and engineering managers perceive important and why. We present a conceptual framework of manager attributes, and find that technical skills are not the sign of greatness for an engineering manager. Through statistical analysis we identify how engineers and managers relate in their views, and how software engineering differs from other knowledge work groups in its perceptions about what makes great managers. We present strategies for putting the attributes to use, discuss implications for research and practice, and offer avenues for further work.}
}

@article{rayyan-727967883,
  title={A systematic review of empirical studies on learning analytics dashboards: A self-regulated learning perspective},
  year={2020},
  journal={IEEE Transactions on Learning Technologies},
  issn={1939-1382},
  volume={13},
  number={2},
  pages={226-245},
  author={Matcha, Wannisa and Uzir, Nora'ayu Ahmad and Gašević, Dragan and Pardo, Abelardo},
  keywords={Systematics, Bibliographies, Recommender systems, Australia, empirical research, Informatics, Analytical models, Computational modeling, Dashboards, feedback, information visualization, learning analytics, self-regulated learning, Learning},
  abstract={This paper presents a systematic literature review of learning analytics dashboards (LADs) research that reports empirical findings to assess the impact on learning and teaching. Several previous literature reviews identified self-regulated learning as a primary focus of LADs. However, there has been much less understanding how learning analytics are grounded in the literature on self-regulated learning and how self-regulated learning is supported. To address this limitation, this review analyzed the existing empirical studies on LADs based on the well-known model of self-regulated learning proposed by Winne and Hadwin. The results show that existing LADs are rarely grounded in learning theory, cannot be suggested to support metacognition, do not offer any information about effective learning tactics and strategies, and have significant limitations in how their evaluation is conducted and reported. Based on the findings of the study and through the synthesis of the literature, the paper proposes that future research and development should not make any a priori design decisions about representation of data and analytic results in learning analytics systems such as LADs. To formalize this proposal, the paper defines the model for user-centered learning analytics systems (MULAS). The MULAS consists of the four dimensions that are cyclically and recursively interconnected including: theory, design, feedback, and evaluation.}
}

@article{rayyan-727967884,
  title={A systematic review of software maintainability prediction and metrics},
  year={2009},
  pages={367-377},
  author={Riaz, Mehwish and Mendes, Emilia and Tempero, Ewan},
  keywords={Software engineering, Software measurement, Software performance, Software quality, Software maintenance, Software systems, Strontium, Costs, Predictive models, State estimation, Metronidazole, Software},
  abstract={This paper presents the results of a systematic review conducted to collect evidence on software maintainability prediction and metrics. The study was targeted at the software quality attribute of maintainability as opposed to the process of software maintenance. The evidence was gathered from the selected studies against a set of meaningful and focused questions. 710 studies were initially retrieved; however of these only 15 studies were selected; their quality was assessed; data extraction was performed; and data was synthesized against the research questions. Our results suggest that there is little evidence on the effectiveness of software maintainability prediction techniques and models.}
}

@article{rayyan-727967885,
  title={A study on Risk Management for software engineering},
  year={2012},
  pages={47-51},
  author={Lobato, Luanna Lopes and Silveira Neto, Paulo Anselmo da Mota and Machado, Ivan do Carmo},
  keywords={Software Product Lines, Narrative Synthesis, Risk Management, Single System Development, Software},
  abstract={Explicit Risk Management (RM) in Software Product Lines Engineering (SPL) is considered an open question, as posed in literature, and confirmed by industrial practices, unlike Single System Development (SSD), which contains a large set of evidence. The goal of this research is to synthesize the available evidence gathered in previous research, in a form of two scoping studies, which considered RM in SPL and SSD, using the narrative synthesis method. Through the synthesis we could identify common risks to both development paradigm, as well as RM activities and practices most commonly used to apply RM in projects. In addition, was observed that RM to SPL is still an open question if compared to SSD.}
}

@article{rayyan-727967886,
  title={Crowdsourcing in systematic reviews: A systematic mapping and survey},
  year={2020},
  pages={404-412},
  author={Felizardo, Katia R and de Souza, Érica F and Lopes, Rafael and Moro, Geovanne J and Vijaykumar, Nandamudi L},
  keywords={Software engineering, Systematics, Search problems, Crowdsourcing, Task analysis, Systematic Review, Outsourcing, SM, SR, Systematic Mapping, World Wide Web},
  abstract={Context: Systematic reviews (SRs) have been adopted in the Software Engineering (SE) field for more than a decade to provide synthesis of evidence on various topics. However, the process in conducting an SR remains laborious-intensive and expensive, specially in terms of hours that SR researchers dedicate. It is worth exploring approaches to conduct SRs at lower costs (quicker, using less resources - time of researchers). One such approach is crowdsourcing, since conducting SRs activities among a large number of researchers is a promising alternative to reduce costs associated to SR conduction. Goal: The main goal of this study is to identify and summarize the body of knowledge on crowdsourcing to support the conduction of SRs in SE. Method: Two empirical research methods were used. Initially, we conducted a Systematic Mapping to identify the available and relevant studies on crowdsourcing in SRs in SE. Secondly, a survey was performed with 39 SE researchers aiming to identify their perception related to the value of performing SRs collaboratively. Results: Our results show that how to speed up the SR process; reduce bias through broad participation; and expand team expertise were most potential benefits linked to the use of crowdsourcing in SR. The main challenges were associated with quality control to ensure the quality of results. Conclusions: In spite of the challenges, we believe that crowdsourcing could be successfully employed in SR context. More empirical research is needed on how to use crowdsourcing to support SR conduction in SE and how to minimize the identified challenges.}
}

@article{rayyan-727967887,
  title={Speech recognition using deep neural networks: A systematic review},
  year={2019},
  journal={IEEE Access},
  issn={2169-3536},
  volume={7},
  pages={19143-19165},
  author={Nassif, Ali Bou and Shahin, Ismail and Attili, Imtinan and Azzeh, Mohammad and Shaalan, Khaled},
  keywords={systematic review, Neural networks, Deep learning, Computer architecture, Feature extraction, Acoustics, deep neural network, Hidden Markov models, Speech recognition, Nerve Net, Neural Networks (Computer), Speech},
  abstract={Over the past decades, a tremendous amount of research has been done on the use of machine learning for speech processing applications, especially speech recognition. However, in the past few years, research has focused on utilizing deep learning for speech-related applications. This new area of machine learning has yielded far better results when compared to others in a variety of applications including speech, and thus became a very attractive area of research. This paper provides a thorough examination of the different studies that have been conducted since 2006, when deep learning first arose as a new area of machine learning, for speech applications. A thorough statistical analysis is provided in this review which was conducted by extracting specific information from 174 papers published between the years 2006 and 2018. The results provided in this paper shed light on the trends of research in this area as well as bring focus to new research topics.}
}

@article{rayyan-727967888,
  title={A systematic review protocol on shared transportation},
  year={2016},
  pages={1-4},
  author={Silva, Elis and Rossetti, Rosaldo J F and Kokkinogenis, Zafeiris and Pinto, José},
  keywords={Artificial intelligence, Systematics, Systematic Review, Methodology, Computer science, Protocols, Mobility, Protocol, Public transportation, Shared Transportation, Signal to noise ratio},
  abstract={Recently, many studies in the literature have been intensified in the scope of mobility services, presenting new technological solutions that can minimize problems, such as traffic congestion and the emission of greenhouse gases resulting from the population growth in large cities. Some services, namely shared services, are mainly dedicated to trying to encourage people to use transport that can be shared. In this paper, we propose a systematic literature review protocol oriented to shared transportation, so as to support bibliographical reviews by researchers on this domain. From the proposed systematic review methodology, we present preliminary results of a survey conducted on shared services.}
}

@article{rayyan-727967889,
  title={The issues of solving staffing and scheduling problems in software development projects},
  year={2014},
  pages={1-10},
  author={Peixoto, Daniela C C and Mateus, Geraldo R and Resende, Rodolfo F},
  keywords={Software, Software engineering, Literature review, Project management, Optimization, Scheduling, Resource allocation, Organizations, Search-Based Software Engineering, Resource management, Staffing},
  abstract={Search-Based Software Engineering (SBSE) applies search-based optimization techniques in order to solve complex Software Engineering problems. In the recent years there has been a dramatic increase in the number of SBSE applications in areas such as Software Test, Requirements Engineering, and Project Planning. Our focus is on the analysis of the literature in Project Planning, specifically the researches conducted in software project scheduling and resource allocation. SBSE project scheduling and resource allocation solutions basically use optimization algorithms. Considering the results of a previous Systematic Literature Review, in this work, we analyze the issues of adopting these optimization algorithms in what is considered typical settings found in software development organizations. We found few evidence signaling that the expectations of software development organizations are being attended.}
}

@article{rayyan-727967890,
  title={A mapping study of the definitions used for Service Oriented Architecture},
  year={2012},
  pages={57-61},
  author={Anjum, Maria and Budgen, David},
  abstract={Context: Service Oriented Architecture (SOA) has formed the basis for many workshops, conferences and books. However these do not always seem to have used a consistent set of concepts. Objectives: To identify the key characteristics of an SOA as identified from the literature. Method: We report on our experience of undertaking a mapping study (a form of systematic literature review) to identify the different definitions of SOA being used in the literature. Results: We found 921 candidate studies, of which only 98 were explicit about the definitions of SOA that were used. Conclusion: We have demonstrated that a mapping study can be usefully employed to identify inconsistencies and help to create a model of SOA.}
}

@article{rayyan-727967891,
  title={Factors for communication technologies selection within virtual software teams},
  year={2015},
  pages={76-80},
  author={Gheni, Ali Yahya and Jabar, Marzanah A and Jusoh, Yusmadi Yah and Mohd Ali, Norhayati and Abdullah, Rusli Hj.},
  keywords={Software, Security, Organizations, communication, performance, Electronic mail, Virtual groups, Virtual teams, Fibrinogen},
  abstract={Virtual teams are established to during software development environment. Virtual team members are gradually engaged in globalized business environments across space, time and organizational boundaries with links ensured by information and communication technologies. A virtual team relies on communication, collaboration, and information exchange is the most important criteria in virtual teams operations and their efficiency directly influences team performance. The aim of this paper is to identify the factors of selecting and using of technologies within virtual teams communication to improve virtual team communication to enhance the virtual team performance. This researcher uses systematic literature review by Kitchenham to review existing literature on the issues and implementation of virtual team members. In this review 22 papers were analyzed to answer the research questions.}
}

@article{rayyan-727967892,
  title={Convergence analysis of ISO/IEC 12207 and CMMI-DEV: A systematic literature review},
  year={2016},
  pages={1-8},
  author={Crisóstomo, Javier and Flores, Luis and Melendez, Karin and Dávila, Abraham},
  keywords={Software, Context modeling, CMM, CMMI, Libraries, ISO Standards, IEC Standards, Benchmark testing, CMMI-DEV, Convergence, ISO/IEC 12207, model harmonization},
  abstract={The organizations and people are demanding more and better software products and services, which implies adequate processes for its development. In the context of the software industry, there are two models, the CMMI-DEV and ISO/IEC 12207 that are influencing it. Though, they are evolving separately, recurrently they have been compared to determine its coverage (in both directions). In this study is analyzed the results of those comparisons (partials and completed) to determine if the models ISO/IEC 12207 and CMMI-DEV converge at processes level. This study identified eight articles where the comparison is carried out between ISO/IEC 12207 and CMMI-DEV. The results show that technique most used is the mapping comparisons between the models and according to the analyzed studies is not possible to determine whether there is convergence in the time. However, we found some items and criterions for use in comparisons.}
}

@article{rayyan-727967893,
  title={Consolidating evidence based studies in software cost/effort estimation — A tertiary study},
  year={2017},
  pages={833-838},
  author={Pillai, Sreekumar P and Madhukumar, S D and Radharamanan, T},
  keywords={Software, Systematics, Systematic Literature Review, Bibliographies, Measurement, Estimation, Software Effort Estimation, Libraries, Industries, Effort Prediction, Software Cost Estimation, Tertiary Survey},
  abstract={Software Effort Estimation is key to the success of any project since all downstream activities such as planning, budgeting, developing and Monitoring cannot be executed without clarity on the scope of the activity that needs to be performed. This is a tertiary study that follows the Systematic Literature Review (SLR) process as put forth by Kitchenham in her seminal paper, based on five criteria: estimation technique, estimation accuracy, type of dataset and independent variables used in empirical research on effort estimation. Our study covering 820 Primary Studies through 14 SLRs, shows that Software Effort Estimation studies focus more on statistical techniques and Machine Learning is taking precedence in comparison to the others; whereas Expert Judgement is preferred by the industry due to its intuitiveness. There is a need for models that are simple to understand and global, due to the distributed nature of software development. The studies are inconclusive about the accuracy benefits of using a within company dataset vs.external datasets. Machine learning techniques such FL and GA in combination with Analogy methods generate more accurate estimates. There is increasing consensus on the use of Mean Magnitude of Relative Error (MMRE), Median Magnitude of Relative Error (MdMRE) and Prediction Pred (25%) as the accuracy metric. 78% of the Primary Studies reported accuracy using MMRE. The best MMRE reported is in the range of 7 to 75. ISBSG (International Software Benchmarking Standards Group) and Desharnais datasets with 27% and 17% usage respectively are the most widely used datasets in empirical studies on effort estimation. Fewer than 20 independent variables account for more than 90% impact of variables in empirical analysis on Software effort estimation.}
}

@article{rayyan-727967894,
  title={Organising evidence to support software engineering practice},
  year={2003},
  pages={25-32},
  author={Budgen, D and Boegh, J and Mohan, A},
  keywords={Software engineering, Software quality, Technology transfer, Computer science, History, Conferences, Books, Acoustics, Acoustical engineering, Silver, Software},
  abstract={Evidence (in different forms) is widely employed to establish and refine ideas about what constitutes good practice in many domains of science and engineering. However, software engineering (and computing in general) has so far been a notable exception to this, and our paper sets out to explore some of the reasons why this should be so, and how the situation might be changed in the future. We examine the ways in which some other domains collect and use evidence, and then examine both 'commercial' and 'open source' development practices to see how these currently make use of previous experience and evidence. Finally, we consider the possible role of systematic review groups in strengthening the way that evidence could be employed in future software engineering practices.}
}

@article{rayyan-727967895,
  title={Systematic literature review on penetration testing for mobile cloud computing applications},
  year={2019},
  journal={IEEE Access},
  issn={2169-3536},
  volume={7},
  pages={173524-173540},
  author={Al-Ahmad, Ahmad Salah and Kahtan, Hasan and Hujainah, Fadhl and Jalab, Hamid A},
  keywords={Software testing, Cloud computing, Mobile cloud computing, Complexity theory, Analytical models, penetration testing, Penetration testing, cloud testing, mobile testing, offloading},
  abstract={Mobile cloud computing (MCC) enables mobile devices to exploit seamless cloud services via offloading, and has numerous advantages and increased security and complexity. Penetration testing of mobile applications has become more complex and expensive due to several parameters, such as the platform, device heterogeneity, context event types, and offloading. Numerous studies have been published in the MCC domain, whereas few studies have addressed the common issues and challenges of MCC testing. However, current studies do not address MCC and penetration testing. Therefore, revisiting MCC and penetration testing domains is essential to overcoming the inherent complexity and reducing costs. Motivated by the importance of revisiting these domains, this paper pursues two objectives: to provide a comprehensive systematic literature review (SLR) of the MCC, security and penetration testing domains and to establish the requirements for penetration testing of MCC applications. This paper has systematically reviewed previous penetration testing models and techniques based on the requirements in Kitchenham's SLR guidelines. The SLR outcome has indicated the following deficiencies: the offloading parameter is disregarded; studies that address mobile, cloud, and web vulnerabilities are lacking; and a MCC application penetration testing model has not been addressed by current studies. In particular, offloading and mobile state management are two new and vital requirements that have not been addressed to reveal hidden security vulnerabilities, facilitate mutual trust, and enable developers to build more secure MCC applications. Beneficial review results that can contribute to future research are presented.}
}

@article{rayyan-727967896,
  title={How to evaluate solutions in pareto-based search-based software engineering? A critical review and methodological guidance},
  year={2020},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  pages={1},
  author={Li, Miqing and Chen, Tao and Yao, Xin},
  keywords={Software engineering, Systematics, Search-based software engineering, Computer science, Indexes, Licenses, multi-objective optimization, Pareto optimization, preferences, quality evaluation, quality indicators, Software},
  abstract={With modern requirements, there is an increasing tendency of considering multiple objectives/criteria simultaneously in many Software Engineering (SE) scenarios. Such a multi-objective optimization scenario comes with an important issue - how to evaluate the outcome of optimization algorithms, which typically is a set of incomparable solutions (i.e., being Pareto nondominated to each other). This issue can be challenging for the SE community, particularly for practitioners of Search-Based SE (SBSE). On one hand, multi-objective optimization could still be relatively new to SE/SBSE researchers, who may not be able to identify the right evaluation methods for their problems. On the other hand, simply following the evaluation methods for general multi-objective optimization problems may not be appropriate for specific SBSE problems, especially when the problem nature or decision maker's preferences are explicitly/implicitly known. This has been well echoed in the literature by various inappropriate/inadequate selection and inaccurate/misleading use of evaluation methods. In this paper, we first carry out a systematic and critical review of quality evaluation for multi-objective optimization in SBSE. We survey 717 papers published between 2009 and 2019 from 36 venues in seven repositories, and select 95 prominent studies, through which we identify five important but overlooked issues in the area. We then conduct an in-depth analysis of quality evaluation indicators/methods and general situations in SBSE, which, together with the identified issues, enables us to codify a methodological guidance for selecting and using evaluation methods in different SBSE scenarios.}
}

@article{rayyan-727967897,
  title={Eye-tracking metrics in software engineering},
  year={2015},
  pages={96-103},
  author={Sharafi, Zohreh and Shaffer, Timothy and Sharif, Bonita and Guéhéneuc, Yann-Gaël},
  keywords={Software, Software engineering, Visualization, Measurement, Unified modeling language, metrics, Mathematical model, eye-tracking, Gaze tracking, Metronidazole},
  abstract={Eye-tracking studies are getting more prevalent in software engineering. Researchers often use different metrics when publishing their results in eye-tracking studies. Even when the same metrics are used, they are given different names, causing difficulties in comparing studies. To encourage replications and facilitate advancing the state of the art, it is important that the metrics used by researchers be clearly and consistently defined in the literature. There is therefore a need for a survey of eye-tracking metrics to support the (future) goal of standardizing eye-tracking metrics. This paper seeks to bring awareness to the use of different metrics along with practical suggestions on using them. It compares and contrasts various eye-tracking metrics used in software engineering. It also provides definitions for common metrics and discusses some metrics that the software engineering community might borrow from other fields.}
}

@article{rayyan-727967898,
  title={A comprehensive investigation of modern test suite optimization trends, tools and techniques},
  year={2019},
  journal={IEEE Access},
  issn={2169-3536},
  volume={7},
  pages={89093-89117},
  author={Kiran, Ayesha and Butt, Wasi Haider and Anwar, Muhammad Waseem and Azam, Farooque and Maqbool, Bilal},
  keywords={Software, Software testing, Tools, Optimization, Databases, multi-objective optimization, Clustering algorithms, Greedy algorithms, single objective optimization, test suite optimization},
  abstract={Software testing is an important but expensive activity of software development life cycle, as it accounts for more than 52% of entire development cost. Testing requires the execution of all possible test cases in order to find the defects in the software. Therefore, different test suite optimization approaches like the genetic algorithm and the greedy algorithm, etc., are widely used to select the representative test suite without compromising the effectiveness. Test suite optimization is frequently researched to enhance its competences but there is no study published until now that analyzes the latest developments from 2016 to 2019. Hence, in this article, we systematically examine the state-of-the-art optimizations' approaches, tools, and supporting platforms. Principally, we conducted a systematic literature review (SLR) to inspect and examine 58 selected studies that are published during 2016-2019. Subsequently, the selected researches are grouped into five main categories, i.e., greedy algorithm (seven studies), meta-heuristic (28 studies), hybrid (six studies), clustering (five studies), and general (12 studies). Finally, 32 leading tools have been presented, i.e., existing tools (25 tools) and proposed/developed tools (seven tools) along 14 platform supports. Furthermore, it is noted that several approaches aim at solving the single-objective optimization problem. Therefore, researchers should focus on dealing with the multi-objective problem, as multi-objective versions outperform the single-objective ones. Moreover, less attention has been given to clustering-based techniques. Thus, we recommend exploring the machine learning and artificial intelligence-based optimization approaches in the future. A broad exploration of tools and techniques, in this article, will help researchers, practitioners, and developers to opt for adequate techniques, tools, or platforms as per requirements.}
}

@article{rayyan-727967899,
  title={Investigating requirement engineering techniques in the context of small and medium software enterprises},
  year={2016},
  pages={519-523},
  author={Besrour, Souhaib and Bin Ab Rahim, Lukman and Dominic, P D D},
  keywords={Software, software engineering, Systematics, Bibliographies, Requirements engineering, Industries, Companies, Computers, Requirement analysis, Software requirement, Technique evaluation},
  abstract={Numerous requirement techniques have been proposed in the last decade in order to reduce requirement engineering challenges. However, Large number of requirement engineering techniques causes confusion while technique selection process. Thus, there is a need to investigate requirement techniques. To address this objective, techniques have been studied in the last two decade between the years 1995 and 2016 using systematic literature review (SLR) to explore other studies findings and results. A total of 15 techniques has been specified in the context of requirement engineering. Additionally, identified techniques have been further investigated using quantitative study through small and medium software enterprises. Where a total of 250 surveys have been collected, after eliminating incomplete surveys 217 completed surveys used for data analysis. Concluding results show that out of 15 techniques, 12 techniques have been rated as supported. Lastly identified techniques aim to benefit software enterprises particularly new enterprises to have a better understanding about techniques support level in the context of requirement engineering.}
}

@article{rayyan-727967900,
  title={An empirical assessment of a systematic search process for systematic reviews},
  year={2011},
  pages={56-65},
  author={Zhang, He and Babar, Muhammad Ali and Bai, Xu and Li, Juan and Huang, Liguo},
  abstract={Background: Systematic Literature Reviews (SLRs) have been gaining significant attention from Software Engineering (SE) researchers. Several researchers are also working on improving SLR methodology for SE. Objective: The study reported in this paper aims to validate the QGS-based search process for SLR, i.e. whether a more effective (sensitive) and/or productive (precise) search can be achieved by following the QGS-based process. Method: We used a dual-case study, in which each case includes two observations of SE literature search for the same SLR but using and not using the QGS-based approach. The overall sensitivity and precision were calculated for each observation that implemented different search design. Results: The use of QGS-based search process resulted in higher sensitivity and precision in this dual-case study. Conclusions: A systematic search process can help capture more relevant studies as well as save researchers' time spent on literature search activities. Our observations also support that an integrated search strategy is recommended for SLRs in SE to avoid the possible limitations of applying a single (manual or automated) search method.}
}

@article{rayyan-727967901,
  title={A systematic review on the effectiveness of web usability evaluation methods},
  year={2012},
  pages={52-56},
  author={Fernandez, Adrian and Abrahão, Silvia and Insfran, Emilio},
  keywords={Systematic Review, Method Effectiveness, Usability Evaluation, Web Development},
  abstract={Usability evaluation methods have become critical in the Web domain to ensure the success of Web applications. Aim: Since a large number of proposals have been presented during the last few years, a question arises: Which usability evaluation methods have proven to be the most effective in the Web domain? Method: This paper presents a systematic review that was motivated by previous results obtained from a systematic mapping study in the Web usability evaluation field. Results: A total of 18 studies were selected from an initial set of 206 in order to extract, code, and synthesize empirical data concerning the effectiveness of usability evaluation methods for the Web. Conclusions: We detected a need of more empirical studies and more standardized effectiveness measures for comparing usability evaluation methods. Our results suggest several evaluation methods which may be useful in allowing researchers and practitioners to perform effective Web usability evaluations.}
}

@article{rayyan-727967902,
  title={Security compliance in agile software development: A systematic mapping study},
  year={2020},
  pages={413-420},
  author={Moyón, Fabiola and Almeida, Pamela and Riofrío, Daniel and Mendez, Daniel and Kalinowski, Marcos},
  keywords={Software, Software engineering, Systematics, Security, Systematic Mapping Study, Programming, Regulation, Standards, Agile Software Engineering, Secure Software Engineering, Security Compliance},
  abstract={Companies adopting agile development tend to face challenges in complying with security norms. Existing research either focuses on how to integrate security into agile methods or on discussing compliance issues of agile methods but independently of the regulation type, in particular of security standards. A comprehensive overview of this scattered field is still missing and we know little about how to achieve security compliance in agile software development. Existing secondary studies (mapping studies and literature reviews) analyze publications on secure agile development, but they do not analyze implications of security standard compliance, e.g., integration of specific standard requirements or compliance assessments. To close this gap, we report on a systematic mapping study. Starting with a set of 2,383 papers, our work distills 11 relevant publications addressing security compliance in agile software development. With this study, we contribute by describing the maturity of the field, as well as domains where security compliant agile software engineering was investigated. Moreover, we make explicit which phases of a secure development process are covered by the field and which agile principles are analyzed when aiming at compliance with international security standards, country-specific security regulations, industry-specific security standards, and other well-known security frameworks.}
}

@article{rayyan-727967903,
  title={Qualitative vs. Quantitative software process simulation modeling: Conversion and comparison},
  year={2009},
  pages={345-354},
  author={Zhang, He and Kitchenham, Barbara and Jeffery, Ross},
  keywords={Software engineering, Software performance, Software quality, Information systems, Australia, Computer science, Computational modeling, Computer simulation, Helium, Mathematical model, qualitative modeling, software evolution, Software process simulation modeling, system dynamics, Software},
  abstract={Software process simulation modeling (SPSM) research has increased in the past two decades. However, most of these models are quantitative, which require detailed understanding and accurate measurement. As the continuous work to our previous studies in qualitative modeling of software process, this paper aims to investigate the structure equivalence and model conversion between quantitative and qualitative process modeling, and to compare the characteristics and performance of these two approaches by modeling and simulating a software evolution process. Following the model conversion scheme, the reference quantitative (SD) model and the corresponding qualitative model become comparable. The results present their different capabilities and interesting perspectives, and further the potential use of qualitative modeling in software process research.}
}

@article{rayyan-727967904,
  title={A systematic literature review of requirements volatility prediction},
  year={2017},
  pages={55-64},
  author={Alsalemi, Ahmed Mubark and Yeoh, Eng-Thiam},
  keywords={Software, Systematics, Bibliographies, Search problems, Databases, Market research, Predictive models, Volatilization},
  abstract={Requirements volatility is a crucial risk factor in software projects as it directly results in cost and time overruns. Accurately predicting requirements volatility is important for better project management. This paper presents a systematic literature review that focuses on the prediction of the requirements volatility. This literature review aims to answer four research questions: 1) how is requirements volatility prediction applied to different software development methods? 2) What are the machine learning algorithms used to predict requirements volatility in software development? 3) What are the attributes (predictors) used to predict requirements volatility in software development? 4) What are the performance metrics for evaluating existing prediction models? This study presents predictors used in the literature and their performances.}
}

@article{rayyan-727967905,
  title={Requirements for smart cities: Results from a systematic review of literature},
  year={2018},
  pages={1-6},
  author={Daneva, Maya and Lazarov, Boyan},
  keywords={Systematic literature review, Systematics, Data mining, Requirements engineering, Empirical study, Security, Software architecture, Instruments, Intelligent systems, Smart cities, Smart city systems},
  abstract={Smart cities are gaining increasingly more importance in both research and business circles. Much effort is spent to defining what a smart city is and how it could be realized with today's or future technologies. However, from requirements engineering perspective, our knowledge of smart cities is fragmented; little is known about the requirements for smart cities as complex systems, or as systems of systems, in specific application domains. In this paper, we elicit requirements for smart city systems by carrying out a systematic review of scientific literature focused on the so-called “hard” domains of smart cities. Based on 32 selected publications, we gathered and classified requirements in respect to three types of smart city systems (instrumented, interconnected, and intelligent systems) and four classes of requirements: end-to-end experience, architectural, security, and infrastructure requirements. Our most important findings are that: (1) most authors took a bottom-up approach to defining requirements for smart cities; their efforts focused mainly on requirements important for designing an architecture that could scale up to any size and include any device or system; and (2) very little is mentioned on the newly emerging security and privacy challenges that are critical to gain the citizens' acceptance of smart city apps.}
}

@article{rayyan-727967906,
  title={A systematic mapping study on software engineering testbeds},
  year={2011},
  pages={107-116},
  author={Barreiros, Emanoel and Almeida, Adauto and Saraiva, Juliana and Soares, Sergio},
  keywords={Systematics, Empirical software engineering, Software architecture, Context, Software systems, Protocols, technology evaluation, technology transfer, testbeds, Software},
  abstract={Even though empirical research has grown in interest, techniques, methodologies and best practices are still in debate. In this context, test beds are effective when one needs to evaluate and compare technologies. The concept is well disseminated in other areas such as Computer Networks, but remains poorly explored in Software Engineering (SE). This paper presents a systematic mapping study on the SE test beds literature. From the initial set of 4239 studies, 13 primary studies were selected and categorized. Based on that, we found that Software Architecture is the most investigated topic, controlled experiment is the most used method to evaluate such test beds, 20 benefits of using test beds in SE have been identified and that test beds comprise very heterogeneous structural elements.}
}

@article{rayyan-727967907,
  title={State of mobile crowdsourcing applications: A review},
  year={2015},
  pages={27-32},
  author={Mahmud, Farahidayah and Aris, Hazleen},
  keywords={systematic review, Crowdsourcing, Databases, Google, Industries, Mobile handsets, Meteorology, Mobile communication, crowdsourcing, crowdsourcing application, mobile crowdsourcing, secondary study},
  abstract={The proliferation of mobile devices has changed the way people communicate and perform their day-to-day dealings. In particular, the widespread availability of mobile devices has enabled information distribution and sharing to be done almost instantly at your fingertips. Companies and organisations are also taking advantage of this phenomenon by changing the way they accomplish their tasks. From traditionally employing people to do the tasks, they now crowdsource the tasks to the public who can easily access the tasks through their mobile devices. As a result, many mobile crowdsourcing applications have been developed for this purpose. In this paper, the result of a systematic literature review performed in an attempt to understand the current state of mobile crowdsourcing applications that exist today is presented. The aim is to identify the potential categorisation of the applications in a way that enables the researchers to understand the current situation and hence determine the direction of future research in this area. Mobile crowdsourcing applications under review came from both; research and industry. Results obtained from the review showed that mobile crowdsourcing applications from the traffic and navigation category are mostly developed and majority of the mobile crowdsourcing applications only exist in mobile form. Finding from the review also showed that most mobile crowdsourcing applications from the industry are offered in both platforms; iOS and Android.}
}

@article{rayyan-727967908,
  title={Systematic review of software product certification},
  year={2012},
  pages={1-6},
  author={Rodríguez, Moisés and Piattini, Mario},
  keywords={systematic review, Software, Systematics, Silicon compounds, Quality management, ISO standards, Certification, IEC standards, ISO/IEC 25000, software product certification, software product quality},
  abstract={Software quality is becoming in recent years a great importance because of the growing need to ensure proper operation of computer systems, which are increasingly present in most areas of our lives. This focus on quality has been focused primarily on the processes used for software development, emerging standards such as CMMI certifications under or ISO / IEC 15504. However, each day the concern for the quality of software product itself and not only by the processes used for development is greater. Thus, standards have emerged as the ISO / IEC 25000, which define a quality model and an evaluation process for the product. However, certifications that take as their object the software product itself and ensure compliance of the product of a set of requirements, are less widespread. In this sense, the present systematic review aims to identify existing initiatives related to the certification of software products, the features they have and the state of maturity in their application.}
}

@article{rayyan-727967909,
  title={Effective collection and selection of research articles for a systematic review},
  year={2018},
  pages={1394-1399},
  author={Rožanc, I},
  abstract={The first step in any research work is to gain a better understanding of selected research topic and its placement in the research field. To achieve this a Systematic Review (SR) is usually performed in one of two forms. A Systematic Literature Review (SLR) addresses a specific research question by providing insight into selected literature, while a Systematic Mapping Study (SMS) uses a principle of classification of a large number of collected articles to present a wider picture of the research topic. In both cases, the collection and selection of proper articles are crucial. Due to a lot of available articles from different sources and its diverse quality, a lot of tedious (often manual) work is required. This article addresses the issues of effective collection and selection of an appropriate set of articles. First, the importance of using suitable guidelines for conducting an SR is presented. Then, the practical considerations for the collection of articles are shown with an emphasis on effective search in Digital Libraries (DLs). Finally, the issues connected with an effective selection of an appropriate set of articles (from all collected) is described. In this part, we advise to collect as many articles as possible and perform sequential three-step selection to efficiently eliminate the obviously incorrect ones without a tedious manual screening of entire article contents.}
}

@article{rayyan-727967910,
  title={A systematic review of literature on methodologies, practices, and tools for programming teaching},
  year={2018},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={16},
  number={5},
  pages={1468-1475},
  author={Borges, R P and Oliveira, P R F and da R. Lima, R G and de Lima, R W},
  keywords={Systematics, Tools, Education, Conferences, IEEE transactions, Programming profession, Systematic Review of Literature, Teaching of Programming},
  abstract={It has been frequent the discussion about the teaching and learning of Programming, from the initial series to the undergraduate courses. It is noticed that many students have difficulty to learn programming by several reasons: methodology, tools, programming languages, lack of programming logic in basic education, motivation, among others. Thus, this carries out a survey of the state of the art of existing and documented approaches in the literature, through a mapping of published works in the last five years (2012 to 2016) in two of Brazil's leading scientific computing platforms (CEIE and RENOTE), whose focus is to present solutions that address methodologies and tools that can be used in the different teaching modalities. As methodology was used the Systematic Review of Literature. As a result, it was found that, although studies still focus on higher education, in recent years there has been an increasing interest in programming teaching projects for children and teenagers, using gamification and tools such as Scratch. The results also demonstrate the growing interest of researchers in the search for approaches that provide better results in this area.}
}

@article{rayyan-727967911,
  title={A systematic review of software product lines applied to mobile middleware},
  year={2009},
  pages={1024-1029},
  author={Bezerra, Yuri Morais and Pereira, Thaís Alves Burity and da Silveira, Glêdson Elias},
  keywords={systematic review, Middleware, Mobile computing, Information technology, Software systems, Application software, Strontium, Communications technology, Hardware, customizable middleware, middleware, mobile computing, Personal digital assistants, software product line, Wireless communication, Software},
  abstract={Mobile computing imposes several restrictions to software development due to diversities in network connectivity, platform capability and resource availability. Middleware has been used to abstract such issues from application developers, resolving common challenges of distributed systems. However, middleware is considered a heavy-weight technology to mobile computing, as it is a general purpose solution. By enabling the customization of the set of features provided by a middleware, the adoption of Software Product Lines techniques seems to be a promising systematic approach. Nevertheless, this is an incipient subject that has been explored for few research groups. In this sense, this paper goal is to present a systematic review of Product Line techniques applied to middleware for mobile computing.}
}

@article{rayyan-727967912,
  title={A study of the publications of educational robotics: A Systematic Review of Literature},
  year={2018},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={16},
  number={4},
  pages={1193-1199},
  author={Bezerra Junior, Jose Etiene and Queiroz, Paulo Gabriel Gadelha and de Lima, Rommel Wladimir},
  keywords={Systematics, Bibliographies, Tools, Manuals, Education, IEEE transactions, Robots, Systematic Review of Literature, Educational Robotics, Pedagogical tool, Robotics},
  abstract={Educational Robotics has been presented as a great pedagogical tool because it demonstrates an attractive way of working the theoretical knowledge put into practice. Thus, several educational technologies have emerged with different approaches, with the purpose of applying robotics in the educational area in a more attractive and playful way. This article presents the conduction of a Systematic Review of Literature (SRL), whose objective is to identify the teaching approaches used with educational robotics. With this, we present experiences reports, and at the same time show the skills and competencies that are explored through robotics and education. This review uses scientific papers published in the period from 2011 to 2016.}
}

@article{rayyan-727967913,
  title={An empirical study to investigate the impact of communication issues in GSD in pakistan's IT industry},
  year={2019},
  journal={IEEE Access},
  issn={2169-3536},
  volume={7},
  pages={171648-171672},
  author={Ammad, Ghana and Iqbal Janjua, Uzair and Madni, Tahir Mustafa and Cheema, Muhammad Faisal and Shahid, Ahmed R},
  keywords={Software, systematic literature review, Systematics, Bibliographies, Organizations, global software development, empirical study, Cultural differences, Industries, distributed software development, communication challenges, communication issues, Communication risk, Face, Pakistan},
  abstract={Global software development (GSD) practice has been increasingly emerging in the recent few decades in the field of business and software industry. On the one hand, many software development organizations get the benefits of GSD, including but not limited to reduced cost, cheap labor, round the clock working and skilled professionals. On the other hand, these organizations have to face several challenges because of GSD. These challenges pose serious threats to the stability of the GSD projects. Communication between distributed team members is one of the most crucial challenges in GSD. Therefore, the current study aims to identify the communication risk in GSD and also evaluate the impact of these communication risks in GSD environment. A Systematic Literature Review (SLR) has been performed to identify all the communication-related issues in GSD. After that, a conceptual framework has been proposed for evaluating the impact of those issues on communication risk in GSD. An empirical evaluation has been performed on data collected from the software organizations of Pakistan working in GSD based environment. The finding of our study demonstrates that geographical distance, socio- temporal distance, socio-culture distance, team member's attitude, team issues, organizational & architectural issue and customer issue have a significant direct impact on communication risk in GSD. The study also shows that there is a significant correlation between findings of SLR and empirical investigation (r = 0.460, P = 0.005). Further, we believe that the results of our research can help to tackle the issues related to communication in GSD. Therefore, it will help to improve the performance of the development activities of GSD organizations.}
}

@article{rayyan-727967914,
  title={Software product maintainability prediction: A survey of secondary studies},
  year={2017},
  pages={702-707},
  author={Elmidaoui, Sara and Cheikhi, Laila and Idri, Ali},
  keywords={Software, Software measurement, Systematics, Systematic Literature Review, Bibliographies, Data mining, Measures, Secondary Study, Libraries, Predictive models, Prediction Models, Software Product Maintainability},
  abstract={Software Product Maintainability Prediction (SPMP) has received more attention from researchers to control the high costs of software maintenance. Many studies related to this topic have been published in the recent years. Some of them, referred to as secondary studies, focused on the interpretation and synthesis of available published research by giving an up-to-date state of the art about SPMP. This state of the art is provided in a form of literature survey or in a rigorous systematic literature review. The objective of this paper is to investigate these secondary studies to discuss methods that they use and contents that they cover. A set of survey research questions have been proposed and discussed through the investigation of nine selected secondary studies collected from different digital libraries. Based on the results, the analysis shows that maintainability prediction models/techniques as well as maintainability key predictors measures/factors are the most studied aspects in SPMP. Moreover, there is a need to address in depth the performance of SPMP models as well as their validation. We believe that this study will be a reliable basis for further research in software maintainability studies.}
}

@article{rayyan-727967915,
  title={Organization type and size based identification of requirements change management challenges in global software development},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={94089-94111},
  author={Akbar, Muhammad Azeem and Mahmood, Sajjad and Alsanad, Ahmed and Shafiq, Muhammad and Gumaei, Abdu and Alsanad, Abeer Abdul-Aziz},
  keywords={Software, Systematics, Bibliographies, Global software development (GSD), systematic literature review (SLR), Organizations, challenges, Requirements management, Standards organizations, Capability maturity model, client, requirements change management (RCM), size-based classification, vendor},
  abstract={Global Software Development (GSD) is adopted by organizations to develop quality software at relatively low cost. Requirement Change Management (RCM) plays a key role in overall success of a GSD project. The objective of this study is to identify the challenges of RCM process by adopting systematic literature review (SLR) and validate them by employing questionnaire survey approach in real world practices. A total of 25 challenges were identified through SLR and empirical study. We have further classified the identified challenges into client and vendor organizations with an aim to understand RCM challenges in the context of both types of GSD organizations. The identified challenges were also categorized into three core types of the organization size (small, medium, large), that highlights the significance of each challenge for specific organizational size. The results indicate that there is a moderate correlation between the ranking of these challenges in the literature and the survey results. The finding of this study has the potential to help the GSD organizations in addressing the problems related with RCM in GSD projects.}
}

@article{rayyan-727967916,
  title={Tool-supported requirement engineering: A categorization of the state of the art and research trends},
  year={2019},
  pages={514-523},
  author={Cadena-Romero, Mariana and Ocharán-Hernández, Jorge Octavio and de los Ángeles Arenas-Valdés, María and Pérez-Arriaga, Juan Carlos},
  keywords={systematic literature review, empirical studies, tools, requirements engineering, safety requirements},
  abstract={The multiple activities that are part of Requirements Engineering (RE) are benefited from the support of software tools that assists the application of diverse techniques. Given the importance of this phase in software development and specifically in product safety, this article aims to compile the current status of software tools used in the different RE activities to direct future development efforts in this type of tools and identify the activities of the RE, the implemented techniques and which tools focus on the requirements associated with safety. We used systematic literature review method for reviewing the literature on RE tools. We used automatic search strategies for searching the relevant papers published between 1 January 2013 and 31 December 2018 and Meta-aggregation to synthesize the data extracted from the primary studies and to obtain a categorization of findings. We selected 50 articles, 33 conference and 17 journal papers, from the initially retrieved 10,191 articles. The results from the data analysis 248 findings enabled us to classify RE tools in 15 categories and 29 subcategories. This review has enabled us to identify the following areas for further research in software tool assisted RE: 1) is necessary to perform more research on the development of RE tools that specialize in software requirements associated to a quality attribute of interest as safety; 2) it is important to development RE tools activities different areas from requirement specification, validation and management; 3) in is important to conduct evaluation the RE tools founds in non-academic environment.}
}

@article{rayyan-727967917,
  title={Prioritization based taxonomy of DevOps challenges using fuzzy AHP analysis},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={202487-202507},
  author={Akbar, Muhammad Azeem and Naveed, Wishal and Mahmood, Sajjad and Alsanad, Abeer Abdulaziz and Alsanad, Ahmed and Gumaei, Abdu and Mateen, Ahmed},
  keywords={Software, systematic literature review, Systematics, Bibliographies, DevOps, Taxonomy, Organizations, challenges, Industries, Collaborative software, fuzzy AHP},
  abstract={The DevOps (development and operations) is a collaborative software development environment which offers the continues development and deployment of quality software project within short time. The DevOps practices are not yet mature enough, and the software organizations hesitate to adopt it. This study aims: 1) to explore the DevOps challenges by conducting systematic literature review (SLR) and to get the insight of industry experts via questionnaire survey study and 2) to prioritize the investigated challenges using fuzzy analytical hierarchy process (FAHP). The study findings provide the set of critical challenges faced by the software organizations while adopting DevOps and a prioritization-based taxonomy of the DevOps challenges. The application of FAHP is novel in this research area as it assists in addressing the vagueness of practitioners concerning the influencing factors of DevOps. We believe that the finding of this study will serve as a body of knowledge for real world practitioners and researchers to revise and develop the new strategies for the successful implementation of DevOps practices in the software industry.}
}

@article{rayyan-727967918,
  title={Multicriteria decision making taxonomy of cloud-based global software development motivators},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={185290-185310},
  author={Akbar, Muhammad Azeem and Alsalman, Hussain and Khan, Arif Ali and Mahmood, Sajjad and Meshram, Chandrashekhar and Gumaei, Abdu H and Riaz, Muhammad Tanveer},
  keywords={Software, systematic literature review, Systematics, Bibliographies, Data mining, Cloud computing, Organizations, Protocols, Cloud-based global software development, fuzzy-AHP, motivators, Decision Making},
  abstract={The software organizations widely consider the cloud based global software development (CGSD) as it offer the quality projects with low cast. The adoption of CGSD is challenging due to the geographical distance between practitioners. This study aims to identify and analyses the motivators that could positively impact the implementation of CGSD paradigm. The systematic literature review approach was applied to identify the CGSD motivators reported in the literature, and were further validated with industry experts using questionnaire survey study. Moreover, the fuzzy-AHP approach was applied to prioritize the investigated motivators concerning their significance for the successful adoption of CGSD. The findings of the study provide the prioritization-based taxonomy of the investigated motivators that assists the software organizations to develop and revise their strategies for the successful implementation of CGSD.}
}

@article{rayyan-727967919,
  title={A comparison of software cost, duration, and quality for waterfall vs. iterative and incremental development: A systematic review},
  year={2009},
  pages={511-515},
  author={Mitchell, Susan M and Seaman, Carolyn B},
  keywords={Software engineering, Software measurement, Project management, Software quality, Databases, Programming, Software development management, Costs, Iterative methods, Maintenance, Software},
  abstract={The objective of this study is to present a body of evidence that will assist software project managers to make informed choices about software development approaches for their projects. In particular, two broadly defined competing approaches, the traditional ldquowaterfallrdquo approach and iterative and incremental development (IID), are compared with regards to development cost and duration, and resulting product quality. The method used for this comparison is a systematic literature review. The small set of studies we located did not demonstrate any identifiable cost, duration, or quality trends, although there was some evidence suggesting the superiority of IID (in particular XP). The results of this review indicate that further empirical studies, both quantitative and qualitative, on this topic need to be undertaken. In order to effectively compare study results, the research community needs to reach a consensus on a set of comparable parameters that best assess cost, duration, and quality.}
}

@article{rayyan-727967920,
  title={Model-driven reverse engineering approaches: A systematic literature review},
  year={2017},
  journal={IEEE Access},
  issn={2169-3536},
  volume={5},
  pages={14516-14542},
  author={Raibulet, Claudia and Arcelli Fontana, Francesca and Zanoni, Marco},
  keywords={Systematics, Bibliographies, Tools, Models, Object oriented modeling, Analytical models, Search engines, reverse engineering, Reverse engineering, legacy system, model transformation, model-driven reverse engineering},
  abstract={This paper explores and describes the state of the art for what concerns the model-driven approaches proposed in the literature to support reverse engineering. We conducted a systematic literature review on this topic with the aim to answer three research questions. We focus on various solutions developed for model-driven reverse engineering, outlining in particular the models they use and the transformations applied to the models. We also consider the tools used for model definition, extraction, and transformation and the level of automation reached by the available tools. The model-driven reverse engineering approaches are also analyzed based on various features such as genericity, extensibility, automation of the reverse engineering process, and coverage of the full or partial source artifacts. We describe in detail and compare fifteen approaches applying model-driven reverse engineering. Based on this analysis, we identify and indicate some hints on choosing a model-driven reverse engineering approach from the available ones, and we outline open issues concerning the model-driven reverse engineering approaches.}
}

@article{rayyan-727967921,
  title={Attributes and metrics of internal quality that impact the external quality of object-oriented software: A systematic literature review},
  year={2016},
  pages={1-12},
  author={Santos, Danilo and Resende, Antônio and Junior, Paulo Afonso and Costa, Heitor},
  keywords={Measurement, Software quality, Usability, Computer science, Software Quality, External Quality, Internal Quality Attributes, Internal Quality Metrics, Software reliability, Metronidazole, Software},
  abstract={Quality metrics of software can be categorized into internal quality metrics, external quality metrics, and quality in use metrics. Although existing a close relationship between internal and external quality of software systems, there are no explicit evidences in literature of what are the attributes and metrics of internal quality that impact external quality. Thus, we carried out a systematic literature review for identifying that relationship. After the analysis of 664 papers, 12 papers were studied in depth. As result, we found 65 metrics related primarily to the maintainability, usability, and reliability quality characteristics and the main attributes that impact external metrics are size, coupling, and cohesion.}
}

@article{rayyan-727967922,
  title={Software support for the Fuzzy Front End stage of the innovation process: a systematic literature review},
  year={2010},
  pages={426-431},
  author={Monteiro, Cleviton and Arcoverde, Daniel F and da Silva, Fabio Q B and Ferreira, Henrique S},
  keywords={Benefits, Informatics, Computer industry, Protocols, Software tools, Costs, Technological innovation, Collaborative tools, Computer Aided Innovation, Fuzzy Front End, Fuzzy sets, Fuzzy systems, New Product Development, Product development, Software Tools, Software},
  abstract={The early stages of new product development, called the Fuzzy Front End (FFE), are essential for the success of innovation. Therefore, various software tools have been proposed to support FFE activities. However, little evidence is provided about the benefits of using such tools. The objective of this study is to present evidence that will assist industry practitioners to make informed choices about software support tools to be used in the FFE. The method used for this study was a systematic literature review that analyzed 1090 articles published between 1997 and 2009. The results show that software tools can speed up the FFE, reduce costs, increase collaboration, improve decision quality and knowledge management, reduce risks, and enhance overall creativity.}
}

@article{rayyan-727967923,
  title={Maintenance effort estimation for open source software: A systematic literature review},
  year={2016},
  pages={32-43},
  author={Wu, Hong and Shi, Lin and Chen, Celia and Wang, Qing and Boehm, Barry},
  keywords={Software, Data mining, Measurement, Estimation, Protocols, Planning, Maintenance engineering},
  abstract={Open Source Software (OSS) is distributed and maintained collaboratively by developers all over the world. However, frequent personnel turnover and lack of organizational management makes it difficult to capture the actual development effort. Various OSS maintenance effort estimation approaches have been developed to provide a way to understand and estimate development effort. The goal of this study is to identify the current state of art of the existing maintenance effort estimation approaches for OSS. We performed a systematic literature review on the relevant studies published in the period between 2000-2015 by both automatic and manual searches from different sources. We derived a set of keywords from the research questions and established selection criteria to carefully choose the papers to evaluate. 29 out of 3,312 papers were selected based on a well designed selection process. Our results show that the commonly used OSS maintenance effort estimation methods are actual effort estimation and maintenance activity time prediction, the most commonly used metrics and factors for actual effort estimation are source code measurements and people related metrics, the most commonly mentioned activity for maintenance activity time prediction is bug fixing. Accuracy measures and cross validation is used for validating the estimation models. Based on the above findings, we identified the issues in evaluation methods for actual maintenance effort estimations and the needs for quantitative OSS maintenance effort inference from size-related metrics. Meanwhile, we highlighted individual contribution and performance measurement as a novel and promising research area.}
}

@article{rayyan-727967924,
  title={Augmented reality for learning of children and adolescents with autism spectrum disorder (ASD): A systematic review},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={78779-78807},
  author={Khowaja, Kamran and Banire, Bilikis and Al-Thani, Dena and Sqalli, Mohammed Tahri and Aqle, Aboubakr and Shah, Asadullah and Salim, Siti Salwah},
  keywords={Systematics, Databases, Autism, virtual reality, Data collection, Maintenance engineering, technology, Augmented reality, autism spectrum disorder, computer, data collection, inclusive education, intervention, mixed reality, research design, smartglass, smartphone, social communication, tablet, Child Development Disorders, Pervasive, Autistic Disorder, Adolescent, Only Child, Child},
  abstract={This paper presents a systematic review of relevant primary studies on the use of augmented reality (AR) to improve various skills of children and adolescents diagnosed with autism spectrum disorder (ASD) from years 2005 to 2018 inclusive in eight bibliographic databases. This systematic review attempts to address eleven specific research questions related to the learing skills, participants, AR technology, research design, data collection methods, settings, evaluation parameters, intervention outcomes, generalization, and maintenance. The social communication skill was the highly targeted skill, and individuals with ASD were part of all the studies. Computer, smartphone, and smartglass are more frequently used technologies. The commonly used research design was pre-test and post-test. Almost all the studies used observation as a data collection method, and classroom environment or controlled research environment were used as a setting of evaluation. Most of the evaluation parameters were human-assisted. The results of the studies show that AR benefited children with ASD in learning skills. The generalization test was conducted in one study only, but the results were not reported. The results of maintenance tests conducted in five studies during a short-term period following the withdrawal of intervention were positive. Although the effect of using AR towards the learning of individuals was positive, given the wide variety of skills targeted in the studies, and the heterogeneity of the participants, a summative conclusion regarding the effectiveness of AR for teaching or learning of skills related to ASD based on the existing literature is not possible. The review also proposes the research taxonomy for ASD. Future research addressing the effectiveness of AR among more participants, different technologies supporting AR for the intervention, generalization, and maintenance of learning skills, and the evaluation in the inslusive classroom environment and other settings is warranted.}
}

@article{rayyan-727967925,
  title={Risk Management in software engineering: A scoping study},
  year={2012},
  pages={243-252},
  author={Lobato, Luanna Lopes and do Carmo Machado, Ivan and da Mota Silveira Neto, Paulo Anselmo and de Almeida, Eduardo Santana and de Lemos Meira, Silvio Romero},
  keywords={Software Engineering, Risk Management, Scoping Study, Software},
  abstract={Background - Risk Management (RM) practices are usually established towards avoiding or minimizing problems, likely to occur during software development. It can be stated as the task of analyzing and managing the impact of every important action to be performed in the project. Aim - In the context of RM practices, we developed a scoping study, aiming at analyzing the current scenario of RM practices in software development. Method - We analyzed 74 studies published by the most important venues published up to the year 2011. Based on the analyzed dataset, we sketched a set of useful practices for applying RM in software projects. Results - The analysis indicate that most of the studies subjectively describe ways to evaluate risks, instead of providing readers with details on how RM is to be performed. Conclusions - Such findings points out to the need of further research in the field of RM, specially due to its importance for software development projects.}
}

@article{rayyan-727967926,
  title={Smart tools in software engineering: A systematic mapping study},
  year={2019},
  pages={1509-1513},
  author={Savchenko, Dmitrii and Kasurinen, Jussi and Taipale, Ossi},
  keywords={Software, Software engineering, Systematics, Tools, Unified modeling language, Industries, Technological innovation},
  abstract={Software development processes such as waterfall development model have been around for over fifty years, but still, even modern software development approaches, such as DevOps or Test-driven development, fundamentally rely on the same principles and phases as everything before them. Yet, the modern world imposes new challenges for software businesses, and new ways of digital distribution require new ways of resource provisioning and ability to reduce the time-to-market to its absolute minimum. In this study, we identify and analyze the technologies which may be useful for software companies to ease the development and maintenance work by assisting the experts to collect relevant information and observe issues before they cause process disturbances. As a result, we describe a mapping study, which identifies different approaches to developing a smart software engineering tools applying potential technologies such as artificial intelligence, cloud-based service models, adaptive measurement, and other approaches, which could offer significant benefits to the software development process.}
}

@article{rayyan-727967927,
  title={Decision support for offshore insourcing software development},
  year={2011},
  pages={111-113},
  author={Jabangwe, Ronald and S?mite, Darja},
  keywords={Software, Software engineering, Systematics, Empirical, Programming, Distributed development, Conferences, Companies, Global Software Engineering, OffshoreInsourcing},
  abstract={Proximity to customers and the possibility of accessing new and prospective markets are some of the factors that compel companies to consider various sourcing options and go global. One sourcing option that has consequently become popular is Offshore insourcing. Though it is appealing as a business strategy due to the perceived benefits, there are some strategies that may inexplicably impact software quality. The objective of this research work is to focus on producing empirical evidence that is valuable input to support decision-making during offshore insourcing projects. The input will help with evaluating different strategies on their impact on quality.}
}

@article{rayyan-727967928,
  title={An architecture for software engineering gamification},
  year={2020},
  journal={Tsinghua Science and Technology},
  issn={1007-0214},
  volume={25},
  number={6},
  pages={776-797},
  author={Pedreira, Oscar and García, Félix and Piattini, Mario and Cortiñas, Alejandro and Cerdeira-Pena, Ana},
  keywords={Software, software engineering, Software engineering, Tools, Computer architecture, Companies, gamification, Engines, gamification architecture, gamification engine},
  abstract={Gamification has been applied in software engineering to improve quality and results by increasing people's motivation and engagement. A systematic mapping has identified research gaps in the field, one of them being the difficulty of creating an integrated gamified environment comprising all the tools of an organization, since most existing gamified tools are custom developments or prototypes. In this paper, we propose a gamification software architecture that allows us to transform the work environment of a software organization into an integrated gamified environment, i.e., the organization can maintain its tools, and the rewards obtained by the users for their actions in different tools will mount up. We developed a gamification engine based on our proposal, and we carried out a case study in which we applied it in a real software development company. The case study shows that the gamification engine has allowed the company to create a gamified workplace by integrating custom-developed tools and off-the-shelf tools such as Redmine, TestLink, or JUnit, with the gamification engine. Two main advantages can be highlighted: (i) our solution allows the organization to maintain its current tools, and (ii) the rewards for actions in any tool accumulate in a centralized gamified environment.}
}

@article{rayyan-727967929,
  title={A systematic review on architecting for software evolvability},
  year={2010},
  pages={13-22},
  author={Breivold, Hongyu Pei and Crnkovic, Ivica},
  keywords={Software engineering, Data mining, Guidelines, Software architecture, Australia, Software systems, Computer architecture, Computer industry, Protocols, Costs, software architecture evolution, Software evolvability, Software},
  abstract={For long-lived systems, there is a need to address evolvability (i.e. a system's ability to easily accommodate changes) explicitly during the entire lifecycle. In this paper, we undertake a systematic review to obtain an overview of the existing studies in promoting software evolvability at architectural level. The search strategy identified 58 studies that were catalogued as primary studies for this review after using multi-step selection process. The studies are classified into five main categories of themes, including techniques that support quality considerations during software architecture design, architectural quality evaluation, economic valuation, architectural knowledge management and modeling techniques. The review investigates what is currently known about architecting software evolvability at architecture level. Implications for research and practice are presented.}
}

@article{rayyan-727967930,
  title={The evolution from traditional to intelligent web security: Systematic literature review},
  year={2020},
  pages={1-9},
  author={Martinez Santander, Carlos José and Moreno, Hugo and Hernandez Alvarez, Myriam Beatriz},
  keywords={Security, Cross-site scripting, SQL injection, Vulnerabilities, Analytical models, Performance analysis, Heuristic algorithms, Attacks, Cross-site script, Defacement, Denial of Service, Dynamic programming, Web Security, Intelligence},
  abstract={Information security is fundamental in the area of computing science with new metrics and technologies developed to secure the private data of entities and individuals related to banking, defense, education, business, medical, among others. The web servers require a high-security level to guarantee data protection and transactions to store, to manage, and to allow the interaction between the user or users and the website. The number of cybercriminals is increasing, looking for vulnerable web servers or applications to obtain the necessary information, modify it, or sell it to the highest bidder. There are multiple solutions created over the years; however, the technological advance is not only to secure the data, but the cybercriminals are also updated and use sophisticated techniques; therefore, these solutions are also obsolete if there is no update. The purpose of this document is to present a systematic review of the literature on web security. Identified Studies try to mitigate various attacks or detect vulnerabilities, the solutions proposed so far are not sufficient, and most attacks occur at the application layer level. The advances in this field are dizzying, where some studies, although very small, that already use automatic learning techniques or Cognitive Security is present in this area.}
}

@article{rayyan-727967931,
  title={A quantitative study to identify critical requirement engineering challenges in the context of small and medium software enterprise},
  year={2016},
  pages={606-610},
  author={Besrour, Souhaib and Rahim, Lukman Bin A B and Dominic, P D D},
  keywords={Software, Software engineering, Systematics, Bibliographies, Requirements engineering, Industries, Companies, Computers, Requirement analysis, Requirements challenges},
  abstract={Requirement engineering field has been intensively studied in last decade. Even though requirement still one of the most critical process in software development. Recent studies shows that 56% of system defects are coming from requirement. Additionally, by one estimate requirement errors cost 10 times more than coding errors. Thus, there is a need to identify those critical requirement engineering challenges. To address this issues, critical challenges have been investigated in the last two decades between the year 1995 and 2016. Where systematic literature review (SLR) has been used to explore previous studies findings and results. Additionally, identified critical challenges have been further investigated using quantitative study through software small and medium enterprises (SME). Where a total of 250 surveys have been collected, after eliminating incomplete surveys 217 completed surveys used for data analysis. After performing all process 12 challenges has been specified as critical challenges in the context of software SME. Where identified critical challenges can benefit software enterprises particularly new enterprises to pay further attention to specified critical challenges.}
}

@article{rayyan-727967932,
  title={Extracting information from experimental software engineering papers},
  year={2007},
  pages={105-114},
  author={Cruzes, Daniela and Mendonca, Manoel and Basili, Victor and Shull, Forrest and Jino, Mario},
  keywords={Software testing, Software engineering, Data mining, Project management, Quality assurance, Software quality, Software maintenance, Quality management, In vitro, Information analysis, Software, Information Storage and Retrieval},
  abstract={Experiments have been conducted to investigate analysis, design, implementation, testing, maintenance, quality assurance and reuse techniques, but, a body of evidence has not yet been built that enables a project manager to know with confidence what software processes produce what product characteristics and under what conditions. This paper extends an approach we proposed earlier to extract information from papers so that systematically analyzing results from several papers is possible. It also describes an in-vitro experiment we did with graduate students to validate the approach. The results show that the approach is feasible and can be taught to less experienced researchers.}
}

@article{rayyan-727967933,
  title={The use of computational tools in teaching pharmacology: A systematic review on the topic},
  year={2014},
  pages={1-6},
  author={Rauta, Leonardo Ronald Perin and Batista, Alex Fernando and da Rocha Fernandes, Anita Maria},
  keywords={systematic review, Systematics, Visualization, Tools, Abstracts, Materials, IEEE Xplore, teaching, Electronic learning, pharmacology},
  abstract={In pharmacology, one of the problems found in teaching is precisely the existing abstract concepts, because it has chemistry as a requirement for your understanding. Due to these abstract concepts, students try to end difficult to visualize and understand these concepts. To try to reduce the degree of abstraction, was started to use computational tools for teaching and learning. This paper presents a systematic literature review of computational tools which are being used for teaching Pharmacology. This systematic review included only studies published in 2008 and 2013 in three different databases and it was noted that are no many computational tools directed to the area of pharmacology.}
}

@article{rayyan-727967934,
  title={Factors promoting software project escalation: A systematic review},
  year={2020},
  volume={1},
  pages={280-289},
  author={Holgeid, Knut Kjetil and Stray, Viktoria},
  keywords={Software, Risk management, Organizations, Psychology, Informatics, Standards organizations, Investment, De-escalation, Escalation of commitment, Software Engineering Process, Fibrinogen},
  abstract={A vast amount of resources is wasted on software projects delivering less than the planned benefits. The objective of this paper is to investigate the tendency to continue a project even when it is evident that it will not provide the expected benefits, often referred to as “project escalation” or “escalation of commitment.” We aim to identify factors that empirically have been found to promote software project escalation. We examined 1376 papers related to the phenomenon of escalating commitment to software projects and found that 44 of them included relevant empirical research. After reviewing these papers, we synthesized the results. We provide an overview of 46 factors that have been found to promote software project escalation. Thirteen of the factors were project-related, 20 were psychological factors, nine were social factors, and four were structural factors related to the project's contextual dimensions. We contribute to practice by systemizing empirical evidence of software project escalation that can be of help in avoiding it in the first place and uncovering already escalated situations. As most studies are investigations of only a few factors, we propose further research to study how factors potentially interplay as they contribute to escalation of software projects.}
}

@article{rayyan-727967935,
  title={A systematic review on test suite reduction: Approaches, experiment's quality evaluation, and guidelines},
  year={2018},
  journal={IEEE Access},
  issn={2169-3536},
  volume={6},
  pages={11816-11841},
  author={Rehman Khan, Saif Ur and Lee, Sai Peck and Javaid, Nadeem and Abdul, Wadood},
  keywords={Software testing, Systematics, Guidelines, Optimization, Testing, Web services, Software systems, experiments, Clustering algorithms, guidelines, regression testing, test suite reduction},
  abstract={Regression testing aims at testing a system under test (SUT) in the presence of changes. As a SUT changes, the number of test cases increases to handle the modifications, and ultimately, it becomes practically impossible to execute all of them within limited testing budget. Test suite reduction (TSR) approaches are widely used to improve the regression testing costs by selecting representative test suite without compromising effectiveness, such as fault-detection capability, within allowed time budget. The aim of this systematic review is to identify state-of-the-art TSR approaches categories, assess the quality of experiments reported on this subject, and provide a set of guidelines for conducting future experiments in this area of research. After applying a two-facet study selection procedure, we finalized 113 most relevant studies from an initial pool of 4230 papers published in the field of TSR between 1993 and 2016. The TSR approaches are broadly classified into four main categories based on the literature including greedy, clustering, search, and hybrid approaches. It is noted that majority of the experiments in TSR do not follow any specific guidelines for planning, conducting, and reporting the experiments, which may pose validity threats related to their results. Thus, we recommend conducting experiments that are better designed for the future. In this direction, an initial set of recommendations is provided that are useful for performing well-designed experiments in the field of TSR. Furthermore, we provide a number of future research directions based on current trends in this field of research.}
}

@article{rayyan-727967936,
  title={Situational factors affecting requirement engineering process in global software development},
  year={2013},
  pages={118-122},
  author={Khan, Huma Hayat and bin Mahrin, Mohd. Naz'ri and bt Chuprat, Suriayati},
  keywords={Software, Software engineering, Context, Organizations, Conferences, Standards organizations, Global Software Development (GSD), Open systems, process definition, Software Requirement Engineering (RE)},
  abstract={A most favorable Requirement Engineering process is considered to be subject of situational characteristics of Software Requirement Engineering settings. These characteristics are related to organizations, project, process, requirements, stakeholders etc. However, list of situational factors affecting the Requirement Engineering process during Global Software Development is presently not available. The lack of such study is challenging as it not only restrain the ability to improve Requirement Engineering process, but also it can undermine the competence of Requirement Engineering team to determine the core constraints and characteristics of developing software. To address this scarcity, we have merged a considerable related research into an initial list of situational factors that affect the Requirement Engineering Process during Global Software Development. We have performed Systematic Literature Review for situational factors identification. To carry on data merging, we have used thorough data coding techniques adopted from Grounded Theory. The initial list of situational factors consists of 37 factors, which is the sound initial reference list for Requirement Engineering process definition in Global Software Development environment. The outcome of this study symbolizes a significant contribution to the Requirement Engineering body of knowledge.}
}

@article{rayyan-727967937,
  title={Synthesizing a comprehensive framework for lean software development},
  year={2013},
  pages={1-8},
  author={Jonsson, Henrik and Larsson, Stig and Punnekkat, Sasikumar},
  keywords={Software, Systematics, Systematic Literature Review, State of the art, Software Process Improvement, Organizations, Product development, Concrete, Lean production, Lean Software Development},
  abstract={Lean principles, originating from Japanese automotive industry, are anticipated to be useful to improve software development processes. Albeit its popularity there is still no generally accepted, clear and detailed definition of what lean software development actually means. This makes it difficult to perform research on the effects of lean software development and determine its usefulness in various contexts. To fill in that research gap this paper analyzes the state of the art based on twenty key Lean concepts derived from nine seminal sources identified in a systematic literature review. The original explanations of the key concepts have been elaborated further and synthesized into a framework for lean software development consisting of a set of goals, recommended activities and practices. The detailed results for the key concept Value are reported. The proposed framework is expected to serve as a basis for further research and for Lean assessment of organizations.}
}

@article{rayyan-727967938,
  title={Are you sure you are happy?},
  year={2018},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={16},
  number={4},
  pages={1213-1218},
  author={Barros-Justo, Jose Luis and Zapata, Sergio and Martinez-Araujo, Nelson},
  keywords={Software, Software Engineering, Software engineering, Systematics, Measurement, Metrics, Empirical studies, Silicon compounds, IEEE transactions, Annotated review, Employment, Happiness},
  abstract={Context: The study of emotions, especially those that promote an apparent increase in performance at the workplace, has received considerable attention since the early days of the industrial revolution. In the much newer area of software engineering, these studies include affections, emotions, motivation and happiness, among other feelings. However, there are still many fallacies, misconceptions and ambiguous definitions, which prevent a useful assessment of these issues. Objective: the aim of this study is to summarize the existing definitions of happiness as well as the appropriate metrics to assess its level on software engineers. Method: a systematic search in several online electronic data sources, complemented with snowballing techniques, to identify and select published literature, supporting happiness research in software engineering. We reported the findings as an annotated literature review. Results: The search strategies retrieved 619 publications, from which we selected 18 primary works. These works were grouped into three categories, according to their focus: a) definition of happiness, b) metrics to measure happiness, and c) general purpose articles. The review provides a description of 8 articles, 4 in the first category and 4 in the second. Conclusions: Research on "happiness at work" has its own issues and challenges, as reported in the articles in this review. Our work can contribute by shedding light on these challenges, highlighting the precise definitions and showing the appropriate metrics to assess them. There is still too much to do since research shows a high degree of divergence in the results.}
}

@article{rayyan-727967939,
  title={Use of augmented reality for social communication skills in children and adolescents with autism spectrum disorder (ASD): A systematic review},
  year={2019},
  pages={1-7},
  author={Khowaja, Kamran and Al-Thani, Dena and Banire, Bilikis and Salim, Siti Salwah and Shah, Asadullah},
  keywords={virtual reality, technology, autism spectrum disorder, computer, data collection, intervention, mixed reality, research design, smartphone, social communication, augmented reality, Child Development Disorders, Pervasive, Adolescent, Autistic Disorder},
  abstract={The purpose of this review is to provide a systematic analysis of studies investigating the use of augmented reality (AR) to improve the social communications skills of individuals (children and adults) diagnosed with autism spectrum disorder (ASD). This review synthesizes the AR technology, research design, data collection methods, settings, and intervention outcomes. Across the studies, the effect of using AR towards the learning of individuals was positive. However, given the wide variety of skills targeted in the shortlisted studies, and the heterogeneity of the participants, a summative conclusion regarding the effectiveness of AR for teaching social communication skills to an individual with ASD based on the existing literature is not possible. Future research addressing this area as well as the relative effectiveness of AR among more participants, different technologies supporting AR, its intervention, and the evaluation in the classroom environment is warranted.}
}

@article{rayyan-727967940,
  title={mHealth technologies for chronic diseases and elders: A systematic review},
  year={2013},
  journal={IEEE Journal on Selected Areas in Communications},
  issn={1558-0008},
  volume={31},
  number={9},
  pages={6-18},
  author={Chiarini, Giovanni and Ray, Pradeep and Akter, Shahriar and Masella, Cristina and Ganz, Aura},
  keywords={systematic review, Systematics, Databases, taxonomy, Smart phones, Diseases, Mobile communication, Senior citizens, Chronic, elderly, mobile health, technologies, ubiquitous health, Chronic Disease},
  abstract={mHealth (healthcare using mobile wireless technologies) has the potential to improve healthcare and the quality of life for elderly and chronic patients. Many studies from all over the world have addressed this issue in view of the aging population in many countries. However, there has been a lack of any consolidated evidence-based study to classify mHealth from the dual perspectives of healthcare and technology. This paper reports the results of an evidence-based study of mHealth solutions for chronic care amongst the elderly population and proposes a taxonomy of a broad range of mHealth solutions from the perspective of technological complexity. A systematic literature review was conducted over 10 online databases and the findings were classified into four categories of predominant mHealth solutions, that is, self-healthcare, assisted healthcare, supervised healthcare and continuous monitoring. The findings of the study have major implications for information management and policy development in the context of the Millennium Development Goals (MDGs) related to healthcare in the world.}
}

@article{rayyan-727967941,
  title={Mobile cloud computing for disaster emergency operation: A systematic review},
  year={2015},
  pages={1-8},
  author={Geumpana, Teuku Aulia and Rabhi, Fethi and Lewis, John and Ray, Pradeep K and Zhu, Liming},
  keywords={systematic review, Systematics, Cloud computing, Mobile applications, Context, Reliability, Computer science, reliability, Mobile communication, availability, disaster emergency technology, MCC, mobile cloud solution, Disasters, Emergencies},
  abstract={The advancement of mobile cloud computing (MCC) has the potential to improve communication and information during disaster emergency operation. Many studies from different countries have addressed different approaches to implement mobile cloud technologies during disaster operation. However, there has been a lack of any consolidated evidence-based study to evaluate the MCC from the dual perspectives of disaster emergency situation and technology. In this paper, we provide an extensive study of mobile cloud computing research as a critical system or application based on the latest literature published from 2010 to 2015, and highlights the specific challenges in implementing mobile cloud computing in disaster emergency operation. A systematic literature review was conducted over 4 prominent journals of computer science engineering (IEEE, ACM, ProQuest, Inspec/Elsevier) and the findings were classified into four key challenges of MCC implementation during disaster emergency operation. We present a taxonomy based on the key challenges in this area, and discuss the different solutions taken to solve the challenges. We conclude the paper with a critical analysis of the challenges that have not been solved, and highlight directions for future work.}
}

@article{rayyan-727967942,
  title={Architectural approaches for implementing clinical decision support systems in cloud: A systematic review},
  year={2016},
  pages={42-47},
  author={Tabares, Luis and Hernandez, Jhonatan and Cabezas, Ivan},
  keywords={systematic review, Systematics, Data mining, Cloud computing, Databases, cloud computing, Medical services, Decision support systems, software architecture, clinical decision support systems, e-health, health care, Decision Support Systems, Clinical},
  abstract={Clinical Decision Support Systems (CDSS) were explicitly introduced in the 90's with the aim of providing knowledge to clinicians in order to influence its decisions and, therefore, improve patients' health care. There are different architectural approaches for implementing CDSS. Some of these approaches are based on cloud computing, which provides on-demand computing resources over internet. The goal of this paper is to determine and discuss key issues and approaches involving architectural designs in implementing a CDSS using cloud computing. To this end, we performed a standard Systematic Literature Review (SLR) of primary studies showing the intervention of cloud computing on CDSS implementations. Twenty-one primary studies were reviewed. We found that CDSS architectural components are similar in most of studies. Cloud-based CDSS are most used in Home Healthcare and Emergency Medical Systems. Alerts/Reminders and Knowledge Service are the most common implementations. Major challenges are around security, performance and compatibility. We concluded on the benefits of implementing a cloud-based CDSS, since it allows cost-efficient, ubiquitous and elastic computing resources. We highlight that some studies show weaknesses regarding the conceptualization of a cloud-based computing approach and lack of a formal methodology in the architectural design process.}
}

@article{rayyan-727967943,
  title={Software project economics: a roadmap},
  year={2007},
  pages={304-315},
  author={Shepperd, Martin},
  keywords={Bibliometrics, Software engineering, Software measurement, Scheduling, Productivity, Business, Computer science, Software systems, Costs, Economic forecasting, Software},
  abstract={The objective of this paper is to consider research progress in the field of software project economics with a view to identifying important challenges and promising research directions. I argue that this is an important sub-discipline since this will underpin any cost-benefit analysis used to justify the resourcing, or otherwise, of a software project. To accomplish this I conducted a bibliometric analysis of peer reviewed research articles to identify major areas of activity. My results indicate that the primary goal of more accurate cost prediction systems remains largely unachieved. However, there are a number of new and promising avenues of research including: how we can combine results from primary studies, integration of multiple predictions and applying greater emphasis upon the human aspects of prediction tasks. I conclude that the field is likely to remain very challenging due to the people-centric nature of software engineering, since it is in essence a design task. Nevertheless the need for good economic models will grow rather than diminish as software becomes increasingly ubiquitous.}
}

@article{rayyan-727967944,
  title={Factors limiting industrial adoption of test driven development: A systematic review},
  year={2011},
  pages={337-346},
  author={Causevic, Adnan and Sundmark, Daniel and Punnekkat, Sasikumar},
  keywords={systematic review, Systematics, Data mining, Testing, Databases, Programming, empirical studies, agile software development, Protocols, Limiting, Test driven developmen, unit testing},
  abstract={Test driven development (TDD) is one of the basic practices of agile software development and both academia and practitioners claim that TDD, to a certain extent, improves the quality of the code produced by developers. However, recent results suggest that this practice is not followed to the extent preferred by industry. In order to pinpoint specific obstacles limiting its industrial adoption we have conducted a systematic literature review on empirical studies explicitly focusing on TDD as well as indirectly addressing TDD. Our review has identified seven limiting factors viz., increased development time, insufficient TDD experience/knowledge, lack of upfront design, domain and tool specific issues, lack of developer skill in writing test cases, insufficient adherence to TDD protocol, and legacy code. The results of this study is of special importance to the testing community, since it outlines the direction for further detailed scientific investigations as well as highlights the requirement of guidelines to overcome these limiting factors for successful industrial adoption of TDD.}
}

@article{rayyan-727967945,
  title={Providing a consensus definition for the term "Smart Product"},
  year={2013},
  pages={203-211},
  author={Gutiérrez, César and Garbajosa, Juan and Diaz, Jessica and Yagüe, Agustin},
  keywords={Software, systematic literature review, Systematics, SLR, Quality assessment, Ontologies, Context, Industries, Technological innovation, innovation, intelligent product, smart product, smart thing, software engineering innovation, thematic synthesis},
  abstract={The term "Smart Product" has become commonly used in recent years. This is because there has been an increasing interest in these kinds of products as part of the consumer goods industry, impacting everyday life and industry. Nevertheless, the term "Smart Product" is used with different meanings in different contexts and application domains. The use of the term "Smart Product" with different meanings and underlying semantics can create important misunderstandings and dissent. The aim of this paper is to analyze the different definitions of Smart Product available in the literature, and to explore and analyze their commonalities and differences, in order to provide a consensus definition that satisfies, and can therefore be used by, all parties. To embrace the identified definitions, the concept of "Smart Thing" is introduced. The methodology used was a systematic literature review. The definition is expressed as an ontology.}
}

@article{rayyan-727967946,
  title={A review of effort estimation studies in agile, iterative and incremental software development},
  year={2013},
  pages={27-30},
  author={Nguyen-Cong, Danh and Tran-Cao, De},
  keywords={systematic review, Software, Software engineering, Measurement, Estimation, XP, Planning, Computational modeling, agile, scrum, Data models, effort estimation, cost estimation, incremental software development, iterative software development, RUP, size estimation},
  abstract={This paper presents the results of systematically reviewing the current research literature on effort estimation in agile, iterative and incremental software projects (AIISPs). The primary purpose is to show evidences about common trends, gaps and recommendations for future studies. The results were limited to peer-reviewed conference papers/journal articles, written in English and published before 2013. The synthesis was made through classifying the papers based on different properties. The analysis indicates the need for future research on: 1) estimating effort for the remaining AIISPs, 2) analyzing the impact of properties of historical/current project data, 3) using benchmark data, 4) using composite models and 5) further empirical validation of the estimation models.}
}

@article{rayyan-727967947,
  title={Research landscape of patterns and architectures for IoT security: A systematic review},
  year={2020},
  pages={463-470},
  author={Rajmohan, Tanusan and Nguyen, Phu H and Ferry, Nicolas},
  keywords={Software, Systematics, Security, Internet of Things, Survey, Databases, Privacy, IoT, Architecture, Computer architecture, Patterns},
  abstract={We have entered a tremendous computerized rev-olution of the Internet of Things (IoT) era when everything is connected. The popularity of IoT systems makes security for the IoT of paramount importance. Security patterns consist of domain-independent time-proven security knowledge and expertise. Would they be applicable to develop secure IoT systems? We aim to draw a research landscape of patterns and architectures for IoT security by conducting a systematic literature review. From more than a thousand of candidate papers, we have systematically distinguished and analyzed twenty-two (22) papers that have been published around patterns and architectures for IoT security (and privacy). Our analysis shows a rise in the number of publications tending to security patterns and architectures in the last two years. Within this rise, we see that most patterns and architectures are applicable for all IoT systems, while some are limited within specific domains. However, there are gaps in this research area that can be filled in to promote the utilization of patterns for IoT security and privacy.}
}

@article{rayyan-727967948,
  title={Towards the guidelines for requirements change management in global software development: Client-vendor perspective},
  year={2019},
  journal={IEEE Access},
  issn={2169-3536},
  volume={7},
  pages={76985-77007},
  author={Akbar, Muhammad Azeem and Sang, Jun and Nasrullah and Khan, Arif Ali and Shafiq, Muhammad and Fazal-E-Amin},
  keywords={Software, Systematics, Global software development (GSD), systematic literature review (SLR), Organizations, Best practices, Standards organizations, Capability maturity model, client, requirements change management (RCM), vendor, best practices, empirical investigation},
  abstract={Currently, being deployed by organizations to develop high-quality software at a low cost, global software development (GSD) faces many challenges that make development activities more complex. These GSD challenges are mainly concerned with requirements to change management (RCM). RCM plays a key role in the successful execution of software projects. The objective of this paper is to identify the best practices of the RCM process by adopting a systematic literature review (SLR) and validate them using questionnaire survey with industry experts. A total of 46 best practices were identified through SLR and validated with industry experts. We have further classified the identified practices in the domain of client and vendor GSD organizations with the aim to provide a clear understanding of the RCM best practices in the context of both types of GSD organizations (client, vendor). Moreover, we have conducted a comparison analysis between SLR and questionnaire survey data and found a moderate positive correlation in the ranks of both data sets (rs = 0.522, p=0.003). In addition, the criticality of the identified best practices was assessed using the criteria of a practice having frequency ≥50%. The findings of this paper provide a framework that could help the GSD organizations to address the problems related to RCM in GSD environment.}
}

@article{rayyan-727967949,
  title={A systematic mapping study on the open source software development process},
  year={2012},
  pages={42-46},
  author={Acuña, Silvia T and Castro, John W and Dieste, Oscar and Juristo, Natalia},
  keywords={software engineering, systematic mapping study, open source software development, Software},
  abstract={Background: There is no globally accepted open source software development process to define how open source software is developed in practice. A process description is important for coordinating all the software development activities involving both people and technology. Aim: The research question that this study sets out to answer is: What activities do open source software process models contain? The activity groups on which it focuses are Concept Exploration, Software Requirements, Design, Maintenance and Evaluation. Method: We conduct a systematic mapping study (SMS). A SMS is a form of systematic literature review that aims to identify and classify available research papers concerning a particular issue. Results: We located a total of 29 primary studies, which we categorized by the open source software project that they examine and by activity types (Concept Exploration, Software Requirements, Design, Maintenance and Evaluation). The activities present in most of the open source software development processes were Execute Tests and Conduct Reviews, which belong to the Evaluation activities group. Maintenance is the only group that has primary studies addressing all the activities that it contains. Conclusions: The primary studies located by the SMS are the starting point for analyzing the open source software development process and proposing a process model for this community. The papers in our paper pool that describe a specific open source software project provide more regarding our research question than the papers that talk about open source software development without referring to a specific open source software project.}
}

@article{rayyan-727967950,
  title={A systematic review on methods and techniques for optimizing assistive virtual keyboards},
  year={2015},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={13},
  number={8},
  pages={2687-2693},
  author={Sousa Gomide, Renato and Loja, Luiz Fernando and Lucia Flores, Edna and Pinto Lemos, Rodrigo and de Carvalho, Sirlon and de Matos Silva, Rafael},
  keywords={Software, Internet, Systematics, Optimization, Systematic Review, Portals, IEEE Xplore, Alternative and Augmentative Communication, Keyboards, Speech, Type Performance, Virtual Keyboards},
  abstract={Locked-In Syndrome is admittedly the worst case of motor and speech impairment, seriously damaging the ability of oral and gestural communication of patients. In recent years, alternative and augmentative communication technology has provided resources to restore these patients' ability to communicate. Among the various methods and existing communication software, we can highlight the virtual keyboard. However, the data entry for these keyboards is considerably slower than entering information through physical keyboards. In order to identify the main approaches employed to optimize the data entry on virtual keyboards, we performed a systematic review by searching for papers in databases. As a result, we found 250 articles and selected 31 papers among them to compose this research. Analyzing these articles, we identified five methods for optimizing data entry performance and 15 ways of implementing them. In addition, this review contains a critical analysis regarding the optimization methods and assistive virtual keyboards.}
}

@article{rayyan-727967951,
  title={Computational games in STEM courses: a systematic review of the literature},
  year={2020},
  pages={1-8},
  author={da Silva Neves Lima, Priscila and das Almas Silva, Laira and dos Santos Oliveira, João Lucas and Franco Brandão, Anarosa Alves and de Oliveira Brandão, Leônidas},
  keywords={Systematic review, Databases, Education, Games, Physics, Planning, Programming profession, Computational, digital, Higher education, STEM, STEM education},
  abstract={This full paper of Research Category presents the main results of a systematic review of the literature about the use of computer games in science, technology, engineering, and mathematics (STEM) courses. The concept of lifelong learning has often been encouraged in literature. However, it also reports students facing difficulties in exact sciences since elementary school: a significant challenge to overcome. Among the reasons for such difficulties are demotivation, disinterest, learning difficulties, and even the use of outdated teaching-learning methods. In an attempt to improve the relationship between students and exact sciences, schools are expanding the use of information technologies to offer the students interactive environments in order to enrich their classes. In this context, characteristics of digital games appear as a didactic resource that can benefit the students' learning process. If properly used, digital games can stimulate memory, creativity, socialization, and also incite curiosity. Due to the ease young students have with games and considering the benefits mentioned above, many institutions have been investing in digital games (or environments with some of their characteristics). However, these games can generate compulsive behaviors (WHO classified such disorders in 2018), and it is worth noticing that many articles reporting the use of games in education focus mainly on its acceptance instead of its teaching capabilities. In this article, we report the main findings, such as that technology is the area with the highest concentration of digital games. It is also observed that different guiding theories appear, such as those with a constructivist tendency. Among the arguments for using games stand out: the gain in cognitive skills, the possibility of using simulations, and the ease in understanding complex themes. The methods of didactic-pedagogical evaluation mostly used are questionnaires of acceptance or the student's perception. This review highlights the potential of digital games to promote learning. However, these games should not be focused solely on their ludic aspect, as they have a different purpose from regular games. Active methodologies mediated by information and communication technologies can make class more engaging as students actively participate in the construction of student learning.}
}

@article{rayyan-727967952,
  title={A user evaluation of process discovery algorithms in a software engineering company},
  year={2019},
  pages={142-150},
  author={Agostinelli, Simone and Maggi, Fabrizio Maria and Marrella, Andrea and Milani, Fredrik},
  keywords={Systematics, Data mining, Tools, Complexity theory, Software algorithms, Companies, Process Mining, BPMN, Process Discovery, User Evaluation, Software, Algorithms},
  abstract={Process mining methods allow analysts to use logs of historical executions of business processes in order to gain knowledge about the actual behavior of these processes. One of the most widely studied process mining operations is automated process discovery. An event log is taken as input by an automated process discovery method and produces a business process model as output that captures the control-flow relations between tasks that are described by the event log. In this setting, this paper provides a systematic comparative evaluation of existing implementations of automated process discovery methods with domain experts by using a real life event log extracted from an international software engineering company and four quality metrics: understandability, correctness, precision, and usefulness. The evaluation results highlight gaps and unexplored trade-offs in the field and allow researchers to improve the lacks in the automated process discovery methods in terms of usability of process discovery techniques in industry.}
}

@article{rayyan-727967953,
  title={Implementation of case-method cycle for case-based reasoning in human medical health: A systematic review},
  year={2019},
  pages={1-6},
  author={Elisabet, Damayanti and Sensuse, Dana Indra and Al Hakim, Shidiq},
  keywords={knowledge management, health care, case-based reasoning, case-method cycle, medical health, Humanities, Humanism, Humans},
  abstract={Reasoning diagnosis and treatments in human medical health fields are a complicated task. While the effort of doctors, physicians, or clinicians to study medical journals and another source of knowledge is hardly done. Studies tried to design and develop a medical health system that imitates the way humans determine diagnoses by studying past cases and medical journals using case-based reasoning (CBR). This study reviews research publications in human medical health that used CBR using the Kitchenham method. The result of this study is identification challenge developing the human medical health system and the reason for using CBR, identification kinds of knowledge that have been used and methods for capturing knowledge, and identification of implementation using CBR in human medical health.}
}

@article{rayyan-727967954,
  title={Effects of mindfulness on conceptual modeling performance: a series of experiments},
  year={2020},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  pages={1},
  author={Bernardez, Beatriz and Duran Toro, Amador and Parejo Maestre, Jose Antonio and Juristo, Natalia and Ruiz-Cortes, Antonio},
  keywords={Software engineering, Task analysis, Productivity, Education, Software Engineering Education, Focusing, Conceptual Modeling, Experiment Replication, Family of Experiments, Mindfulness, Public speaking, Stress},
  abstract={Context. Mindfulness is a meditation technique whose main goal is keeping the mind calm and educating attention by focusing only on one thing at a time, usually breathing. The reported benefits of its continued practice can be of interest for Software Engineering students and practitioners, especially in tasks like conceptual modeling, in which concentration and clearness of mind are crucial. Goal. In order to evaluate whether Software Engineering students enhance their conceptual modeling performance after several weeks of mindfulness practice, a series of three controlled experiments were carried out at the University of Seville during three consecutive academic years (2013—2016) involving 130 students. Method. In all the experiments, the subjects were divided into two groups. While the experimental group practiced mindfulness, the control group was trained in public speaking as a placebo treatment. All the subjects developed two conceptual models based on a transcript of an interview, one before and another one after the treatment. The results were compared in terms of conceptual modeling quality (measured as effectiveness, i.e. the percentage of model elements correctly identified) and productivity (measured as efficiency, i.e. the number of model elements correctly identified per unit of time). Results. The statistically significant results of the series of experiments revealed that the subjects who practiced mindfulness developed slightly better conceptual models (their quality was 8.16% higher) and they did it faster (they were 46.67% more productive) than the control group, even if they did not have a previous interest in meditation. Conclusions. The practice of mindfulness improves the performance of Software Engineering students in conceptual modeling, especially their productivity. Nevertheless, more experimentation is needed in order to confirm the outcomes in other Software Engineering tasks and populations.}
}

@article{rayyan-727967955,
  title={Serious game, gamified applications, educational software: A comparative study},
  year={2019},
  pages={55-62},
  author={Carrion, Mayra and Santórum, Marco and Flores, Harold and Aguilar, Jose and Perez, Maria},
  keywords={Software, systematic literature review, Systematics, Bibliographies, Education, education, gamification, serious games, educational software, Systems engineering and theory},
  abstract={This article presents a comparative study of three concepts, beginning with its characteristic elements: serious games, gamified application, and educational software. This research arises from the need to clarify the differences and similarities that exist between the concepts to be studied. Kitchenham and Charters' methodology is used for the systematic reviews of the software engineering literature. To organize the characteristic elements of the criteria to be compared, we used the Design, Play, and Experience (DPE) framework, and adapted it to encompass the features of serious games, educational software, and gamified applications. In general, a total of nine computer applications were analyzed to verify if they are correctly using the definitions in this context, two examples are presented in this study.}
}

@article{rayyan-727967956,
  title={Review of architectural patterns and tactics for microservices in academic and industrial literature},
  year={2018},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={16},
  number={9},
  pages={2321-2327},
  author={Marquez, Gaston and Osses, Felipe and Astudillo, Hernan},
  keywords={Software, Systematic literature review, Systematics, industry, Monitoring, Microservices, Taxonomy, Architectural patterns, taxonomy, Google, Silicon compounds, IEEE transactions, Academy, Architectural tactics},
  abstract={Microservices are an emerging trend for development of service-oriented software. This approach proposes to build each application as a collection of small services running on separate process and inter-communicating with lightweight mechanisms. Systematic development of microservices is hampered by the lack of a catalog of emerging recurrent architectural solutions (architectural patterns) and design decisions (architectural tactics). This article describes a systematic review of academic and industrial literature regarding architectural patterns and architectural tactics for microservices. The review yield 44 architectural patterns in academic sources and 74 in industrial ones, as well as a few architectural tactics originally proposed to address related problems. Most architectural patterns and tactics are associated to one of just five quality attributes: scalability, flexibility, testability, performance, and elasticity. Also, most microservices in academic (but not industrial) literature are related to DevOps and IoT. The findings lead to propose a new taxonomy of microservice architectural patterns.}
}

@article{rayyan-727967957,
  title={MeRinde process model adaptation with Requirements Engineering techniques sopported by Free Software tools},
  year={2013},
  pages={1-12},
  author={Salazar, G Carmen R and Losavio, Francisca and Matteo, Alfredo},
  keywords={Systematics, Context modeling, Requirements Engineering, Systematic Review, Unified modeling language, Software tools, Computational modeling, Adaptation models, Free Software, MeRinde, SPEM 2.0, Software},
  abstract={MeRinde (Metodología de la Red Nacional de Integración y Desarrollo de Software Libre) developed by CNTI (Centro Nacional de Tecnologías de Información), proposes an open standard for software development to support the implementation of Venezuelan State's Decree 3390 on the use of Free Software for governmental projects. With respect to the requirements discipline, the MeRinde process model only uses UML diagrams, and does not offer any guidelines for the Requirements Engineering (capture, analysis, specification, and validation) overall process. This paper proposes an adaptation of the MeRinde requirements discipline, incorporating the use of requirements engineering techniques to complement the final products. Moreover, available free software tools that support the selected techniques are analyzed and discussed using the Systematic Review methodology. The MeRinde adapted process model is described using SPEM 2.0 notation.}
}

@article{rayyan-727967958,
  title={Investigating key areas of research in crowdsourcing software development},
  year={2017},
  pages={1-5},
  author={Sharma, Shruti and Hasteer, Nitasha and Van Belle, Jean-Paul},
  keywords={Software, Software engineering, Systematics, Crowdsourcing, Task analysis, Systematic Review, Information systems, Conferences, Crowdsourced platforms, Crowdsourcing Software Development},
  abstract={Crowdsourcing Software Development implies outsourcing software development to crowd through an open call. This way of development is emerging out to be beneficial for software development organisations. It involves task decomposition and broader participation by way of which we get effective and diverse solutions. This is carried out by an open call invitation to the crowd who voluntarily participate in this activity. We have carried out a systematic review of the literature to identify focus areas of the field. In this work we report the key areas of research in this emerging software development paradigm.}
}

@article{rayyan-727967959,
  title={An empirical study on heterogeneous defect prediction approaches},
  year={2020},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  pages={1},
  author={Chen, Haowen and Jing, Xiao-Yuan and Li, Zhiqiang and Wu, Di and Peng, Yi and Huang, Zhiguo},
  keywords={Software, Measurement, empirical study, Buildings, Libraries, Predictive models, Data models, cross-project, Heterogeneous defect prediction, metric selection, metric transformation, NASA},
  abstract={Software defect prediction has always been a hot research topic in the field of software engineering owing to its capability of allocating limited resources reasonably. Compared with cross-project defect prediction (CPDP), heterogeneous defect prediction (HDP) further relaxes the limitation of defect data used for prediction, permitting different metric sets to be contained in the source and target projects. However, there is still a lack of a holistic understanding of existing HDP studies due to different evaluation strategies and experimental settings. In this paper, we provide an empirical study on HDP approaches. We review the research status systematically and compare the HDP approaches proposed from 2014 to June 2018. Furthermore, we also investigate the feasibility of HDP approaches in CPDP. Through extensive experiments on 30 projects from five datasets, we have the following findings: (1) metric transformation-based HDP approaches usually result in better prediction effects, while metric selection-based approaches have better interpretability. Overall, the HDP approach proposed by Li et al. (CTKCCA) currently has the best performance. (2) Handling class imbalance problems can boost the prediction effects, but the improvements are usually limited. In addition, utilizing mixed project data cannot improve the performance of HDP approaches consistently since the label information in the target project is not used effectively. (3) HDP approaches are feasible for cross-project defect prediction in which the source and target projects have the same metric set.}
}

@article{rayyan-727967960,
  title={A visual text mining approach for systematic reviews},
  year={2007},
  pages={245-254},
  author={Malheiros, Viviane and Hohn, Erika and Pinho, Roberto and Mendonca, Manoel and Maldonado, Jose Carlos},
  keywords={Humans, Software engineering, Software measurement, Text mining, Decision making, Programming, Software development management, Costs, Technology management, Stress},
  abstract={The software engineering research community has been adopting systematic reviews as an unbiased and fair way to assess a research topic. Despite encouraging early results, a systematic review process can be time consuming and hard to conduct. Thus, tools that help on its planning or execution are needed. This article suggests the use of visual text mining (VTM) to aid systematic reviews. A feasibility study was conducted comparing the proposed approach with a manual process. We observed that VTM can contribute to systematic review and we propose a new strategy called VTM-Based systematic review.}
}

@article{rayyan-727967961,
  title={A systematic review of the application and empirical investigation of search-based test case generation},
  year={2010},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={36},
  number={6},
  pages={742-762},
  author={Ali, Shaukat and Briand, Lionel C and Hemmati, Hadi and Panesar-Walawege, Rajwinder Kaur},
  keywords={Software testing, Guidelines, Automation, Automatic testing, System testing, Scalability, Genetic algorithms, Costs, Algorithm design and analysis, Evolutionary computing and genetic algorithms, frameworks, heuristics design, Logic testing, review and evaluation, test generation, testing strategies, validation.},
  abstract={Metaheuristic search techniques have been extensively used to automate the process of generating test cases, and thus providing solutions for a more cost-effective testing process. This approach to test automation, often coined “Search-based Software Testing” (SBST), has been used for a wide variety of test case generation purposes. Since SBST techniques are heuristic by nature, they must be empirically investigated in terms of how costly and effective they are at reaching their test objectives and whether they scale up to realistic development artifacts. However, approaches to empirically study SBST techniques have shown wide variation in the literature. This paper presents the results of a systematic, comprehensive review that aims at characterizing how empirical studies have been designed to investigate SBST cost-effectiveness and what empirical evidence is available in the literature regarding SBST cost-effectiveness and scalability. We also provide a framework that drives the data collection process of this systematic review and can be the starting point of guidelines on how SBST techniques can be empirically assessed. The intent is to aid future researchers doing empirical studies in SBST by providing an unbiased view of the body of empirical evidence and by guiding them in performing well-designed and executed empirical studies.}
}

@article{rayyan-727967962,
  title={Understanding the successes and challenges of model-driven software engineering - a comprehensive systematic mapping},
  year={2018},
  pages={129-138},
  author={Gottardi, Thiago and Vaccare Braga, Rosana Teresinha},
  keywords={Software, Systematics, Tools, Challenges, Review, Secondary Study, Protocols, Runtime, Systematic Mapping, Domains, Model-Driven Software Engineering, Model-Oriented, Models at Run-time, Object recognition, Software Domains},
  abstract={Model-Driven Software Engineering (MDSE) is a development method in which models are used to generate software. Despite documented advantages, projects employing MDSE may fail due to development challenges. In this paper, we study and document these challenges by conducting an up-to-date systematic mapping that goes beyond reviewing MDSE studies: we also include two derived paradigms (Model-Oriented Programming and Models at Run-time). Therefore, we present a systematic mapping with two objectives: The first objective was to identify specific domains in which MDSE is successful, while the second objective was to identify what are the challenges to apply this methodology to general purpose development processes. Following the review of 4859 studies (3727 are unique), we have identified the application and technological domains in which MDSE projects are more likely to succeed. We also discuss challenges presented by 17 primary studies. The analysis of the results indicate that MDSE application is consolidated in specific domains. A common feature identified among studies related to general purpose MDSE processes is that, initially, authors reported lack of proper methods and training. After new techniques have risen, it has been pointed that MDSE projects still face maintenance problems that can discourage their usage in other domains.}
}

@article{rayyan-727967963,
  title={Identifying risks of software project management in Global Software Development: An integrative framework},
  year={2016},
  pages={1-7},
  author={Chadli, Saad Yasser and Idri, Ali and Fernández-Alemán, José Luis and Ros, Joaquín Nicolás and Toval, Ambrosio},
  keywords={systematic review, Software, Systematics, Project management, Risk management, Context, Software Project Management, Organizations, Libraries, Global Software Development, risk identification},
  abstract={Global Software Development (GSD) poses inherent risks to projects success. Project managers are now faced with new challenges related to the geographical, temporal and socio-cultural distances between stakeholders. The objective of this research is to identify challenges associated with Software Project Management (SPM) activities in a GSD context and present an integrative framework encompassing them. Using a Systematic Literature Review (SLR), 39 risk factors were identified and later compiled into a framework inspired from the model of organizational change.}
}

@article{rayyan-727967964,
  title={The use of artificial neural networks in network intrusion detection: A systematic review},
  year={2018},
  pages={1-6},
  author={ÖNEY, Mehmet Uğur and PEKER, Serhat},
  keywords={Systematics, Bibliographies, Data mining, Search problems, Databases, Neural networks, Literature Review, Systematic Mapping, ANNs, Intrusion detection, Network Intrusion Detection, Neural Networks, Nerve Net, Neural Networks (Computer)},
  abstract={Network intrusion detection is an important research field and artificial neural networks have become increasingly popular in this subject. Despite this, there is a lack of systematic literature review on that issue. In this manner, the aim of this study to examine the studies concerning the application artificial neural network approaches in network intrusion detection to determine the general trends. For this purpose, the articles published within the last decade from 2008 to 2018 were systematically reviewed and 43 articles were retrieved from commonly used databases by using a search strategy. Then, these selected papers were classified by the publication type, the year of publication, the type of the neural network architectures they employed, and the dataset they used. The results indicate that there is a rising trend in the usage of ANN approaches in the network intrusion detection with the gaining popularity of deep neural networks in recent years. Moreover, the KDD'99 dataset is the most commonly used dataset in the studies of network intrusion detection using ANNs. We hope that this paper provides a roadmap to guide future research on network intrusion detection using ANNs.}
}

@article{rayyan-727967965,
  title={Interoperability for model-driven development: Current state and future challenges},
  year={2012},
  pages={1-10},
  author={Giachetti, Giovanni and Valverde, Francisco and Marín, Beatriz},
  keywords={Systematics, Systematic Literature Review, Context modeling, Context, Unified modeling language, Proposals, Standards, Interoperability Framework, Model-Driven Development, Model-Driven Interoperability, Weaving},
  abstract={Nowadays, the emergence of several model-driven development (MDD) proposals that are related to multiple domains requires the definition of proper interoperability mechanisms that facilitate the reuse of knowledge in the MDD community by taking advantage of already defined modeling languages, tools, and standards. However, there are no recent studies that cover the existent interoperability alternatives in the model-driven domain nor is there a common interoperability framework. This paper confronts this situation through a systematic analysis of recent interoperability approaches that provide relevant features for MDD processes. From this analysis, a general interoperability framework is depicted, which is complemented with our contributions to solve specific interoperability issues. Therefore, we present those aspects that are already covered by existent proposals as well as those pending subjects that, from our point of view, are future challenges in order to achieve a suitable interoperability framework for MDD approaches.}
}

@article{rayyan-727967966,
  title={The applicability of present estimation models to the context of mobile applications},
  year={2014},
  pages={1-6},
  author={de Souza, Laudson Silva and de Aquino, Gibeon Soares},
  keywords={Software, Software Engineering, Systematics, Estimation, Systematic Review, Context, Mobile Applications, Complexity theory, Mobile handsets, Software Quality, Mobile communication, Estimating Software, Mobile Computing},
  abstract={The growing use of mobile technologies has shown different ways to access information and interact with other computer systems. Thus, the traditional information systems are undergoing a process of adaptation to this new computing environment. Thereafter, there is a need to reassess the current knowledge on the planning and development of systems in this new environment. One area in particular that demand such adaptation is the estimation software. The estimation processes, in general, are based on characteristics of the systems, trying to quantify the complexity of implementing them. Hence, the main objective of this paper is to present a proposal for an estimation model for mobile applications, and to debate about the applicability of the traditional estimation models on this environment. Throughout the paper we analyze existing methods of estimates, identify specific features of systems for mobile devices and finally an adaptation to be proposed for this area of an existing estimation method.}
}

@article{rayyan-727967967,
  title={Analysing the concept of quality in model-driven engineering literature: A systematic review},
  year={2014},
  pages={1-12},
  author={Giraldo, Fáber D and España, Sergio and Pastor, Oscar},
  keywords={systematic review, Software, Context modeling, Context, Unified modeling language, Model quality, software quality, Proposals, Computational modeling, Data models, model-driven engineering, modelling language quality},
  abstract={The Model-Driven Engineering (MDE) research must manage a diversity of conceptions despite the global truth about the use of conceptual models as one way for representing and managing the development of complex information systems. Due to this diversity of conceptions and the multiple MDE compliance interpretations, a pletora of definitions about quality in models are emerging, each one tackling specific dimensions involved in MDE projects. In order to explore a consensus about quality in models and model-driven contexts an identification of the previous proposals for quality in models is needed. The main contribution of this work is the identification of representative trends about quality definition in MDE, and therefore, exposing the implications of the multiple quality interpretations as consequence of the diversity in MDE compliance works.}
}

@article{rayyan-727967968,
  title={Architecting for the cloud: A systematic review},
  year={2014},
  pages={312-318},
  author={Breivold, Hongyu Pei and Crnkovic, Ivica and Radosevic, Iva and Balatinac, Ivan},
  keywords={Data mining, Security, Cloud computing, Architecture, Cloud Computing, Business, Computer architecture, cloud-based architecture, concerns, Elasticity},
  abstract={Cloud Computing has emerged as a new paradigm in the field of network-based services within many industrial and application domains. The major benefits that it provides in terms of IT efficiency and business agility represent a huge competitive advantage for an organization. However, building new services in the cloud or designing cloud-based solutions into existing business context in general is a complex decision process involving many factors. In this paper, we undertake a systematic review to obtain an overview of the existing studies in designing cloud-based solutions. In particular, we investigate the main challenges and concerns when building cloud-based architectures and different architectural approaches and design considerations that are proposed in literatures to meet these specific concerns. The search strategy identified 72 studies that were catalogued as primary studies for this review after using multi-step selection process. The main challenges and concerns are classified into four main categories: security and trustworthiness, elasticity, portability and interoperability, and cloud resilience. We have also categorized studies that describe architectural approaches and design considerations when architecting for the cloud. Implications for research and practice are presented as well.}
}

@article{rayyan-727967969,
  title={Data mining in sports: A systematic review},
  year={2018},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={16},
  number={1},
  pages={232-239},
  author={Parmezan Bonidia, Robson and Duilio Brancher, Jacques and Marques Busto, Rosangela},
  keywords={Systematics, Data mining, Data Mining, Google, Libraries, Electronic mail, IEEE transactions, Training, Data Mining in Sports, Sports},
  abstract={Data mining technique has attracted attention in the information industry and society as a whole, because of the big amount of data and the imminent need to transform that data into useful information and knowledge. Recently conducted studies with successfully demarcated results using this technique, to estimate several parameters in a variety of domains. However, the effective use of data in some areas is still developing, as is the case of sports, which has shown moderate growth. In this context, the objective of this article is to present a systematic review of the literature about research involving sports data mining. As systematic searches were made out in five databases, resulting in 21 articles that answered a question that grounded this article.}
}

@article{rayyan-727967970,
  title={A systematic review on the use of LEGO® robotics in education},
  year={2018},
  pages={1-9},
  author={Souza, Isabelle M L and Andrade, Wilkerson L and Sampaio, Lívia M R and Araujo, Ana Liz Souto O},
  keywords={Systematics, Education, Computer languages, Programming environments, Educational robots, Erbium, Robotics},
  abstract={Educational Robotics (ER) has revealed several benefits in the educational context, not only helping the teaching of disciplines, but also making possible the development of several abilities, such as teamwork, problem-solving, and creativity. Among various robotics kits, LEGO® Robotics has been shown one of the best results considering some evaluated criteria (modularity level, hardware, curriculum, price, etc.). Some studies analyze the teaching practices, some compare technologies, and others evaluate the kits in a pedagogical way. However, it is essential to investigate all these contexts together in order to improve the impact produced by the ER in education and to know the best teaching practices associated with the most powerful technologies. The objective of this Research Full Paper is to identify: a) environments and programming languages adopted in the LEGO® Robotics context, b) educational practices applied during classes based on LEGO® Robotics, and c) the educational levels in which robotics has been applied with positive results. To achieve these goals, we planned and carried out a systematic review of the literature. Our main findings are: a) the most widely used environment and programming language are LabVIEW along with the LEGO®'s block-based programming language, b) we identified LEGO® Robotics is used for teaching programming, interdisciplinary contents, participation in tournaments, robotics, and computational thinking, c) LEGO® Robotics is used with success by students of different levels, such as K12, undergraduate, and graduated. Finally, we discuss some problems and limitations related to ER and point out that there is no standardization of teaching practices or methodologies for evaluating results, indicating that more research is needed to find the best scenario regarding technologies, methods, and target audience.}
}

@article{rayyan-727967971,
  title={Computational intelligence techniques used for stock market prediction: A systematic review},
  year={2020},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={18},
  number={4},
  pages={744-755},
  author={Zavadzki, S and Kleina, M and Drozda, F and Marques, M},
  keywords={Literature Review, Computational intelligence, Computational modeling, Biological system modeling, Artificial neural networks, Autoregressive processes, Computational Intelligence, Economic Engineering, Financial Model, Stock Market Forecast, Stock markets, Intelligence},
  abstract={With the advancement of various computational techniques and the growing search for assertive predictive models, computational intelligence methods have attracted much attention. They are data-based methodologies and mainly include fuzzy logic, artificial neural networks and evolutionary computation. In the economic environment, more specifically, in the stock market forecast, where there is the challenge of the time series volatility, these methods have stood out. In this context, the objective of this paper is to present a systematic review of the literature on recent research involving forecasting techniques in the stock market, and the computational intelligence were the ones that stood out. To define these techniques, articles were collected from four large databases and a keyword filter was applied, which reduced the initial volume. So we selected the articles from the most published journals and remove duplicated articles. The most articles applied hybrid models and for the selection of featured techniques were choose those most frequent ones. A brief description was also made of the most used methods as well as of the selected articles. The review was done with articles published between the years 2014 and 2018 taken from four databases and, after some selection criteria, 24 articles were selected by relation to the subject studied.}
}

@article{rayyan-727967972,
  title={Health ontology and information systems: A systematic review},
  year={2017},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={15},
  number={1},
  pages={103-120},
  author={Corral Diaz, Maria Alexandra and Antonelli, Leandro and Sanchez, Luis Enrique},
  keywords={Software, Ontology, Ontologies, Interoperability, Semantics, Medical services, Biomedical informatics, Health, Medical information system, Semantic, Information Systems},
  abstract={One of the most important aspects for the development of information systems is that they were interoperable, so many initiatives consider that ontologies as useful tools for their development, especially when the application is in complex and dynamic domains as the case of health. In this article, a systematic review (SR) of the existing literature related to ontologies used in the health sector is carried out, not only to interpret and synthesize the available studies but also to provide a framework as a basis for conducting new researches. Recent publications (2010- 2016) in which topics such as the use and impact of ontologies in the development of information systems are discussed, taking into account the organizational objectives and the involved stakeholders were considered. The number of published studies shows a growing interest by researchers because they consider ontologies artifacts that facilitate interoperability, understanding of information and communication structures.}
}

@article{rayyan-727967973,
  title={Virtual reality and fetal medicine — A systematic review},
  year={2017},
  pages={1-10},
  author={Yoshida, Emilia A and Castro, Márcia L A and Martins, Valéria F},
  keywords={Software, Visualization, Virtual reality, Medical diagnostic imaging, Fetal Medicine, Fetus, Lasers, Virtual Reality},
  abstract={This work brings a Systematic Review on the use of Virtual Reality in Fetal Medicine in order to recover the state of the art on the use of Virtual Reality tools in Fetal Medicine and also which Fetal Medicine areas has been focus of Virtual Reality application. The keywords “Virtual Reality” and “Fetal Medicine”, “Virtual Reality” and “Fetus” were used in both Portuguese and English and the “And” operator, where 24 papers were found after using inclusion and exclusion criteria, from 2001 to 2014. It presents some statistics on studied items and uses the most relevant papers found in virtual databases, to present the results and benefits this technology can bring to the field of fetal medicine, especially in the analysis of diseases, abnormalities and birth defects, as well existing and used tools.}
}

@article{rayyan-727967974,
  title={Software productivity measurement using multiple size measures},
  year={2004},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={30},
  number={12},
  pages={1023-1035},
  author={Kitchenham, B and Mendes, E},
  keywords={Software measurement, Productivity, Production, Computer science, Application software, Costs, Predictive models, Computer Society, Equations, Index Terms- Software productivity measurement, Size measurement, software cost estimation., Software},
  abstract={Productivity measures based on a simple ratio of product size to project effort assume that size can be determined as a single measure. If there are many possible size measures in a data set and no obvious model for aggregating the measures into a single measure, we propose using the expression AdjustedSize/Effort to measure productivity. AdjustedSize is defined as the most appropriate regression-based effort estimation model, where all the size measures selected for inclusion in the estimation model have a regression parameter significantly different from zero (p¡0.05). This productivity measurement method ensures that each project has an expected productivity value of one. Values between zero and one indicate lower than expected productivity, values greater than one indicate higher than expected productivity. We discuss the assumptions underlying this productivity measurement method and present an example of its use for Web application projects. We also explain the relationship between effort prediction models and productivity models.}
}

@article{rayyan-727967975,
  title={Software cost estimation for global software development a systematic map and review study},
  year={2015},
  pages={197-206},
  author={El Bajta, Manal and Idri, Ali and Fernández-Alemán, José Luis and Ros, Joaquin Nicolas and Toval, Ambrosio},
  keywords={Software, Systematics, Data mining, Estimation, Systematic Mapping Study, Context, Conferences, Mathematical model, Software Cost Estimation, Global Software Development},
  abstract={Software cost estimation plays a central role in the success of software project management in the context of global software development (GSD). The importance of mastering software cost estimation may appear to be obvious. However, as regards the issue of customer satisfaction, end-users are often unsatisfied with software project management results. In this paper, a systematic mapping study (SMS) is carried out with the aim of summarising software cost estimation in the context of GSD research by answering nine mapping questions. A total, of 16 articles were selected and classified according to nine criteria: publication source, publication year, research type, research approach, contribution type, software cost estimation techniques, software cost estimation activity, cost drivers and cost estimation performances for GSD projects. The results show that the interest in estimating software cost for GSD projects has increased in recent years and reveal that conferences are the most frequently targeted publications. Most software cost estimation for GSD research has focused on theory. The dominant contribution type of software cost estimation for GSD research is that of models, while the predominant activity was identified as being software development cost. Identifying empirical solutions to address software cost estimation for GSD is a promising direction for researchers.}
}

@article{rayyan-727967976,
  title={Decision support system for risk assessment and management strategies in distributed software development},
  year={2017},
  journal={IEEE Access},
  issn={2169-3536},
  volume={5},
  pages={20349-20373},
  author={Aslam, Adeel and Ahmad, Naveed and Saba, Tanzila and Almazyad, Abdulaziz S and Rehman, Amjad and Anjum, Adeel and Khan, Abid},
  keywords={Software, Bibliographies, Guidelines, Decision making, Risk management, Distributed software development, Companies, Decision support systems, decision support system, risk analysis, Risk Assessment},
  abstract={Risk management in distributed software development (DSD) is a well-researched area, providing different methods for assessing risks and suggesting control strategies. However, some of these methods are narrow in scope, only considering few risks, and are too complex to be used in practice whereas others provide many rules and guidelines which are often implicit. Moreover, the knowledge related to risks in DSD is scattered over different publications which make it difficult to find relevant information to be used in practice. This research aims to develop an automated decision support system to aid practitioners in assessing risks and deciding on suitable control strategies. In order to construct the knowledge base for the proposed decision support system, a systematic literature review (SLR) is conducted. Results of SLR are used to identify required questions, options and set of rules to implement our decision support system (DSS). In total 80 studies were identified from which 49 aspects, 53 questions, and a set of rules are extracted. DSS is evaluated through multiple case studies. The results indicate that the developed DSS supports decision-making process in risk assessment and selection of control strategy.}
}

@article{rayyan-727967977,
  title={Identification and prioritization of cloud based global software development best practices},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={191242-191262},
  author={Akbar, Muhammad Azeem and Mahmood, Sajjad and Alsalman, Hussain and Razzaq, Abdul and Gumaei, Abdu and Riaz, Muhammad Tanveer},
  keywords={Cloud computing, Organizations, Computer science, Best practices, Industries, Software as a service, fuzzy-AHP, best practices, Cloud based global software development (CGSD), Software},
  abstract={The cloud based global software development (CGSD) is the most widely adopted development paradigm in software industry. The CGSD offers significant economic and strategic benefits; besides, various complexities are faced by the practitioners while deploying CGSD. Hence, this study aims to identify and prioritize the best practices that are important for the success and progression of CGSD paradigm. Using the systematic literature review a total of 30 best practices were identified and were further verified with industry experts using questionnaire survey study. The identified best practices were further prioritize using fuzzy-AHP approach. The fuzzy-AHP is novel in this domain as it successfully applied in other engineering domain to address the multicriteria decision making problems. The findings of this study will provide a prioritization-based taxonomy of the investigated best practices which assists the academic researchers and industry experts to develop and revise the strategies of CGSD.}
}

@article{rayyan-727967978,
  title={A new web-based method for automatic selection of articles for systematic literature reviews},
  year={2017},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={15},
  number={10},
  pages={1926-1932},
  author={Pelissari, Renata and Infante, Kleber Manoel and Oliveira, Maria Celia and Helleno, Andre Luis and Abackerli, Alvaro Jose},
  keywords={systematic review, Software, Systematics, Databases, IEEE transactions, articles portfolio, automated systematic review, impact factor, Information filters, JCR, Portfolios},
  abstract={A systematic review about a particular subject provides the basis of knowledge for supporting a research. The increase of scientific information available and the easy access to such information in electronic databases have contributed to the rise of systematic review studies. However, one of the problems that arise with traditional research methods is the difficulty of reading all available articles. Thus, some items must be selected according to predefined selection rules. Nevertheless, the effectiveness of a systematic review is directly related to the relevance of the scientific papers selected according to the purpose of the particular study, among others factors. Therefore, there are several indicators that can be used to prioritize the articles that will set the articles portfolio of the review. The objective of this article is to introduce a method and a web system, implemented in R package, designed to apply automated filters to help in the selection of articles for systematic reviews. We propose two filters in the discussed method: the journal impact factor and the number of citations of the articles. The latter was analyzed by using Pareto rule. In the portfolio creation process, articles are first selected from a query performed in the electronic databases Scopus and Web of Science. Then, the proposed method can be automatically applied using the web system introduced here.}
}

@article{rayyan-727967979,
  title={Approaches to co-evolution of metamodels and models: A survey},
  year={2017},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={43},
  number={5},
  pages={396-414},
  author={Hebig, Regina and Khelladi, Djamel Eddine and Bendraou, Reda},
  keywords={software engineering, Survey, Productivity, Taxonomy, Unified modeling language, Libraries, Companies, Biological system modeling, Atmospheric modeling, design notations and documentation, metamodels, models},
  abstract={Modeling languages, just as all software artifacts, evolve. This poses the risk that legacy models of a company get lost, when they become incompatible with the new language version. To address this risk, a multitude of approaches for metamodel-model co-evolution were proposed in the last 10 years. However, the high number of solutions makes it difficult for practitioners to choose an appropriate approach. In this paper, we present a survey on 31 approaches to support metamodel-model co-evolution. We introduce a taxonomy of solution techniques and classify the existing approaches. To support researchers, we discuss the state of the art, in order to better identify open issues. Furthermore, we use the results to provide a decision support for practitioners, who aim to adopt solutions from research.}
}

@article{rayyan-727967980,
  title={What should I document? A preliminary systematic mapping study into API documentation knowledge},
  year={2019},
  pages={1-6},
  author={Cummaudo, Alex and Vasa, Rajesh and Grundy, John},
  keywords={Software, Software engineering, Systematics, documentation, Guidelines, Tools, systematic mapping study, Taxonomy, taxonomy, Documentation, API, DevX},
  abstract={Background: Good API documentation facilitates the development process, improving productivity and quality. While the topic of API documentation quality has been of interest for the last two decades, there have been few studies to map the specific constructs needed to create a good document. In effect, we still need a structured taxonomy that captures such knowledge systematically.Aims: This study reports emerging results of a systematic mapping study. We capture key conclusions from previous studies that assess API documentation quality, and synthesise the results into a single framework.Method: By conducting a systematic review of 21 key works, we have developed a five dimensional taxonomy based on 34 categorised weighted recommendations.Results: All studies utilise field study techniques to arrive at their recommendations, with seven studies employing some form of interview and questionnaire, and four conducting documentation analysis. The taxonomy we synthesise reinforces that usage description details (code snippets, tutorials, and reference documents) are generally highly weighted as helpful in API documentation, in addition to design rationale and presentation.Conclusions: We propose extensions to this study aligned to developer utility for each of the taxonomy's categories.}
}

@article{rayyan-727967981,
  title={The impact of scope creep on project success: An empirical investigation},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={125755-125775},
  author={Komal, Bakhtawar and Janjua, Uzair Iqbal and Anwar, Fozia and Madni, Tahir Mustafa and Cheema, Muhammad Faisal and Malik, Muhammad Noman and Shahid, Ahmad Raza},
  keywords={Software, Systematics, Bibliographies, Project management, Scope management, Organizations, Mathematical model, Creep, partial least squares structural equation modeling, requirement creep, scope creep, software project success},
  abstract={Advocates of software engineering and software project management stated in the literature that creeping of scope is one of the most common causes for the failure of software projects. Also, advocates believed that it could occur in almost every software project, which leads to compromise in quality, delayed schedules, increase cost and decreased customer satisfaction. However, the lack of empirical evidence demands a comprehensive investigation to identify the factors of scope creep and to propose a conceptual framework to empirically evaluate the impact of scope creep on software project success. To determine the scope creep factors in this study, two exploratory methods, i.e. a Systematic Literature Review (SLR) and interview from experts are performed. Following the analysis of these methods, a conceptual framework is proposed. To empirically evaluate the proposed conceptual framework, data is collected through a survey method. Next, the collected data is analyzed through Partial Least Squares' Structural Equation Modelling (PLS-SEM). From the results, it is evident that the identified factors of scope creep are negatively associated with software project success. The results of empirical evaluation also second the findings of SLR. The outcome of the study may help the practitioners to understand the dynamics of factors, which undermine scope creep in software SMEs and to assist them in the development of effective control and mitigation strategies, therefore, to increase the project success rate.}
}

@article{rayyan-727967982,
  title={What is the perception of female and male software professionals on performance, team dynamics and job satisfaction? Insights from the trenches},
  year={2017},
  pages={13-22},
  author={James, Toni and Galster, Matthias and Blincoe, Kelly and Miller, Grant},
  keywords={Software, Software engineering, Business, descriptive survey, diversity, female and male software professionals, perceptions, software development teams, Job Satisfaction},
  abstract={Research has shown that gender diversity correlates positively with innovation and productivity in many professional engineering and technology domains. Yet, software development teams are dominated by males. In this paper, we aim at understanding whether female software professionals, compared to male, have different perceptions on a) team performance and dynamics, b) their own personal performance, c) their immediate supervisors, and d) accomplishment, recognition, and opportunities. Understanding perceptions of different genders can help software professionals, their supervisors and those responsible for staff create and foster environments in which both females and males are comfortable and perform best. To achieve this aim, we conducted a survey targeted at individual software professionals in technical roles. We collected and analyzed data from 55 female and 69 male respondents. Our results show basic differences in demographics (e.g., males tend to be older, have more senior roles, and have longer tenure with their employer). While we did find some differences around perceptions of spirit of team work, productivity, sense of satisfaction and fairness of reviews from supervisors, in general, females and males do not seem to differ significantly in their perceptions. Based on the results from our survey and insights from the current literature, we discuss commonalities and differences between females and males, and explore potential implications for performance reviews, recognition, and career progression.}
}

@article{rayyan-727967983,
  title={An empirical study on developing secure mobile health apps: The developers' perspective},
  year={2020},
  pages={208-217},
  author={Aljedaani, Bakheet and Ahmad, Aakash and Zahedi, Mansooreh and Babar, M Ali},
  keywords={Software, Security, Mobile applications, Wireless sensor networks, Mobile Health, Empirical Software Engineering, Medical services, Faces, Pervasive computing, Secure Software Development, Software Engineering for Mobile},
  abstract={Mobile apps exploit embedded sensors and wireless connectivity of a device to empower users with portable computations, context-aware communication, and enhanced interaction. Specifically, mobile health apps (mHealth apps for short) are becoming integral part of mobile and pervasive computing to improve the availability and quality of healthcare services. Despite the offered benefits, mHealth apps face a critical challenge, i.e., security of health-critical data that is produced and consumed by the app. Several studies have revealed that security specific issues of mHealth apps have not been adequately addressed. The objectives of this study are to empirically (a) investigate the challenges that hinder development of secure mHealth apps, (b) identify practices to develop secure apps, and (c) explore motivating factors that influence secure development. We conducted this study by collecting responses of 97 developers from 25 countries - across 06 continents - working in diverse teams and roles to develop mHealth apps for Android, iOS, and Windows platform. Qualitative analysis of the survey data is based on (i) 8 critical challenges, (ii) taxonomy of best practices to ensure security, and (iii) 6 motivating factors that impact secure mHealth apps. This research provides empirical evidence as practitioners' view and guidelines to develop emerging and next generation of secure mHealth apps.}
}

@article{rayyan-727967984,
  title={Strategies focused on the teaching of programming logic: A systematic review of brazilian literature},
  year={2018},
  pages={292-298},
  author={de Oliveira, Tiago and Stringhini, Denise and Corrêa, Deborah Godoy Martins},
  keywords={Education, Games, Informatics, Conferences, Portals, robotics, Programming profession, games, multiple intelligences, programming logic, Scratch},
  abstract={This article aims to present the results obtained from the systematic of the Brazilian literature on teaching and learning the discipline of Programming Logic (LP), considering the articles published in the Brazilian Journal of Informatics in Education (RBIE), in the annals of the Brazilian Symposium on Informatics (SBIE), in the annals of the Computer Science Workshops (WIE), in the Brazilian Congress of Informatics in Education (CBIE) and in the Journey of Updating in Computer Science at School (JAIE) between 2012 and 2017. From this mapping the strategies used in teaching LP in Basic Education, High School and Higher Education were analyzed, and was verified that the subject remains a great challenge to be overcome. Additionally, this work investigates if Multiple Intelligences are being considered in the adoption of teaching techniques - the goal is to establish a future work on this subject. Results show that Scratch and Games are the most cited in Brazilian literature and that Multiple Intelligences are still an open research field in this context.}
}

@article{rayyan-727967985,
  title={A widening digital platform gap: A systematic review of the sharing economy for small and micro enterprises},
  year={2019},
  pages={1-12},
  author={Abebe, Sertse and Twinomurinzi, Hossana},
  keywords={Market research, Technological innovation, Employment, Entrepreneurship, Manufacturing, Three-dimensional displays, Two dimensional displays},
  abstract={Information and Communication Technology (ICT) presents immense opportunities to SMEs by providing platforms to share resources and to collaborate better. The umbrella concept that explains the coordinated effort of people to share resources and collaborate through digital platforms is generally known as the Sharing Economy (SE). This study investigated SE research trends and gaps from the context of SMEs. The exploration analyzed six different SE aspects from 77 previous SE studies in the context of SMEs. These aspects included SE forms, values of SE for SMEs, SMEs sectors, the context of nations, geographic origin of data and cases, and methodological.}
}

@article{rayyan-727967986,
  title={Current state of research on continuous experimentation: A systematic mapping study},
  year={2018},
  pages={335-344},
  author={Auer, Florian and Felderer, Michael},
  keywords={Software engineering, Systematics, Bibliographies, Data mining, Search problems, systematic mapping study, Databases, continuous experimentation},
  abstract={The systematic evaluation of ideas by experiments are the foundation of continuous experimentation. It allows to assess the value of an idea, remove guessing and subjective opinions from the discussion. The enormous interest of it by practitioners and researchers let the body of knowledge consistently grow. New framework, methods and techniques are developed and its application is constantly expanded to new fields like cyber-physical systems or social networks. In this paper we present a systematic mapping study to characterize the current state of research on continuous experimentation. Our study analyzes the following aspects: intensity of research activity and industry-academia collaboration, influential authors and publications, frequent research types and topics, kind of contributions and terms used for continuous experimentation. Our findings show amongst others that the intensity of research activities increases consistently, the collaboration between industry and academia is high and the most cited publications are experience reports from practitioners.}
}

@article{rayyan-727967987,
  title={Review and analysis of software development team communication research},
  year={2017},
  journal={IEEE Transactions on Professional Communication},
  issn={1558-1500},
  volume={60},
  number={2},
  pages={165-182},
  author={DeFranco, Joanna F and Laplante, Philip A},
  keywords={Software, Software engineering, Bibliographies, Databases, Communication, Google, Protocols, IEEE Xplore, teamwork, software development teams, integrative literature review, software engineering teams},
  abstract={Research problem:Communication affects many aspects of the software engineering process. In addition, poor team communication is often a root cause of failure for complex engineering projects. In global software engineering, communication and coordination become more challenging, and that fact affects the quality of the product. The goal of this study is to analyze the type and quality of research performed on software development team communication, and to present data to guide future research in these areas. Research questions:(1) How much communication research activity in the area of software development teams has been described in the literature from 2005 to 2015? (2) What is the current state of software development team communication research (what has been done and to what degree)? (3) What is the predominant research methodology of software development team communication research? (4) Where are the gaps in the current state of software development team communication research? (5) What are the major, common findings about communication in software development teams?Methodology:We reviewed 184 journal papers and performed a content analysis of the keywords from the relevant papers to create a software engineering team research taxonomy. We utilized this taxonomy to categorize the context of communication research papers. We then used the categorization results to determine the most active software development team communication research areas. In addition, we analyzed the quality of the journals (using impact factor and H-index as metrics) and the type of research performed in these areas (i.e., qualitative, quantitative, survey, social network analysis, or literature review).Results and conclusions:The results showed that the most active software development team communication research areas are global software development, project effectiveness, and effective teamwork. The most prevalent research methodology is a survey among those research areas. We conclude this paper with a presentation and discussion of the major findings of each research paper as well as the common themes among those findings in each of the top research areas.}
}

@article{rayyan-727967988,
  title={Cloud computing and affective computer approach on development programming skills},
  year={2019},
  pages={1-5},
  author={Quezada- Sarmiento, Pablo Alejandro and Mengual-Andrés, Santiago},
  keywords={Software, Cloud computing, Programming, Learning, Education, Information systems, Cloud Computing, Computational modeling, Medical services, Affective Computing, Program Development},
  abstract={This article shows a thesis doctoral proposal; it is development on the framework of Educational Doctoral Program of Valencia University; this research is focus on affective computer on the process to development technological skills on the resolution of programming problems of software engineering. In the same way the use of cloud computing tools as supplementary elements on the teaching and learn computational paradigms is proposed.}
}

@article{rayyan-727967989,
  title={A pearson correlation analysis of the SoftwareEngineering practice in micro and small-sizedsoftware industry of sinaloa, mexico},
  year={2019},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={17},
  number={2},
  pages={210-218},
  author={Aguilar-Calderón, José-Alfonso and Zaldívar-Colado, Aníbal and Tripp-Barba, Carolina and Espinoza-Oliva, Roberto and Zurita-Cruz, Carlos-Eduardo},
  keywords={Software, software engineering, Unified modeling language, Industries, Correlation, IEEE transactions, Capability maturity model, ANOVA, micro and small-sized software factories, pearson correlation analysis, Mexico},
  abstract={Is well-known that software development process fordesktop, web or mobile applications in industry needs the adoptionof best practices of Software Engineering. Nevertheless, notmuch contemporary information with regard to current practicesin micro and small-sized software industry exists especially inSinaloa, Mexico. This work presents an exploratory study whichprovides insight into industrial practices in the software industryof Sinaloa. A combination of both qualitative and quantitativedata is collected, using semi-structured interviews and a detailedquestionnaire from sixteen software factories. A Pearson correlationanalysis was performed independently between the variablesCompany location, Scope of coverage, Number of workers, Timeto live in the market, Projects completed, Time dedicated toactivities related to the project, Outdated projects completed inorder to determine the degree of relationship between each ofthe variables mentioned, with all. A correlation analysis and ananalysis of variance were performed. The quantitative resultswill serve to obtain opportunities for further interpretation andcomparison.}
}

@article{rayyan-727967990,
  title={A study of ontology-based knowledge management system in academic domain},
  year={2019},
  pages={1-5},
  author={Rokhman, Mokhammad Fathoni and Indra Sensuse, Dana and Hakim, Shidiq Al and Satria, Deki},
  keywords={systematic review, Data mining, Search problems, Quality assessment, Knowledge management, Ontologies, ontology, Taxonomy, Libraries, knowledge management, academic},
  abstract={The purpose of this paper is to review the use of ontology in the field of knowledge management in the academic domain. To conducting this systematic review, we used the guidelines from Kitchenham and used collected data sources from IEEE Xplore, Scopus, Emerald Insight, and ACM Digital Library. We collect paper from data sources by using boolean search through available tools in each publisher. The papers collected were only limited to the last five years and those using ontology as taxonomies. Our data search results found 346 articles, and there were 26 suitable articles selected based on the selection criteria we defined. Our findings concluded six types of ontology models contained in ontology-based knowledge management and three areas that mostly used ontology-based knowledge management systems. Finally, we also found important classes that form the basis for making an ontology used in knowledge management.}
}

@article{rayyan-727967991,
  title={Survey of applications for apartment energy consumption monitoring},
  year={2019},
  pages={283-288},
  author={Saari, Mika and Sillberg, Pekka and Grönman, Jere and Rantanen, Petri and Jaakkola, Hannu and Henno, Jaak},
  keywords={Systematic literature review, IoT, Energy consumption, API, Energy saving, Network API, Sensor Networks, Sensors and Monitoring},
  abstract={Nowadays energy consumption and especially energy saving are hot topics. The news about global warming has increased the need to save energy. In Finland, one of the major energy consumers is housing. The heating of residential buildings accounts for up to 68% of housing energy consumption. The second largest consumer of energy is heating water, at 15%. Therefore, it is not surprising that apartment energy consumption is a popular research topic in Finland. In particular, this study tries to find ways to increase energy savings - without forgetting comfortable living. This paper introduces the results of a study exploring the research subjects of energy saving in the area of real estate and housing. The research methodology is a literature survey and primary articles were selected by means of a systematic literature review. This study presents a way to categorize research papers with diverse themes. In addition, the survey reveals the most recent trends in research and practical applications of energy saving as well as efforts toward energy saving in the area of real estate and housing.}
}

@article{rayyan-727967992,
  title={A taxonomy of quality metrics for cloud services},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={131461-131498},
  author={Guerron, Ximena and Abrahão, Silvia and Insfran, Emilio and Fernández-Diego, Marta and González-Ladrón-De-Guevara, Fernando},
  keywords={systematic literature review, Systematics, Cloud computing, Measurement, Software quality, Quality of service, Taxonomy, metrics, Elasticity, cloud services, NIST, Metronidazole},
  abstract={A large number of metrics with which to assess the quality of cloud services have been proposed over the last years. However, this knowledge is still dispersed, and stakeholders have little or no guidance when choosing metrics that will be suitable to evaluate their cloud services. The objective of this paper is, therefore, to systematically identify, taxonomically classify, and compare existing quality of service (QoS) metrics in the cloud computing domain. We conducted a systematic literature review of 84 studies selected from a set of 4333 studies that were published from 2006 to November 2018. We specifically identified 470 metric operationalizations that were then classified using a taxonomy, which is also introduced in this paper. The data extracted from the metrics were subsequently analyzed using thematic analysis. The findings indicated that most metrics evaluate quality attributes related to performance efficiency (64%) and that there is a need for metrics that evaluate other characteristics, such as security and compatibility. The majority of the metrics are used during the Operation phase of the cloud services and are applied to the running service. Our results also revealed that metrics for cloud services are still in the early stages of maturity - only 10% of the metrics had been empirically validated. The proposed taxonomy can be used by practitioners as a guideline when specifying service level objectives or deciding which metric is best suited to the evaluation of their cloud services, and by researchers as a comprehensive quality framework in which to evaluate their approaches.}
}

@article{rayyan-727967993,
  title={On the effectiveness of the UML object diagrams: A replicated experiment},
  year={2011},
  pages={76-85},
  author={Scanniello, Giuseppe and Ricca, Filippo and Torchiano, Marco},
  abstract={Background: In the modeling of object oriented software systems, the UML object diagrams are recognized very useful to complement class diagrams. However, up to now, there exists only one experiment [Torchiano 2004] that investigates this concern. Aim: To confirm or contradict the findings of the original experiment, we have conducted a replication and the achieved results have been presented in this paper. Both the replication and the original experiment have been conducted to investigate whether the use of object diagrams to complement class diagrams affects the comprehension of software systems. Method: The replication has been conducted with a group of 24 graduated subjects in Computer Science of the University of Basilicata. The experiment adopts a counterbalanced design, thus ensuring that each subject work on two comprehension tasks, experimenting each time class and object diagrams together or class diagrams alone. The comprehension on each task has been assessed using a questionnaire-based approach. In particular, we have measured the comprehension level of each subject using an information retrieval based approach that allowed us to get a balance between correctness and completeness of the answers. Results: The results show that the subjects significantly benefit from the use of object diagrams in the comprehension of software systems, thus confirming and strengthening the findings of the original experiment. Conclusions: It is advisable to complement the usual class diagrams with object diagrams to increase the understandability of software systems. To raise the generalizability of the results, replications of this study are necessary especially with professional software engineers.}
}

@article{rayyan-727967994,
  title={An empirical study of model-agnostic techniques for defect prediction models},
  year={2020},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  pages={1},
  author={Jiarpakdee, Jirayus and Tantithamthavorn, Chakkrit and Dam, Hoa Khanh and Grundy, John},
  keywords={Software, Software engineering, Software algorithms, Analytical models, Prediction algorithms, Predictive models, Defect Prediction Models, Electric breakdown, Explainable Software Analytics, Model-Agnostic Techniques, Software Quality Assurance},
  abstract={Software analytics have empowered software organisations to support a wide range of improved decision-making and policy-making. However, such predictions made by software analytics to date have not been explained and justified. Specifically, current defect prediction models still fail to explain why models make such a prediction and fail to uphold the privacy laws in terms of the requirement to explain any decision made by an algorithm. In this paper, we empirically evaluate three model-agnostic techniques, i.e., two state-of-the-art Local Interpretability Model-agnostic Explanations technique (LIME) and BreakDown techniques, and our improvement of LIME with Hyper Parameter Optimisation (LIME-HPO). Through a case study of 32 highly-curated defect datasets that span across 9 open-source software systems, we conclude that (1) model-agnostic techniques are needed to explain individual predictions of defect models; (2) instance explanations generated by model-agnostic techniques are mostly overlapping (but not exactly the same) with the global explanation of defect models and reliable when they are re-generated; (3) model-agnostic techniques take less than a minute to generate instance explanations; and (4) more than half of the practitioners perceive that the contrastive explanations are necessary and useful to understand the predictions of defect models. Since the implementation of the studied model-agnostic techniques is available in both Python and R, we recommend model-agnostic techniques be used in the future.}
}

@article{rayyan-727967995,
  title={The consistency of empirical comparisons of regression and analogy-based software project cost prediction},
  year={2005},
  pages={10-pp.–},
  author={Mair, C and Shepperd, M},
  keywords={Software engineering, Machine learning, Regression analysis, Productivity, Costs, Solids, Artificial neural networks, Conference proceedings, Environmental factors, Sampling methods, Software},
  abstract={The objective is to determine the consistency within and between results in empirical studies of software engineering cost estimation. We focus on regression and analogy techniques as these are commonly used. We conducted an exhaustive literature search using predefined inclusion and exclusion criteria and identified 67 journal papers and 104 conference papers. From this sample we identified 11 journal papers and 9 conference papers that used both methods. Our analysis found that about 25% of studies were internally inconclusive. We also found that there is approximately equal evidence in favour of, and against analogy-based methods. We confirm the lack of consistency in the findings and argue that this inconsistent pattern from 20 different studies comparing regression and analogy is somewhat disturbing. It suggests that we need to ask more detailed questions than just: "What is the best prediction system?".}
}

@article{rayyan-727967996,
  title={Cognitive factors in perspective-based reading (PBR): A protocol analysis study},
  year={2009},
  pages={145-155},
  author={Robbins, Bryan and Carver, Jeff},
  keywords={Software engineering, Software measurement, Testing, Inspection, Computer science, Protocols, Bioreactors, Cognition, Cognitive science, US Department of Transportation},
  abstract={The following study investigated cognitive factors involved in applying the Perspective-Based Reading (PBR) technique for defect detection in software inspections. Using the protocol analysis technique from cognitive science, the authors coded concurrent verbal reports from novice reviewers and used frequency-based analysis to consider existing research on cognition in software inspections from within a cognitive framework. The current coding scheme was able to describe over 98% of the cognitive activities reported during inspection at a level of detail capable of validating multiple hypotheses from literature. A number of threats to validity are identified for the protocol analysis method and the parameters of the current experiment. The authors conclude that protocol analysis is a useful tool for analyzing cognitively intense software engineering tasks such as software inspections.}
}

@article{rayyan-727967997,
  title={5Ws of green and sustainable software},
  year={2020},
  journal={Tsinghua Science and Technology},
  issn={1007-0214},
  volume={25},
  number={3},
  pages={401-414},
  author={Calero, Coral and Mancebo, Javier and García, Félix and Moraga, María Ángeles and Berná, José Alberto García and Fernández-Alemán, José Luis and Toval, Ambrosio},
  keywords={Software, Software Engineering, Bibliometrics, Software engineering, Green products, Sustainable development, Economics, Product development, Energy Efficiency, Energy Utilization, Green Software, Software Sustainability, Sustainable Software},
  abstract={Green and Sustainable Software has emerged as a new and highly active area in the software community. After several years of research and work, we believe that it is now necessary to obtain a general snapshot of how the research in this area is evolving. To do so, we have applied the 5Ws (why, when, who, where, and what), a formula for getting the complete story on a subject. We have therefore carried out a study, using 542 publications related to Green and Sustainable Software research; these were recovered using SCOPUS. The results obtained allow us to conclude that it is important to identify key elements of the research to allow researchers be fully aware of the state of the research on Green and Sustainable Software (why); the study uses papers published between 2000 and the beginning of November 2018 (when); the most prolific authors are mainly from Europe, although the USA is the most active country, Green and Sustainable Software being a very interactive area with a good number of multinational publications (who); the top five keywords related to sustainable aspects are Green Software, Green IT, Software Sustainability, Energy Consumption, and Energy Efficiency (what); finally, as regards the places authors prefer to publish in, there is almost a complete balance between conferences and journals, with a trend towards an increase in the number of publications (where).}
}

@article{rayyan-727967998,
  title={Machine learning applied to software testing: A systematic mapping study},
  year={2019},
  journal={IEEE Transactions on Reliability},
  issn={1558-1721},
  volume={68},
  number={3},
  pages={1189-1212},
  author={Durelli, Vinicius H S and Durelli, Rafael S and Borges, Simone S and Endo, Andre T and Eler, Marcelo M and Dias, Diego R C and Guimarães, Marcelo P},
  keywords={Software testing, Software engineering, Systematics, systematic mapping study, Software systems, software testing, Software algorithms, Machine learning (ML), Software, Learning},
  abstract={Software testing involves probing into the behavior of software systems to uncover faults. Most testing activities are complex and costly, so a practical strategy that has been adopted to circumvent these issues is to automate software testing. There has been a growing interest in applying machine learning (ML) to automate various software engineering activities, including testing-related ones. In this paper, we set out to review the state-of-the art of how ML has been explored to automate and streamline software testing and provide an overview of the research at the intersection of these two fields by conducting a systematic mapping study. We selected 48 primary studies. These selected studies were then categorized according to study type, testing activity, and ML algorithm employed to automate the testing activity. The results highlight the most widely used ML algorithms and identify several avenues for future research. We found that ML algorithms have been used mainly for test-case generation, refinement, and evaluation. Also, ML has been used to evaluate test oracle construction and to predict the cost of testing-related activities. The results of this paper outline the ML algorithms that are most commonly used to automate software-testing activities, helping researchers to understand the current state of research concerning ML applied to software testing. We also found that there is a need for better empirical studies examining how ML algorithms have been used to automate software-testing activities.}
}

@article{rayyan-727967999,
  title={Software project management approaches for global software development: a systematic mapping study},
  year={2018},
  journal={Tsinghua Science and Technology},
  issn={1007-0214},
  volume={23},
  number={6},
  pages={690-714},
  author={El Bajta, Manal and Idri, Ali and Ros, Joaquín Nicolás and Fernández-Alemán, José Luis and de Gea, Juan Manuel and García, Félix and Toval, Ambrosio},
  keywords={Software, Software engineering, Systematics, Monitoring, Project management, Planning, Standards, Global Software Development (GSD), Software Project Management (SPM), SPM approaches, Systematic Mapping Study (SMS)},
  abstract={Global Software Development (GSD) is a well established field of software engineering with the benefits of a global environment. Software Project Management (SPM) plays a key role in the success of GSD. As a result, the need has arisen to study and evaluate the downsides of SPM for GSD, to thereby pave the way for the development of new methods, techniques, and tools with which to tackle them. This paper aims to identify and classify research on SPM approaches for GSD that are available in the literature, to identify their current weaknesses and strengths, and to analyze their applications in industry. We performed a Systematic Mapping Study (SMS) based on six classification criteria. Eighty-four papers were selected and analyzed. The results indicate that interest in SPM for GSD has been increasing since 2006. As a class of approaches, the most frequently reported methods (40%) are those used for coordination, planning, and monitoring, along with estimation techniques that can be used to better match a distributed project. SPM for GSD requires further investigation by researchers and practitioners, particularly with respect to cost and time estimations. These findings will help overcome the challenges that must to be considered in future SPM research for GSD, especially regarding collaboration and time-zone differences.}
}

@article{rayyan-727968000,
  title={How do I know whether to trust a research result?},
  year={2015},
  journal={IEEE Software},
  issn={1937-4194},
  volume={32},
  number={1},
  pages={106-109},
  author={Shepperd, Martin},
  keywords={meta-analysis, Internet, software engineering, Software engineering, Systematics, Matthews correlation coefficient, Research and development, research results, researcher bias, scientific research, software defect prediction, Statistical analysis, statistical significance},
  abstract={A meta-analysis indicated that some areas of computer science research are subject to researcher bias. However, rather than mistrust all scientific research, researchers should examine research to determine its validity.}
}

@article{rayyan-727968001,
  title={Start-ups must be ready to pivot},
  year={2017},
  journal={IEEE Software},
  issn={1937-4194},
  volume={34},
  number={3},
  pages={18-22},
  author={Bajwa, Sohaib Shahid and Wang, Xiaofeng and Duc, Anh Nguyen and Chanin, Rafael Matone and Prikladnicki, Rafael and Pompermaier, Leandro Bento and Abrahamsson, Pekka},
  keywords={software engineering, Software engineering, software, Twitter, software development, Market opportunities, pivot, pivot triggers, pivot types, Social network services, start-up, Tecnopuc},
  abstract={As prominent examples such as Twitter have demonstrated, software start-ups frequently find that their initial product ideas don't pan out commercially. So, they must be prepared to change direction in one or more ways, a process called pivoting.}
}

@article{rayyan-727968002,
  title={State of the art of machine learning for product sustainability},
  year={2020},
  pages={197-202},
  author={Singhal, Swasti and Ahuja, Laxmi and Monga, Himanshu},
  keywords={Systematic literature review, Systematics, Search problems, Machine learning, Production, Business, Predictive models, Sustainable development, Machine Learning (ML), Product sustainability},
  abstract={Nowadays, the feasibility and life-time for every business venture is dependent on the generation of sustainable products and services. Almost all businesses and industries have endorsed Machine Learning (ML) technologies. The application of Machine Learning in determining the sustainability aspects is quite a challenging process. ML models can help in capturing the decision-making processes or predict the sustainability evaluation metrics assuming some key aspects such as triple bottom line and life-cycle factors are provided to the models. The main aim of this paper is to present a systematic review of studies related to the product sustainability assessments, framework development using Machine Learning techniques. Search strategies are applied to the most common computer science digital database libraries from 2010 to 2020 in the field of product sustainability using machine learning techniques. 15 out of 23 papers are shortlisted as was considered suitable for this research. A set of the research queries are defined and addressed by the review process. Based on the findings obtained in this literature review, future outlook in the field of Machine Learning for sustainable production is represented.}
}

@article{rayyan-727968003,
  title={Big data applied to tax evasion detection: A systematic review},
  year={2016},
  pages={435-440},
  author={Abrantes, Paulo Cesar and Ferraz, Felipe},
  keywords={big data, Scientific computing, Computational intelligence, 5G mobile communication, financial fraud, tax evasion, tax fraud detection},
  abstract={Tax evasion fraud is an issue faced by all governments in the world and one way to improve its detection is the application of big data technologies. This work performs a systematic literature review with the objective of identifying primary studies that address the fraud tax detection by the use of big data. This review resulted in the finding of 56 works of which 5 were identified as primary study. An overview of the results is presented categorized by the studies that address the problem by the use of pattern recognition methodologies, natural language processing and data analytics in auditing. The results also present algorithms and models used in each solution.}
}

@article{rayyan-727968004,
  title={Adapting software with affective computing: a systematic review},
  year={2019},
  journal={IEEE Transactions on Affective Computing},
  issn={1949-3045},
  pages={1},
  author={Aranha, Renan Vinicius and Corrêa, Cléber Gimenez and Nunes, Fátima L S},
  keywords={Software, Systematics, Protocols, Affective Computing, Affective Adaptation, Affective computing, Computer applications, Emotion recognition},
  abstract={Strategies aimed at keeping the user's interest in using computer applications are being studied to provide greater user engagement, and can influence how people interact with computers. One of the approaches that can promote user engagement is Affective Computing (AC), based on the premise of recognizing the user's emotional state and adjusting the computer application to respond to such state in real-time. Although it is a relatively new area, over the past few years many research works have investigated the use of AC in various activities and objectives. To provide an overview on the use of AC in computer applications, this article presents a systematic literature review based on available articles on the main scientific databases of the Computer Science area. The main contribution of this review is the analysis of different types of applications. Based on the 58 articles analyzed, the main emotion recognition techniques and approaches to the adaptation of computer applications, as well as the limitations and challenges to be overcome were compiled. Our conclusions present the limitations and challenges still to be overcome in the area of automatic adaptation of computer applications by means of AC.}
}

@article{rayyan-727968005,
  title={Microservices identification strategies : A review focused on model-driven engineering and domain driven design approaches},
  year={2020},
  pages={1-6},
  author={Schmidt, Roger Anderson and Thiry, Marcello},
  keywords={Systematics, Databases, Unified modeling language, MDE, Protocols, Proposals, Data models, microservices, DDD, decomposition, domain-driven., granularity, identification, model-driven},
  abstract={A proper architectural design for a microservices system is crucial for its success. Although there are several design strategies to identify software components in general, microservices demands special consideration. In this context of distributed systems, the component size directly impacts on nonfunctional requirements, such as performance, flexibility, reusability, etc. Design practices of coupling and cohesion have to be fine-tuned to determine the ideal microservices granularity. In order to shed light on this question, this study conducted a Systematic Literature Review that investigates microservices identification proposals. From procedures and guidelines inspired by Kitchenham et al., a rigorous research protocol was defined and performed, that covers publications from 2013 to 2019. Starting with an initial screening of 715 papers, 27 studies were considered relevant to answer four research questions. Besides microservices decomposition strategies, this review underlines Model Driven Engineering and Domain Driven Design, once they represent valuable approaches to support this challenging task. Moreover, this work highlights that only a few studies had explored these approaches in their strategies, which opens promising potential for further research.}
}

@article{rayyan-727968006,
  title={Risks and uncertainties in cloud computing: Literature review, trends and gaps},
  year={2017},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={15},
  number={2},
  pages={349-357},
  author={Zied Milian, Eduardo and Mesquita Spinola, Mauro and Monteiro Carvalho, Marly},
  keywords={Cloud computing, Uncertainty, Computational modeling, Software as a service, NIST, Laser radar, Risks and Uncertainties, Silicon, Systematic Review of the Literature},
  abstract={This study aims to characterize risks and uncertainties in the cloud computing context. A Systematic Review of the cloud computing literature was conducted focusing on publications on risks and uncertainties and combining techniques such as bibliometric analysis, network analysis and content analysis. Using the definitions of risk and uncertainty, this review maps how these concepts are used and in which contexts they occur. The study find that the main sources of risks and uncertainties were: strategic viability of outsourcing or offering certain services, weaknesses in the business model, vendor maturity, behavioral tradition and hiring of third party services.}
}

@article{rayyan-727968007,
  title={Open-source big data analytics architecture for businesses},
  year={2019},
  pages={1-6},
  author={Gökalp, Mert Onuralp and Kayabay, Kerem and Zaki, Mohamed and Koçyiğit, Altan and Eren, P Erhan and Neely, Andy},
  keywords={Systematic Review, Big Data, Big Data Analytics, Big Data Technologies, Reference Architecture},
  abstract={Unaware of existing big data technologies, organizations fail to develop a big data capability despite its disruptive impact on today's competitive business environment. To determine the shortcomings and strengths of developing a big data architecture with open-source tools from technical and managerial perspectives, this study (1) systematically reviews the available open-source big data technologies to present a comprehensive picture, and (2) proposes an open-source architecture for businesses to take as a reference while developing big data analytics capabilities. Lastly, we discuss technical, domain-specific, and firm-specific soft challenges related to establishing a big data architecture in an organization, and how these challenges are reshaping the big data research domain.}
}

@article{rayyan-727968008,
  title={Test driven development contribution in universities in producing quality software: A systematic review},
  year={2014},
  pages={1-6},
  author={Yahya, Norzariyah and Awang Abu Bakar, Normi Sham},
  keywords={experiment, Decision support systems, university, quality, perception, Test driven development, Software},
  abstract={Test driven development (TDD) is one of the Agile techniques adopted in education. TDD is an ideal approach to be taught in university due to its capability in producing quality software and at the same time teaching novice programmers to test and develop a product simultaneously. It helps novice programmers to think before they develop rather than using “trial-and-error” approach in their project. However, based on the existing research, TDD contribution in producing a better quality and the perception among novice programmers towards it needs to be analyzed. This systematic review will identify the quality of a product produced by the students in university and also their perception towards TDD.}
}

@article{rayyan-727968009,
  title={A systematic review of cloud lock-in solutions},
  year={2013},
  volume={2},
  pages={363-368},
  author={Silva, Gabriel Costa and Rose, Louis M and Calinescu, Radu},
  keywords={Systematics, Cloud computing, interoperability, Interoperability, Computer science, Industries, Semantics, Standards, cloud lock-in, portability, review},
  abstract={The heterogeneity of cloud semantics, technology and interfaces limits application and platform portability and interoperability, and can easily lead to vendor lock-in. We identify, analyse and classify existing solutions to cloud vendor lock-in, and highlight unresolved challenges. Our survey is based on a systematic review of 721 primary studies that describe the state-of-the-art in managing cloud lock-in, portability and interoperability. 78 of these primary studies were selected and used for a thorough analysis of cloud standards, commercial products and academic work related to cloud lock-in. Our review shows that most solutions proposed so far are platforms, APIs or architectures addressing infrastructure-as-a-service (IaaS) interoperability. From our review, we identify a need for: (i) exploiting established solutions from areas that are closely related to cloud computing, (ii) increasing empirical evidence to raise confidence in existing solutions, and (iii) addressing the socio-technical and business challenges related to cloud lock-in.}
}

@article{rayyan-727968010,
  title={Security and privacy in cloud computing via obfuscation and diversification: A survey},
  year={2015},
  pages={529-535},
  author={Hosseinzadeh, Shohreh and Hyrynsalmi, Sami and Conti, Mauro and Leppänen, Ville},
  keywords={systematic literature review, Cloud computing, Data privacy, Privacy, cloud computing, Organizations, security, Malware, diversification, obfuscation, privacy},
  abstract={The development of cloud computing has facilitate the organizations with its services. This makes the security and privacy of the cloud even more significant. Diversification and obfuscation approaches are of the most promising proactive techniques that protect computers from harmful malware, by preventing them to take advantage of the security vulnerabilities. There is a large body of research on the use of diversification and obfuscation techniques for improving the security in various domains, including cloud computing. Cloud computing provides an excellent setting for applying diversification/obfuscation, as the computing platforms (virtual machines) are implemented in software. The main objective of this study is to determine in what ways obfuscation and diversification techniques are used to enhance the security and privacy of the cloud computing, and discover the potential avenues for the further research. To achieve this goal, we systematically review and report the papers that discuss/propose a technique to enhance the security and privacy of the cloud, using diversification and obfuscation techniques. As the result of the search we collected 43 papers published on the topic. In this report we present the process of data collection, analysis of the results, and classification of the related studies. The classification is done based on how the diversification/obfuscation techniques are used to enhance the security in cloud computing environment. The presented study gives a clear view of the state of the art of the existing works in the field, and sheds light on the areas remained intact which could be avenues for further research. The existing works cover surprisingly a small set of the wealth of opportunities for diversification/obfuscation.}
}

@article{rayyan-727968011,
  title={What do we know about software product management? - a systematic mapping study},
  year={2011},
  pages={26-35},
  author={Maglyas, Andrey and Nikula, Uolevi and Smolander, Kari},
  keywords={Software, systematic literature review, Systematics, Project management, systematic mapping study, Databases, Context, Companies, cloud environment, services, software product management},
  abstract={Software product management (SPM) offers tools and practices for achieving business goals of a company as well as for increasing the predictability and profitability of software product development. Despite the importance of this topic, the studies of SPM have this far been fragmented. The goal of the present study is to summarize the existing knowledge in software product management and identify the areas which need further research. The paper reports the conduct and the results of a systematic mapping study which identified 25 studies on SPM. Still, most of the papers had only hypotheses and theories that were not empirically confirmed or the confirmation was based on a small set of cases. The existing knowledge of software product management consists of small and unconnected pieces. In addition to this, our specific interest, software product management in the cloud environment has not been studied at all. However, since both researchers and practitioners find research in SPM important, this area needs more research in the future.}
}

@article{rayyan-727968012,
  title={A survey of secondary studies in software process improvement},
  year={2016},
  pages={1-8},
  author={Idri, Ali and Cheikhi, Laila},
  keywords={Software, Secondary studies, systematic literature review, Systematics, Bibliographies, Software process improvement, Libraries, Industries, Standards, Face, literature survey},
  abstract={Software Process Improvement (SPI) has become one of the main strategic objectives in software industry. Companies make more investments in implementing software quality standards and models that focus on process assessment to improve their performance and productivity. To achieve these goals, companies focus on improving their process by means of improvement initiatives which may be implemented. To help practitioners find more innovative ways to manage and implement software process improvement initiatives efficiently, an important number of studies related to this topic have been emerged in recent years. Some of them, referred to as secondary studies, focused on the interpretation and synthesis of available published research works by giving an up to date state of art about SPI. This state of the art is provided in a form of literature surveys or in a methodological form using well established approaches such as systematic reviews or systematic mappings or tertiary studies. The objective of this paper is to identify and present the current secondary studies on SPI. The purpose is to discuss methods that these literature reviews of SPI use, their quality, and specific subjects that they cover. A set of survey research questions have been proposed and discussed through the investigation of 70 selected secondary studies collected from different digital libraries. The results show that success factors and issues related to implementation of SPI initiatives are the most studied, and there is a need to address in depth the measurement aspects in SPI.}
}

@article{rayyan-727968013,
  title={Redefinition of fault classes in logic expressions},
  year={2012},
  pages={144-153},
  author={Paul, T K and Lau, M F},
  keywords={Software, Software testing, systematic literature review, Systematics, Terminology, fault class, Fault detection, Logic expression, Syntactics},
  abstract={Fault-based testing selects test cases to detect hypothesized faults. In logic expression testing, many fault classes have been defined by researchers based on the syntax of the expressions. Due to the syntactic nature of the logic expressions, some fault classes may exist in one form (say, disjunctive normal form - DNF) of the logic expressions but not in other forms (say, general form). As a result, different fault-based testing techniques have been developed for different types of logic expressions and these techniques have different fault detecting capabilities. For example, some have high detecting power in DNF but low detecting power in the general form. Another complication arises when software developers decide which forms of logic expressions should be used in the first place. Should software developers use the general form for flexibility but compromise that with fewer fault classes and less fault detection? Or should they use DNF for more fault classes and, hence, better fault detection (because software developers have more ”hypothesized faulty” scenarios to test) but sacrificing the generality of the expressions. In this paper, we propose a set of uniform definitions of fault classes such that they can be applied irrespective of the syntactic nature of the logic expressions, to produce consistent fault-based testing techniques and fault detection capabilities.}
}

@article{rayyan-727968014,
  title={A survey of metrics use in finnish software companies},
  year={2011},
  pages={49-57},
  author={Soini, Jari},
  keywords={Software, Software measurement, Current measurement, Companies, Process control, case study, empirical software measurement, software metrics, software process measurement, Metronidazole},
  abstract={This study looks at software measurement practices and experiences related to software engineering in Finland. This paper uses the results of an empirical case study to examine how measurement was implemented in practice from the perspective of the software process. The research was motivated by the challenges recognized in the implementation and execution of measurement in software development work. The empirical results presented and also analyzed in this paper were captured by interviews and questionnaires during a research project (Software Measurement - SoMe) carried out in Finland. The project focused on clarifying and identifying the current software process and product metrics used and their utilization in Finnish software companies. The results of the case study show how the measurement was targeted and emphasized in the software process. The paper points out the key issues that arose when the empirical results were compared to the existing theoretical knowledge on measurement in software engineering.}
}

@article{rayyan-727968015,
  title={Prioritization based taxonomy of DevOps security challenges using PROMETHEE},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={105426-105446},
  author={Rafi, Saima and Yu, Wu and Akbar, Muhammad Azeem and Alsanad, Ahmed and Gumaei, Abdu},
  keywords={Software, Systematics, Bibliographies, Security, Taxonomy, Organizations, challenges, Industries, DevOps security, empirical investigations},
  abstract={DevOps is a combination of collaborative and multidisciplinary efforts of an organization to control continuous delivery and updates of new software while guaranteeing their reliability and correctness. In the software industry, the implementation of DevOps (development and operations units) faces many challenges that are specifically associated with the security. The objective of this study is to identify and develop a prioritization based taxonomy of DevOps security challenges. The total of eighteen DevOps security challenges were extracted using systematic literature review approach and were further evaluated with experts using questionnaire survey study. Finally, the multi criteria decision making PROMETHEE-II approach was used to prioritize and develop the taxonomy of identified factors and their categories. The implications of PROMETHEE-II approach are novel in this research domain as it has been used successfully in various other domains e.g. medical, banking, internet techniques and management etc. The contribution of this study is not limited to develop the taxonomy based structure of DevOps security challenges, but also the proper prioritization of these challenges by introducing PROMETHEE-II approach in the research field of DevOps. The study results will assist the practitioners to remove the uncertainty and vagueness in the opinion of DevOps experts to secure DevOps implementation for better and continuous software development process.}
}

@article{rayyan-727968016,
  title={Conceptual developments in genetic programming for time series forecasting},
  year={2015},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={13},
  number={8},
  pages={2728-2733},
  author={A. Martinez, Carlos and David Velasquez, Juan},
  keywords={Systematics, Manuals, Forecasting, Genetic programming, Computational modeling, Predictive models, Neural Networks, Genetic Algorithms, Nonlinear models, Time series, Time series analysis},
  abstract={Objective: The aim of this paper is to analyze the main research areas in Genetic Programming (GP). Method: We used the systematic literature review method employing an automatic search with manual refining of papers published on GP between 1992 to 2012. Results: Just 63 studies meet all the requirements of the inclusion criteria. Conclusion: Although studies relating to the application of genetic programming in the forecast of time series were frequently presented, we find that the studies proposing changes in the original algorithm of GP with a theoretical support and a systematic procedure for the construction of model were scarce in the time 1992-2012.}
}

@article{rayyan-727968017,
  title={Towards a metamodel for a requirements engineering process of embedded systems},
  year={2016},
  pages={93-100},
  author={Pereira, Tarcísio and Albuquerque, Deivson and Sousa, Aêda and Alencar, Fernanda and Castro, Jaelson},
  keywords={requirements engineering, embedded systems, Systems engineering and theory, metamodel},
  abstract={In the embedded systems (ES) area, more than 50% of problems occur at system delivery and are related to misconceptions in capturing requirements. According to our systematic literature review (SLR), no evidence explicitly depicts how an embedded system must be elicited and specified. However, understanding the embedded systems and their environment is a strenuous activity. Even though current approaches present some contributions, the definition of a systematic requirements engineering process remains a challenging issue. Based on this shortcoming, we developed a metamodel that defines concepts and relationships that must be taken into account the development of an ES. From the metamodel, we define a precise requirements engineering process. This research presents the main results of a SLR, a resource model, and a sketch of a process to guide the requirements development of embedded systems.}
}

@article{rayyan-727968018,
  title={Multicriteria based decision making of DevOps data quality assessment challenges using fuzzy TOPSIS},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={46958-46980},
  author={Rafi, Saima and Yu, Wu and Akbar, Muhammad Azeem and Alsanad, Ahmed and Gumaei, Abdu},
  keywords={Software, Systematics, Bibliographies, Organizations, Industries, Data integrity, empirical investigation, Object recognition, DevOps data quality assessment, fuzzy TOPSIS, Decision Making, Research Design},
  abstract={In current era, DevOps gain much interaction in software industry as it provides the flexible development environment. To meet the continuous development and operations, DevOps mainly focus, to integrate the data from heterogeneous source. While DevOps adoption, the quality assessment of data integrated from heterogeneous environment, is important and challenging at the same time. This study aims to identify the critical factors that could negatively impact the data quality assessment process in DevOps. We have used the systematic literature review (SLR) approach and identify a total of 13 critical challenging factors. The finding of SLR are further validated with industry experts via questionnaire survey. Finally, we have applied the Fuzzy TOPSIS approach to prioritize the investigated challenging factors with respect to their significance of DevOps data quality assessment process. The results show that analyzing data in real time, visualization of data and missing information and other invalid data are the highest ranked challenging factors which need to be addressed on priority basis, to successfully measure the quality of heterogeneous data in DevOps. We believe that the finding of this study will assist the practitioner to consider the most significant factors for measuring the quality of heterogeneous data in DevOps.}
}

@article{rayyan-727968019,
  title={Towards a collaborative repository for the documentation of service-based antipatterns and bad smells},
  year={2019},
  pages={95-101},
  author={Bogner, Justus and Boceck, Tobias and Popp, Matthias and Tschechlov, Dennis and Wagner, Stefan and Zimmermann, Alfred},
  keywords={Collaboration, SOA, Service-oriented architecture, Microservices, Taxonomy, Libraries, Search engines, Documentation, antipatterns, bad smells, service-based systems, Smell},
  abstract={While the concepts of object-oriented antipatterns and code smells are prevalent in scientific literature and have been popularized by tools like SonarQube, the research field for service-based antipatterns and bad smells is not as cohesive and organized. The description of these antipatterns is distributed across several publications with no holistic schema or taxonomy. Furthermore, there is currently little synergy between documented antipatterns for the architectural styles SOA and Microservices, even though several antipatterns may hold value for both. We therefore conducted a Systematic Literature Review (SLR) that identified 14 primary studies. 36 service-based antipatterns were extracted from these studies and documented with a holistic data model. We also categorized the antipatterns with a taxonomy and implemented relationships between them. Lastly, we developed a web application for convenient browsing and implemented a GitHub-based repository and workflow for the collaborative evolution of the collection. Researchers and practitioners can use the repository as a reference, for training and education, or for quality assurance.}
}

@article{rayyan-727968020,
  title={Consensus mechanisms in distributed ledgers for the protection of confidential data: A multivocal literature review},
  year={2020},
  pages={166-173},
  author={Vargas-Gómez, Renato and Peréz-Arriaga, Juan Carlos and Ocharán-Hernández, Jorge Octavio and Sánchez-García, Angel J},
  keywords={Software, Systematics, Bibliographies, Multivocal literature review, Blockchain, Distributed ledger, Privacy, Distributed ledger technology, Proposals, Data protection, Distributed databases, Confidentiality},
  abstract={Distributed Ledger Technologies (DLT) open new opportunities for data protection since they bring decentralization and sovereignty over the ownership of data. On the practical side, implementations of this technology are limited. Due to its novelty, the fundamentals for their design and development are still emerging. An essential feature of DLT is the consensus mechanism, which is used so that the members of the DLT network validate and append data to the ledger. The design decision of which mechanism to use will significantly impact the functionality of the system. Therefore, it is a decision that cannot be taken lightly. Software engineers, developers, and other practitioners in the field can use the knowledge and understanding of the implementations that already exist to help with these decisions. This paper presents the results of a multivocal literature review carried out to identify the consensus mechanisms used in DLT for the protection of confidential data. Twenty-seven studies were selected; in those studies, twenty-one different consensus mechanisms were identified. The review showcases the mechanisms that have been identified and their contexts, alongside a discussion of their relevance and characteristics.}
}

@article{rayyan-727968021,
  title={Teaching multidisciplinary teams requirements for undergraduate students: an approach to augmented reality software in design thinking context},
  year={2018},
  pages={1-7},
  author={de Almeida, Eduarda Maganha and Damasceno, Eduardo Filgueiras and L'Erário, Alexandre},
  keywords={Software, Software engineering, Systematics, Requirements engineering, Process, Education, Augmented reality, Augmented Reality, Requirements, Virtual environments, Software Design, Thinking},
  abstract={Augmented Reality (AR) has the advantage of allowing the use of tangible actions and multimodal operations. It is believed that the development of AR scenarios is a difficult task, requiring knowledge and understanding in several areas. With this, the disciplines of Software Engineering play a fundamental role in the development of these scenarios. The teaching of requirements and concepts of elicitation is primordial in the first years of the courses of Computing. A systematic review to map the use of Requirements Engineering in the development of AR scenarios pointed to a gap regarding the requirements elicitation process for AR, which led us to think of a teaching strategy to help undergraduate students to specify AR scenario requirements. Therefore, in 2016, we conducted experimental tests at the University Technological Federal of Paraná with the undergraduate students of Systems Analysis and Development and Software Engineering, to present a strategy that would contribute to the elicitation of requirements for RA scenarios. These experimental tests verified the strategy, and the results indicate if it is possible to help the students in the elicitation of requirements for AR scenarios from this approach..}
}

@article{rayyan-727968022,
  title={An ontology for task allocation to teams in distributed software development},
  year={2013},
  pages={21-30},
  author={Marques, Anna Beatriz and Carvalho, José Reginaldo and Rodrigues, Rosiane and Conte, Tayana and Prikladnicki, Rafael and Marczak, Sabrina},
  keywords={Software, Systematics, Ontologies, Distributed software development, Analytical models, Proposals, Planning, Resource management, Domain Ontology, Task allocation},
  abstract={An adequate task allocation plan is an effective strategy to reduce collaboration issues in distributed software development. Practitioners adopt distinct processes to allocate tasks as well as diverse labels for the same activities and artifacts. This diversity is also found in literature. Task allocation proposals consider different elements and use distinct names for the same concepts. The lack of a standardized vocabulary and of an understanding of the elements involved impairs knowledge acquisition and sharing. Our paper presents a domain ontology to represent concepts related to task allocation in distributed teams. The ontology was defined based on a literature systematic mapping and on the opinion of experts. Preliminary evaluation suggests that the relationships among concepts are valid in real projects. The ontology brings awareness to managers regarding the factors related to task allocation planning and provides researchers with a framework to define processes and design tools to support such activity.}
}

@article{rayyan-727968023,
  title={Theoretical maximum prediction accuracy for analogy-based software cost estimation},
  year={2008},
  pages={495-502},
  author={Keung, Jacky W},
  keywords={Analogy, Software engineering, Software measurement, Software performance, Software metrics, Programming, Australia, Application software, Costs, Predictive models, Accuracy, Software Cost Estimation, K-NN, MMRE, Software Metrics and Measurement, Software},
  abstract={Software cost estimation is an important area of research in software engineering. Various cost estimation model evaluation criteria (such as MMRE, MdMRE etc.) have been developed for comparing prediction accuracy among cost estimation models. All of these metrics capture the residual difference between the predicted value and the actual value in the dataset, but ignore the importance of the dataset quality. What is more, they implicitly assume the prediction model to be able to predict with up to 100% accuracy at its maximum for a given dataset. Given that these prediction models only provide an estimate based on observed historical data, absolute accuracy cannot be possibly achieved. It is therefore important to realize the theoretical maximum prediction accuracy (TMPA) for the given model with a given dataset. In this paper, we first discuss the practical importance of this notion, and propose a novel method for the determination of TMPA in the application of analogy-based software cost estimation. Specifically, we determine the TMPA of analogy using a unique dynamic K-NN approach to simulate and optimize the prediction system. The results of an empirical experiment show that our method is practical and important for researchers seeking to develop improved prediction models, because it offers an alternative for practical comparison between different prediction models.}
}

@article{rayyan-727968024,
  title={The adoption of JavaScript linters in practice: A case study on ESLint},
  year={2020},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={46},
  number={8},
  pages={863-891},
  author={Tómasdóttir, Kristín Fjóla and Aniche, Maurício and Van Deursen, Arie},
  keywords={Software, Tools, Static analysis, Interviews, empirical software engineering, Standards, Encoding, Face, ASATs, eslint, javascript linters, linters, Static analysis tools},
  abstract={A linter is a static analysis tool that warns software developers about possible code errors or violations to coding standards. By using such a tool, errors can be surfaced early in the development process when they are cheaper to fix. For a linter to be successful, it is important to understand the needs and challenges of developers when using a linter. In this paper, we examine developers' perceptions on JavaScript linters. We study why and how developers use linters along with the challenges they face while using such tools. For this purpose we perform a case study on ESLint, the most popular JavaScript linter. We collect data with three different methods where we interviewed 15 developers from well-known open source projects, analyzed over 9,500 ESLint configuration files, and surveyed 337 developers from the JavaScript community. Our results provide practitioners with reasons for using linters in their JavaScript projects as well as several configuration strategies and their advantages. We also provide a list of linter rules that are often enabled and disabled, which can be interpreted as the most important rules to reason about when configuring linters. Finally, we propose several feature suggestions for tool makers and future work for researchers.}
}

@article{rayyan-727968025,
  title={Crafting a global teaming model for architectural knowledge},
  year={2010},
  pages={55-63},
  author={Beecham, Sarah and Noll, John and Richardson, Ita and Ali, Nour},
  keywords={Software, Distributed Software Development, Knowledge management, Programming, Global software development, Software Process Improvement, Global software engineering, Organizations, Computer architecture, Global communication, Book reviews, Software Processes, Architectural Knowledge Management, Global Teaming Model, Virtual Teams},
  abstract={In this paper, we present the Global Teaming Model (GTM), which is empirically grounded, and outlines practices that managers need to consider when managing virtual teams. We explain how the model can be adapted to specific areas of software development, and use architectural knowledge management (AKM) as our exemplar. We focus on specific practices relating to how teams collaborate and share essential architectural knowledge across multiple sites. Through a review of the literature, we develop an in-depth view of recommended practices associated with AKM in a global environment. We then consider how we can incorporate these AKM practices into our Global Teaming model to ensure managers are given the necessary support. Our contribution to research therefore is to present AKM practices within the context of all other Global Software Development processes.}
}

@article{rayyan-727968026,
  title={Formative evaluation of a tool for managing software quality},
  year={2017},
  pages={297-306},
  author={Guzman, Liliana and Vollmer, Anna Maria and Ciolkowski, Marcus and Gillmann, Michael},
  keywords={software engineering, Tools, Software quality, Reliability, Interviews, software quality, Instruments, Prototypes, software measurement, Software},
  abstract={Context/Background: To achieve high software quality, particularly in the context of agile software development, organizations need tools to continuously analyze software quality. Several quality management (QM) tools have been developed in recent years. However, there is a lack of evidence regarding the quality of QM tools, standardized definitions of such quality, and reliable instruments for measuring it. This, in turn, impedes proper selection and improvement of QM tools. Goals: We aimed at operationalizing the quality of a research QM tool, namely the ProDebt prototype, and evaluating its quality. The goal of the ProDebt prototype is to provide practitioners with support for managing software quality and technical debt. Method: We performed interviews, workshops, and a mapping study to operationalize the quality of the ProDebt prototype and to identify reliable instruments to measure it. We designed a mixed-method study aimed at formative evaluation, i.e., at assessing the quality of the ProDebt prototype and providing guidance for its further development. Eleven practitioners from two German companies evaluated the ProDebt prototype. Results: The participants assessed the information provided by the ProDebt prototype as understandable and relevant. They considered the ProDebt prototype's functionalities as easy to use but of limited usability. They identified improvement needs, e.g., that the analysis results should be linked to other information sources. Conclusions: The evaluation design was of practical value for evaluating the ProDebt prototype considering the limited resources such as the practitioners' time. The evaluation results provided the developers of the ProDebt prototype with guidance for its further development. We conclude that it can be used and tailored for replication or evaluation of other QM tools.}
}

@article{rayyan-727968027,
  title={Usable results from the field of API usability: A systematic mapping and further analysis},
  year={2012},
  pages={179-182},
  author={Burns, Chris and Ferreira, Jennifer and Hellmann, Theodore D and Maurer, Frank},
  keywords={meta-analysis, systematic review, Software engineering, Systematics, Visualization, API usability, Usability, Conferences, Abstracts, systematic map, application programming interface},
  abstract={Modern software development often involves the use of complex, reusable components called Application Programming Interfaces (APIs). Developers use APIs to complete tasks they could not otherwise accomplish in a reasonable time. These components are now vital to mainstream software development. But as APIs have become more important, understanding how to make them more usable is becoming a significant research question. To assess the current state of research in the field, we conducted a systematic mapping. A total of 28 papers were reviewed and categorized based on their research type and on the evaluation method employed by its authors. We extended the analysis of a subset of the papers we reviewed beyond the usual limits of a systematic map in order to more closely examine details of their evaluations - such as their structure and validity - and to summarize their recommendations. Based on these results, common problems in the field are discussed and future research directions are suggested.}
}

@article{rayyan-727968028,
  title={A systematic review: B-cell conformational epitope prediction from epitope characteristics view},
  year={2017},
  pages={93-98},
  author={Solihah, Binti and Winarko, Edi and Afiahayati and Hartati, Sri and Wibowo, Moh. Edi},
  keywords={Systematics, Bibliographies, Databases, Feature extraction, 3D structure based feature, Amino acids, conformational epitope prediction method, epitope characteristics, Prediction methods, Sun},
  abstract={B-cell conformational epitope identification is the crucial issue in vaccinology. Limitation on experimental methods in the biological side, dataset and availability of computational resources opens a chance on developing prediction method which can accelerate epitope identification. A number of methods have been developed but their performance is still medium. Epitope prediction is a knowledge-based method. Presenting the statistical or computational based epitope characteristics together with epitope prediction method will facilitate the newcomer on identifying importance feature, improve the existing feature and propose the new feature. To reach the goal of the review, the research papers are collected from both epitope analysis research and epitope prediction methods research. The prediction methods are evaluated on what characteristics of epitope have been implemented on feature representation and are shown in a mapping table.}
}

@article{rayyan-727968029,
  title={Investigating the impact of development task on external quality in test-driven development: An industry experiment},
  year={2019},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  pages={1},
  author={Tosun, Ayse and Dieste, Oscar and Vegas, Sira and Pfahl, Dietmar and Rungi, Kerli and Juristo, Natalia},
  keywords={Bibliographies, Task analysis, Test-driven development, Productivity, Organizations, Industries, Programming profession, experimental task, external quality, incremental test-last development, industry experiment},
  abstract={Reviews on test-driven development (TDD) studies suggest that the conflicting results reported in the literature are due to unobserved factors, such as the tasks used in the experiments, and highlight that there are very few industry experiments conducted with professionals. The goal of this study is to investigate the impact of a new factor, the chosen task, and the development approach on external quality in an industrial experiment setting with 17 professionals. The participants are junior to senior developers in programming with Java, beginner to novice in unit testing, JUnit, and they have no prior experience in TDD. The experimental design is a 2 ×2 cross-over, i.e., we use two tasks for each of the two approaches, namely TDD and incremental test-last development (ITLD). Our results reveal that both development approach and task are significant factors with regards to the external quality achieved by the participants. More specifically, the participants produce higher quality code during ITLD in which splitting user stories into subtasks, coding, and testing activities are followed, compared to TDD. The results also indicate that the participants produce higher quality code during the implementation of Bowling Score Keeper, compared to that of Mars Rover API, although they perceived both tasks as of similar complexity. An interaction between the development approach and task could not be observed in this experiment. We conclude that variables that have not been explored so often, such as the extent to which the task is specified in terms of smaller subtasks, and developers' unit testing experience might be critical factors in TDD experiments. The real-world appliance of TDD and its implications on external quality still remain to be challenging unless these uncontrolled and unconsidered factors are further investigated by researchers in both academic and industrial settings.}
}

@article{rayyan-727968030,
  title={Choosing component origins for software intensive systems: In-house, COTS, OSS or Outsourcing?—A case survey},
  year={2018},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={44},
  number={3},
  pages={237-261},
  author={Petersen, Kai and Badampudi, Deepika and Shah, Syed Muhammad Ali and Wnuk, Krzysztof and Gorschek, Tony and Papatheocharous, Efi and Axelsson, Jakob and Sentilles, Séverine and Crnkovic, Ivica and Cicchetti, Antonio},
  keywords={Software, Decision making, OSS, Outsourcing, COTS, Computer architecture, Industries, Companies, outsourcing, in-house},
  abstract={The choice of which software component to use influences the success of a software system. Only a few empirical studies investigate how the choice of components is conducted in industrial practice. This is important to understand to tailor research solutions to the needs of the industry. Existing studies focus on the choice for off-the-shelf (OTS) components. It is, however, also important to understand the implications of the choice of alternative component sourcing options (CSOs), such as outsourcing versus the use of OTS. Previous research has shown that the choice has major implications on the development process as well as on the ability to evolve the system. The objective of this study is to explore how decision making took place in industry to choose among CSOs. Overall, 22 industrial cases have been studied through a case survey. The results show that the solutions specifically for CSO decisions are deterministic and based on optimization approaches. The non-deterministic solutions proposed for architectural group decision making appear to suit the CSO decision making in industry better. Interestingly, the final decision was perceived negatively in nine cases and positively in seven cases, while in the remaining cases it was perceived as neither positive nor negative.}
}

@article{rayyan-727968031,
  title={Patterns of evolution in the practice of distributed software development in wholly owned subsidiaries: A preliminary capability model},
  year={2008},
  pages={99-108},
  author={Prikladnicki, Rafael and Damian, Daniela and Audy, Jorge Luis Nicolas},
  keywords={Software, Programming, Distributed software development, Business, Organizations, Companies, Book reviews, capability model, Evolution (biology), internal offshoring, offshore outsourcing},
  abstract={In this paper, we describe a preliminary capability model that captures patterns of evolution in the practice of distributed software development in internal offshoring projects. In our research we seek to understand how the practices of organizations involved in the internal offshoring of software development evolve over time, from a software engineering perspective, and from the point of view of the subsidiaries. Based on a combination of qualitative and quantitative methods, we propose a capability model that encompasses the evolution of software development activities within and among several subsidiaries owned by an organization.}
}

@article{rayyan-727968032,
  title={Systematic mapping study of dealing with error in software development effort estimation},
  year={2016},
  pages={140-147},
  author={Koutbi, Salma El and Idri, Ali and Abran, Alain},
  keywords={Software, Systematics, Quality assessment, systematic mapping study, Uncertainty, Databases, error, Estimation error, software development effort estimation, uncertainty},
  abstract={Over the last decades, the software engineering community has investigated new techniques for software development effort estimation. Unfortunately, the estimates were not always accurate. Error approaches are then, aninteresting track for improving the projects running performances and their financial profitability. The aim of this systematic mapping study is to summarize and synthesize theexisting studies dealing with effort estimation error and uncertainty and to classify them based on research approaches, contribution types, accuracy criteria, datasets, error approaches and effort estimation techniques used. In total 19papers published between 1990 and 2015 were selected. We observed a balance between the managerial approaches and the technical ones. Furthermore, the proposed errortechniques and frameworks improve in general the accuracy ofeffort estimation techniques. Fuzzy logic, bootstrapping andrisk analysis are promising avenues that could be combined with various estimation techniques.}
}

@article{rayyan-727968033,
  title={Domain ontologies in the context of Requirements Engineering},
  year={2015},
  pages={1-8},
  author={Parreira, Paulo Afonso and Dellosso Penteado, Rosângela Aparecida},
  keywords={Software Engineering, Systematics, Requirements Engineering, Ontologies, Literature Review, Computers, Systematic Mapping, Domain Ontologies},
  abstract={Context: obtaining accurate information about the problem domain for which the software is being developed is one of most important and difficult activity of the Requirements Engineering (RE). One of the strategies to deal with problem domain concepts and their relationships is the usage of domain ontologies. However, there are not studies in the literature that describe which ontology-based approaches exist and how they have been applied in the RE context. Goal: this paper aims to identify, understand and catalog existing primary studies in the literature regarding to this subject. Method: for this, a Systematic Mapping was conducted and sixty-seven studies were analyzed and cataloged. Some results are: (i) there are five main ways of using ontologies in the context of RE and they are related to “Requirements Elicitation”, “Requirements Analysis”, “Requirements Verification”, “Conflict Identification and Analysis” and “Unification among requirements formalisms”; (ii) there are few evaluation studies about this subject; among others.}
}

@article{rayyan-727968034,
  title={Energy management software systems based on ISO 50001 standard: A preliminary systematic mapping study},
  year={2017},
  pages={1-7},
  author={Castro, John W and Fonseca, C Efraín R and Meléndez, M Pablo},
  keywords={software engineering, Systematics, systematic mapping study, Software systems, Standards organizations, ISO Standards, Energy management, energy management systems, ISO 50001, Software},
  abstract={The United Nations Organization for Industrial Development requested the International Organization for Standardization the development of an international standard for energy management, within the framework of the industries' needs to have an effective response to climate changes. As a result, the ISO 50001 standard was published in 2011. This standard specifies the guidelines for the implementation of Energy Management Systems in companies, through a continuous cycle of improvements. Due to the standard's recentness, it is necessary to determine the software systems and its application that could support the industry to facilitate the standard's application. This work aims to know the current software systems development trend regarding the use of ISO 50001 standard. We perform a preliminary Systematic Mapping Study (SMS), as a research method, in order to establish the state of the art about the incorporation of ISO 50001 standard into software systems for energy management. 8 primary studies were found as a result of this research, which were categorized into two groups: studies who describe a methodology to apply the standard, and studies who propose a software system for implement the standard. There are few studies reporting software systems for energy management based on ISO 50001 standard; hence, we believe that it is necessary to make a greater effort for the research and development of this area.}
}

@article{rayyan-727968035,
  title={Analysis of requirements engineering techniques for IT-enabled product service systems},
  year={2011},
  pages={50-58},
  author={Berkovich, Marina and Leimeister, Jan Marco and Hoffmann, Axel and Krcmar, Helmut},
  keywords={Software, Software engineering, Visualization, Context, Electronic mail, Companies, requirements engineering, Documentation, complex solution, product service system, technique},
  abstract={Product service systems (PSS) are introduced by many companies to increase their differentiation and to provide integrated solutions to customers. PSS are integrated solutions consisting of physical products, software and services aiming at providing an individualized solution to a customer's problem. The specific attributes of PSS lead to specific requirements for requirements engineering (RE). The goal of this paper is to analyze to which degree the analysis techniques of software engineering are suitable for PSS. We therefore conducted a structured literature review of software engineering techniques. The criteria for assessing the suitability of the techniques were based on the characteristics of PSS and the task of RE in the development process of PSS. We analyzed five textbooks and 144 scientific articles and identified 27 groups of techniques. The result is that there are major gaps in techniques for RE for PSS. Two of ten criteria are not satisfied by any technique. Moreover, for the majority of tasks of RE multiple techniques have to be combined for satisfying the criteria. In summary, the literature review shows that the techniques of software engineering are largely not directly applicable to PSS.}
}

@article{rayyan-727968036,
  title={Adopting software product lines: A systematic mapping study},
  year={2011},
  pages={11-20},
  author={Bastos, Jonatas Ferreira and da Mota Silveira Neto, Paulo Anselmo and de Almeida, Eduardo Santana and de Lemos Meira, Silvio Romero},
  keywords={Software},
  abstract={Context: The benefits of taking a product line approach in order to achieve significant reductions in cost and time to market and, at the same time, increasing the quality has encouraged product line adoption. Objective: In this context, this study focuses on some SPL adoption aspects and has the following goals: investigate state-of-the-art SPL adoption, synthesize available evidence, and identify gaps between required strategies, organizational structures, maturity level and existing adoption barriers, available in the literature. Method: A systematic mapping study was undertaken to analyze the important aspects that should be considered when adopting SPL approaches. A set of four questions were defined in which 34 primary studies were evaluated. Results: A total of 34 primary studies were considered. They reported four different strategies (Incremental, Big Bang, Tactical and Pilot project), however there is insufficient information about how such strategies link to factors as organizational structure and process maturity. By investigating all primary studies we found 23 barriers to adoption. Conclusions: Researchers need to consider the relationships between SPL adoption and factors such as company maturity and organization structure in more detail. There is also a need for patterns to assist in SPL adoption and overcoming SPL adoption barriers.}
}

@article{rayyan-727968037,
  title={Whence to learn? Transferring knowledge in configurable systems using BEETLE},
  year={2020},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  pages={1},
  author={Krishna, Rahul and Nair, Vivek and Jamshidi, Pooyan and Menzies, Tim},
  keywords={Software engineering, Tools, Optimization, Computer science, Software systems, Data collection, Bellwether, Performance Optimization, SBSE, Transfer Learning},
  abstract={As software systems grow in complexity and the space of possible configurations increases exponentially, finding the near-optimal configuration of a software system becomes challenging. Recent approaches address this challenge by learning performance models based on a sample set of configurations. However, collecting enough sample configurations can be very expensive since each such sample requires configuring, compiling, and executing the entire system using a complex test suite. When learning on new data is too expensive, it is possible to use Transfer Learning to “transfer” old lessons to the new context. Traditional transfer learning has a number of challenges, specifically, (a) learning from excessive data takes excessive time, and (b) the performance of the models built via transfer can deteriorate as a result of learning from a poor source. To resolve these problems, we propose a novel transfer learning framework called BEETLE, which is a “bellwether”-based transfer learner that focuses on identifying and learning from the most relevant source from amongst the old data. This paper evaluates BEETLE with 57 different software configuration problems based on five software systems (a video encoder, an SAT solver, a SQL database, a high-performance C-compiler, and a streaming data analytics tool). In each of these cases, BEETLE found configurations that are as good as or better than those found by other state-of-the-art transfer learners while requiring only a fraction 1/7th of the measurements needed by those other methods. Based on these results, we say that BEETLE is a new high-water mark in optimally configuring software.}
}

@article{rayyan-727968038,
  title={A taxonomy of metrics for software fault prediction},
  year={2020},
  pages={429-436},
  author={Caulo, Maria and Scanniello, Giuseppe},
  keywords={Software, Software engineering, Systematics, Measurement, Software metrics, Taxonomy, taxonomy, Object oriented modeling, fault prediction, software metrics, Metronidazole},
  abstract={Researchers in the field of Software Fault Prediction (SFP) make use of software metrics to build predictive models, for example, by means of machine learning and statistical techniques. The number of metrics used for SFP has increased dramatically in the last few decades. Therefore, a taxonomy of metrics for SFP could be useful to standardize the lexicon, to simplify the communication among researchers/practitioners, and to organize and classify such metrics. In this research, we built a taxonomy of metrics for SFP with the aim of making it as comprehensive as possible. We exploited and extended two Systematic Literature Reviews (SLRs) to collect and classify a total of 512 metrics for SFP and then to build our taxonomy. We also provide information on the metrics in this taxonomy in terms of: acronym(s), extended name, description, granularity of the prediction, category, and research papers in which they were used. To allow the taxonomy to be constantly updated over time, we provide external contributors the possibility to ask for changes via pull-requests on GitHub.}
}

@article{rayyan-727968039,
  title={Bellwethers: A baseline method for transfer learning},
  year={2019},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={45},
  number={11},
  pages={1081-1105},
  author={Krishna, Rahul and Menzies, Tim},
  keywords={Software, Software engineering, Task analysis, Transfer learning, Estimation, Complexity theory, Analytical models, prediction, effort estimation, Benchmark testing, bad smells, defect prediction, issue close time},
  abstract={Software analytics builds quality prediction models for software projects. Experience shows that (a) the more projects studied, the more varied are the conclusions; and (b) project managers lose faith in the results of software analytics if those results keep changing. To reduce this conclusion instability, we propose the use of “bellwethers”: given N projects from a community the bellwether is the project whose data yields the best predictions on all others. The bellwethers offer a way to mitigate conclusion instability because conclusions about a community are stable as long as this bellwether continues as the best oracle. Bellwethers are also simple to discover (just wrap a for-loop around standard data miners). When compared to other transfer learning methods (TCA+, transfer Naive Bayes, value cognitive boosting), using just the bellwether data to construct a simple transfer learner yields comparable predictions. Further, bellwethers appear in many SE tasks such as defect prediction, effort estimation, and bad smell detection. We hence recommend using bellwethers as a baseline method for transfer learning against which future work should be compared.}
}

@article{rayyan-727968040,
  title={Analyzing excessive user feedback: A big data challenge},
  year={2018},
  pages={206-211},
  author={Bukhsh, Faiza Allah and Arachchige, Jeewanie Jayasinghe and Malik, Furqan},
  keywords={Software, Tools, Natural language processing, Big data analytics, Big Data, Feature extraction, Proposals, Data analysis, Feedback analysis, User Reviews, Feedback},
  abstract={User involvement in the process of discovering and shaping the product is the base of software systems. In recent years, however, a shift in the user feedback has been observed: repositories of user data have become increasingly more subjected to analysis for improvement purposes. Significant surge has been seen in feedback collected from users in the form of reviews and ratings along with app usage statistics. This led software engineering researchers to deploy big data analytics techniques in order to figure out the requirements that should be met in the future software system releases. While a variety of big data analytics methods exist, it is not clear which ones have been used and what are the benefits and disadvantages of these proposals. In this paper, we have aimed to outline the recently published proposals for big data analytics techniques for user feedback analysis. We found that the majority of the techniques rest on natural language processing concepts and visualization. Our findings also indicate that the majority of the proposals come from the United States, Germany and the United Kingdom. Moreover, we also found the proposed techniques perform well with the chosen data-sets however the generalizability and scalability of these method raised concerns as these methods are not evaluated based on real-world cases.}
}

@article{rayyan-727968041,
  title={Using the findings of a mapping study to conduct a research project: A case in knowledge management in software testing},
  year={2015},
  pages={208-215},
  author={Souza, Érica F and Falbo, Ricardo A and Vijaykumar, Nandamudi L},
  keywords={Software, Software testing, Software engineering, Systematics, Software Testing, Survey, Knowledge Management, Ontologies, Mapping Study, Planning, Systematic Mapping},
  abstract={A mapping study provides a broad overview of a research area in order to determine whether there is research evidence on a particular topic. Results of a systematic mapping may identify suitable areas for performing future research. In this paper, we discuss our experience in using the findings of a mapping study on Knowledge Management (KM) in Software Testing for performing a real research project, which also applied other empirical approaches. The main goals of this paper are: (i) to reinforce the importance of a systematic mapping in the conduction of a research project by discussing a real case of such application, and (ii) to present the results of our survey on the most important aspects of KM when applied to software testing.}
}

@article{rayyan-727968042,
  title={A systematic mapping study on security in agile requirements engineering},
  year={2018},
  pages={454-461},
  author={Villamizar, Hugo and Kalinowski, Marcos and Viana, Marx and Fernández, Daniel Méndez},
  keywords={Software, Software engineering, Systematics, Requirements engineering, Security, systematic mapping study, Libraries, security, Proposals, requirements engineering, agile methods},
  abstract={[Background] The rapidly changing business environments in which many companies operate is challenging traditional Requirements Engineering (RE) approaches. This gave rise to agile approaches for RE. Security, at the same time, is an essential non-functional requirement that still tends to be difficult to address in agile development contexts. Given the fuzzy notion of "agile" in context of RE and the difficulties of appropriately handling security requirements, the overall understanding of how to handle security requirements in agile RE is still vague. [Objective] Our goal is to characterize the publication landscape of approaches that handle security requirements in agile software projects. [Method] We conducted a systematic mapping to outline relevant work and contemporary gaps for future research. [Results] In total, we identified 21 studies that met our inclusion criteria, dated from 2005 to 2017. We found that the approaches typically involve modifying agile methods, introducing new artifacts (e.g., extending the concept of user story to abuser story), or introducing guidelines to handle security issues. We also identified limitations of using these approaches related to environment, people, effort and resources. [Conclusion] Our analysis suggests that more effort needs to be invested into empirically evaluating the existing approaches and that there is an avenue for future research in the direction of mitigating the identified limitations.}
}

@article{rayyan-727968043,
  title={Partnership models for software ecosystems: A systematic mapping study},
  year={2019},
  pages={387-394},
  author={Belo, Ítalo and Alves, Carina},
  keywords={Software engineering, systematic mapping study, Three-dimensional displays, C# languages, partnership models, software ecosystem, Software},
  abstract={[Background] Partnership models enable the generation and sharing of value among stakeholders of software ecosystems. Large companies like SAP and Microsoft form partnerships with complementors to develop applications and service solutions for customers. Partnership models are important strategic tools to manage software ecosystems. Several partnership models have been proposed and discussed in the literature. So far, there is no comprehensive understanding of the key elements and goals of such models. [Objective] This paper aims to synthesize the research knowledge in partnership models for software ecosystems. [Method] We conducted a systematic mapping study to understand the goals of partnership models and discuss their benefits and application domains. [Results] We found 23 studies published between 2006 and 2018. Partnership models provide coordination mechanisms that structure the definition of benefits, roles and responsibilities of actors in an ecosystem. We identified the key goals of the proposed models are related to the creation and maintenance of partnerships. [Conclusion] We observed the opportunity to develop partnership models focusing on the co-creation of value and the definition of business models. Our analysis also suggests the need for further studies in the context of open source ecosystems.}
}

@article{rayyan-727968044,
  title={Quality and success in open source software: A systematic mapping},
  year={2019},
  pages={363-370},
  author={Gezici, Bahar and Özdemir, Nurseda and Yılmaz, Nebi and Coşkun, Evren and Tarhan, Ayça and Chouseinoglou, Oumout},
  keywords={Software, Software engineering, Systematics, Bibliographies, Data mining, systematic mapping, Measurement, OSS, software, Organizations, open source, success, quality, software metrics},
  abstract={As the number of available Open Source Software (OSS) and the interest they attract are increasing, numerous product attributes are provided to developers and users for evaluating the quality and success of an OSS. Accordingly, various articles in the literature assess the quality and success of OSS, by using different quality attributes and metrics and different approaches. Though this variety can be considered as a positive indicator of research interest and maturation on one side, it creates a kind of jungle in defining and understanding the terms 'quality' and 'success' on the other side. Based on this challenge, in this study, we targeted a systematic mapping (SM) of the articles on quality and success of OSS. More than 474 articles have appeared in this area between the years 2002 and 2017, and the final pool of 128 articles is obtained by defining and applying inclusion and exclusion criteria. SM was employed to develop a classification scheme and categorized the existing body of articles with respect to five research questions (RQs) on: contribution and research types, quality criteria and metrics, success criteria and metrics, the relation of quality and success, and demographics. We observed that the majority of the articles assess the concept of quality as 'code quality', whereas the concept of success is mostly perceived as 'market success' and/or 'developer activity'. Moreover, the metrics of 'contributing developers/users', and the quality attribute of 'functionality' are the quality criteria most employed in the assessment of success.}
}

@article{rayyan-727968045,
  title={Building software cost estimation models using homogenous data},
  year={2007},
  pages={393-400},
  author={Premraj, Rahul and Zimmermann, Thomas},
  keywords={Software engineering, Software measurement, Data mining, Human factors, Productivity, Business, Companies, Costs, Predictive models, Accuracy, Software},
  abstract={Several studies have been conducted to determine if company-specific cost models deliver better prediction accuracy than cross-company cost models. However, mixed results have left the question still open for further investigation. We suspect this to be a consequence of heterogenous data used to build cross-company cost models. In this paper, we build cross-company cost models using homogenous data by grouping projects by their business sector. Our results suggest that it is worth to train models using only homogenous data rather than all projects available.}
}

@article{rayyan-727968046,
  title={Role of requirement prioritization technique to improve the quality of highly-configurable systems},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={27549-27573},
  author={Ali, Atif and Hafeez, Yaser and Hussain, Shariq and Yang, Shunkun},
  keywords={Systematics, Bibliographies, Data mining, Testing, Automotive engineering, Software systems, Customer value creation, highly-configurable system, quantitative approach, requirement prioritization},
  abstract={Highly-configurable systems are such systems which are not developed for single scenario. However, perhaps they have variable functionality and they are developed for hybrid scenarios. Producing a good highly-configurable system within time and with customer satisfaction is not easy. Handling requirements effectively in such a way that it take least time to market, is one of the most difficult tasks in highly-configurable system. In this paper, a quantitative requirement prioritization technique for highly-configurable systems is proposed. This technique involves all stakeholders and can be used primarily for large scale software projects. The proposed technique is evaluated by taking a case study of the highly-configurable point of sale for automotive industry. The result shows that the proposed technique provides promising results and can be enhanced with more future work.}
}

@article{rayyan-727968047,
  title={What motivates software developers?},
  year={2018},
  pages={50-59},
  author={Kachorowski, Ana and Wendler, Janaina and Albuquerque, Regina and Mantovani Fontana, Rafaela and Malucelli, Andreia and Reinehr, Sheila},
  keywords={Software, Software engineering, Context modeling, Human factors, Education, Organizations, Informatics, motivation, software developer, software process, Motivation},
  abstract={One of the main objectives in software engineering is that software projects are well succeeded, that is, delivering a high quality software product within due date. Considering that software development is an activity highly dependent on people, recent research has been showing that human factors in software engineering - such as motivation - impact projects results. This study aims thus at identifying motivating factors for practitioners that develop software, having a motivation theory as a foundation. Our research approach was the survey. We got 79 responses from practitioners from different organizations. Results show that motivation is related to existence, growth, relationships and interpersonal issues. We also identified a relation between motivation and turnover, motivation and the time working in a company, and motivation and the software process model used by practitioners.}
}

@article{rayyan-727968048,
  title={Literature review on the theory of constraints applied in the software development process},
  year={2018},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={16},
  number={11},
  pages={2747-2756},
  author={Ribeiro, S and Schmitz, E and Alencar, A and Silva, M},
  keywords={Software, Software engineering, Systematics, Bibliographies, Optimization, Literature Review, IEEE transactions, Systematic Mapping, Job shop scheduling, Software Development Process, Theory of Constraints},
  abstract={This work is the result of a systematic mapping on the application of Theory of Constraints in the software development process. The objective of the research is to identify in the literature relevant primary studies published on the Theory of Constraints related to the software production process and its execution environment. The search principle was based on works that used the concepts of the Theory of Constraints (TOC), originally proposed by Israeli physicist Eliyahu Moshe Goldratt in the 1980s. In this research, we also sought to explore works that applied to TOC (Theory of Constraints) in other productive environments in an attempt to visualize possible applications that can be adapted to the software development process. The idea is to select jobs with examples and applications that can be extended and applied in the software development process. Thus, works with approaches in process optimization, process improvement, and process scheduling was also observed and cataloged in the mapping performed. The gaps found showed some research opportunities theoretical and practical on the application of TOC in the software process, improvement of the software process, identification and treatment of bottlenecks in software process and optimization of the software process, using heuristics, metaheuristics, mathematical models And Job Shop scheduling. As a result, this secondary study presents potential research areas relating TOC to PDS in the hope of contributing researchers and developers with an interest in this broader research front.}
}

@article{rayyan-727968049,
  title={Awareness, collaboration and comunication's tools for distributed software development: A systematic mapping},
  year={2016},
  pages={1-8},
  author={Damato, Marco Aurélio Panizza and L'Erario, Alexandre and Fabri, José Augusto},
  keywords={Software, Software engineering, Collaboration, Tools, Project management, Distributed Software Development, Context, Communication, Libraries, IEEE Xplore, Awareness, DSD},
  abstract={Context: Increased competitiveness in the market, coupled with other factors such as the lack of skilled labor and cost reduction, makes the distributed software development a common practice in nowadays. Aims: With the complex scenario involving DSD environment (distributed software development), we aim to research tools to supporting this challenging environment. Method: We use a systematic mapping, in order to select primary studies and later refine the search, in order to show solutions and tools that support this scenario. Result: As a result, we found 20 tools divided into areas, which greatly facilitated the understanding and study, it will substantially succeed in DSD environments. Conclusion: We conclude that there are several tools for DSD scenario, and that are of high relevance in supporting collective awareness, collaboration, knowledge and communication.}
}

@article{rayyan-727968050,
  title={Methodologies for development of mobile applications},
  year={2016},
  pages={688-692},
  author={Stapić, Z and Mijač, M and Strahonja, V},
  keywords={Software, Systematics, Bibliographies, Mobile applications, Planning, Mobile communication, Application programming interfaces},
  abstract={The paper presents the results of literature review performed in order to identify newly created methodologies targeting the development of mobile applications. As mobile development differs from development of other software systems we also discuss the challenges of such development and recon that methodological approach in development is necessary. However, there were no systematic studies identifying methodologies for mobile applications development. Systematic literature review we performed included more than 6700 initial studies, identified 49 (0.73%) potentially relevant studies, and after extracting data and applying quality check procedures 14 methodologies are identified. Only one of these methodologies is reported to be used in practice.}
}

@article{rayyan-727968051,
  title={A systematic identification of formal and semi-formal languages and techniques for software-intensive systems-of-systems requirements modeling},
  year={2019},
  journal={IEEE Systems Journal},
  issn={1937-9234},
  volume={13},
  number={3},
  pages={2201-2212},
  author={Lana, Cristiane Aparecida and Guessi, Milena and Antonino, Pablo Oliveira and Rombach, Dieter and Nakagawa, Elisa Yumi},
  keywords={Software, Systematics, Tools, systematic mapping, Unified modeling language, Industries, Semantics, requirements modeling, Formal languages, semi-formal languages, systems-of-systems (SoS)},
  abstract={Software-intensive systems-of-systems (SoS) refer to an arrangement of managerially and operationally independent systems (i.e., constituent systems), which work collaboratively toward the achievement of global missions. Because some SoS are being developed for critical domains, such as healthcare and transportation, there is an increasing need to attain higher quality levels, which often justifies the additional costs that can be incurred by adopting formal and semi-formal approaches (i.e., languages and techniques) for modeling requirements. Various approaches have been employed, but a detailed landscape is still missing, and it is not well known whether these approaches are appropriate for addressing the inherent characteristics of SoS. The main contribution of this paper is to present this landscape by reporting on the state of the art in SoS requirements modeling. This landscape was built by means of a systematic mapping and shows formal and semi-formal approaches grouped from model-based to property-oriented ones. Most of them have been tested in safety-critical domains, where formal approaches such as finite-state machines are aimed at critical system parts, whereas semi-formal approaches (e.g., unified modeling language and i*) address non-critical parts. Although formal and semi-formal modeling is an essential activity, the quality of SoS requirements does not rely solely on the formalism that is used, but also on the availability of supporting tools/mechanisms that enable, for instance, requirements verification along the SoS life cycle.}
}

@article{rayyan-727968052,
  title={A comprehensive analysis of healthcare big data management, analytics and scientific programming},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={95714-95733},
  author={Nazir, Shah and Khan, Sulaiman and Khan, Habib Ullah and Ali, Shaukat and García-Magariño, Iván and Atan, Rodziah Binti and Nawaz, Muhammad},
  keywords={Data mining, big data, Healthcare, Big Data, big data analytics, Diseases, Medical diagnostic imaging, Data models, big data management},
  abstract={Healthcare systems are transformed digitally with the help of medical technology, information systems, electronic medical records, wearable and smart devices, and handheld devices. The advancement in the medical big data, along with the availability of new computational models in the field of healthcare, has enabled the caretakers and researchers to extract relevant information and visualize the healthcare big data in a new spectrum. The role of medical big data becomes a challenging task in the form of storage, required information retrieval within a limited time, cost efficient solutions in terms care, and many others. Early decision making based healthcare system has massive potential for dropping the cost of care, refining quality of care, and reducing waste and error. Scientific programming play a significant role to overcome the existing issues and future problems involved in the management of large scale data in healthcare, such as by assisting in the processing of huge data volumes, complex system modelling, and sourcing derivations from healthcare data and simulations. Therefore, to address this problem efficiently a detailed study and analysis of the available literature work is required to facilitate the doctors and practitioners for making the decisions in identifying the disease and suggest treatment accordingly. The peer reviewed reputed journals are selected for the accumulated of published research work during the period ranges from 2015 - 2019 (a portion of 2020 is also included). A total of 127 relevant articles (conference papers, journal papers, book section, and survey papers) are selected for the assessment and analysis purposes. The proposed research work organizes and summarizes the existing published research work based on the research questions defined and keywords identified for the search process. This analysis on the existence research work will help the doctors and practitioners to make more authentic decisions, which ultimately will help to use the study as evidence for treating patients and suggest medicines accordingly.}
}

@article{rayyan-727968053,
  title={Linked data approach for selection process automation in systematic reviews},
  year={2011},
  pages={31-35},
  author={Tomassetti, Federico and Rizzo, Giuseppe and Vetro, Antonio and Ardito, Luca and Torchiano, Marco and Morisio, Maurizio},
  abstract={Background: a systematic review identifies, evaluates and synthesizes the available literature on a given topic using scientific and repeatable methodologies. The significant workload required and the subjectivity bias could affect results. Aim: semi-automate the selection process to reduce the amount of manual work needed and the consequent subjectivity bias. Method: extend and enrich the selection of primary studies using the existing technologies in the field of Linked Data and text mining. We define formally the selection process and we also develop a prototype that implements it. Finally, we conduct a case study that simulates the selection process of a systematic literature published in literature. Results: the process presented in this paper could reduce the work load of 20% with respect to the work load needed in the fully manually selection, with a recall of 100%. Conclusions: the extraction of knowledge from scientific studies through Linked Data and text mining techniques could be used in the selection phase of the systematic review process to reduce the work load and subjectivity bias.}
}

@article{rayyan-727968054,
  title={Performing systematic literature reviews with novices: An iterative approach},
  year={2014},
  journal={IEEE Transactions on Education},
  issn={1557-9638},
  volume={57},
  number={3},
  pages={175-181},
  author={Lavallée, Mathieu and Robillard, Pierre-N. and Mirsalari, Reza},
  keywords={Software, Systematics, Search problems, Computer science education, Planning, Training, Iterative methods, engineering education, engineering students, reviews, Tutorials},
  abstract={Reviewers performing systematic literature reviews require understanding of the review process and of the knowledge domain. This paper presents an iterative approach for conducting systematic literature reviews that addresses the problems faced by reviewers who are novices in one or both levels of understanding. This approach is derived from traditional systematic literature reviews and based on observations from four systematic reviews performed in an academic setting. These reviews demonstrated the importance of defining iterations for the eight tasks of the review process. The iterative approach enables experiential learning from the two levels of understanding: the process level and the domain level.}
}

@article{rayyan-727968055,
  title={Acceptance of blockchain based supply chain management system: Research model proposal},
  year={2019},
  pages={1-6},
  author={Gökalp, Ebru and Çoban, Selin and Gökalp, Mert Onuralp},
  keywords={Blockchain, Organizational Acceptance, Supply Chain Management, Technology-Organization-Environment Framework},
  abstract={Blockchain technology provides emerging solutions including decentralized management, security, privacy and robustness. In Supply Chain Management (SCM) applications, blockchain technology enable us to increase customer satisfaction, operational excellence, and to decrease operational costs and risks. Despite of these significant benefits, there are limited number of studies that combines SCM and blockchain in the literature. The main aim of this study is to investigate the factors influencing the adoption and acceptance of the blockchain based SCM in organizations. To this end, a comprehensive systematic literature review is conducted and a research model based on Technology-Organization-Environment (TOE) framework is proposed. The proposed research model consists of factors of the relative advantage, complexity, interoperability/compatibility, standardization, trust and scalability in the context of technology; organization's IT resource, top management support, organization size and financial resources in the context of organization; competitive pressure, trading partner pressure, government policy and regulations and inter-organizational trust in the context of environment.}
}

@article{rayyan-727968056,
  title={A taxonomy and qualitative comparison of program analysis techniques for security assessment of android software},
  year={2017},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  volume={43},
  number={6},
  pages={492-530},
  author={Sadeghi, Alireza and Bagheri, Hamid and Garcia, Joshua and Malek, Sam},
  keywords={Systematics, Security, Taxonomy, Malware, Androids, Humanoid robots, Mobile communication, android platform, program analysis, security assessment, Taxonomy and survey, Methyltestosterone, Software},
  abstract={In parallel with the meteoric rise of mobile software, we are witnessing an alarming escalation in the number and sophistication of the security threats targeted at mobile platforms, particularly Android, as the dominant platform. While existing research has made significant progress towards detection and mitigation of Android security, gaps and challenges remain. This paper contributes a comprehensive taxonomy to classify and characterize the state-of-the-art research in this area. We have carefully followed the systematic literature review process, and analyzed the results of more than 300 research papers, resulting in the most comprehensive and elaborate investigation of the literature in this area of research. The systematic analysis of the research literature has revealed patterns, trends, and gaps in the existing literature, and underlined key challenges and opportunities that will shape the focus of future research efforts.}
}

@article{rayyan-727968057,
  title={Open government data: Analysing benefits and challenges},
  year={2019},
  pages={1-6},
  author={Çaldağ, Murat Tahir and Onuralp Gökalp, Mert and Gökalp, Ebru},
  keywords={Benefit and Challenge Analysis, Open Data, Open Government Data, Road Map, Socio-Technic Approach},
  abstract={Open Government Data enables stakeholders to monitor and participate in governance processes by accessing governance information and decision-making areas while providing transparency, accountability, cooperation, participation, new job opportunities that provide public benefits. However, many open government data initiatives either fail or never start because of the challenges it has. After conducting the systematic literature review, it was determined that there is a lack of study covering analyzing the benefits and challenges of open government data in a holistic perspective and providing a road map to overcome these difficulties for our country. In order to satisfy this gap, this study aims to define the benefits and challenges of open government data from a holistic socio-technical perspective and derive a roadmap including guidelines to overcome these difficulties for the benefit of organizations aiming to transition to open government data.}
}

@article{rayyan-727968058,
  title={An approach to support the specification of agile artifacts in the development of safety-critical systems},
  year={2017},
  pages={526-531},
  author={Leite, Ana Isabella Muniz},
  keywords={Software, Software engineering, Safety, Stakeholders, Maintenance engineering, Certification, Agile Development, Safety Requirements, Safety-Critical Systems, User Stories},
  abstract={Providing a correct, complete, and unambiguous requirements specification is still one of the biggest challenges in software engineering. In the case of safety-critical systems, this challenge is even greater, since misinterpretations can lead to catastrophic damages to humans and to the environment. Agile development proposes minimizing the challenges in requirements specifications through short iterations, quick feedback, and active stakeholders. However, in safety-critical systems development, there is a gap (either geographical, cultural, educational, or temporal) between safety engineers and developers. [Question Problem] Therefore, it is not possible to be assured by agile development teams that safety aspects are well understood by developers, and, if the latter are aware of the criticality of the problem, that they can implement them accordingly. [Principal Ideas Results] The proposed research aims to provide adequate support for more accurate specification of agile development artifacts in the development of safety-critical systems. In this regard, the first contribution of this research aims at defining an Agile Safety Process, whose purpose is to identify which artifacts or parts thereof are enough to specify failure detection and containment, as well as measures for taking the system to a safe state. The second contribution aims at providing a semi-automated methodology for supporting the specification of agile artifacts, taking into account safety aspects. As a consequence, this research will have a significant impact in terms of improving the creation of evidences to be submitted for certification in terms of timing and accuracy.}
}

@article{rayyan-727968059,
  title={A process model for Requirements Change Management in collocated software development},
  year={2012},
  pages={1-6},
  author={Khan, Arif Ali and Basri, Shuib and Dominic, P D D and Fazal-e-Amin},
  keywords={Data mining, Process, Model, Organizations, Software systems, Educational institutions, Planning, Collocated Software Development, Requirements Change Management, Spirals, Software},
  abstract={Requirements Change Management (RCM) could occur at any phase of the software development life cycle. Therefore, RCM is considered to be a difficult task in software development organizations. The purpose of this study is to propose a model for RCM that will be implemented in collocated software development organizations. The model is based on the data collected from different RCM reports, models, research papers and different case studies related to RCM. The model has seven core stages, i.e., change request, validate, reject, batch, implement, verify and update. Propose model will eliminate the limitations of the available RCM models.}
}

@article{rayyan-727968060,
  title={A systematic mapping study for intelligent user interfaces - IUI},
  year={2017},
  pages={361-368},
  author={Sanchez, Cristina and Cedillo, Priscila and Bermeo, Alexandra},
  keywords={Software, Systematics, Ambient assisted living, Libraries, Conferences, User interfaces, Ambient Assisted Living, Systematic Mapping, Adaptive User Interface, Human – Computer Interaction, Intelligent User Interface, Intelligence},
  abstract={Intelligent User Interfaces (IUI) facilitate human-machine interaction, through which the user makes use of a general system in a more efficient way. These interfaces are helpful for different types of users, specifically for people with disabilities, seniors, among others. These interfaces belong to a type of intelligent systems that are capable of self-adapt to users with different health problems, this is possible through the determination of behavior characteristics that distinguishes each user from another. In this paper, a systematic mapping of the Intelligent User Interfaces is presented, which allow developers to determine which applications are adopting those interfaces with emphasis on Ambient Assisted Living (AAL) technologies. The Kitchenham's methodology has been applied in order to perform this secondary study, after the execution of the review, a total of 43 primary studies was selected and classified, thus allowing us to obtain the results presented in this contribution.}
}

@article{rayyan-727968061,
  title={Aspect-oriented software maintenance metrics: A systematic mapping study},
  year={2012},
  pages={253-262},
  author={Saraiva, Juliana and Barreiros, Emanoel and Almeida, Adauto and Lima, Flávio and Alencar, Aline and Lima, Gustavo and Soares, Sergio and Castor, Fernando},
  keywords={Metronidazole, Software},
  abstract={Background: Despite the number of empirical studies that assess Aspect-Oriented Software Development (AOSD) techniques, more research is required to investigate, for example, how software maintainability is impacted when these techniques are employed. One way to minimize the effort and increase the reliability of results in further research is to systematize empirical studies in Aspect-Oriented Software Maintainability (AOSM). In this context, metrics are useful as indicators to quantify software quality attributes, such as maintenance. Currently, a high number of metrics have been used throughout the literature to measure software maintainability. However, there is no comprehensive catalogue showing which metrics can be used to measure AOSM. Aim: To identify an AOSM metrics suite to be used by researchers in AOSM research. Method: We performed a systematic mapping study based on Kitchenham and Charters' guidelines, which derived a research protocol, and used well known digital libraries engines to search the literature. Conclusions: A total of 138 primary studies were selected. They describe 67 aspect-oriented (AO) maintainability metrics. Also, out of the 575 object-oriented maintainability metrics that we analyzed, 469 can be adapted to AO software. This catalogue provides an objective guide to researchers looking for maintainability metrics to be used as indicators in their quantitative and qualitative assessments. We provide information such as authors, metrics description, and studies that used the metric. Researchers can use this information to decide which metrics are more suited for their studies.}
}

@article{rayyan-727968062,
  title={A systematic mapping study on architectural analysis},
  year={2013},
  pages={661-664},
  author={Catal, Cagatay and Atalay, Muratcan},
  keywords={Software, Evidence-based software engineering, Systematics, survey, ATAM, Software architecture, Inspection, Databases, literature review, Computer architecture, software architecture},
  abstract={We conduct a Systematic Mapping Study to categorize the primary research papers in architectural analysis. Systematic mapping studies can change the research perspective in an area and they can help to find the ignored researched areas. The objective of this study is to investigate the techniques used in papers on architectural analysis, identify the current trends, and provide suggestions for the future research studies. We search for papers published within the last ten years in the following databases: IEEE Explorer, ACM Digital Library, Science Direct, and Wiley. We conclude that more validation and evaluation research is needed to provide a better foundation for architectural analysis.}
}

@article{rayyan-727968063,
  title={Activity diagram inspection on requirements specification},
  year={2010},
  pages={168-177},
  author={de Mello, Rafael Maiani and Pereira, Wallace Martinho and Travassos, Guilherme Horta},
  keywords={Software, Inspection, Unified modeling language, Business, Computational modeling, Book reviews, Adaptation model},
  abstract={The requirements specification of contemporary software applications usually is composed by diverse artifacts describing lots of activities, flows, dependencies among the flows, branches and business rules. For instance, web or scientific workflow (e-science) based applications require structural representations for the various activities and functionalities involved in their execution, usually described trough activity diagrams. The quality assurance of such specifications represents a challenge for software engineers. The results of a quasi-systematic review indicated there is a lack of software technologies to support the inspection of this type of requirements specification. Therefore, in this paper, besides the review results, an inspection technique (checklist) to review Activity Diagrams on the requirements specifications is introduced. A proof of concept on applying the checklist for the inspection of a real requirements specification concerned with a scientific workflow based application indicated some advantages on its defect detection capacity when compared with previously executed ad-hoc inspection by software engineers.}
}

@article{rayyan-727968064,
  title={Towards taxonomical-based situational model to improve the quality of agile distributed teams},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={6812-6826},
  author={Sarwar, Amber and Hafeez, Yaser and Hussain, Shariq and Yang, Shunkun},
  keywords={Software, Systematics, Bibliographies, Software architecture, Databases, Modeling, Organizations, Agile distributed taxonomy, agile distributed teams, situational agile distributed development, situational model, taxonomical model},
  abstract={At present, software organizations are developing software products that employ global software development (GSD) teams. Organizations tend to adopt new methodologies for global software development, among which is the use of agile in the GSD industry, which yields both benefits and challenges. However, software development teams do not consider situational needs that delay software delivery, resulting in the late discovery of incompatible assumptions and architecture level rework. In this study, we conduct a systematic literature review (SLR) to identify the situational factors that need to be considered by software development teams before developing a software product. We further present taxonomical classification and comprehensively map the situational factors that impact design development and advancement in the proposed Situational Agile Distributed Development (SADD) model. We propose 18 directed hypotheses against each situational factor that supports our SADD model. In order to evaluate our directed hypotheses, statistical analysis method is used, and the level of confidence of each directed hypothesis is validated. The result of our study confirms that global software development teams are highly reliant on the SADD Model. Our study will largely contribute by devising a multilevel taxonomy of situational factors that elevate the performance of global software development teams. This taxonomical classification will allow to better map the relationships between multiple situational factors and elevate the process of creating a holistic model to handle situational needs in the context of Agile Distributed Software Development (ADSD).}
}

@article{rayyan-727968065,
  title={Set of usability heuristics for quality assessment of mobile applications on smartphones},
  year={2019},
  journal={IEEE Access},
  issn={2169-3536},
  volume={7},
  pages={116145-116161},
  author={Parente Da Costa, Ruyther and Canedo, Edna Dias and De Sousa, Rafael Timóteo and De Oliveira Albuquerque, Robson and García Villalba, Luis Javier},
  keywords={Task analysis, Mobile applications, Usability, usability, ISO Standards, Smart phones, IEC Standards, cognitive load, heuristic evaluation, usability heuristics},
  abstract={The innovations proposed by the cell phone market have grown steadily in recent years, along with the increasing complexity of the hardware, operating systems, and applications available in this market. These changes bring new challenges related to usability that need to be considered during the development process of these applications since the new forms of user-application interactions increasingly require adapting the behavior of smartphone users. In this situation, usability is an important issue that depends on factors such as the Users, their characteristics and abilities, the Task which the users intend to achieve and also the application usage Context. This work presents a systematic literature review with the objective of identifying the heuristics and usability metrics used in the literature and/or industry. Based on the review results, this work presents another contribution with a proposal of a set of usability heuristics focused in mobile applications on smartphones, considering the User, Task and Context as usability factors and Cognitive Load as an important attribute of usability. The components of this set are detailed in a model intended to be used in empirical validations allowing to dynamically incorporate improvements to the proposal.}
}

@article{rayyan-727968066,
  title={Exploring user centered design in healthcare: A literature review},
  year={2020},
  pages={1-8},
  author={Chandran, Srijith and Al-Sa'di, Ahmed and Ahmad, Esraa},
  keywords={Software, Systematics, Bibliographies, Literature Review, Healthcare, Best practices, Medical services, UCD, UCD Methods, User centered design, User Centered Design},
  abstract={Recently User Centered Design (UCD) has been frequently used in software development to place the user's needs and goals in the centre of the development process to deliver a highly usable system. This paper aims to capture the current usage of UCD methods in healthcare system development and briefly discusses the best usage of these methods. We conducted a systematic review of the literature using selection criteria and resulted in retrieving 20 relevant publications. The results were analyzed to identify the most common use of UCD methods in the healthcare system. We hope this paper will contribute to the body of knowledge on healthcare software development,by providing a broad overview of the existing works on UCD in healthcare along with identifying the best practices of the most common method in UCD.}
}

@article{rayyan-727968067,
  title={Evaluating code readability and legibility: An examination of human-centric studies},
  year={2020},
  pages={348-359},
  author={Oliveira, Delano and Bruno, Reydne and Madeiral, Fernanda and Castor, Fernando},
  keywords={Software engineering, Systematics, Bibliographies, Task analysis, Software maintenance, Programming, Taxonomy, code legibility, Code readability, code understandability, code understanding, program comprehension, Humanities, Humanism, Humans},
  abstract={Reading code is an essential activity in software maintenance and evolution. Several studies with human subjects have investigated how different factors, such as the employed programming constructs and naming conventions, can impact code readability, i.e., what makes a program easier or harder to read and apprehend by developers, and code legibility, i.e., what influences the ease of identifying elements of a program. These studies evaluate readability and legibility by means of different comprehension tasks and response variables. In this paper, we examine these tasks and variables in studies that compare programming constructs, coding idioms, naming conventions, and formatting guidelines, e.g., recursive vs. iterative code. To that end, we have conducted a systematic literature review where we found 54 relevant papers. Most of these studies evaluate code readability and legibility by measuring the correctness of the subjects' results (83.3%) or simply asking their opinions (55.6%). Some studies (16.7%) rely exclusively on the latter variable. There are still few studies that monitor subjects' physical signs, such as brain activation regions (5%). Moreover, our study shows that some variables are multi-faceted. For instance, correctness can be measured as the ability to predict the output of a program, answer questions about its behavior, or recall parts of it. These results make it clear that different evaluation approaches require different competencies from subjects, e.g., tracing the program vs. summarizing its goal vs. memorizing its text. To assist researchers in the design of new studies and improve our comprehension of existing ones, we model program comprehension as a learning activity by adapting a preexisting learning taxonomy. This adaptation indicates that some competencies, e.g., tracing, are often exercised in these evaluations whereas others, e.g., relating similar code snippets, are rarely targeted.}
}

@article{rayyan-727968068,
  title={Solution to interoperability protocols smarthomes},
  year={2014},
  pages={1-6},
  author={Albuquerque, Héldon José Oliveira and de Aquino, Gibeon Soares},
  keywords={systematic review, Internet, interoperability, Middleware, Interoperability, Protocols, Mobile handsets, communication protocol, connectivity, IP networks, mobile devices, proxy, XML},
  abstract={During the recent years, we can observe how mobile devices entered in the lives of people, becoming their main personal assistants and helping in various daily tasks. However not just mobile devices have evolved. And others electronic devices we use every day also experienced changes that have become smarter as the example of home devices. All these devices interconnected in the same environment or the same network, making use of services and exchange information with other devices, characterize a smart environment. smarthomes are special class of such environments and, increasingly, has become a scene with a variety of heterogeneous devices. However, due to the rapid progress of technology and the rise of a large number of heterogeneous devices, a variety of independent communication protocols were created, establishing a complex scenario to ensure interoperability between them. In this context, the main objective of this paper is to analyze the state of the art with an emphasis on the interoperability of current Mobile Devices with other digital devices present in a smarthome. Thus, it will be defined which protocols are currently most commonly used for this purpose, what are the current ongoing projects, what are the limitations of the solutions found in the research and, finally, it will be proposed an alternative solution for interoperability between the devices at a smarthome.}
}

@article{rayyan-727968069,
  title={Is there a "Golden" rule for code reviewer recommendation? : —An experimental evaluation},
  year={2020},
  pages={497-508},
  author={Hu, Yuanzhe and Wang, Junjie and Hou, Jie and Li, Shoubin and Wang, Qing},
  keywords={Systematics, Guidelines, Task analysis, Software quality, Software reliability, code review, experimental evaluation, reviewer recommendation, Sensitivity, Training data},
  abstract={Peer code review has been proven to be an effective practice for quality assurance, and widely adopted by commercial companies and open source communities as GitHub. However, identifying an appropriate code reviewer for a pull request is a non-trivial task considering the large number of candidate reviewers. Several approaches have been proposed for reviewer recommendation, yet none of them has conducted a complete comparison to explore which one is more effective. This paper aims at conducting an experimental evaluation of the commonly-used and state-of-the-art approaches for code reviewer recommendation. We begin with a systematic review of approaches for code reviewer recommendation, and choose six approaches for experimental evaluation. We then implement these approaches and conduct reviewer recommendation on 12 large-scale open source projects with 53,005 pull requests spanning two years. Results show that there is no golden rule when selecting code reviewer recommendation approaches, and the best approach varies in terms of different evaluation metrics (e.g., Top-5 Accuracy, MRR) and experimental projects. Nevertheless, TIE, which utilizes the textual similarity and file path similarity, is the most promising one. We also explore the sensitivity of these approaches to training data, and compare their time cost. This approach provides new insights and practical guidelines for choosing approaches for reviewer recommendation.}
}

@article{rayyan-727968070,
  title={Smart contract development: Challenges and opportunities},
  year={2019},
  journal={IEEE Transactions on Software Engineering},
  issn={1939-3520},
  pages={1},
  author={Zou, Weiqin and Lo, David and Kochhar, Pavneet Singh and Le, Xuan-Bach D and Xia, Xin and Feng, Yang and Chen, Zhenyu and Xu, Baowen},
  keywords={Software, Blockchain, Empirical Study, Smart contracts, Challenges, Interviews, Law, Smart Contract},
  abstract={Smart contract, a term which was originally coined to refer to the automation of legal contracts in general, has recently seen much interest due to the advent of blockchain technology. Recently, the term is popularly used to refer to low-level code scripts running on a blockchain platform. Our study focuses exclusively on this subset of smart contracts. Such smart contracts have increasingly been gaining ground, finding numerous important applications (e.g., crowdfunding) in the real world. Despite the increasing popularity, smart contract development still remains somewhat a mystery to many developers largely due to its special design and applications. Are there any differences between smart contract development and traditional software development? What kind of challenges are faced by developers during smart contract development? Questions like these are important but have not been explored by researchers yet. In this paper, we performed an exploratory study to understand the current state and potential challenges developers are facing in developing smart contracts on blockchains, with a focus on Ethereum (the most popular public blockchain platform for smart contracts). Toward this end, we conducted this study in two phases. In the first phase, we conducted semi-structured interviews with 20 developers from GitHub and industry professionals who are working on smart contracts. In the second phase, we performed a survey on 232 practitioners to validate the findings from the interviews. Our interview and survey results revealed several major challenges developers are facing during smart contract development: (1) there is no effective way to guarantee the security of smart contract code; (2) existing tools for development are still very basic; (3) the programming languages and the virtual machines still have a number of limitations; (4) performance problems are hard to handle under resource constrained running environment; and (5) online resources (including advanced/updated documents and community support) are still limited. Our study suggests several directions that researchers and practitioners can work on to help improve developers? experience on developing high-quality smart contracts.}
}

@article{rayyan-727968071,
  title={Analytical Hierarchy Process issues and mitigation strategy for large number of requirements},
  year={2012},
  pages={1-8},
  author={Nidhra, Srinivas and Kelapanda Satish, Likith Poovanna and Ethiraj, Vinay Sudha},
  keywords={Software, Systematics, Interviews, Complexity theory, Industries, Companies, Requirement Engineering, Face, Analytical Hierarchy Process, NAcAHP, Requirement Prioritization},
  abstract={Now-a-days most software projects have more candidate requirements. So it is vital for software companies to use different prioritization techniques to select valuable requirements among the candidate requirements. But software companies usually face a lot of challenges in using AHP such as increase in time and complexity with respect to number of comparisons. In this paper, we present previous work carried out in this research area and industrial study to identify the challenges software companies face while prioritizing large number of requirements using AHP. Different types of prioritization techniques have been developed to resolve these challenges. This paper focus on Numeral assignment technique which groups requirements into three categories: critical, standard and optional and AHP which prioritize requirements based on pair-wise comparisons. In this article we proposed a model i.e., NAcAHP where in which pair-wise comparison of AHP is applied on critical group of Numeral assignment technique for prioritizing the requirements. The result shows that the proposed model minimizes the time and complexity of pair wise comparison.}
}

@article{rayyan-727968072,
  title={Process improvement in the acquisition domain: A decision model for IT supplier selection},
  year={2012},
  pages={1-6},
  author={Grossi, Lucas and Calvo-Manzano, Jose A},
  keywords={Software, Decision Making, Outsourcing, Information technology, Abstracts, Capability maturity model, Silicon, Eigenvalues and eigenfunctions, Supplier Selection, Decision Support Techniques},
  abstract={The main objective of this work is to develop a decision model for Information Technology (IT) supplier selection to reduce the number of failures in the relationship between client and supplier. Also, it is defined a software engineering systematic review for the IT supplier selection decision and a case study to help to define and evaluate the model. The motivation for defining this model is the absence of any decision model based on the acquisition models and the decision methods for selecting and IT supplier.}
}

@article{rayyan-727968073,
  title={Towards a terminology unification in software interoperability},
  year={2018},
  pages={478-485},
  author={Torres Ricaurte, Diana Maria and Villavicencio Cabezas, Mónica K and Zapata Jaramillo, Carlos Mario},
  keywords={Software, Task analysis, Terminology, Interoperability, Proposals, Semantics, Standards, Main concepts, Software enineering, Terminology unification},
  abstract={Interoperability is commonly referred as a software quality sub-characteristic. Some proposals for formalizing interoperability include sets of concepts linked to this phenomenon. Such concepts differ among the proposals, resulting in a different terminology for referring to the same thing. Non-unified terminology of interoperability concepts exhibits three main problems: homonymy, synonymy and missing concepts. Due to such problems, crucial aspects of the interoperability process could be either misunderstood or just omitted, resulting in an ambiguous and incomplete description of interoperability. To solve these problems, we propose a terminology unification of interoperability concepts. This unification is based on published studies about the interoperability formalization. Results lead us to recognize six main concepts useful to unify the interoperability vocabulary.}
}

@article{rayyan-727968074,
  title={How to make best use of cross-company data for web effort estimation?},
  year={2015},
  pages={1-10},
  author={Minku, Leandro and Sarro, Federica and Mendes, Emilia and Ferrucci, Filomena},
  keywords={Software, Estimation, Databases, Optical wavelength conversion, Companies, Predictive models, Training},
  abstract={[Context]: The numerous challenges that can hinder software companies from gathering their own data have motivated over the past 15 years research on the use of cross-company (CC) datasets for software effort prediction. Part of this research focused on Web effort prediction, given the large increase worldwide in the development of Web applications. Some of these studies indicate that it may be possible to achieve better performance using CC models if some strategy to make the CC data more similar to the within-company (WC) data is adopted. [Goal]: This study investigates the use of a recently proposed approach called Dycom to assess to what extent Web effort predictions obtained using CC datasets are effective in relation to the predictions obtained using WC data when explicitly mapping the CC models to the WC context. [Method]: Data on 125 Web projects from eight different companies part of the Tukutuku database were used to build prediction models. We benchmarked these models against baseline models (mean and median effort) and a WC base learner that does not benefit of the mapping. We also compared Dycom against a competitive CC approach from the literature (NN-filtering). We report a company-by- company analysis. [Results]: Dycom usually managed to achieve similar or better performance than a WC model while using only half of the WC training data. These results are also an improvement over previous studies that investigated the use of different strategies to adapt CC models to the WC data for Web effort estimation. [Conclusions]: We conclude that the use of Dycom for Web effort prediction is quite promising and in general supports previous results when applying Dycom to conventional software datasets.}
}

@article{rayyan-727968075,
  title={The EMFIS model — Enable more frequent integration of software},
  year={2017},
  pages={10-17},
  author={Mårtensson, Torvald and Ståhl, Daniel and Bosch, Jan},
  keywords={Bibliographies, Testing, Embedded systems, Interviews, continuous delivery, continuous integration, Companies, embedded systems, large-scale, size, software integration, Software},
  abstract={The EMFIS model allows companies to explicate a representation of the organization's current situation regarding continuous integration impediments, and visualizes what the organization must focus on in order to enable more frequent integration of software. The model is used to perform an assessment of twelve factors, where the ratings from participants representing the developers are summarized separately from ratings from participants representing the enablers (responsible for processes, development tools, test environments etc.). The EMFIS model is based on semi-structured interviews with 20 developers from two companies which develop large-scale software systems, and a literature review that included 74 research papers and four books. The model has been validated in workshops and interviews, which in total included 28 individuals in five different companies. The model was well received during the validation, and was appreciated for its simplicity and its ability to show differences in rating between developers and enablers.}
}

@article{rayyan-727968076,
  title={Reporting empirical evidence in distributed software development: An extended taxonomy},
  year={2015},
  pages={71-80},
  author={da Rosa Techio, Antonio Rafael Da Rosa and Prikladnicki, Rafael and Marczak, Sabrina},
  keywords={Distributed Software Development, Context, Taxonomy, Terminology, Interviews, Organizations, Industries, Instruments, Empirical Evidence, Expert Opinion Survey, Systematization of Knowledge, Software},
  abstract={Distributed Software Development (DSD) has been discussed by industry and academia for almost two decades now, and, as consequence, there is a large number of empirical scientific papers and industrial reports on it. However, the description of the context in which the empirical study was conducted is not always clear or complete, making the process of searching for empirical evidence burdensome. It becomes difficult to understand or to judge the relevance of study given that DSD scenarios are diverse. What works in one context might not apply to another. To reduce such difficulty, we need, as a research community, to have means to standardize how we report empirical studies and their findings aiming to make them more readily available to practitioners and researchers. In this paper we present an extended taxonomy to classify empirical DSD evidence. We conducted an expert opinion survey with researchers and practitioners to identify elements to compose the taxonomy. Preliminary evaluation of the proposed taxonomy suggests that it can be used to synthesize existing knowledge, to identify gaps in literature, to identify related work and to help researchers who will publish or review further empirical work, as well as practitioners who are interested in published empirical studies.}
}

@article{rayyan-727968077,
  title={An extended assessment of data-driven bayesian networks in software effort prediction},
  year={2013},
  pages={157-166},
  author={Tierno, Ivan A P and J.Nunes, Daltro},
  keywords={Software, Measurement, Predictive models, Data models, Accuracy, Benchmark testing, Bayes methods},
  abstract={Software prediction unveils itself as a difficult but important task which can aid the manager on decision making, possibly allowing for time and resources sparing, achieving higher software quality among other benefits. Bayesian Networks are one of the machine learning techniques proposed to perform this task. However, the data pre-processing procedures related to their application remain scarcely investigated in this field. In this context, this study extends a previously published paper, benchmarking data-driven Bayesian Networks against mean and median baseline models and also against ordinary least squares regression with a logarithmic transformation across three public datasets. The results were obtained through a 10-fold cross validation procedure and measured by five accuracy metrics. Some current limitations of Bayesian Networks are highlighted and possible improvements are discussed. Furthermore, we assess the effectiveness of some pre-processing procedures and bring forward some guidelines on the exploration of data prior to Bayesian Networks' model learning. These guidelines can be useful to any Bayesian Networks that use data for model learning. Finally, this study also confirms the potential benefits of feature selection in software effort prediction.}
}

@article{rayyan-727968078,
  title={Bibliography},
  year={2020},
  journal={An introduction to self-adaptive systems: A contemporary software engineering perspective},
  issn={978-1-119-57493-4},
  pages={241-262},
  author={Weyns, Danny},
  url={https://ieeexplore.ieee.org/document/9261306},
  publisher={IEEE}
}

@article{rayyan-727968079,
  title={On the dependability for dynamic software product lines: A comparative systematic mapping study},
  year={2016},
  pages={323-330},
  author={Dirce Alves Sandim Eleuterio, Jane and Gaia, Felipe Nunes and Bondavalli, Andrea and Lollini, Paolo and Rodrigues, Genaína Nunes and Fischer Rubira, Cecília Mary},
  keywords={Software, Software product lines, Systematics, systematic mapping study, Context, Reliability, Proposals, Runtime, dependability, dynamic software product line, dynamically adaptive systems},
  abstract={Software Product Lines (SPLs) are techniques where several artefacts are reused (domain), and some are customised (variation points). An SPL can bind variation points statically (compilation time) or dynamically (runtime). Dynamic Software Product Lines (DSPLs) use dynamic binding to adapt to the environment or requirements changes. DSPLs are commonly used to build dependable systems, defined as systems with the ability to avoid more frequent or severe service failures than the acceptable. The main dependability attributes are availability, confidentiality, integrity, reliability, maintainability, and safety. To better understand this context, a Systematic Mapping Study (SMS) was applied searching proposals that include dependability attributes in DSPLs. Our results suggest that few solutions handle dependability in DSPL context. We selected nine primary studies in this regard. We performed a comparative study of the results, analysing other dimensions, and facets, aiming for a better understanding of this research area.}
}

@article{rayyan-727968080,
  title={On the use of requirements measures to predict software project and product measures in the context of android mobile apps: A preliminary study},
  year={2015},
  pages={357-364},
  author={Francese, Rita and Gravino, Carmine and Risi, Michele and Scanniello, Giuseppe and Tortora, Genoveffa},
  keywords={Software, Software measurement, Empirical study, Estimation, Context, Smart phones, mobile app development, Predictive models, Mobile communication, software development effort estimation, requirements measures, Methyltestosterone},
  abstract={In this paper, we study the value of software project and product measures in the context of Android mobile apps. In particular, we focus on the effort to develop mobile apps and the number of graphical components in these apps. Estimation models are based on information from requirements specification documents (e.g., Number of actors, number of use cases, and number of classes). We have used a dataset containing information on 23 Android apps and employed a stepwise linear regression to build estimation models. The predictions have been compared with those obtained considering models built on software measures (e.g., Number of classes, number of files, and number of line of code). The results suggest that the measures from the artifacts produced in requirements engineering process are not worse predictors than those measures from source code. That is, requirements measures can effectively employed to estimate software project and product measures of a mobile app and estimations can be done early in the software development process.}
}

@article{rayyan-727968081,
  title={A hybrid architecture for one-stop e-government portal integration and interoperability},
  year={2014},
  pages={96-101},
  author={Sedek, Khairul Anwar and Omar, Mohd Adib and Sulaiman, Shahida},
  keywords={Service-oriented architecture, Integration, Interoperability, Computer architecture, Electronic government, Portals, Software Architecture, Prototypes, One-stop E-government Portal, Service Component Architecture (SCA)},
  abstract={An effective one-stop e-government portal needs e-government system with good integration and interoperability. However, most e-government portals lack in integration and interoperability. This problem causes each government agency tend to have its own portal. Hence, it prevents the government to provide services in a single access point. Therefore, e-government system needs a better architecture for integration and interoperability among e-government system components, applications, and services. The architecture should provide a seamless approach for integration and interoperability of e-government applications and services using a hybrid and distributed e-government architecture. This work presents e-government system architecture and the proof-of-concept prototype of the proposed architecture as a case study in the Malaysian One-Stop E-government (MyOneEG) system.}
}

@article{rayyan-727968082,
  title={Visualization to support decision-making in cities: advances, technology, challenges, and opportunities},
  year={2020},
  pages={198-207},
  author={Cepero García, M Teresa and Montané-Jiménez, Luis G},
  keywords={visualization, Visualization, Tools, Decision making, Data visualization, Big Data, Smart cities, cities, decision support, Urban areas, Decision Making},
  abstract={Data is a valuable asset to the management of a city. With the growing integration of technology, some tools help to collect, process, and visualize urban data, aiding the interpretation and understanding of how urban systems work. Despite the wide use of visualization to support decisionmaking in urban management, the understanding of urban data visualization in cities is limited in the current literature. Therefore, this paper analyzes the relevant body of literature to identify the advances in information visualization for decisionmaking in cities, the visualization technology used, and the areas of opportunity in the field of study. As a result of this analysis, we identified eleven visualization technologies used for urban data visualization. The literature review suggests that specialized visualization tools do not consider the users and their real needs. For this reason, we recommend incorporating the User-Centered Design approach in urban data visualization design.}
}

@article{rayyan-727968083,
  title={What does research say about agile and architecture?},
  year={2010},
  pages={32-37},
  author={Breivold, Hongyu Pei and Sundmark, Daniel and Wallin, Peter and Larsson, Stig},
  keywords={Software, Data mining, Agile, Software architecture, Programming, Architecture, Agile methodology, Conferences, Computer architecture},
  abstract={Agile has been used to refer to a software development paradigm that emphasizes rapid and flexible development. In the meanwhile, we have through our practical experiences in scaling up agile methods, noticed that architecture plays an important role. Due to the inter-relationship between agile methods and architecture, as well as divergent perceptions on their correlation stated in numerous sources, we are motivated to find out how these perceptions are supported by findings in the research community in general and in empirical studies in particular. To fully benefit from agile practices and architectural disciplines, we need empirical data on the perceived and experienced impacts of introducing agile methods to existing software development process, as well as correlations between agile and architecture. In this paper, we survey the research literature for statements made regarding the relationship between agile development and software architecture. The main findings are that there is a lack of scientific support for many of the claims that are concerned with agile and architecture, and more empirical studies are needed to fully reveal the benefits and drawbacks implied by an agile software development method.}
}

@article{rayyan-727968084,
  title={A systematic mapping study on test generation from Input/Output transition systems},
  year={2015},
  pages={333-340},
  author={Da Costa Paiva, Sofia Larissa and Da Silva Simao, Adenilso},
  keywords={Systematics, Data mining, Testing, systematic mapping study, Taxonomy, Protocols, Data models, test generation, Inference algorithms, Input/Output Transition Systems},
  abstract={Context: The construction of complex systems has increased the adoption of technologies that aim at automating the testing activity. Model-Based Testing (MBT) has emerged as an approach to automate the generation of high-quality test suites from behavioural models. Input/Output Transition Systems(IOTSs) have been used in MBT because they are more expressive than other formalisms. Objective: This paper focuses on methods for test generation from IOTSs, aiming at synthesizing available knowledge and identifying gaps in the existing approaches. Method: A systematic mapping was conducted, in which 84 studies were evaluated and categorized in the taxonomy of MBT approaches. Results: The results indicate most of the reported approaches apply non-deterministic algorithms to test generation which do not employ measures of coverage or quality. This scenario underscores the importance of further research into this topic. Conclusion: The evidences indicate that the generation of complete test suites is guaranteed in theory without satisfying a certain test selection criterion. This result points out the need of additional investigation in this topic.}
}

@article{rayyan-727968085,
  title={Functional safety in product lines – a systematic mapping study},
  year={2016},
  pages={313-322},
  author={Baumgart, Stephan and Fröberg, Joakim},
  keywords={Software, Systematics, Safety, Systematic Mapping Study, Databases, Context, Standards, Focusing, Functional Safety, Product Line Engineering},
  abstract={Software product line engineering is a widely used approach to plan and manage reuse of software. When safety critical products are developed, achieving functional safety standard compliance must be shown. The requirements stated in the functional safety standards also apply when safety critical products are developed in product lines. Managing functional safety in industrial product lines is challenging and work around solutions are applied in practice. The objective of this research is to collect and review reported research publications focusing on achieving safety in product lines and to identify gaps in todays research. We conduct a systematic mapping study of research publications reported until January 2016. We identify 39 research articles to be included in a list of primary studies and analyze how product lines are documented, which safety-related topics are covered and which evaluation method the studies apply. Generally, we find that the area of how to achieve functional safety in product lines needs more attention. Our study provides an overview on which topics have been discussed until now and which safety-related topics need more attention.}
}

@article{rayyan-727968086,
  title={A mapping study on requirements engineering in agile software development},
  year={2015},
  pages={199-207},
  author={Heikkilä, Ville T and Damian, Daniela and Lassenius, Casper and Paasivaara, Maria},
  keywords={mapping study, Bibliographies, Requirements engineering, Agile software development, Context, Usability, requirements engineering, Documentation, scrum, agile development, Software},
  abstract={Agile software development (ASD) methods have gained popularity in the industry and been the subject of an increasing amount of academic research. Although requirements engineering (RE) in ASD has been studied, the overall understanding of RE in ASD as a phenomenon is still weak. We conducted a mapping study of RE in ASD to review the scientific literature. 28 articles on the topic were identified and analyzed. The results indicate that the definition of agile RE is vague. The proposed benefits from agile RE included lower process overheads, a better requirements understanding, a reduced tendency to over allocate development resources, responsiveness to change, rapid delivery of value, and improved customer relationships. The problematic areas of agile RE were the use of customer representatives, the user story requirements format, the prioritization of requirements, growing technical debt, tacit requirements knowledge, and imprecise effort estimation. We also report proposed solutions to the identified problems.}
}

@article{rayyan-727968087,
  title={Systems-of-systems development: Initiatives, trends, and challenges},
  year={2016},
  pages={1-12},
  author={Lana, Cristiane A and Souza, Nilton M and Delamaro, Márcio E and Nakagawa, Elisa Y and Oquendo, Flávio and Maldonado, José C},
  keywords={Software, Systematics, Modeling, Computer architecture, Market research, Systems-of-Systems, Standards, Software Development Process, Software Process Standards},
  abstract={Systems-of-Systems (SoS) refer to large, complex, and software-intensive systems, resulted from the interoperability among heterogeneous, independent constituent systems. The main purpose of SoS is to perform tasks that could not be achieved by these constituents separately; besides, unique SoS characteristics impose new challenges to their development processes. We conducted a Systematic Mapping to identify initiatives, trends, and challenges in the SoS development and, as a result, 32 initiatives were identified. We also evaluated these initiatives with regard to the adherence to the technical processes of IEEE/ISO/IEC 15288:2015 and to the software implementation essential processes of IEEE/ISO/IEC 12207:2008. In general, these initiatives emphasize artifacts in the early stages of the development processes, addressing mainly requirements and architecture. Moreover, 17 of them emphasize the testing activity. We also summarized the SoS characteristics addressed in the studies and discuss the main trends and challenges.}
}

@article{rayyan-727968088,
  title={Business process management in digital and software ecosystems: A systematic mapping study},
  year={2020},
  pages={226-233},
  author={Afonso, Anderson Tavares Queiroz and Chueri, Luciana Vilanova and dos Santos, Rodrigo Pereira},
  keywords={Software, Ecosystems, Software architecture, Systematic Mapping Study, Business process management, Conferences, Process control, Business Process Management, Digital Ecosystem, Software Ecosystem},
  abstract={Modern companies need to revisit and constantly improve their business practices in order to remain profitable in the light of the global competition. Digital Ecosystems (DE) have emerged as an innovative approach to business because they transcend traditional collaborative environments towards an interactive digital environment with the intensive participation of diverse external actors. However, adjusting the existing business processes to the digital environment is not simple, especially taking into account the culture of the DE actors and the characteristics of such environment. The same issue applies when the ecosystem is focused on software development as investigated in a Software Ecosystem (SECO). In this context, this study aims to understand how the management of traditional business processes can be applied into DE or SECO management. To this end, a systematic mapping study regarding business process management in DE and SECO is presented. This study is important since it helps researchers and practitioners to understand how these processes can provide inputs for modeling relationships among the DE actors, besides providing support to ecosystem managers. The study confirms that the use of business process management techniques or methods as a support for DE or SECO managers or developers is an emergent field.}
}

@article{rayyan-727968089,
  title={Investigating integration challenges and solutions in global software development},
  year={2011},
  pages={291-297},
  author={Zafar, Atique and Ali, Sajad and Shahzad, Raja Khurram},
  keywords={Software, Collaboration, Integration, Programming, Context, Documentation, Global Software Development, Failure Factor, Success Factor},
  abstract={One of the major challenges of Global Software Development (GSD) is associated with overcoming integration problems that remain hidden throughout development and surface at the end of a project. Incompatibilities and other integration complexities can lead to extra costs, delays, lowered quality and even failure of a GSD project. A solid integration strategy can be an effective solution to overcome integration challenges, but this requires a good understanding of what causes failure and how the failures can be mitigated. The aim of this study is to investigate integration problems that occurred in different phases of GSD, and successful integration practices with their relative importance through an extensive literature review and a Delphi survey. As a result, a prioritized list of failure and success factors is obtained. The findings are applicable for planning new projects at an early stage of GSD or improving ongoing projects.}
}

@article{rayyan-727968090,
  title={Decision trees based software development effort estimation: A systematic mapping study},
  year={2019},
  pages={1-6},
  author={Najm, Assia and Zakrani, Abdelali and Marzak, Abdelaziz},
  keywords={Software, Systematics, Data mining, Estimation, Systematic Mapping Study, Libraries, Market research, Decision Tree, Decision trees, Regression Tree, Software Development Effort Estimation, Decision Trees, Trees},
  abstract={The decision tree (DT) represents a nonparametric estimation method that has been mostly used for both classification and regression problems. DTs were adopted for software development effort estimation (SDEE) generally for their simplicity of use and interpretation contrary to other learning methods. Nevertheless, to our self-knowledge, no systematic mapping has been devoted especially to decision trees. The aim of this study is to elaborate a systematic mapping study that classifies DTs papers in conformity with the succeeding criteria: research approach, contribution type, techniques employed in combination with DT methods besides identifying publication channels and trends. An automated search of five digital libraries was made to carry out a systematic mapping of DT studies mainly devoted to SDEE that were published in the period 1985-2017. We identify 46 relevant studies. Basically, the results revealed that most researchers focus on technique contribution type. In addition, the majority of papers deal with improving the existing DT models while few studies have proposed novel models to improve the reliability of SDEE. Furthermore, solution proposal and case study are the most frequently used approaches.}
}

@article{rayyan-727968091,
  title={Software developers sentiment analysis: A systematic mapping},
  year={2018},
  pages={60-69},
  author={Coelho, André A N and Silva, Thalita T O and Oliveira, Alessandreia M and David, José Maria N},
  keywords={Software, Systematics, Tools, Software Development, Software algorithms, Delays, Emotion Analysis, Sentiment analysis, Sentiment Analysis, Tagging},
  abstract={Sentiment Analysis is a research area that has been also focused on software developers in the latest years. However, there is a need to map the way in which this area is structured. The goal of this article is to answer research questions through a systematic mapping in order to be aware of the existing works in this area. As a result, a growth curve related to the number of publications has been perceived as well as the association between the emotional condition of a developer and his/her performance in software projects.}
}

@article{rayyan-727968092,
  title={Spreadsheet smells: A systematic mapping study},
  year={2019},
  pages={345-3455},
  author={Azam, Awais and Alam, Khubaib Amjad and Umair, Areeba},
  keywords={Software Engineering, Systematic Mapping, Spreadsheet Programming, Spreadsheet Smells, Spreadsheets, Smell},
  abstract={Spreadsheets provide a very flexible programming environment and are used by almost every organization, company, institution or business for data processing and data storage tasks. The main objectives of this research are: (1) to have an overview of the research that is being done related to spreadsheet smells; (2) to classify the spreadsheet smells research according to ten criteria: techniques, year of publication, publication venues, publication channels, datasets, countries, contribution type, research approaches, tools used, tools/techniques proposed; and (3) analyze studies from different perspectives like study objectives, methods, method accuracy and limitations. We performed a systematic mapping on the spreadsheet smells studies published in the time span of 2010-2019, and adopted proper methods to classify, review and analyze them. In total, we were able to identify 28 studies and map the results of these studies.}
}

@article{rayyan-727968093,
  title={Safe and secure: re-engineering a software process set for the challenges of the 21st century},
  year={2014},
  pages={1-6},
  author={Wallace, K R},
  keywords={Software, Safety, Security, Process, Risk},
  abstract={This paper discusses a risk-based approach to re-engineering a legacy software engineering process set in the context of a large-scale engineering enterprise responsible for the design and production of surface warships. The increasing integrity requirements on software deployed on modern naval platforms, principally in respect of safety and security, have been addressed through elicitation and analysis of key software integrity risks. The results of this analysis have been applied to assess the extent of mitigation of the identified risks provided in the legacy process set. This assessment provides a basis for the further development and improvement of the process set in respect of treatment of software integrity. More generally the approach provides a template for risk elicitation and analysis that can be extended to treat further categories of software-related risk such as acquisition/supply chain, legal and human factors.}
}

@article{rayyan-727968094,
  title={A systematic mapping study on dynamic metrics and software quality},
  year={2012},
  pages={326-335},
  author={Tahir, Amjed and MacDonell, Stephen G},
  keywords={Software, mapping study, Manuals, Software metrics, software quality, Conferences, Performance analysis, software metrics, dynamic analysis, dynamic metrics, Metronidazole},
  abstract={Several important aspects of software product quality can be evaluated using dynamic metrics that effectively capture and reflect the software's true runtime behavior. While the extent of research in this field is still relatively limited, particularly when compared to research on static metrics, the field is growing, given the inherent advantages of dynamic metrics. The aim of this work is to systematically investigate the body of research on dynamic software metrics to identify issues associated with their selection, design and implementation. Mapping studies are being increasingly used in software engineering to characterize an emerging body of research and to identify gaps in the field under investigation. In this study we identified and evaluated 60 works based on a set of defined selection criteria. These studies were further classified and analyzed to identify their relativity to future dynamic metrics research. The classification was based on three different facets: research focus, research type and contribution type. We found a strong body of research related to dynamic coupling and cohesion metrics, with most works also addressing the abstract notion of software complexity. Specific opportunities for future work relate to a much broader range of quality dimensions.}
}

@article{rayyan-727968095,
  title={Bibliometric analysis of the tertiary study on agile software development using social network analysis},
  year={2020},
  pages={1-4},
  author={Bayram, Egemen and Doğan, Buket and Tunalı, Volkan},
  keywords={Software, Citation analysis, tertiary study, Systematics, systematic literature reviews, Bibliographies, Tools, social network analysis, Agile software development, agile software development, bibliometric analysis, Social networking (online), Social Support, Bibliometrics},
  abstract={This study aims to examine the systematic literature reviews published on the Agile Software Development subject between 2013 and 2018 and to examine the citation relationships among the studies within the scope of the tertiary study with the help of social network analysis. In this study, the publications within the scope were visualized with VOSviewer and Gephi social network analysis tools, and the relations between publishing institutions, countries were revealed. In citation analysis; according to the total link strength, it is seen that UK, Spain and Slovenia are at the forefront at institution and country level. Brazil has the highest citation value and provides the link between the two large clusters obtained in the analysis. In the bibliographic coupling analysis, the five most active countries at the country level were Brazil, Germany, Finland, Malaysia and Pakistan. When the same analysis is made at the institution level, the top five institutions are in Brazil, Switzerland, Peru and Pakistan. The findings of the study indicate that developing countries have more studies on the subject and that the cited publications are mostly from developed countries; European countries seem to be more collaborative based on citation analysis yet developing countries such as Brazil and Malaysia have also relations with them; the number of publications is not directly proportional to the citations.}
}

@article{rayyan-727968096,
  title={Handling of categorical data in software development effort estimation: A systematic mapping study},
  year={2019},
  pages={763-770},
  author={Amazal, Fatima Azzahra and Idri, Ali},
  keywords={Software, Software engineering, Systematics, Data mining, Computer science, Conferences, Market research},
  abstract={Producing reliable and accurate estimates of software effort remains a difficult task in software project management, especially at the early stages of the software life cycle where the information available is more categorical than numerical. In this paper, we conducted a systematic mapping study of papers dealing with categorical data in software development effort estimation. In total, 27 papers were identified from 1997 to January 2019. The selected studies were analyzed and classified according to eight criteria: publication channels, year of publication, research approach, contribution type, SDEE technique, Technique used to handle categorical data, types of categorical data and datasets used. The results showed that most of the selected papers investigate the use of both nominal and ordinal data. Furthermore, Euclidean distance, fuzzy logic, and fuzzy clustering techniques were the most used techniques to handle categorical data using analogy. Using regression, most papers employed ANOVA and combination of categories.}
}

@article{rayyan-727968097,
  title={Does pair programming work in a data science context? An initial case study},
  year={2017},
  pages={2348-2354},
  author={Saltz, Jeffrey S and Shamshurin, Ivan},
  keywords={Software, Software engineering, Project Management, Pair Programming, Big Data, Process Methodology, Programming profession, Data science, Data Science},
  abstract={While pair programming has been studied extensively for software programmers, very little has been reported with respect to pair programming in a data science project. This paper reports on a case study evaluating the effectiveness of pair programming within a data science / big data context. Our findings show that pair programming can be useful for data science teams. In addition, while the driver role was similar to what has been described for software programmers, we note that the observer role had an expanded set of responsibilities, which we termed researcher activities. Further exploration is required to explore if these expanded roles are specific to data science pair programming.}
}

@article{rayyan-727968098,
  title={Supporting customizable architectural design decision management},
  year={2010},
  pages={232-240},
  author={Chen, Lianping and Babar, Muhammad Ali},
  keywords={Software engineering, Software architecture, Software design, Software systems, Engineering management, Costs, Data models, software architecture, design rationale, Systems engineering and theory, architectural design decision, Architectural knowledge, Conference management, customizability, Design engineering},
  abstract={When engineering complex software systems, the key Architectural Design Decisions (ADD) and the reasoning underlying those decisions need to be fully understood by all stakeholders. Achieving such understanding usually requires the use of ADD management tools. Most existing ADD management tools apply prescriptive ADD models and do not provide sufficient customizability. However, forcing architects to follow an ADD model that does not fit their specific needs can cause significant problems (e.g., extra cost is needed, and architects' willingness and motivation can negatively be affected). This research project aims at solving this issue by developing a highly customizable solution, which can enable practitioners to define ADD models according to their preferences and working situations. The detailed needs for ADD model customization will be identified by multiple case studies and semi-structured interviews; the proposed solution will be evaluated using different empirical research methods.}
}

@article{rayyan-727968099,
  title={Goal-oriented requirements engineering: A systematic literature map},
  year={2016},
  pages={106-115},
  author={Horkoff, Jennifer and Aydemir, Fatma Başak and Cardoso, Evellin and Li, Tong and Maté, Alejandro and Paja, Elda and Salnitri, Mattia and Mylopoulos, John and Giorgini, Paolo},
  keywords={Systematics, systematic literature survey, Requirements engineering, Conferences, Analytical models, Proposals, requirements engineering, Adaptation models, Syntactics, evidence-based requirements engineering, goal mode, systematic literature map},
  abstract={Over the last two decades, much attention has been paid to the area of Goal-Oriented Requirements Engineering(GORE), where goals are used as a useful conceptualization to elicit, model and analyze requirements, capturing alternatives and conflicts. Goal modeling has been adapted and applied to many sub-topics within RE and beyond, such as agent-orientation, aspect-orientation, business intelligence, model-driven development, security, and so on. Despite extensive efforts in this field, the RE community lacks a recent, general systematic literature review of the area. As a first step towards providing a GORE overview, we present a Systematic Literature Map, focusing on GORE-related publications at a high-level, categorizing and analyzing paper information in order to answer several research questions, while omitting a detailed analysis of individual paper quality. Our Literature Map covers the 246 top-cited GORE-related conference and journal papers, according to Scopus, classifying them into a number of descriptive paper types and topics, providing an analysis of the data, which is made publicly available. We use our analysis results to make recommendations concerning future GORE research.}
}

@article{rayyan-727968100,
  title={Quantifying the performance impact of SQL antipatterns on mobile applications},
  year={2019},
  pages={53-64},
  author={Lyu, Yingjun and Alotaibi, Ali and Halfond, William G J},
  keywords={Software engineering, Bibliographies, Security, Mobile applications, Databases, performance, security, mobile applications, Mobile handsets, Benchmark testing, database, energy, SQL antipatterns},
  abstract={In mobile applications, local databases have become an important component, providing mobile users with a responsive and secure service for data access and management. However, using local databases comes with a cost. Studies have shown that they are one of the most resource consuming components on mobile devices. Improper usage of the local database can even severely impact the responsiveness of an application. In this paper, we conducted a literature review and a benchmark study to investigate problematic programming practices with respect to database usage. Our results present a comprehensive overview of the current knowledge about these practices, and introduce new knowledge about the impact of these practices on the resource consumption of mobile applications.}
}

@article{rayyan-727968101,
  title={What are the used activity diagram constructs? a survey},
  year={2014},
  pages={87-98},
  author={Reggio, Gianna and Leotta, Maurizio and Ricca, Filippo and Clerissi, Diego},
  keywords={Software, Software engineering, Survey, Empirical Study, Unified modeling language, Object oriented modeling, Educational institutions, Tutorials, Jacobian matrices, UML Usage},
  abstract={UML Activity diagrams offer a very large set of constructs, however many of them seem scarcely used or even their existence is not known. Here, we present a precise view of the usage levels of these constructs by means of a survey, covering preliminarily books, courses, tutorials, and tools about UML. Results show that, among the 47 Activity diagrams constructs, a large majority of them seem to be scarcely used, while, only nine result widely used. This work is part of a larger project aimed at investigating the usage level of the UML diagrams and their constructs, also by means of a personal opinion survey intended for UML users. UML is really a huge notation, and as consequence, on one hand, it is difficult and time consuming to master it, and on the other hand, people tend, naturally, to consider only a part of it; by means of this empirical study we want to assess what are the most/less used UML diagrams/constructs.}
}

@article{rayyan-727968102,
  title={A survey on project factors that motivate Finnish software engineers},
  year={2014},
  pages={1-9},
  author={Misirli, Ayse Tosun and Verner, June and Markkula, Jouni and Oivo, Markku},
  keywords={Software, Software engineering, Productivity, Educational institutions, Cultural differences, Maintenance engineering, Schedules, culture, motivational factors, software development team motivation, software project outcome},
  abstract={Previous studies suggest that motivation is a critical factor in developer productivity and project outcome, i.e., software project failures are significantly associated with low motivation of software teams. Surveys with software engineers also indicate that culture can affect software development team motivation through differences in developers' motivational factors. In this study, we conduct a survey with 15 Finnish software engineers and 21 non-Finnish software engineers who live in Finland to investigate 1) the relationship between team motivation and project outcome, and 2) the factors that motivate Finnish software engineers. We compare the motivational factors found from the Finnish data with those identified in prior research with developers from four different countries. An analysis of the data from our 36 subjects indicates that project outcome is not associated with team motivation in Finland though this result contradicts prior research. There is a single motivational factor, “team work” that appears to be culturally independent. Unique team motivational factors for Finnish software engineers are found to be “the authority of project manager” and “the vision of project manager”. Our results indicate that cultural differences affect team motivation and that project managers need to consider these when working in a global environment.}
}

@article{rayyan-727968103,
  title={Layered architecture revisited — Comparison of research and practice},
  year={2009},
  pages={317-320},
  author={Savolainen, Juha and Myllarniemi, Varvana},
  keywords={Guidelines, Software architecture, Software design, Databases, Software systems, Computer architecture, Computer industry, Protocols, Books, Organizing},
  abstract={Organizing a software architecture into layers has been one of the earliest architectural styles ever used. Even today layered structure is a very common architectural style used in various industrial systems. However, we have observed that the usage of layered architectural style varies greatly in different contexts. This paper aims to compare the notion of software architecture layers in research literature as well as in industrial practice. Firstly, we performed a systematic literature review of research articles on layered software architectures; we also reviewed selected books of software architecture. Secondly, to understand the practice, we investigated a number different recent architecture documents to cover the current usage of layered architectures. Our results indicate that there is very little actual research done on layered architectures. The current usage of layered structures seems to be more complex than reported before. This gap between the research and practice needs to be bridged by researchers.}
}

@article{rayyan-727968104,
  title={The evolution of architectural decision making as a key focus area of software architecture research: A semi-systematic literature study},
  year={2020},
  pages={69-80},
  author={Bhat, Manoj and Shumaiev, Klym and Hohenstein, Uwe and Biesdorf, Andreas and Matthes, Florian},
  keywords={Bibliographies, Data mining, Tools, Decision making, Software architecture, literature review, decision making, Computer architecture, Biological system modeling, software architecture, architectural design decisions, Decision Making, Software},
  abstract={Literature review studies are essential and form the foundation for any type of research. They serve as the point of departure for those seeking to understand a research topic, as well as, helps research communities to reflect on the ideas, fundamentals, and approaches that have emerged, been acknowledged, and formed the state-of-the-art. In this paper, we present a semi-systematic literature review of 218 papers published over the last four decades that have contributed to a better understanding of architectural design decisions (ADDs). These publications cover various related topics including tool support for managing ADDs, human aspects in architectural decision making (ADM), and group decision making. The results of this paper should be treated as a getting-started guide for researchers who are entering the investigation phase of research on ADM. In this paper, the readers will find a brief description of the contributions made by the established research community over the years. Based on those insights, we recommend our readers to explore the publications and the topics in depth.}
}

@article{rayyan-727968105,
  title={Barriers to implement electronic health records},
  year={2016},
  pages={1-6},
  author={Carlos, Maldonado and Sussy, Bayona Oré},
  keywords={Systematics, Security, barriers, ISO Standards, Silicon compounds, EHR, electronic healt records, Electronic medical records, HCE},
  abstract={The aim of implementing Electronic Health Record (EHR) is to replace the use of handwritten medical records in patient care. Despite of the benefits of EHR implementation, still there are still limitations. The aim of this study is to identify the barriers that impact the EHR implementation in health. A systematic review was conducted in order to identify the barriers. As a result 156 articles were identified and finally were selected 9 primary articles and 8 articles based on systematic review of the literature. A list of barriers were identified and categorized. Financial, technological, security, and organizational aspects are the most important limitations.}
}

@article{rayyan-727968106,
  title={An ontology-based approach to semi-automate systematic literature reviews},
  year={2018},
  pages={9-16},
  author={Ali, Asad and Gravino, Carmine},
  keywords={Software, SLR, Data mining, Ontology, Ontologies, Semantic Web, Protocols, Jena, Maximum likelihood estimation, User Interface},
  abstract={A Systematic Literature Review (SLR) allows us to combine and analyze data from multiple (published and unpublished) studies. Though it provides a complete and comprehensive empirical evidence of an area of interest, the results we usually get from the data synthesis phase of an SLR include huge tables and graphs and thus, for users, it is a tedious and time-consuming job to get the required results. In this work, we propose to semi-automate some steps which can be used to fetch the information from an SLR, beyond the traditional tables, graphs, and plots. The automation is performed using Semantic Web technologies like ontology, Jena API and SPARQL queries. The Semantic Web, also called Web 3.0, provides a common framework and thus allows us to share and re-use the data across the applications and enterprises. It can be used to integrate, extract, and infer the most relevant data required by the users, which are hidden behind the huge information on the Web. We also provide an easy-to-use user interface in order to allow users to perform different searches and find their required SLR results easily and quickly. Finally, we present the results of a preliminary user study performed to analyze the amount of time users need to extract their required information, both via the SLR tables and our proposal. The results revealed that with our system the users get their required information in less time compared to the manual system.}
}

@article{rayyan-727968107,
  title={Computer-aided diagnosis of chronic kidney disease in developing countries: A comparative analysis of machine learning techniques},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={25407-25419},
  author={Sobrinho, Alvaro and Queiroz, Andressa C M Da S and Dias Da Silva, Leandro and De Barros Costa, Evandro and Eliete Pinheiro, Maria and Perkusich, Angelo},
  keywords={Software, machine learning, Guidelines, Machine learning, Diseases, Developing countries, Kidney, medical diagnosis, Reviews, Kidney Diseases, Developing Countries, Acquired Immunodeficiency Syndrome, Developed Countries},
  abstract={The high incidence and prevalence of chronic kidney disease (CKD), often caused by late diagnoses, is a critical public health problem, especially in developing countries such as Brazil. CKD treatment therapies, such as dialysis and kidney transplantation, increase the morbidity and mortality rates, besides the public health costs. This study analyses the usage of machine learning techniques to assist in the early diagnosis of CKD in developing countries. Qualitative and quantitative comparative analyses are, respectively, conducted using a systematic literature review and an experiment with machine learning techniques, with the k-fold cross-validation method based on the Weka© software and a CKD dataset. These analyses enable a discussion on the suitability of machine learning techniques for screening for CKD risk, focusing on low-income and hard-to-reach settings of developing countries, due to the specific problems faced by them, e.g., inadequate primary health care. The study results show that the J48 decision tree is a suitable machine learning technique for such screening in developing countries, due to the easy interpretation of its classification results, with 95.00% accuracy, reaching a nearly perfect agreement with an experienced nephrologist`s opinion. Conversely, random forest, naive Bayes, support vector machine, multilayer perceptron, and k-nearest neighbor techniques, respectively, yield 93.33%, 88.33%, 76.66%, 75.00%, and 71.67% accuracy, presenting at least moderate agreement with the nephrologist, at the cost of a more difficult interpretation of the classification results.}
}

@article{rayyan-727968108,
  title={Psychological effects and their role in online privacy interactions: A review},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={21236-21260},
  author={Kitkowska, Agnieszka and Shulman, Yefim and Martucci, Leonardo A and Wästlund, Erik},
  keywords={Visualization, Bibliographies, Decision making, Privacy, Psychology, Economics, privacy, design, attitude, behavior, decision-making, Government, HCI, visual cues},
  abstract={Because of the increasing dependency on online technologies in even the most ordinary activities, people have to make privacy decisions during everyday online interactions. Visual design often influences their choices. Hence, it is in the hands of choice architects and designers to guide users towards specific decision outcomes. This “nudging” has gained much interest among scholars in interdisciplinary research, resulting in experimental studies with visual cues that may have the potential to alter attitudes and behaviors. Attitude and behavior changes are often attributed to several psychological effects manifesting in cognitive processing and decision-making. This article presents the results of a systematic literature review carried out to identify which psychological effects have been previously studied in the context of online privacy interactions. Subsequently, fifteen articles were selected and thoroughly reviewed, resulting in the identification of twenty psychological effects. The visual cues triggering these effects were recognized and classified against their capabilities to alter privacy attitudes and behaviors. Specifically, the visual cues were divided into two categories: privacy-enhancing and privacy-deteriorating. This review discusses the applicability of such cues in research and UI design. Further, the findings are discussed against the existing research on digital nudges. The authors conclude with a discussion on issues of research quality in the privacy-related field and outline the road to improvement.}
}

@article{rayyan-727968109,
  title={Enterprise architects should follow the money},
  year={2014},
  volume={1},
  pages={135-142},
  author={van den Berg, Martin and van Vliet, Hans},
  keywords={Systematics, Bibliographies, Decision making, Information systems, Organizations, Investment, enterprise architecture, IT decision-making, IT governance},
  abstract={Enterprise architecture (EA) offers ways to steer and guide the design and evolution of the enterprise including its information technology (IT). One of the outputs of EA is improved decision-making about IT. Objective: This study aims to provide EA researchers and practitioners with insights into how IT decision-making actually takes place and what that means for them. Method: A systematic literature review was conducted in order to find and analyze primary studies about IT decision-making. Results: We found that IT investment and prioritization is by far the largest decision category. Money seems much more important than content. The IT decision-making process itself is subject to different variables and factors making every IT decision unique. We also found that both rational and bounded rational approaches are used in IT decision-making. EA has not a prominent role in IT decision-making. Conclusions: IT decision-making is a messy and complex process where money plays a prominent role. We argue that, if enterprise architects want to influence IT decision-making, they should follow the money by combining content with investment planning and prioritization. Further research is required into what distinguishes enterprise architects that are successful in IT decision-making, from those that are less successful.}
}

@article{rayyan-727968110,
  title={Resistence in Software Process Improvement: A classification proposal of causes, resistance models and gamification},
  year={2019},
  pages={1-6},
  keywords={Software, Information systems, Proposals, ISO Standards, Silicon compounds, software process improvement, gamification, change resistance, Immune system, Resistance},
  abstract={Software development companies are interested in best practices implementation looking for the improvement of their processes and software products. However, software process improvement (SPI) initiatives are complex because search to ameliorate the conditions and behavior of the professionals involved in software development. For this reason, indicators about change resistance in SPI initiatives show that underestimating the complexity of a change process and its consequent resistance to change weakens the results of such improvement. Therefore, there is a growing search of strategies for reduce the change resistance and as a result of this search, gamification is being used in this context because it helps to modify and influence the behavior of people and thus achieve the proposed goals. Motivated by the gamification benefits, in this article we propose a classification of the models of change resistance found in the literature looking to define a strategy that facilitate SPI initiatives implementation. Such classification proposal will be an input for the gamification use to decrease change resistance common causes in SPI initiatives. This classification is the result of a systematic literature review and the work with an experts group using transitivity law for relating models, causes and gamification principles.}
}

@article{rayyan-727968111,
  title={How to design tools for supporting self-regulated learning in MOOCs? Lessons learned from a literature review from 2008 to 2016},
  year={2016},
  pages={1-12},
  author={Pérez-Álvarez and Pérez-Sanagustín, R and Maldonado, M Jorge J},
  keywords={Software, Bibliographies, Tools, Literature review, Monitoring, Libraries, Silicon compounds, Mobile communication, Design tools, MOOC, Online, Self-Regulated Learning, System, Technologies},
  abstract={This paper presents a systematic literature review that examines and analyzes the articles from 2008 to 2016 that have addressed the development of tools to support Self-Regulated Learning (SRL) in online and MOOC environments. The findings denote that: (1) there is a lack of tools to support SRL in MOOC environments; (2) the evaluation of the existing tools are not aligned whit the objectives of the research; (3) current research presents proposal of tools but very few achieve the stage of implementation; and (4) current existing tools tend to support many SRL strategies at the same time. We end up with a set of lessons learned for guiding the implementation of tools to support SRL strategies in MOOCs environments.}
}

@article{rayyan-727968112,
  title={Bridging the gap between software architecture and business model development: A literature study},
  year={2019},
  pages={1519-1524},
  author={Hyrynsalmi, Sami and Rauti, Sampsa and Kaila, Erkki},
  keywords={Systematics, Software architecture, literature review, Software systems, Computer architecture, Companies, software architecture, Pricing, business model, mercury business, software-intensive business, Software},
  abstract={The software architecture plan describes the high-level structure and logic of a software system. The architectural plan acts as a constitution and dictates the fundamental principles of the system; therefore, the plan also eventually determines which kinds of business models the software system can support. In the modern mercury business, there is need for experimentation in business model and flexibility in architecture. This paper uses a systematic literature review method to collect primary studies from the extant literature addressing business models and software architectures. The aim is to summarize the current knowledge. The selected primary studies (n=10) are qualitatively analyses and synthesized. The results show that the area remain mostly unaddressed and there is need to develop new methods to support flexible architecture design, tools and development methods.}
}

@article{rayyan-727968113,
  title={Urban cableway systems: State-of-art and analysis of the emirates air line, london},
  year={2020},
  pages={1-8},
  author={Tiessler, Michaela and Ricci, Gysele Lima and Bogenberger, Klaus},
  keywords={Systematics, Bibliographies, Planning, Maintenance engineering, Public transportation, Urban areas, Automobiles, London},
  abstract={Cableway systems are often associated with mountains and skiing. Nonetheless, they established in several cities for public transportation in the last years. This paper describes the state of the art in urban cable car systems and focuses especially on advances and experiences of planning processes in Germany. For this purpose, we conduct a systematic review of academic literature regarding urban ropeway systems, including analyses of the international context, the methodologies used and different perspectives on the topic in an international context. Additionally, we investigate the Emirates Air Line which operates since 2012 in London in order to identify the challenges for urban ropeway systems for Germany. The results indicate that urban ropeway systems have a potential to establish in public transportation in German cities, but still exist several obstacles that need to be settled.}
}

@article{rayyan-727968114,
  title={Mobile mental health: A review of applications for depression assistance},
  year={2019},
  pages={708-713},
  author={Teles, Ariel and Rodrigues, Ivan and Viana, Davi and Silva, Francisco and Coutinho, Luciano and Endler, Markus and Rabêlo, Ricardo},
  keywords={Systematics, Monitoring, Quality assessment, Mobile applications, Mobile Applications, Psychology, Google, mHealth, Depression, Mental disorders, Mobile Mental Health},
  abstract={Depression is a mental disorder characterized by persistent sadness, loss of interest, and a set of behavioral changes. The high prevalence of depression imposes a significant burden on the world population, demanding methods capable of monitoring and treating this mental disorder. Currently, a large number of mobile applications have been designed to provide support to depressive people. This paper aims to identify, analyze and characterize the current state of mobile applications focused on depression. To do so, we conducted a systematic review of applications for depression assistance. The two most popular mobile app stores (Google Play Store and Apple App Store) have been explored to find the most relevant apps. After applying the inclusion and exclusion criteria and performing the quality assessment of the results, 216 applications were selected for the data extraction phase, where we summarized their benefits and limitations and identified gaps and trends. The results of this review evidenced that there is a growth in the diversity of apps' purposes such as chatbot, online therapy, educational tools, mood tracker, testing, and self-help.}
}

@article{rayyan-727968115,
  title={TANDEM: A taxonomy and a dataset of real-world performance bugs},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={107214-107228},
  author={Sánchez, Ana B and Delgado-Pérez, Pedro and Medina-Bulo, Inmaculada and Segura, Sergio},
  keywords={Software, Systematics, Testing, Programming, Taxonomy, taxonomy, Computer bugs, Graphical user interfaces, dataset, Performance bugs, performance testing},
  abstract={The detection of performance bugs, like those causing an unexpected execution time, has gained much attention in the last years due to their potential impact in safety-critical and resource-constrained applications. Much effort has been put on trying to understand the nature of performance bugs in different domains as a starting point for the development of effective testing techniques. However, the lack of a widely accepted classification scheme of performance faults and, more importantly, the lack of well-documented and understandable datasets makes it difficult to draw rigorous and verifiable conclusions widely accepted by the community. In this paper, we present TANDEM, a dual contribution related to real-world performance bugs. Firstly, we propose a taxonomy of performance bugs based on a thorough systematic review of the related literature, divided into three main categories: effects, causes and contexts of bugs. Secondly, we provide a complete collection of fully documented real-world performance bugs. Together, these contributions pave the way for the development of stronger and reproducible research results on performance testing.}
}

@article{rayyan-727968116,
  title={Metrics to evalute fuctional quality: A sistematic review},
  year={2012},
  pages={1-6},
  author={Carrillo, Alberto Blanco and Mateo, Pedro Reales and Monje, Moisés Rodríguez},
  keywords={Software, Measurement, metrics, Silicon compounds, ISO standards, IEC standards, Functional Quality, ISO, ISO 25010, Sistematic Review, Metronidazole},
  abstract={Nowadays, quality assurance of software products is very important. To assure this quality, there exist several international standards like the standard ISO 25010. These standards define quality models, characteristics and sub-characteristics that must be taken into account in order to evaluate the quality of a software product. In this context, metrics are needed to evaluate quality objectively. This document shows a systematic review of the literature about the existing metrics that can be used to measure functional quality (as defined in the international standard ISO 25010) of a software product. As results of the conducted review, 23 metrics were found, most of them based on tests. The conclusions obtained from this study show that, although more metrics should be defined, there exists a enough number of metrics to measure all the functional quality characteristics defined in the international standard ISO 25010.}
}

@article{rayyan-727968117,
  title={An analysis of the inclusion of environmental cost factors in software cost estimation datasets},
  year={2018},
  pages={623-630},
  author={Mustafa, Emtinan and Osman, Rasha},
  keywords={Software, Estimation, Productivity, Organizations, Cultural differences, Benchmark testing, Environmental factors, cost attributes, datasets, environmental factors, software effort estimation},
  abstract={Many factors influence the cost estimation of software projects. Environmental factors are country-specific cultural, societal or technical factors that impact software development cost. In this paper, we analyzed the characteristics of 31 cost estimation datasets to identify the incorporation of environmental factors within their cost attributes. To analyze the heterogeneous attributes of the datasets, we combined all attributes within six categories and 48 representative attributes. We observed that the majority of datasets represent organizational environments of Europe and North America. Furthermore, recent datasets provided more diverse attributes and increased coverage of organizational factors (users, developers and project attributes). However, environmental factors, i.e., cultural and societal norms, were not represented. This limits the application of these datasets in environments in which these factors have a high impact on software development cost. This paper highlights the need for further research into cultural and environmental factors and their impact on software cost estimation.}
}

@article{rayyan-727968118,
  title={Eye-tracking technologies supporting vision screening in children},
  year={2020},
  pages={471-478},
  author={Ali, Qasim and Heldal, Ilona and Helgesen, Carsten G and Costescu, Cristina and Kovari, Attila and Katona, Jozsef and Thill, Serge},
  keywords={Conferences, Stakeholders, stakeholders, eye-tracking, Gaze tracking, functional vision, oculomotor problems, school children, vision screening, Only Child, Child},
  abstract={The aim of this paper is to contribute to a better understanding of children's vision care from the identification to treatment, with, in particular, a better understanding of the use of eye tracking (ET) technologies. While there are indications that these technologies can support vision care, a comprehensive understanding of the possibilities is lacking. Here, we review cross-disciplinary research on performing vision care, and identify current challenges for using and further developing ET technologies. To this end, we describe (1) the involved stakeholders, (2) screening possibilities at schools, and (3) how technology-supported vision screening is used today. Data come from a literature survey of peer-reviewed journal and conference articles, complemented by secondary sources from related projects and products. The focus is on literature after 2000, and in particular, on screening oculomotor dysfunctions (OMD), for school children. The results show that the contributions to state of the art from various research areas are fragmented, in particular regarding the communication between the necessary stakeholders influencing vision care, the handling of general and functional vision care, and between screening and treatment. Further development of ET technologies will likely depend on overcoming these fragmentations. A first step in this direction consists of providing a thorough description of stakeholders, their roles, and requirements enabling communication on children with vision problems.}
}

@article{rayyan-727968119,
  title={Heuristic optimization for the resource constrained Project Scheduling Problem: A systematic mapping},
  year={2016},
  pages={619-626},
  author={Ciupe, Aurelia and Meza, Serban and Orza, Bogdan},
  keywords={Systematics, Guidelines, Optimization, Databases, Filtering, Heuristic algorithms, Job shop scheduling},
  abstract={Context: Heuristic optimization has been of strong focus in the recent modeling of the Resource Constrained Project Scheduling Problem (RCPSP), but lack of evidence exists in systematic assessments. New solution methods arise from random evaluation of existing studies. Objective: The current work conducts a secondary study, aiming to systemize existing primary studies in heuristic optimization techniques applied to solving classes of RCPSPs. Method: The systemizing framework consists of performing a systematic mapping study (SM), following a 3-steped protocol. Results: 371 primary studies have been depicted from the multi-stage search and filtering process, to which inclusion and exclusion criteria have been applied. Results have been visually mapped in several distributions. Conclusions: Specific RCPSP classes have been grounded and therefore a rigorous classification is required before performing a systematic mapping. Focusing on recent developments of the RCPSP (2010-2015, a strong interest has been acknowledged on solution methods incorporating AI techniques in meta- and hyper-heuristic algorithms.}
}

@article{rayyan-727968120,
  title={Blockchain application for central banks: A systematic mapping study},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={139918-139952},
  author={Dashkevich, Natalia and Counsell, Steve and Destefanis, Giuseppe},
  keywords={mapping study, Systematics, literature review, challenges, Law, blockchain, assets ownership, Assets transfer, audit trail, CBDC, central bank, central bank digital currency, distributed ledger technology, DLT, financial regulation, opportunities, payment clearing and settlement, PCS, regulatory compliance, research maturity, research trend, use-case},
  abstract={Blockchain is a novel technology capturing the attention of Central Banks and a technology with significant disruptive potential. However, a gap in research effort between practitioners and academics seems to have emerged. This paper analyses and maps that gap by exploring trends in peer-reviewed research contributions through thematic categorisation of academic literature on Distributed Ledger Technology (DLT) use-cases for services, operations and functions performed by central banks. Furthermore, this paper provides summaries of opportunities and challenges for central banks arising from blockchain adaptation to each of those use-cases. To achieve this goal, we utilise a Systematic Mapping Study approach. The paper presents an in-depth assessment of statistical and thematic analysis of research maturity and the types of researchers, with specific emphasis on types of central bank use-cases considered for blockchain adaptation. Our work contributes to an understanding of where the most or least attention is directed, allowing for identification of gaps and opportunities for both academics, practitioners and combinations of each. Results show that the research topic is a comparatively new domain. It confirms the gap between depth and volume of the research provision from industry and academia, with industry leading the trend. Our study also found that the most research-intensive use-cases are those for: 1) Central Bank issued Digital Currency (CBDC), 2) Regulatory Compliance and 3) Payment Clearing and Settlement Systems (PCS) operated by central banks; a comparatively low engagement was found in the areas of 4) Assets Transfer/Ownership and 5) Audit Trail.}
}

@article{rayyan-727968121,
  title={Using GORE in big data: A systematic mapping study},
  year={2019},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={17},
  number={3},
  pages={493-504},
  author={Cravero, Ania and Sepúlveda, Samuel},
  keywords={Software, Systematics, systematic mapping, Big Data, Business, Adaptation models, IEEE transactions, Data models, requirements, goal-oriented, GORE},
  abstract={Big Data has developed rapidly into a hot topic that generates great attention in academia, industry and governments around the world because of its ability to process large volumes of data at a reasonable speed. The real value of Big Data lies in the help to business decision making, so it requires appropriate methods and techniques to find the requirements of the system to be developed. However, the Big Data community has focused mainly on technologies to obtain information, and not on the goal of the business. The objective of this paper is to offer an overview of how some Goal Oriented Requirements Engineering (GORE) proposals are used to obtain the requirements of the Big Data system, serving as a starting point for future research. The methodology used is the Systematic Mapping. The goal models identified it is used as conceptual models for different situations: understanding the nature of business and analyzing the technical aspects for purposes of functional and non-functional requirements of Big Data system.}
}

@article{rayyan-727968122,
  title={Small software organizations need explicit project portfolio management},
  year={2010},
  journal={IBM Journal of Research and Development},
  issn={0018-8646},
  volume={54},
  number={2},
  pages={1:1-1:12},
  author={Vahaniitty, J and Rautiainen, K and Lassenius, C},
  keywords={Software, Project management, Enterprise resource planning, Resource management, Portfolios, Personnel},
  abstract={The concept of managing new product development projects as an explicit portfolio originates from the context of large organizations. However, the question as to whether explicit portfolio management is relevant for small organizations is rarely discussed. We conducted a qualitative multiple-case study of six small organizations (with 15–40 people) that developed software and provided related services. Five of the organizations did not practice explicit portfolio management. They also seemed to suffer from problems that, in the literature, are considered symptomatic of inadequate portfolio management, such as having too many simultaneous projects, overcommitment in terms of workload, and ineffective executive decision making. In one of the studied organizations, the management personnel had recognized the need for explicit portfolio management and introduced portfolio management practices such as regular reviews of the project portfolio, appointing specific people for resolving cross-project conflicts, and limiting the number of concurrent projects to which a person can be assigned. The personnel we interviewed perceived clear improvements with respect to various challenges since the introduction of these practices. Our preliminary study suggests that explicit portfolio management is relevant for small software organizations, at least in cases in which the development personnel possess multiple roles and responsibilities and are concurrently performing many different types of activities.}
}

@article{rayyan-727968123,
  title={A semantic approach to support the analysis of abstracts in a bibliographical review},
  year={2019},
  pages={259-264},
  author={Sergio, Marbilia Possagnolo and Costa, Talita de Souza and Pessoa, Marcelo S de Paula and Pedro, Paulo S M},
  keywords={Software, Systematics, Tools, Databases, Semantics, Resource management, Bibliographical Review, LDA, Probabilistic logic, Semantic Analysis},
  abstract={A large amount of scientific information is found in digital databases. This fact turns the search for the state of the art an increasingly challenging task. This work has the goal of developing a technological solution to support researchers in bibliographic review using the Latent Dirichlet Allocation. The Sw3T software, developed in Python, analyses the abstracts in scientific publications that interest the researcher. Through the use of semantic analysis, Sw3t supplies themes, which are defined by a group of terms. Themes can subsidize the researcher in the process of identification of complementary terms through the restriction on the number of publications of interest and it contributes to the analysis of the publications selected by their classification by theme as well. Sw3t has shown to be efficient and increased reliability in the search process regarding complementary terms, as well it contributes to the promptness of the detailed analysis of the publications. The results of the bibliographical review, regarding the application of Sw3T, have shown to be consistent and replicable. Future work will address the use of Sw3T for other bibliographic reviews as well as improvements and broadening of its functionalities such as search in DDB automation and previous content processing in abstracts.}
}

@article{rayyan-727968124,
  title={Database-access performance antipatterns in database-backed web applications},
  year={2020},
  pages={58-69},
  author={Shao, Shudi and Qiu, Zhengyi and Yu, Xiao and Yang, Wei and Jin, Guoliang and Xie, Tao and Wu, Xintao},
  keywords={Tools, Software maintenance, Databases, Conferences, Computer bugs, Focusing, characteristic study, database-backed web applications, performance antipatterns, performance bugs, Back},
  abstract={Database-backed web applications are prone to performance bugs related to database accesses. While much work has been conducted on database-access antipatterns with some recent work focusing on performance impact, there still lacks a comprehensive view of database-access performance antipatterns in database-backed web applications. To date, no existing work systematically reports known antipatterns in the literature, and no existing work has studied database-access performance bugs in major types of web applications that access databases differently.To address this issue, we first summarize all known database-access performance antipatterns found through our literature survey, and we report all of them in this paper. We further collect database-access performance bugs from web applications that access databases through language-provided SQL interfaces, which have been largely ignored by recent work, to check how extensively the known antipatterns can cover these bugs. For bugs not covered by the known antipatterns, we extract new database-access performance antipatterns based on real-world performance bugs from such web applications. Our study in total reports 24 known and 10 new database-access performance antipatterns. Our results can guide future work to develop effective tool support for different types of web applications.}
}

@article{rayyan-727968125,
  title={A literature review on mining cyberthreat intelligence from unstructured texts},
  year={2020},
  pages={516-525},
  author={Rahman, Md Rayhanur and Mahdavi-Hezaveh, Rezvan and Williams, Laurie},
  keywords={Bibliographies, Data mining, Natural language processing, Literature Review, Cyber threat intelligence, Semantics, Social networking (online), Computer hacking, Feeds, Intelligence},
  abstract={Cyberthreat defense mechanisms have become more proactive these days, and thus leading to the increasing incorporation of cyberthreat intelligence (CTI). Cybersecurity researchers and vendors are powering the CTI with large volumes of unstructured textual data containing information on threat events, threat techniques, and tactics. Hence, extracting cyberthreat-relevant information through text mining is an effective way to obtain actionable CTI to thwart cyberattacks. The goal of this research is to aid cybersecurity researchers understand the source, purpose, and approaches for mining cyberthreat intelligence from unstructured text through a literature review of peer-reviewed studies on this topic. We perform a literature review to identify and analyze existing research on mining CTI. By using search queries in the bibliographic databases, 28,484 articles are found. From those, 38 studies are identified through the filtering criteria which include removing duplicates, non-English, non-peer-reviewed articles, and articles not about mining CTI. We find that the most prominent sources of unstructured threat data are the threat reports, Twitter feeds, and posts from hackers and security experts. We also observe that security researchers mined CTI from unstructured sources to extract Indicator of Compromise (IoC), threat-related topic, and event detection. Finally, natural language processing (NLP) based approaches: topic classification; keyword identification; and semantic relationship extraction among the keywords are mostly availed in the selected studies to mine CTI information from unstructured threat sources.}
}

@article{rayyan-727968126,
  title={No title},
  year={2019},
  journal={Qualitative and critical research in information systems and human-computer interaction: Divergent and convergent paths},
  issn={978-1-68083-557-1},
  author={Wynn, Eleanor and Hult, Helena Vallo},
  url={https://ieeexplore.ieee.org/document/8752265},
  publisher={now},
  abstract={Qualitative and Critical Research in Information Systems and Human Computer Interaction explores the history and adoption of qualitative and critical research in Information Systems (IS) and contrasts it with the growth of similar methods/theories in Human Computer Interaction (HCI) and, to a lesser, extent Computer Supported Collaborative Work (CSCW). The supposition behind the comparison was that the areas overlap in subject matter and would overlap in methods and authors. However, marked differences were observed in the structure of publications, conferences, and on social media that led to questions about the extent to which the fields shared a common framework. The authors find that the history of each discipline reflects institutional factors that affected the respective timelines for the use of these approaches. This leads them to consider a sociological epistemic framework, which explains the differences quite well. It also supports characterizations of the culture of IS made by members, as having open paradigm and high collegiality, described as an adhocracy. The authors propose that qualitative and critical research developed interdependently in IS. Aside from institutional factors, a further difference in uptake of methods and critical framework comes from the US/Europe divide in research traditions and the political/epistemic climates affecting research in the respective regions. Research from beyond the transatlantic traditions postdates the developments covered here but is touched on at the end of the monograph. The primary goal of Qualitative and Critical Research in Information Systems and Human Computer Interaction is to better understand the ways the IS research community differentiates itself into diverse constituencies, and how these constituencies interact in the field's complex processes of knowledge creation and dissemination. Another goal is to create cross-disciplinary discussion and build on related work in the fields. This is important in the era of platforms with global reach, and the concurrent development of powerful AI and analytics capabilities that both intrude on daily life and try to emulate human intelligence.}
}

@article{rayyan-727968127,
  title={Advances in the application of Ontologies in the area of Digital Forensic Electronic Mail},
  year={2019},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={17},
  number={10},
  pages={1694-1705},
  author={Parra, Beatriz and Vegetti, Marcela and Leone, Horacio},
  keywords={Ontology, Ontologies, Traceability, Electronic mail, Digital forensics, Digital Forensic, Email},
  abstract={This article presents a descriptive review of the research published in the last five years to identify areas of unavailability in the study of Digital Forensics problems. Particularly, it is important to define the state of the art related to the application of ontologies, especially in the forensics of emails. The following objectives of the review are proposed: identify and study the most up-to-date research contributions on Ontologies and Digital Forensics; establish the gaps in current research related with the application of Ontologies to Digital Forensics; and correlate these works from attributes of proximity (or distance) with the application of ontologies to the forensic analysis of emails. In addition, a systematic method is defined to select the research works that are considered of interest for this review. It is expected that it will lead to the identification of gaps in the investigation of characteristic problems in digital forensic analysis, and the definition of an updated theoretical framework linked to the forensic analysis of emails with the application of ontologies.}
}

@article{rayyan-727968128,
  title={Regression test selection and product line system testing},
  year={2010},
  pages={512-515},
  author={Engström, Emelie},
  keywords={Software testing, Visualization, Programming, literature review, System testing, Large-scale systems, Computer science, Computer industry, Application software, Costs, regression testing, industrial practices, software product line testing, test case selection, test coverage},
  abstract={Context: Software product lines (SPL) are used in industry to achieve more efficient software development. To test a SPL is complex and costly and often becomes a bottleneck in the product line organization. Objective: This research aims to develop and evaluate strategies for improving system test selection in a SPL. Method: Initially industrial practices and research in both SPL testing and traditional regression test selection have been surveyed. Two systematic literature reviews, two industrial exploratory surveys and one industrial evaluation of a pragmatic test selection approach have been conducted. Results: There is a lack of industrial evaluations as well as of useful solutions, both regarding regression test selection and SPL testing. Test selection is an activity of varying scope and preconditions, strongly dependent on the context in which it is applied. Conclusions: Continued research will be done in close cooperation with industry with the goal to define a tool for visualizing system test coverage in a product line and the delta between a product and the covered part of the product line.}
}

@article{rayyan-727968129,
  title={Characterizing educational data mining},
  year={2019},
  pages={1-5},
  author={da Silva, Victor Regis Lyra Beserra and de Albuquerque Silva, Fábio and Burégio, Vanilson},
  keywords={Software, Data mining, Machine learning, Information systems, Market research, Art, Educational data mining, Pattern Recognition},
  abstract={Educational data mining (EDM) is intended to uncover trends and patterns hidden in the thousands of data sets in an educational system and has drawn the attention of academic authorities for being able to bring benefits to educational institutions. This work aims to investigate in more detail the mining of educational data and to suggest a preliminary classification scheme. As a result, we hope to provide an overview of such research area by identifying key topics, types and trends of preliminary research, as well as the maturity of existing contributions. Since the EDM is part of an interdisciplinary area, mobilizing mainly knowledge of statistics, machine learning, pattern recognition, etc., we believe that with the present work one can have a better understanding of the area and its aspects.}
}

@article{rayyan-727968130,
  title={A generic analogy-centered software cost estimation based on differential evolution exploration process},
  year={2019},
  journal={The Computer Journal},
  issn={1460-2067},
  volume={64},
  number={1},
  pages={462-472},
  author={Wani, Zahid Hussain and Bhat, Javaid Iqbal and Giri, Kaisar Javeed},
  keywords={software cost estimation, analogy-based estimation, differential evolution, genetic algorithms, Software},
  abstract={Software cost estimation is the prediction of development effort and calendar time required to develop any software project. It is considered to be the very fundamental task for successful execution of an on-going project as well as budgetary requirements of futuristic projects. As accuracy in software cost estimation is very hard because of the availability of vague information at the time of inception of the software project, it prompted many researchers to explore in this domain from past decades. Their pioneer works suggest a bulk of techniques for this purpose. However, because of the availability of large number of estimation techniques, it becomes hard for any software practitioners to select an appropriate one. To help the industry practitioners in these situations, a novel analogy-centered model based on differential evolution exploration process is proposed in this research study. The proposed model has been assessed on 676 projects from 5 different data sets and the results achieved are significantly better when compared with other benchmark analogy-based estimation studies. Furthermore, being the very less computational cost of the proposed model, it is suggested that the proposed model be considered as the preliminary stage of any analogy-based software estimation technique.}
}

@article{rayyan-727968131,
  title={Towards a software sustainability-quality model: Insights from a multi-case study},
  year={2019},
  pages={1-11},
  author={Condori-Fernandez, Nelly and Lago, Patricia},
  keywords={Software quality, Interviews, Software systems, Companies, sustainability, Sustainable development, Economics, Customer relationship management, multiple case study, Software},
  abstract={Background. Software sustainability is defined in terms of multiple and interdependent dimensions (economic, social, technical and environmental). Preliminary initiatives have investigated the contribution of certain quality attributes to sustainability dimensions. Problem. Despite these valuable efforts, the characterization of software sustainability is still a key challenge. This entails how sustainability can be embraced in the design of software systems by identifying the relevant software quality attributes (QAs) and their dependencies. Both attributes and dependencies vary heavily with amongst others the type of software system and its operational context. Aim and Method. We followed a multiple case study research method with the main objective of investigating the applicability of the sustainability model in different contexts. We aim also to enrich our model, by means of identifying missing quality attributes or new contributions to the sustainability dimensions. We selected two software projects as cases of our study, where each one was independently conducted in specific situations. Results. The results of the study show that the relevant quality requirements identified in both projects (cases) are covered by most of the QAs related to the social (82%) and technical (83%) dimensions. Moreover, some QAs that were not addressed in the corresponding projects, their relevance like context completeness, and flexibility were acknowledged. These results suggest that the software sustainability model could support the identification of relevant QAs. The case study also contributed to identify QAs that had not been considered in the economic, technical and social dimensions of the sustainability-quality model.}
}

@article{rayyan-727968132,
  title={Adaptive 3D virtual learning Environments—A review of the literature},
  year={2017},
  journal={IEEE Transactions on Learning Technologies},
  issn={1939-1382},
  volume={10},
  number={3},
  pages={262-276},
  author={Scott, Ezequiel and Soria, Alvaro and Campo, Marcelo},
  keywords={Context, Adaptation models, Data models, Three-dimensional displays, Two dimensional displays, Adaptive systems, 3D virtual learning environments, Adaptive virtual environments, personalized virtual environments, Solid modeling},
  abstract={New ways of learning have emerged in the last years by using computers in education. For instance, many Virtual Learning Environments have been widely adopted by educators, obtaining promising outcomes. Recently, these environments have evolved into more advanced ones using 3D technologies and taking into account the individual learner needs and preferences. This focus has led a shift to more personalized learning approaches, requiring that the environments adapt themselves to the learner. Then, many adaptive 3D environments have explored adaptive features to create new and enhanced learning experiences in different contexts. However, very little is known about both what factors are involved with adaptive 3D environments to achieve learning benefits and what assessment factors are present in current studies. For this reason, this review analyzes the recent publications on Adaptive 3D Virtual Learning Environments. Findings have revealed that these environments have covered factors on defining the learner's model, the instructional strategies and contents, and the adaptations mechanisms. Nearly half of the environments have addressed thorough assessments whereas the rest has not reported any evaluation at all. Moreover, when they report assessment, promising outcomes have also been shown not only in multiple domains of knowledge but also at various stages of education. These findings indicate that the field of Adaptive 3D Virtual Learning Environments is an active and ongoing area, and this study highlights several promising directions and suggestions for future research.}
}

@article{rayyan-727968133,
  title={Requirements of software visualization tools: A literature survey},
  year={2007},
  pages={2-9},
  author={Kienle, Holger M and Muller, Hausi A},
  keywords={Visualization, Software quality, Software maintenance, Usability, Scalability, Software tools, Reverse engineering, Information filters, Conference proceedings, Information filtering, Software},
  abstract={Our objective is to identify requirements (i.e., quality attributes and functional requirements) for software visualization tools. We especially focus on requirements for research tools that target the domains of visualization for software maintenance, reengineering, and reverse engineering. The requirements are identified with a comprehensive literature survey based on relevant publications in journals, conference proceedings, and theses. The literature survey has identified seven quality attributes (i.e., rendering scalability, information scalability, interoperability, customizability, interactivity, usability, and adoptability) and seven functional requirements (i.e., views, abstraction, search, filters, code proximity, automatic layouts, and undo/history). The identified requirements are useful for researchers in the software visualization field to build and evaluate tools, and to reason about the domain of software visualization.}
}

@article{rayyan-727968134,
  title={A systematic mapping study of the empirical MOOC literature},
  year={2019},
  journal={IEEE Access},
  issn={2169-3536},
  volume={7},
  pages={124809-124827},
  author={Rasheed, Rasheed Abubakar and Kamsin, Amirrudin and Abdullah, Nor Aniza and Zakari, Abubakar and Haruna, Khalid},
  keywords={Systematics, systematic mapping study, Computer science, Information technology, Market research, Focusing, Electronic learning, MOOC, distance education, E-learning, massive open online course},
  abstract={Massive open online courses (MOOCs) have revolutionized todays education by offering a global accessible form of online learning. Over the years, MOOCs have been an attractive research area and have yielded an ample amount of research publications. However, the existing review studies in MOOCs are characterized by short year coverage or focusing on a specific theme. As such, a systematic mapping methodology was adopted to provide a fine-grain overview of MOOC research domain by identifying the quantity, types of research, available results and publication trends in educational aspects of MOOCs from 2009 to 2018. Key findings show that I) MOOC research have been on the rise since MOOCs became mainstream in 2011. II) MOOC research largely resides in the United States and few European countries. II) Most of MOOC studies focused on addressing learners' completion/dropout/retention. In addition, we proposed some recommendations for future research on MOOCs.}
}

@article{rayyan-727968135,
  title={Enhancing privacy in robotics via judicious sensor selection},
  year={2020},
  pages={7156-7165},
  author={Eick, Stephen and Antón, Annie I},
  keywords={Task analysis, Data privacy, Privacy, Law, robotics, privacy, Cameras, compliance, privacy by design, privacy impact assessments, robot design, Robot sensing systems, sensor selection, Robotics},
  abstract={Roboticists are grappling with how to address privacy in robot design at a time when regulatory frameworks around the world increasingly require systems to be engineered to preserve and protect privacy. This paper surveys the top robotics journals and conferences over the past four decades to identify contributions with respect to privacy in robot design. Our survey revealed that less than half of one percent of the ∼89,120 papers in our study even mention the word privacy. Herein, we propose privacy preserving approaches for roboticists to employ in robot design, including, assessing a robot's purpose and environment; ensuring privacy by design by selecting sensors that do not collect information that is not essential to the core objectives of that robot; embracing both privacy and performance as fundamental design challenges to be addressed early in the robot lifecycle; and performing privacy impact assessments.}
}

@article{rayyan-727968136,
  title={RAModel: A reference model for reference architectures},
  year={2012},
  pages={297-301},
  author={Nakagawa, Elisa Yumi and Oquendo, Flavio and Becker, Martin},
  keywords={Software architecture, Reference architecture, Architecture, Business, Software systems, Computer architecture, Standards, reference architecture model, reference model},
  abstract={Reference architectures have emerged as a special type of software architecture that achieves well-recognized understanding of specific domains, promoting reuse of design expertise and facilitating the development, standardization, and evolution of software systems. Designed for various domains and purpose, they have increasingly impacted important aspects of system development, such as productivity and quality of such systems. However, reference architectures have been sometimes established without an adequate concern about which elements they should encompass. Besides that, there is a lack of work that investigate the essence of reference architectures, their dimensions and elements that they should contain. In this perspective, the main contribution of this paper is to present a reference model for reference architectures, named RAModel (Reference Architecture Model), that intends to improve the understanding about what reference architectures are, as well as their components and relationships, supporting the establishment, use, and evolution of such architectures.}
}

@article{rayyan-727968137,
  title={Challenges and solutions of project management in distributed software development},
  year={2019},
  pages={1-10},
  author={Moreno, Wallace and Afonso, Paulo and Costa, Heitor},
  keywords={Distributed Software Development, Software Project Management, Global Software Development, Software},
  abstract={Challenge of developing software has increased because of several factors, e.g. complexity of these systems and lack of skilled available professional next of development companies. Thus, they should seek alternatives; one of them is global software development. Therefore, software project management should adapt to this reality and resolve challenges not previously found in the “traditional” management. In this paper, we present an initial list of challenges in project management in the context of global software development and solutions proposed by researchers and project managers to try to solve these challenges. We used Literature Systematic Mapping for finding challenges and solutions for software project management. We found 29 papers and used 20 initial papers, totalizing 49 papers. The findings showed 18 challenges which were listed with its solutions.}
}

@article{rayyan-727968138,
  title={Supportive metrics to estimate the effort to develop Business Intelligence system},
  year={2016},
  pages={1-6},
  author={Endo, Luciano and Mendes, Fabiana Freitas and Dias Canedo, Edna},
  keywords={Software, Monitoring, Measurement, Metrics, Business, Complexity theory, Process control, Biological system modeling, BI System Development, Business Intelligence, Business Process Complexity, Metronidazole, Intelligence},
  abstract={Goal: This work aims to identify metrics to estimate the effort to develop Business Intelligence (BI) system. Methodology: It was conducted a secondary study in order to identify metrics related to BI and to business process complexity. The results conducted to a hypothesis that relates business process complexity to effort required to implement it on BI System. This hypothesis was developed on a case study. Results: The hypothesis was verified and seems to be true, but more studies are required to confirm the relation. It was also proposed a method that explore this relation. By using the method it is possible to estimate the effort to implement BI System. Conclusions: This work emphasizes the importance of measuring BI Systems and it is an information source to academics and BI system developers that want to increase their effort estimative precision.}
}

@article{rayyan-727968139,
  title={Blockchain-based solutions for IoT: A tertiary study},
  year={2020},
  pages={124-131},
  author={Xu, Qianwen and Chen, Xiudi and Li, Shanshan and Zhang, He and Babar, Muhammad Ali and Tran, Nguyen Khoi},
  keywords={Data mining, Internet of Things, Blockchain, Software quality, IoT, Tertiary Study, Focusing, Software reliability, Supply chains},
  abstract={The combination of blockchain and Internet of Things (IoT) brings revolutionary changes to Industry 4.0. However, blockchain technology is immature now and the combination is under exploration. To understand the research status of applying blockchain-based solutions for IoT, we conduct a tertiary study that systematically reviews secondary studies in this domain. Through the processes of automated search and selection, this study obtains 11 secondary studies published between 2016 and 2020. Based on the rigorous data extraction and analysis, we have established a comprehensive overview of the current state, concerns, and critical findings of relevant studies focusing on blockchain-based solutions for IoT. Results involve typical challenges and characteristics of blockchain itself, the constrained nature of IoT, and specific domains that blockchain-based solutions are commonly applied to. Three directions are also highlighted for future research, i.e. addressing challenging quality attributes, consensus optimization, and integrating blockchain-based IoT with other new technologies.}
}

@article{rayyan-727968140,
  title={GiveMe trace: A software evolution traceability support tool},
  year={2016},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={14},
  number={7},
  pages={3444-3454},
  author={Silveira Lelis, Claudio Augusto and Fernandes Tavares, Jacimar and Pereira Araujo, Marco Antonio and Nazar David, Jose Maria},
  keywords={Software, Visualization, Manuals, Context, Configuration management, Statistics, Sociology, Software Evolution, Change Management, Software Repository, Software Traceability, Software Visualization},
  abstract={Traceability is a key factor in the analysis of the changes that software undergoes throughout its evolution. The main purpose of analysis is to minimize the side effects of these changes and, when it is made to the source at a lower level of abstraction (methods) and in an integrated manner, it can provide more accurate data in order to support decision making. This article presents the GiveMe Trace tool, integrated with a multiple view interactive environment that, among other features, can generate information about the traceability between source code and artifacts its different versions. This information is based on software versions analysis from software repository. As a result, occurrences of changes in classes or methods are shown. A proof of concept was carried out through which repositories versions of two distinct real projects were analyzed. At the end, it was possible to obtain evidences on the feasibility of the use of GiveMe Trace to support traceability between the source code and versions.}
}

@article{rayyan-727968141,
  title={A systematic mapping study on the verification of cyber-physical systems},
  year={2018},
  journal={IEEE Access},
  issn={2169-3536},
  volume={6},
  pages={59043-59064},
  author={Duan, Pengfei and Zhou, Ying and Gong, Xufang and Li, Bixin},
  keywords={Systematic mapping study, Systematics, Guidelines, Tools, Cyber-physical systems, Databases, Statistics, Sociology, abstraction methods, assistance tools, verification challenges, verification of cyber-physical system, verification scenarios, verification techniques},
  abstract={Cyber-physical system (CPS) is a kind of complex real-time hybrid system which involves deep interactions between computation processors, communication network, and physical environments are deemed as the key enablers of next generation computer applications. However, how to verify CPS effectively is always a great challenge. Based on current scientific works about CPS verification, this paper aims at identifying the gap of current studies and suggesting promising areas for the future works. For this purpose, we conduct a systematic mapping study over the topic on verification of cyber-physical system. We carry out a widely search of publications from 2006 to 2018 in 11 electronic databases. After the step of study selection, 80 papers are selected as primary studies for answering proposed research questions, focused questions, and statistical questions. According to these questions and their answers, this paper not only presents a quantitative and comprehensive analysis of verification challenges, abstraction methods, verification techniques, assistance tools, and verification scenarios that represent each step of verification works, but also summarizes CPS systematic natures, main routine of verification and future research directions. We believe that this survey can identify gaps in current research works and reveal new insights for the future works.}
}

@article{rayyan-727968142,
  title={Sarasvati: Diagnostic method for software process improvement},
  year={2017},
  pages={1-6},
  author={da Silva, Marcelo Pereira and Brancher, Jacques Duílio},
  keywords={Software, Libraries, Companies, ISO Standards, IEC Standards, Software Quality, Capability maturity model, Diagnosis, Improvement of Software Process, Radar, Reference Models},
  abstract={The growing demand for software brings companies in this sector the need to constantly improve their processes. However, not all companies obtained the expected return from Process Improvement, and one of the causes of this problem is the diagnosis that precedes the implementations of reference models. This article presents the Sarasvati method of diagnosis, whose objective is to provide information for the selection of the model / level of maturity that best meets the objectives of the company. A tool was created to get company data and turn them into a map that shows your situation. Finally, this map is compared to the maps of the reference models to show the improvement that each one can provide to the company. The method was evaluated in two ways: in the first stage, the methodology was evaluated by four experts in the subject. Then, the method was applied in a set of companies. The result of the evaluation was considered satisfactory, concluding that there are indications that the method is a viable option to support companies in the diagnosis for Process Improvement projects.}
}

@article{rayyan-727968143,
  title={Parallel programming in computing undergraduate courses: a systematic mapping of the literature},
  year={2019},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={17},
  number={8},
  pages={1371-1381},
  author={Lara Soares, Felipe Augusto and Neri Nobre, Cristiane and de Freitas, Henrique},
  keywords={Learning, IEEE transactions, Parallel programming, Computer aided instruction, Distributed Computing, Instruction sets, Parallel processing, Parallel Programming, Teaching, Undergraduate, Software},
  abstract={Due to the current scenario in which multi-core architectures are predominant in most personal computers and servers, the knowledge of parallel programming content becomes fundamental for computer students to develop software capable of obtaining the best performance of these architectures. Considering the importance of this context, this paper presents the results of a systematic mapping of the literature related to the teaching-learning process of parallel programming in the computing programmes in three important databases: ACM, IEEE and Science Direct. The results obtained showed that in order to solve the challenges and differences found in teaching-learning parallel programming, reorganization is necessary in the undergraduate programmes. A standard for parallel programming teaching is important. This can be established by defining where and how to insert parallelism in the courses, adopting a methodology to teach the contents of parallelism in several different courses, beginning the study in the first year. The main languages, libraries, difficulties encountered and methods of classroom and distance teaching for parallel programming are presented in this paper. Distance learning is still little explored in this area of knowledge, but it can support the teaching and study of these contents.}
}

@article{rayyan-727968144,
  title={The MSR cookbook: Mining a decade of research},
  year={2013},
  pages={343-352},
  author={Hemmati, Hadi and Nadi, Sarah and Baysal, Olga and Kononenko, Oleksii and Wang, Wei and Holmes, Reid and Godfrey, Michael W},
  keywords={Software, Data mining, Context, Best practices, Electronic mail, Communities, Data acquisition},
  abstract={The Mining Software Repositories (MSR) research community has grown significantly since the first MSR workshop was held in 2004. As the community continues to broaden its scope and deepens its expertise, it is worthwhile to reflect on the best practices that our community has developed over the past decade of research. We identify these best practices by surveying past MSR conferences and workshops. To that end, we review all 117 full papers published in the MSR proceedings between 2004 and 2012. We extract 268 comments from these papers, and categorize them using a grounded theory methodology. From this evaluation, four high-level themes were identified: data acquisition and preparation, synthesis, analysis, and sharing/replication. Within each theme we identify several common recommendations, and also examine how these recommendations have evolved over the past decade. In an effort to make this survey a living artifact, we also provide a public forum that contains the extracted recommendations in the hopes that the MSR community can engage in a continuing discussion on our evolving best practices.}
}

@article{rayyan-727968145,
  title={The intensity of the research activities on e learning for care givers of autistic children},
  year={2015},
  pages={1-7},
  author={Shminan, Ahmad Sofian and Fauzan, Norsiah and Aren, Merikan},
  keywords={Systematics, Databases, Autism, telehealth, Training, Sections, Electronic learning, Tutorials, autism intervention, e learning},
  abstract={Autism, known as a “spectrum disorder” (ASD) is seen in early childhood or by three years of age. ASD is a neuro-developmental disorder characterized by deficits in social responsiveness, impairments in verbal and nonverbal communication. The purpose of this paper is to demonstrate the relevance of e-learning technology to the area of training the caregivers of autistic children. Our search focused on a number of data banks that contain numerous references to autism and home based treatment. Our results found only ten papers published since 2010 that met our criteria for inclusion. Six were demonstrations of e learning to teach caregivers and professionals the basics of applied behavior analysis and some techniques for skill training. The remainder of the studies fell into the telehealth category which involved direct communication between a professional and caregiver in the home. The studies suggest that home based service delivery is effective and offers both the social service system and parents considerable financial savings. The use of Web based platform (e learning and telehealth) is depicted as an aid to caregivers of autistic children. The need for more studies of the variables related to home based service delivery is noted.}
}

@article{rayyan-727968146,
  title={Addressing legal requirements in requirements engineering},
  year={2007},
  pages={5-14},
  author={Otto, Paul N and Anton, Annie I},
  keywords={Monitoring, Computer science, Information security, Software systems, Law, Systems engineering and theory, Government, Legal factors, Legislation, Logic programming},
  abstract={Legal texts, such as regulations and legislation, are playing an increasingly important role in requirements engineering and system development. Monitoring systems for requirements and policy compliance has been recognized in the requirements engineering community as a key area for research. Similarly, regulatory compliance is critical in systems that are governed by regulations and law, especially given that non-compliance can result in both financial and criminal penalties. Working with legal texts can be very challenging, however, because they contain numerous ambiguities, cross-references, domain-specific definitions, and acronyms, and are frequently amended via new regulations and case law. Requirements engineers and compliance auditors must be able to identify relevant regulations, extract requirements and other key concepts, and monitor compliance throughout the software lifecycle. This paper surveys research efforts over the past 50 years in handling legal texts for systems development. These efforts include the use of symbolic logic, logic programming, first-order temporal logic, deontic logic, defeasible logic, goal modeling, and semi-structured representations. This survey can aid requirements engineers and auditors to better specify, monitor, and test software systems for compliance.}
}

@article{rayyan-727968147,
  title={No title},
  year={2018},
  journal={Enterprise personal analytics: The next frontier in individual information systems research},
  issn={978-1-68083-461-1},
  author={Clohessy, Trevor and Acton, Thomas and Whelan, Eoin and Golden, Willie},
  url={https://ieeexplore.ieee.org/document/8428806},
  publisher={now},
  abstract={Enterprise Personal Analytics: The Next Frontier in Individual Information Systems Research examines an emergent category of personal analytics – enterprise personal analytics – that encompasses the concept of organizations enabling their employees to use their individual analytics to manage their digital working lives from descriptive, diagnostic, predictive and prescriptive points of view. The monograph is structured as follows: First, the individuation of IS is examined. Second, the methodology is explained. Third, an overview of the recent personal analytical trends is provided. Fourth, an EPA research framework comprising specific perspectives with regards to stakeholders and concerns is delineated. Finally, the monograph concludes with a discussion pertaining to theoretical and practical implications and limitations.}
}

@article{rayyan-727968148,
  title={Evaluation of requirements models},
  year={2016},
  pages={432-437},
  author={Gralha, Catarina},
  keywords={Measurement, Unified modeling language, metrics, Analytical models, Stakeholders, Biological system modeling, quality evaluation, biometrics, Brain models, requirements models},
  abstract={Requirements Engineering (RE) approaches, following paradigms such as goal-oriented [1] or scenario-based [2], provide expressive model elements for requirements elicitation and analysis. However, these approaches are still struggling when it comes to managing the quality of their models. Problems in quality can cause difficulties in anaging and understanding requirements, which in turn leads to increased development costs. The models' quality should then be a permanent concern. We propose a quantitative assessment of the goal-oriented and scenario-based models' quality, namely its complexity, completeness, appropriateness recognizability, understandability and learnability. To this end, we propose a combination of techniques to be applied to the RE models, the modelling process, and the models' notation. We are going to define metrics about the models, through the Goal-Question-Metric (GQM) approach, and incorporate them in a common evaluation framework that helps in the requirements modelling process. The quality of the RE models, the modelling process and the model's notation will be measured by collecting biometric data from stakeholders, by using eye-tracking devices, electroencephalography (EEG) scanners, and electro-dermal activity (EDA) scanners. Furthermore, we will collect metrics about the model during the modelling process, and the subjective opinion of stakeholders about the usage of these models, through questionnaires like NASA TLX [4] (which measures perceived effort while working on tasks). All metrics and biometrics are going to be theoretically and experimentally evaluated, through a set of case studies and experiments with different types of participants (including researchers, practitioners and students).}
}

@article{rayyan-727968149,
  title={Towards implementation of process and product quality assurance process area for saudi arabian small and medium sized software development organizations},
  year={2018},
  journal={IEEE Access},
  issn={2169-3536},
  volume={6},
  pages={41643-41675},
  author={Keshta, Ismail and Niazi, Mahmood and Alshayeb, Mohammad},
  keywords={Software, Quality assessment, Companies, Standards organizations, quality assurance, Capability maturity model, Product design, capability maturity model integration (CMMI), small- and medium-sized software development organizations, Software process improvement (SPI)},
  abstract={There is a significant need to give more careful consideration to the process and product quality assurance (PPQA) process area of the capability maturity model integration (CMMI) Level 2, especially in the context of smalland medium-sized software development organizations, to help such organizations achieve CMMI Level 2. The objective of this paper is to report the implementation of the PPQA process area for smalland medium-sized software development organizations in Saudi Arabia. An abstract-level model for each specific practice of the PPQA process area has been developed. In addition, an initial evaluation of the proposed models has been discussed. Data have been collected by exploring published research articles and high-level software process descriptions. Moreover, previous research works that dealt with the implementation of CMMI Level 2 process areas have been reviewed. Furthermore, research articles that provide guidance to software development organizations for implementing the process areas of CMMI Level 2 in their environments have been considered. The evaluation of the proposed models was also carried out using an expert panel review process and one case study. After careful analysis of the collected data, we have proposed a model for each specific practice in the PPQA process area. Each model is divided into core stages, and different activities associated with each stage are clearly indicated. Based on the evaluation, we are confident that our proposed models are clear, easy to use and learn, and can be applied to smalland medium-sized software development organizations in Saudi Arabia. In addition, the evaluation results show that the proposed models can help such organizations in implementing the PPQA process area according to the CMMI Level 2 maturity requirement.}
}

@article{rayyan-727968150,
  title={Factors of influence in software process improvement: An ISO/IEC 29110 for very-small entities},
  year={2015},
  pages={12-17},
  author={Wongsai, Noppachai and Siddoo, Veeraporn and Wetprasit, Rattana},
  keywords={Software, barriers, Companies, Standards organizations, ISO Standards, IEC Standards, ISO/IEC 29110, critical success factors, software process improvement (SPI), very-small entity (VSE), Fibrinogen},
  abstract={The recently introduced ISO/IEC 29110 standard Lifecycle profile for Very Small Entities (VSE) has been adopted and practiced in many small and medium software companies, including in Thailand's software industry. Many Thai companies complete their software process improvement (SPI) initiative program and have been certified. There are, however, a number of participants who fail to succeed. This study was concerned with the factors that influence the accomplishment of the standard implementation in various VSE characteristics. In order to achieve this goal, surveying and extracting critical factors from prior studies were carried out and then the surveyed factors were evaluated by the experts' opinion. The analysis of comments and recommendations from the experts was performed using a qualitative content analysis method. This paper presents the initial set of influence factors on both positive and negative impacts of ISO/IEC 29110 implementation. The aim was to help such SPI practitioners with some considerations to manage their approach of adoption appropriately.}
}

@article{rayyan-727968151,
  title={A systematic mapping study of quality assessment models for software products},
  year={2017},
  pages={63-71},
  author={Yan, Meng and Xia, Xin and Zhang, Xiaohong and Xu, Ling and Yang, Dan},
  keywords={Systematic mapping study, Measurement, Software quality, Databases, Predictive models, Q-factor, Quadrature amplitude modulation, Quality assessment model, Software},
  abstract={Quality model is regarded as a well-accepted approach for assessing, managing and improving software product quality. There are three categories of quality models for software products, i.e., definition model, assessment model, and prediction model. Quality assessment model (QAM) is a metric-based approach to assess the software quality. It is typically regarded as of high importance for its clear method on how to assess a system. However, the current state-of-the-art in QAM research is under limited investigation. To address this gap, the paper provides an organized and synthesized summary of the current QAMs. In detail, we conduct a systematic mapping study (SMS) for structuring the relevant articles. We obtain a total of 716 papers from the five databases, and 31 papers are selected as relevant studies at last. In summary, our work focuses on QAMs from the following aspects: software metrics, quality factors, evaluation methods and tool support.}
}

@article{rayyan-727968152,
  title={Reference architecture for integration platforms},
  year={2017},
  pages={113-122},
  author={Singh, Prince Mayurank and Van Sinderen, Marten and Wieringa, Roel},
  keywords={Software, Systematics, Integration, Interoperability, Logistics, Best practices, Computer architecture, IP networks, Integration Platform, Reference Architetcure},
  abstract={In addition to in-house applications, networked enterprises are increasingly using data and services from various external sources. Conversion of data to useful information and IT alignment with business goals are big challenges faced by these enterprises. Integration platforms (IPs) aid enterprises in solving such challenges. However, the large number of commercial and academic IPs currently available have created a new problem for enterprises, namely whether to build their own IP or buy/rent a existing IP. Also, how to choose from the plethora of different design/solution options that are available? This paper presents a study and analysis of 31 IPs to bring out best practices in IP design. Following a commonality analysis of IPs from different research domains, an IP reference architecture is proposed. The reference architecture will aid enterprises in making better IP design/solution choices. It can also contribute to IP research by acting as a common reference point for future IP analysis.}
}

@article{rayyan-727968153,
  title={Maker in electrical engineering education based on emergent technology: Mapping study},
  year={2019},
  journal={IEEE Revista Iberoamericana de Tecnologias del Aprendizaje},
  issn={1932-8540},
  volume={14},
  number={4},
  pages={135-144},
  author={Martinez-Lopez, Ruth},
  keywords={technology, reviews, Computer aided instruction, Educational courses, educational technology, Electrical engineering, Electrical engineering computing, Electrical engineering education},
  abstract={“Hands-on activities” with emergent technology is being integrated into the curriculum of Electrical Engineering education. This mapping study carry out an overview of the evidence of the Making movement using emergent technology in Electrical Engineering education. Primary studies in the literature were selected, classified and analysed according to the type of technology used and, the design-based research (DBR) intervention. 20 primary studies were identified. Making related to activities with Robotics is the application most used at the undergraduate level. The most of the studies reported an intervention focused on the instructional approach of the curriculum unit and, that students improved motivation.}
}

@article{rayyan-727968154,
  title={Practice: Risk assessment in globally distributed projects},
  year={2012},
  journal={Global software and IT: A guide to distributed development, projects, and outsourcing},
  issn={978-1-118-13507-5},
  pages={179-187},
  author={Ebert, Christof},
  url={https://ieeexplore.ieee.org/document/6382088},
  publisher={IEEE},
  keywords={Software, Tools, Project management, Productivity, Risk management, Computational modeling, Schedules, Risk Assessment},
  abstract={This chapter contains sections titled: Background Results Take-Away Tips ]]¿}
}

@article{rayyan-727968155,
  title={Agile testing: A systematic mapping across three conferences: Understanding agile testing in the XP/Agile universe, agile, and XP conferences},
  year={2013},
  pages={32-41},
  author={Hellmann, Theodore D and Chokshi, Apoorve and Abad, Zahra Shakeri Hossein and Pratte, Sydney and Maurer, Frank},
  keywords={Software, Systematics, systematic mapping, Testing, Context, software testing, empirical, agile software development, Abstracts, Graphical user interfaces, Electric breakdown, test-driven development, testing tools},
  abstract={Unit and acceptance testing are central to agile software development, but is that all there is to agile testing? We build on previous work to provide a systematic mapping of agile testing publications at major agile conferences. The analysis presented in this paper allows us to answer research questions like: what is agile testing used for, what types of studies on agile testing have been published, what problems do people have when performing agile testing, and what benefits do these publications offer? We additionally explore topics such as: who are the major authors in this field, in which countries do these authors work, what tools are mentioned, and is the field driven by academics, practitioners, or collaborations? This paper presents our analysis of these topics in order to better structure future work in the field of agile testing and to provide a better understanding of what this field actually entails.}
}

@article{rayyan-727968156,
  title={Taking goal models downstream: A systematic roadmap},
  year={2014},
  pages={1-12},
  author={Horkoff, Jennifer and Li, Tong and Li, Feng-Lin and Salnitri, Mattia and Cardoso, Evellin and Giorgini, Paolo and Mylopoulos, John and Pimentel, João},
  keywords={Software, Systematics, systematic literature survey, Taxonomy, Unified modeling language, Market research, requirements engineering, model transformation, Syntactics, evidence-based requirements engineering, systematic literature map, goal model, Transforms},
  abstract={Creating and reasoning with goal models is useful for capturing, understanding, and communicating about requirements in the early stages of information system (re)development. However, the utility of goal models is greatly enhanced when an awareness of system intentions can feed into other stages in the requirements analysis process (e.g. requirements elaboration, validation, planning), and can be used as part of the entire system life cycle (e.g., architecture, process design, coding, testing, monitoring, adaptation, and evolution). In order to understand the progress that has been made in integrating goal models with downstream system development, we ask: what approaches exist which map/integrate/transform goal-oriented languages to other software artifacts or languages? To answer this question, we conduct a systematic survey, producing a roadmap of work summarizing 174 publications. Results include a categorization of the “why?” and “how?” for each approach. Findings show that there are a wide variety of proposals with many proposed sources and targets, covering multiple paradigms, motivated by a variety of purposes. We conclude that although much work has been done in this area, the work is fragmented and is often still in a proposal stage.}
}

@article{rayyan-727968157,
  title={A preliminary fault taxonomy for multi-tenant SaaS systems},
  year={2019},
  pages={178-187},
  author={Santiago C. Pinto, Victor Hugo and Souza, Simone R S and Souza, Paulo S L},
  keywords={Fault taxonomy, Multi tenancy, Software as a Service},
  abstract={Multi-tenancy is the key feature for every Software as a Service (SaaS), as it enables multiple customers, so-called tenants, to transparently share a system's resources reducing costs. Tenants can customize a system according to their particular needs, however, such a high level of complexity may open possibilities for a failure. In addition, there is a lack of a reference architecture for such applications and once the implementations differ significantly, ensuring that all executions flows have been verified without impacting the working features for other tenants is a complex task. The clear understanding of the possible faults is fundamental for the identification, tolerance and definition of appropriate testing techniques. This paper presents a preliminary fault taxonomy for multi-tenant cloud applications considering their foundational features. A literature review previously carried out, a survey with practitioners and analysis of some applications were performed to achieve this classification. In addition, an e-commerce called MtShop was developed for a case study. The expressiveness of the proposed taxonomy is illustrated with critical faults identified in the MtShop through the automated and parallel testing. We conclude with the benefits that our taxonomy can bring to testing, prediction and regression testing activity of multi-tenant cloud applications.}
}

@article{rayyan-727968158,
  title={Mitigating challenges in the elicitation and analysis of transparency requirements},
  year={2019},
  pages={470-475},
  author={Chazette, Larissa},
  keywords={Bibliographies, Requirements engineering, Requirements Elicitation, Privacy, Usability, Software systems, Stakeholders, Requirements Analysis, Non-Functional Requirements, Software Transparency},
  abstract={Software systems are getting more and more complex, with an increasing integration of machine-learning based decisions. The ubiquitous presence of these systems makes users more dependent on them and their correctness in many aspects of daily life. Thus, there is a rising need to make software systems and their decisions more comprehensible. This seems to call for more transparency in software-supported decisions. Therefore, transparency requirements have to be understood, elicited and translated to lower level requirements. However, there is a lack of understanding about the requirements engineering process for transparency and how the different roles, e.g., UX designers, data scientists, and other stakeholders, have to interact in this process. In order to fill this gap, the requirements engineering process for transparency requirements needs to be thoroughly investigated. For this purpose, I intend to conduct empirical studies with practitioners and other stakeholders to gain a deeper understanding of the process and its associated problems. Based on the results, additional research will be conducted to investigate and propose solutions with the purpose of supporting requirements engineering when transparency is required.}
}

@article{rayyan-727968159,
  title={Usability evaluation in Brazil: A systematic mapping},
  year={2014},
  pages={1-7},
  author={Francisco, Lourival and Benitti, Fabiane Barreto Vavassori},
  keywords={Systematics, Guidelines, systematic mapping, Testing, Usability, Google, usability, TV, Brazil},
  abstract={This paper examines the Brazilian research on techniques for usability evaluation with regard to the scope of use. Through a systematic mapping this study identified 116 studies that reported empirical evidence on the different technical and evaluations in different environments. Results indicate that the empirical technique is the most used and the web environment was chosen by 60% of the studies. This study noted that assessments of an educational product are the most applied ones (33 studies). In addition to mapping research in the area, this study also contributes to point out areas for further research.}
}

@article{rayyan-727968160,
  title={Identification and relationship between notation and tool for feature models with graphic representation},
  year={2016},
  pages={1-12},
  author={Vale, Gustavo and Abílio, Ramon and Pereira, Juliana and Figueiredo, Eduardo and Afonso, Paulo and Costa, Heitor},
  keywords={Software, Software product lines, Context modeling, Software product line, Programming, Object oriented modeling, tools, Computational modeling, notations, feature models},
  abstract={Feature models are composed of features and their relationships. They are used to represent the common features and variabilities of a Software Product Line (SPL). Many proposals for notations and tools to feature models can be found in the literature. Thus, the choosing the notation and the tool that best fits the needs of the software engineer is a hard task. This paper shows a study on which tools allow to represent some notations. Fifteen notations were identified and compared, which are related to five tools. The eight attributes of notations related to tools are: graphical structure, type of features (mandatory, optional, and external), type of decomposition (OR, XOR, and AND) and cross restrictions. In the analysis, notations, attributes, and tools were related. As main contribution, we presented a relationship between tools and notations of feature models that can assist in the choice of one notation/tool for modeling a domain.}
}

@article{rayyan-727968161,
  title={Automatic support for multi-domain model management},
  year={2020},
  pages={830-833},
  author={Torres, Weslley and van den Brand, Mark G J and Serebrenik, Alexander},
  keywords={Systematics, Bibliographies, Tools, Software maintenance, Optical character recognition software, Model Management, Systems Engineering, Robots, OCR, Shape},
  abstract={The process of developing complex systems often involves knowledge of engineers from multiple domains: e.g., to develop a robot one needs to combine expertise about mechanics, electronics, and software. Such domain-specific knowledge is often represented in a form of interdependent models, consequently a change in a model of one domain might impact a model from a different domain. Thus, identifying which models are affected due to a change is an important problem, which is further exacerbated due to heterogeneity of modeling notations used. The aim of this PhD research project is to facilitate model management in a multi-domain setting. In the earlier stage of this study, we investigated the available approaches used to manage models from different domains. We concluded that the available approaches are tool-dependent, and do not fully support co-evolution of the models. Additionally, previous research recommends to explicitly indicate the dependency between models in order to support the co-evolution of models from different domains. Since these models are created using different modeling notations we believe that it is not reasonable to develop a tool to parse every notation. Furthermore, it is possible that the source code of the model is missing, but engineers still have an image of the model. Thus, to ensure the maintenance of multi-domain systems we investigated the suitability of optical character recognition (OCR) as a uniform approach. We observed that even though OCR has shortcomings, it produces satisfactory results, and once the identified shortcomings are addressed, OCR can become a crucial technology to support the evolution of multi-domain systems. To this end we envision the development of an infrastructure where we can use OCR to identify relationships between models from different domains, store them in a structured manner making it easier to maintain the consistency of the entire system.}
}

@article{rayyan-727968162,
  title={Data collection for Software Defect Prediction - An exploratory case study of open source software projects},
  year={2015},
  pages={463-469},
  author={Mauša, Goran and Grbac, Tihana Galinac and Bašić, Bojana Dalbelo},
  keywords={Software, Systematics, Software metrics, Data collection, Computer bugs, Joining processes, Data Collection},
  abstract={Software Defect Prediction (SDP) empirical studies are highly biased with the quality of data and widely suffer from limited generalizations. The main reasons are the lack of data and its systematic data collection procedures. Our research aims at producing the first systematically defined data collection procedure for SDP datasets that are obtained by linking separate development repositories. This paper is the first step to achieving that objective, performing an exploratory study. We review the existing literature on approaches and tools used in the collection of SDP datasets, derive a detailed collection procedure and test it in this exploratory study. We quantify the bias that may be caused by the issues we identified and we review 35 tools for software product metrics collection. The most critical issues are many-to-many relation between bug-file links, duplicated bug-file links and the issue of untraceable bugs. Our research provides more detailed, experience based data collection procedure, crucial for further development of SDP body of knowledge. Furthermore, our findings enabled us to develop the automatic data collection tool.}
}

@article{rayyan-727968163,
  title={10 Challenges for the specification of self-adaptive software},
  year={2018},
  pages={1-12},
  author={Muñoz-Fernández, Juan C and Mazo, Raúl and Salinesi, Camille and Tamura, Gabriel},
  keywords={Software, Systematics, Requirements engineering, Uncertainty, requirements engineering, Adaptation models, Runtime, requirements specification, self-adaptive software systems, software requirements, Synthetic aperture sonar},
  abstract={The demand for systems that continue on operation by adapting themselves in response to disturbing changes in their environment has increased in the last decades. Those systems, termed self-adaptive software (SAS) systems, should be developed with techniques and methods appropriated for analysing and designing this kind of systems, starting from the requirements phase. Several contributions propose approaches to improve the specification of requirements for those systems. This paper aims to review the most significant challenges still open in the domains of languages for requirements specification and methods for model verification of self-adaptive systems, independently of their particular application areas. More concretely, the main contribution of this paper is a list of ten challenges to achieve a better-defined specification of requirements for SAS systems, and a more effective verification of such specifications. These challenges are well worthy of being addressed in both communities, the requirements engineering (RE) and the SAS one.}
}

@article{rayyan-727968164,
  title={Evaluating the use of pareto efficiency to optimize non-functional requirements satisfaction in i* modeling},
  year={2016},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={14},
  number={1},
  pages={331-338},
  author={Zubcoff, Jose Jacobo and Garrigos, Irene and Casteleyn, Sven and Mazon, Jose Norberto and Aguilar, Jose Alfonso},
  keywords={Optimization, Context modeling, Reliability, Usability, Interviews, Web Engineering, Analytical models, Adaptation models, goal-oriented requirements, i*, NFRs Optimization, Pareto Efficiency},
  abstract={Due to the large, heterogeneous audience of Web applications, and its rapidly changing expectations, holistic requirement analysis approaches are crucial to ensure the success of Web engineering projects. To increase the quality of resulting Web applications, non-functional requirements (NFRs) must be considered. Satisfying them is a non-trivial task that depends on making decisions about which functional requirements (FRs) to implement, and how to prioritize the NFRs. A satisfactory solution is a trade-off, where competing NFRs must be balanced. In this paper, we outline how the Pareto efficiency can complement a goal-oriented requirement analysis modelling to evaluate and select optimal configurations of requirements for a Web application, while NFRs are balanced and maximized according to a priority list. We hereby focus on an empirical evaluation to verify whether our Pareto method improves the accuracy of design decisions during the requirements analysis phase, and/or if it reduces the time needed by designers.}
}

@article{rayyan-727968165,
  title={Interactive data exploration of distributed raw files: A systematic mapping study},
  year={2019},
  journal={IEEE Access},
  issn={2169-3536},
  volume={7},
  pages={10691-10717},
  author={Alvarez-Ayllon, Alejandro and Palomo-Duarte, Manuel and Dodero, Juan-Manuel},
  keywords={Systematics, Data mining, systematic mapping study, Time factors, Indexing, Distributed databases, Big data applications, data analysis, data engineering, data exploration, Data structures, database systems, interactive systems},
  abstract={When exploring big amounts of data without a clear target, providing an interactive experience becomes really difficult, since this tentative inspection usually defeats any early decision on data structures or indexing strategies. This is also true in the physics domain, specifically in high-energy physics, where the huge volume of data generated by the detectors are normally explored via C++ code using batch processing, which introduces a considerable latency. An interactive tool, when integrated into the existing data management systems, can add a great value to the usability of these platforms. Here, we intend to review the current state-of-the-art of interactive data exploration, aiming at satisfying three requirements: access to raw data files, stored in a distributed environment, and with a reasonably low latency. This paper follows the guidelines for systematic mapping studies, which is well suited for gathering and classifying available studies. We summarize the results after classifying the 242 papers that passed our inclusion criteria. While there are many proposed solutions that tackle the problem in different manners, there is little evidence available about their implementation in practice. Almost all of the solutions found by this paper cover a subset of our requirements, with only one partially satisfying the three. The solutions for data exploration abound. It is an active research area and, considering the continuous growth of data volume and variety, is only to become harder. There is a niche for research on a solution that covers our requirements, and the required building blocks are there.}
}

@article{rayyan-727968166,
  title={An empirical analysis of software practices in Malaysian Small and Medium Enterprises},
  year={2016},
  pages={442-447},
  author={Almomani, Malek Ahmad and Basri, Shuib and Mahmood, Ahmad Kamil B and Baashar, Yahia Mohamed},
  keywords={Survey, Software quality, Companies, Documentation, Computers, Training, Quantitative analyzes, Small and Medium Enterprises - SMEs, Software Practices, Software},
  abstract={Small and Medium Enterprises are growing rapidly and becoming a key component in the industrial profile in many countries around the world. Therefore, good organizational management practices are strongly affected by the organizational performance. Unfortunately, there has been little empirical evidence about organizational management practices through Small and Medium Enterprises, especially in the Southeast Asian region. Hence, the main purpose of this paper is to investigate the current level of software development practices through Malaysian Small and medium Enterprises from an organizational perspective. The researchers conducted a survey of seventeen companies which are specialized in software development. The results indicated that Malaysian small and medium software companies still in the low level of organizational management practices. Finally, the present study helps senior managers in order to be realize and aware about the shortcoming issues whilst providing clear vision to develop comprehensive strategies to improve organization performance in Malaysian SME companies.}
}

@article{rayyan-727968167,
  title={Fuzzy analogy based effort estimation: An empirical comparative study},
  year={2017},
  pages={114-121},
  author={Idri, Ali and Abnane, Ibtissam},
  keywords={Software, Project management, Estimation, Fuzzy sets, Artificial neural networks, Software Development Effort Estimation, Accuracy Evaluation, Analogy Based Software Effort Estimation, Fuzzy Analogy, Pragmatics, Support vector machines},
  abstract={Software Development Effort Estimation (SDEE) plays a primary role in software project management. Among several techniques suggested for estimating software development effort, analogy-based software effort estimation approaches stand out as promising techniques. In this paper, the performance of Fuzzy Analogy is compared with that of six other SDEE techniques (Linear Regression, Support Vector Regression, Multi-Layer Perceptron, M5P and Classical Analogy). The evaluation of the SDEE techniques was performed over seven datasets with two evaluation techniques (All-in and Jackknife). The first step of the evaluation aimed to ensure that the SDEE techniques outperformed random guessing by using the Standardized Accuracy (SA). Then, we used a set of reliable performance measures (Pred(0.25), MAE, MBRE, MIBRE and LSD) and Borda count to rank them and identify which techniques are the most accurate. The results suggest that when using All-in evaluation, Fuzzy Analogy statistically outperformed the other SDEE techniques regardless of the dataset used. However, when using Jackknife evaluation, the results obtained depended on the dataset and the SDEE technique used. The results suggest that Fuzzy Analogy is a promising technique for software development effort estimation.}
}

@article{rayyan-727968168,
  title={Learning outcomes and the teaching-learning process: A proposal},
  year={2019},
  pages={1-8},
  author={Sepúlveda, Samuel and Diéguez, Mauricio},
  keywords={Software, Programming, Taxonomy, Proposals, Electronic publishing, Training, Bloom's Taxonomy, Engineering profession, Learning outcomes, Teaching-learning},
  abstract={Nowadays computer programming and disciplines associated with informatics are increasingly necessary for the training of future professionals. The challenge is even greater if we think about the training of future computer engineers, as specialists in the development of software systems. On the other hand, the Faculty of Engineering and Sciences of the University of La Frontera is developing a review of the curricula of its 14 Civil Engineering careers, in the context of which the process of re-designing the career of Computer Engineering is framed. This paper presents a proposal called EM-RA2, that aims to discuss the learning outcomes and its relationship with Bloom's Taxonomy and the teaching-learning process. The results show that a more detailed review of the learning outcomes and their alignment with the levels of curricular advancement, the training cycles, and Bloom's Taxonomy must be carried out.}
}

@article{rayyan-727968169,
  title={Extending UML testing profile towards non-functional test modeling},
  year={2014},
  pages={488-497},
  author={Rodríguez, Federico Toledo and Lonetti, Francesca and Bertolino, Antonia and Usaola, Macario Polo and Lamancha, Beatriz Pérez},
  keywords={Software, Systematics, Testing, Unified modeling language, Analytical models, Standards, Load modeling, Model-based Testing, Non-functional Test Cases, UML-TP},
  abstract={The research community has broadly recognized the importance of the validation of non-functional properties including performance and dependability requirements. However, the results of a systematic survey we carried out evidenced the lack of a standard notation for designing non-functional test cases. For some time, the greatest attention of Model-Based Testing (MBT) research has focused on functional aspects. The only exception is represented by the UML Testing Profile (UML-TP) that is a lightweight extension of UML to support the design of testing artifacts, but it only provides limited support for non-functional testing. In this paper we provide a first attempt to extend UML-TP for improving the design of non-functional tests. The proposed extension deals with some important concepts of non-functional testing such as the workload and the global verdicts. As a proof of concept we show how the extended UML-TP can be used for modeling non-functional test cases of an application example.}
}

@article{rayyan-727968170,
  title={Artificial intelligence for cybersecurity: A systematic mapping of literature},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={146598-146612},
  author={Wiafe, Isaac and Koranteng, Felix Nti and Obeng, Emmanuel Nyarko and Assyne, Nana and Wiafe, Abigail and Gulliver, Stephen R},
  keywords={machine learning, systematic reviews, Systematics, Machine learning, Protocols, Computer crime, Artificial intelligence and cybersecurity, information security, Intelligence},
  abstract={Due to the ever-increasing complexities in cybercrimes, there is the need for cybersecurity methods to be more robust and intelligent. This will make defense mechanisms to be capable of making real-time decisions that can effectively respond to sophisticated attacks. To support this, both researchers and practitioners need to be familiar with current methods of ensuring cybersecurity (CyberSec). In particular, the use of artificial intelligence for combating cybercrimes. However, there is lack of summaries on artificial intelligent methods for combating cybercrimes. To address this knowledge gap, this study sampled 131 articles from two main scholarly databases (ACM digital library and IEEE Xplore). Using a systematic mapping, the articles were analyzed using quantitative and qualitative methods. It was observed that artificial intelligent methods have made remarkable contributions to combating cybercrimes with significant improvement in intrusion detection systems. It was also observed that there is a reduction in computational complexity, model training times and false alarms. However, there is a significant skewness within the domain. Most studies have focused on intrusion detection and prevention systems, and the most dominant technique used was support vector machines. The findings also revealed that majority of the studies were published in two journal outlets. It is therefore suggested that to enhance research in artificial intelligence for CyberSec, researchers need to adopt newer techniques and also publish in other related outlets.}
}

@article{rayyan-727968171,
  title={Understanding uncertainty in self-adaptive systems},
  year={2020},
  pages={242-251},
  author={Calinescu, Radu and Mirandola, Raffaela and Perez-Palacin, Diego and Weyns, Danny},
  keywords={survey, Uncertainty, Self-adaptation, Taxonomy, Adaptation models, Encoding, Runtime, models, Adaptive systems, uncertainty, Synthetic aperture sonar, modeling formalism, unanticipated change},
  abstract={The following topics are dealt with: cloud computing; learning (artificial intelligence); self-adjusting systems; Web services; decision making; fault tolerant computing; resource allocation; parallel processing; multi-agent systems; and telecommunication traffic.}
}

@article{rayyan-727968172,
  title={A study on the active methodologies applied to teaching and learning process in the computing area},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={219083-219097},
  author={Calderon Ribeiro, Maria Ivanilse and Passos, Odette Mestrinho},
  keywords={Systematics, Programming, Education, Games, Market research, learning, Programming profession, Licenses, Active methodologies, students' perception, teaching in computing, Learning},
  abstract={Active Methodologies allow an active process in teaching and learning contents, promote responsible student involvement and bring satisfaction and enrichment to educational practices and active learning. Generally, students have learning difficulties in Computer Science courses, as they need to develop computational skills and thinking. The goals of this article is to characterize and analyze the types of Active Methodologies that are being applied in teaching and learning activities in Computer Science. Thus, this investigation was carried out through a Systematic Mapping Study, focusing on the use of the types of methodologies in view of the results achieved. It presents students' perceptions, benefits, and difficulties in adopting these methodologies in the classroom. The results show 6 types of different Active Methodologies used in 35 publications selected, different types of techniques or studies that were used, the publications trend per year, the courses that were worked in analyzed publications, and some benefits and difficulty related to the adoption of Active Methodologies. Regarding to students' perception, we identified different type feelings. Thus, the contributions of this study consist in a research focused on the use of Active Methodologies in a very broad sense, including the perceptions of teachers and students regarding the use of different teaching and learning methodologies. In addition, it shows the specific benefits and possible difficulties experienced in the use of Active Methodologies as teaching strategies. Consequently, some findings from this study may have the potential to support or direct choices of these methodologies in different Computer Science courses.}
}

@article{rayyan-727968173,
  title={A survey on graduates' curriculum-based knowledge gaps in software testing},
  year={2018},
  pages={1-8},
  author={Scatalon, Lilian P and Fioravanti, Maria Lydia and Prates, Jorge M and Garcia, Rogério E and Barbosa, Ellen F},
  keywords={Software},
  abstract={This research full paper presents a study on graduates' knowledge gaps in software testing according to industry needs. Several studies indicate that students graduate from computing programs with a knowledge gap in software testing. In this sense, we aimed to investigate in details this broader testing gap, by considering gaps in the level of testing topics. We conducted a survey with Brazilian practitioners in order to collect data (N=90). For each testing topic, knowledge gaps were calculated as the difference between what respondents' learned/practiced in undergraduate courses and what they actually applied in industry after graduating. Results provide evidence on points that could be improved in software testing education. Firstly, for all testing topics there was a negative gap on practice activities. This means that students could benefit from more testing assignments throughout the curriculum. Regarding gaps in concepts, some testing topics presented negative gaps (such as test in web applications) and others positive (such as test in aspect oriented software and mutation analysis). Therefore, results suggest that it is possible to counterbalance them in order to reduce the existing gaps. We also present respondents' opinions about their experience in software testing education and industry practices.}
}

@article{rayyan-727968174,
  title={PROMETHEUS: Procedural methodology for developing heuristics of usability},
  year={2017},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={15},
  number={3},
  pages={541-549},
  author={Jimenez, Cristhy and Allende Cid, Hector and Figueroa, Ismael},
  keywords={Usability, Robustness, Software systems, empirical studies, usability, Silicon compounds, IEEE transactions, human-computer interaction, heuristic evaluation, usability heuristics, TV, procedural methodology},
  abstract={Usability is a key discipline related to the development of modern software systems. Its goal is to assess the user-friendliness and effectiveness of a software product from the user point of view. Therefore, proper methodologies and techniques to perform this assessment are definitely relevant. Heuristic evaluation is probably the most commonly used method for usability assessment. Developed initially by Nielsen and Molich in the 90s, traditional heuristic evaluations rely on Nielsen's well-known 10 usability heuristics. However, recent evidence suggests that such heuristics are not sufficiently complete for dealing with new domains such as interactive television, virtual worlds, and many others. In addition to the lack of suitability of the traditional heuristics, it has been stated in the past years the lack of a robust methodology or process to effectively develop and validate these new domain-specific heuristics. In this paper we summarize current evidence regarding the lack of suitability of traditional heuristics, as well as the need for the development of new domain-specific heuristics. After identifying and acknowledging existing gaps in the state-of-the-art pointed by other researchers, we present PROMETHEUS, a PROcedural METhodology for developing HEuristics of USability. PROMETHEUS refines the methodology of Rusu et al. (2011), and is composed of 8 stages. PROMETHEUS clearly defines the artifacts that are required and produced by each stage, and also presents a set of quality indicators in order to assess the need for further refinement in the development of new heuristics. As an initial validation of PROMETHEUS, we apply a questionnaire to several researchers that have used the methodology of Rusu∼etal, and we have also performed a small retrospective study, computing the quality indicators of several previous studies. Our results suggest that PROMETHEUS is a very promising methodology, and that the metrics and indicators are indeed pertinent with respect to the conclusions of previous works.}
}

@article{rayyan-727968175,
  title={A catalogue of agile smells for agility assessment},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={79239-79259},
  author={Telemaco, Ulisses and Oliveira, Toacy and Alencar, Paulo and Cowan, Don},
  keywords={Software, Bibliographies, Tools, Industries, Companies, agile development, agile smell, Agility assessment, Crystals, Smell},
  abstract={Background: The Manifesto for Agile Development has already inspired many software development methods such as Scrum, XP, and Crystal Reports. However, being “agile” is not trivial and only a few companies are capable of mastering so-called agile practices. Failure to apply the agile approach properly can do more harm than good and may jeopardize the benefits of an agile method. Thus, evaluating an organization's ability to apply agile practices using an agility assessment tool is critical. Aims: In this paper, we extend the metaphor of code smell and introduce the term agile smell to denote the issues and practices that may impair the adoption of the agile approach. The focus of the paper is defining and validating a catalogue of agile smells that can support agility assessment. Method: A literature review and a survey were conducted to identify and confirm the characterization of agile smells. Once identified, the agile smells were organized in a structured catalogue. Results: The literature review found 2376 references published between 2001 and 2018. We selected 55 papers for full consideration and identified 20 agile smells. The survey consulted 20 participants to determine the relevance of the selected agile smells. Conclusion: We have identified a set of 20 agile smells that were ranked according to their relevance. For each smell, we proposed at least one strategy to identify the smell's presence in real projects. The catalogue can be used by companies to support the assessment of their agility ability.}
}

@article{rayyan-727968176,
  title={A systematic mapping study of usability vs security},
  year={2018},
  pages={1-6},
  author={MERDANOĞLU, Nur and ONAY DURDU, Pınar},
  keywords={Systematic mapping study, Systematics, Usability, Software systems, security, usability, Market research, Authentication, Control engineering, usable security},
  abstract={Usability and security are two of the inevitable but conflicting quality factors in today's multi-user modern computing environment. Although usability is considered to be an essential component of a secure system, not many studies focused on these two factors together. This study aimed to determine the general trends regarding the usability and security issues. Therefore, a systematic mapping approach was applied with the articles published on these two issues together between years 2006 and 2016 to reveal the research trends in this research area. 179 papers, which were indexed in web of science, were included for the mapping. The results revealed that many researches dealt with usability and security issues. They reported problems of efficiency and learnability from usability perspective and authentication from security perspective. They mainly adopt usability evaluation methods rather than any specific usability-security specific method and a few proposed recommendations or a method for design or improvement of usable security.}
}

@article{rayyan-727968177,
  title={Design guidelines for SaaS development process},
  year={2018},
  pages={825-831},
  author={Aleem, Saiqa and Batool, Rabia and Ahmed, Faheem and Khattak, Asad Masood},
  keywords={Guidelines, Cloud computing, QoS, Quality of service, Computer architecture, Software as a service, Parallel processing, Design for Failure, Design for Parallelism, Domain Analysis Design patterns, SaaS Design guidelines},
  abstract={A novel and widespread business model in cloud computing is to provide on-demand software as a service (SaaS) over the Internet. The software runs on a server and the user access it through an Internet connection. A single application instance can be shared by multiple users which provide a cost-effective solution to SaaS providers. Varying requirements from multiple users increase complexity in SaaS application design. The success of SaaS depends on its design. SaaS is different than traditional web-based application, so traditional application design model cannot full fill many SaaS specific design requirements. This paper provides a better understanding of key design factors in SaaS development process which results in a successful SaaS product following an improved design process. This study identifies key design factors through literature review and provides guidelines for key design factors on the SaaS application development. Ultimately, it will be beneficial for SaaS developers to improve the SaaS application development process and have a positive impact on the final product.}
}

@article{rayyan-727968178,
  title={Formulation of metric based software project performance monitoring model: A roadmap},
  year={2014},
  pages={48-53},
  author={Doraisamy, Mariayee and bin Ibrahim, Suhaimi and Mahrin, Mohd Naz'ri},
  keywords={Software, Systematics, Bibliographies, Data mining, Monitoring, Measurement, Metrics, Interviews, Performance Measurement Criteria, Software project monitoring, Software project performance, Metronidazole},
  abstract={Successful implementation of software projects are completely depending upon successful monitoring and controlling mechanism. The ignorance of monitoring and controlling mechanism by the project managers and the team members in a software projects development, leading to inadequate decision with fruitless outcomes. One of the ways to monitor the software projects is by looking at the performance of the elements that involved in a software project development. Development of metrics for each element in a software project development will lead the software projects to complete its objectives. This paper presents on formulation of Metric based Software Project Performance Monitoring Model which consists of metrics for each software project elements. Generally, this model will be a guideline for project managers to monitor and manage software projects especially in the perspectives of the public sectors.}
}

@article{rayyan-727968179,
  title={A holistic view of data warehousing in education},
  year={2018},
  journal={IEEE Access},
  issn={2169-3536},
  volume={6},
  pages={64659-64673},
  author={Moscoso-Zea, Oswaldo and Paredes-Gualtor, Joel and LujáN-Mora, Sergio},
  keywords={Data mining, Decision making, systematic mapping, Education, Business intelligence, Industries, Data analysis, data warehouse, Data warehouses, educational data warehouse},
  abstract={Data warehousing (DW) is a widespread and essential practice in business organizations that support the data analytic and decision-making process. Despite the importance of DW in complex organizations, the adoption of a data warehouse (DWH) in education is apparently lower compared with other industries. To clarify this situation, this paper presents a systematic mapping that includes the study of empirical research papers from 2008 to 2018 on the topic of DW in education. For this paper, we applied a qualitative and quantitative approach based on a four-stage research method with the objective to have a holistic view of DWHs in education. After filtering and applying the proposed method, 34 relevant papers were identified and studied in detail. The study revealed interesting facts; for example, Kimball's approach is the most applied methodology for DWH design in education. In addition, a mapping between this comprehensive collection of research papers covering educational DW and six dimensions of analysis (schema proposal, analysis of the user requirements, analysis of the business requirements, effectiveness, implementation, and data analysis) was performed. From this analysis, we discovered that the star schema is the most implemented approach. The purpose of the mapping was to explore and identify the priority areas of research and the research gaps within the academic community. These gaps are a source of opportunities to start new lines of research.}
}

@article{rayyan-727968180,
  title={Consolidation of usability problems with novice evaluators re-examined in individual vs. Collaborative settings},
  year={2019},
  journal={Interacting with Computers},
  issn={1873-7951},
  volume={31},
  number={3},
  pages={525-538},
  author={Hoffmann, Rebekka and Jónsdóttir, Anna Helga and Hvannberg, Ebba Thora},
  keywords={usability testing, consolidation, evaluator effect, group decision effects, HCI design and evaluation methods, human-centered computing, usability problems},
  abstract={Usability testing can involve multiple users and evaluators. In such cases, consolidating usability problems (UPs) constitutes an essential part of data analysis. In a between-subjects design, this study aims to re-examine a previous study by comparing the results of novice evaluators merging UPs individually vs. collaboratively and to assess the quality of the final UP lists, by computing the merging rate and the accuracy rate, respectively. Law and Hvannberg compared the results of evaluators merging UPs individually vs. collaboratively in a within-subjects design, revealing a tendency towards merging UPs in collaborative settings. In the present study, 45 novice evaluators consolidated four UP lists into a single UP master list while working alone or with a partner. The results showed no significant difference between evaluators in the two settings, suggesting that the UP consolidation process does not benefit from positive group decision effects.}
}

@article{rayyan-727968181,
  title={References},
  year={2015},
  journal={Software project estimation: The fundamentals for providing high quality information to decision makers},
  issn={978-1-118-95930-5},
  pages={253-256},
  author={Abran, Alain},
  url={https://ieeexplore.ieee.org/document/7111494},
  publisher={IEEE},
  abstract={No abstract.}
}

@article{rayyan-727968182,
  title={Built-in testing in component-based software - a mapping study},
  year={2015},
  pages={159-168},
  author={Divya and Gill, Nasib S and Singh, Latika},
  keywords={Software, Data mining, Testing, Computational modeling, Maintenance engineering, Heuristic algorithms, Runtime, Built-in test, Component based software},
  abstract={Component-based software development (CBSD)has proved to be a highly useful way of developing software using re-usable components, especially within a short time frame. The biggest challenge faced during development (specially testing) of component-based software is that the source-code of components is not available. Due to this, the traditional testing techniques cannot be applied directly while testing the software. Amongst several techniques proposed to bridge this gap, built-in testing has emerged out to be a popular choice. Built-in tests are test-cases/testing functionalities that are added to a component that analyze the component within the software during development and runtime. It is increasingly being used to perform testing during development and maintenance of various kinds of component-based software. This paper aims to identify the state-of-the-art research on built-in testing in component-based software. A mapping study has been performed employing searches in journals, conference proceedings, and electronic databases. A total of 26 studies have been reviewed in order to extract relevant information regarding a previously defined set of research questions. Although only 26 studies have addressed the use of built-in tests in component-based software, this number can be considered significant due to the specific nature of the reviewed topic. The results of the present study have shown the limitations and help to set further guidelines.}
}

@article{rayyan-727968183,
  title={Agile testing: Past, present, and future – charting a systematic map of testing in agile software development},
  year={2012},
  pages={55-63},
  author={Hellmann, Theodore D and Sharma, Abhishek and Ferreira, Jennifer and Maurer, Frank},
  keywords={Software, Systematics, Collaboration, systematic mapping, Testing, Databases, software testing, empirical, agile software development, Industries, Encoding, test-driven development, testing tools},
  abstract={Testing has been a cornerstone of agile software development methodologies since early in the history of the field. However, the terminology used to describe the field - as well as the evidence in existing literature - is largely inconsistent. In order to better structure our understanding of the field and to guide future work, we conducted a systematic mapping of agile testing. We investigated five research questions: which authors are most active in agile testing; what is agile testing used for; what types of paper tend to be published in this field; how do practitioners and academics contribute to research in this field; and what tools are used to conduct agile testing? Of particular interest is our investigation into the source of these publications, which indicates that academics and practitioners focus on different types of publication and, disturbingly, that the number of practitioner papers in the sources we searched is strongly down since 2010.}
}

@article{rayyan-727968184,
  title={Code of conduct in open source projects},
  year={2017},
  pages={24-33},
  author={Tourani, Parastou and Adams, Bram and Serebrenik, Alexander},
  keywords={Software, Collaboration, Productivity, Google, Cultural differences, Companies, Standards},
  abstract={Open source projects rely on collaboration of members from all around the world using web technologies like GitHub and Gerrit. This mixture of people with a wide range of backgrounds including minorities like women, ethnic minorities, and people with disabilities may increase the risk of offensive and destroying behaviours in the community, potentially leading affected project members to leave towards a more welcoming and friendly environment. To counter these effects, open source projects increasingly are turning to codes of conduct, in an attempt to promote their expectations and standards of ethical behaviour. In this first of its kind empirical study of codes of conduct in open source software projects, we investigated the role, scope and influence of codes of conduct through a mixture of quantitative and qualitative analysis, supported by interviews with practitioners. We found that the top codes of conduct are adopted by hundreds to thousands of projects, while all of them share 5 common dimensions.}
}

@article{rayyan-727968185,
  title={A systematic mapping on the role-permission relationship in role based access control models},
  year={2012},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={10},
  number={1},
  pages={1243-1250},
  author={Ueda, Eduardo Takeo and Ruggiero, Wilson Vicente},
  keywords={Systematics, Data mining, Context modeling, Libraries, Computational modeling, Adaptation models, Access control, RBAC, Role Based Access Control, Role-Permission Relationship},
  abstract={Access control is a key component of security in any computer system. In the last two decades, the research on Role Based Access Control Models was intense. One of the most important components of a Role Based Model is the Role-Permission Relationship. In this paper, the technique of systematic mapping is used to identify, extract and analyze many approaches applied to establish the Role-Permission Relationship. The main goal of this mapping is pointing directions of significant research in the area of Role Based Access Control Models.}
}

@article{rayyan-727968186,
  title={RESPECT 2020 prepint proceedings},
  year={2020},
  volume={1},
  pages={1-246},
  abstract={Full Conference PDF.}
}

@article{rayyan-727968187,
  title={Identification of terminology used by business people to express data quality},
  year={2019},
  journal={IEEE Latin America Transactions},
  issn={1548-0992},
  volume={17},
  number={3},
  pages={365-371},
  author={Ortega, Luis and Caro, Angélica and Rodríguez, Alfonso and Velásquez, Ignacio},
  keywords={Business Process, Unified modeling language, Terminology, Business, Information technology, Google, IEEE transactions, Data integrity, Business People, Data Quality, Data Quality Problems, Data Quality Terminology, Information Technologies People, Research Design},
  abstract={Problems associated with data quality cost companies millions of dollars every year because of bad decision-making based on inaccurate information, noncompliance with regulations, or failure to timely deal with customer problems. Nowadays, data have become one of the most valuable assets of organizations; data quality is no longer a task involving only information technology managers, business managers are increasingly more implicated. In the context of business management processes, the early specification of data quality requirements can be crucial. This article reveals the results of a research study conducted in specialized sites, blogs, forums, and literature related to how business people (not specialized in information technology) express data quality needs. Additionally, how the result was validated is shown through the participation of business experts and analysts. The outcome were lists with 18 types of problems associated with data quality and 35 characteristics that data must comply with in relation to quality.}
}

@article{rayyan-727968188,
  title={Towards implementation of requirements management specific practices (SP1.3 and SP1.4) for saudi arabian small and medium sized software development organizations},
  year={2017},
  journal={IEEE Access},
  issn={2169-3536},
  volume={5},
  pages={24162-24183},
  author={Keshta, Ismail and Niazi, Mahmood and Alshayeb, Mohammad},
  keywords={Software quality, Organizations, Requirements management, Standards organizations, Computational modeling, Capability maturity model, capability maturity model integration (CMMI), small- and medium-sized software development organizations, Software process improvement (SPI), project management, Software},
  abstract={There is a significant need to give careful consideration to Capability Maturity Model Integration (CMMI) Level 2 specific practices-SP 1.3 “manage requirements changes,”and SP 1.4 “maintain bidirectional traceability of requirements,”especially in the context of smalland medium-sized software development organizations in Saudi Arabia, in order to assist such organizations in getting one step closer to achieving CMMI Level 2 certification. The objective of this research is to implement CMMI Level 2 specific practices-SP 1.3 and SP 1.4. In this paper, a workflow model for each specific practice has been developed. In addition, initial evaluation of the models has been discussed. It is necessary to highlight that this paper contributes not only to the implementation of SP 1.3 and SP 1.4 of Requirements management process area in the context of smalland medium-sized software development organizations but also to the body of empirical studies in various context. Data has been collected by exploring published research articles and high-level software process descriptions. Moreover, previous research works that dealt with the implementation of CMMI Level 2 process areas have been reviewed. Furthermore, research articles that provide guidance to software development organizations for implementing process areas of CMMI Level 2 in their environments have been considered. After careful analysis of the collected data, we have proposed the models for two specific practices of CMMI level 2, i.e., managing requirements change and maintaining bidirectional traceability of requirements. Each model is divided into core stages, and different activities associated with each stage are clearly indicated. Initial evaluation of the proposed models was also conducted using the expert review process. Based on the initial evaluation, we are confident that our proposed models are clear and easy to learn, follow, and use. Moreover, our models are applicable to smalland medium-sized software development organizations in Saudi Arabia. The proposed models can also assist such organizations in implementing these two specific practices. For further evaluation, we need to perform multiple case studies in an industrial setting to test the proposed models.}
}

@article{rayyan-727968189,
  title={Towards ethical data ecosystems: A literature study},
  year={2019},
  pages={1-9},
  author={Rantanen, Minna M and Hyrynsalmi, Sami and Hyrynsalmi, Sonja M},
  keywords={Systematics, Guidelines, Ecosystems, Big Data, Companies, Biological system modeling, data economy, data ecosystem, ecosystem ethics, ecosystem governance, ethical governance, Ethics, systematic literature study},
  abstract={While the importance of data is growing as the fuel of the new data economy, also the role of the data ecosystems is growing. The new data ecosystems enables the use, reuse and enrichment of big data sets by or together with third parties. However, in the context of technology management, the governance of these kinds of data ecosystems raises ethical questions and issues that should be acknowledged by researchers and practitioners. This study reviews the extant literature regarding the given advice about ethical considerations. The method of systematic literature study is used to collect the primary articles (N=20). The selected articles are analyzed and themed according to reoccurring themes: privacy, accountability, ownership, accessibility, and motivation. The results show the discussion is fragmented and concrete ethical guidelines are lacking. Thus, this study requires more work for governing data ecosystems in an ethical way.}
}

@article{rayyan-727968190,
  title={How to describe SPL variabilities in textual use cases: A systematic mapping study},
  year={2014},
  pages={64-73},
  author={Santos, Ismayle Sousa and Andrade, Rossana M C Castro and Santos Neto, Pedro A},
  keywords={Software, Systematics, Data mining, systematic mapping study, Databases, Protocols, Abstracts, Adaptation models, software product line, use case},
  abstract={In the Software Product Line (SPL) paradigm, the variability description is an important issue for the requirements engineering process. In this scenario, there are several approaches in the literature focusing on how to describe variability in use cases. However, to the best of our knowledge, no efforts have been made to collect and summarize the existing templates for textual use case description in the SPL paradigm. This paper addresses this gap, presenting a systematic mapping study about SPL variability description in textual use cases. We found with this mapping, nine use case templates and four different ways to describe SPL variabilities in a use case description. From these templates, only one deal with the five variability types identified and we did not find any experimental study comparing these templates in terms of ease of use or comprehensibility.}
}

@article{rayyan-727968191,
  title={Dynamic re-configuration of software product lines towards an exploratory study on DSPLs},
  year={2016},
  pages={1-6},
  author={Sprovieri, Danillo},
  keywords={Software, Software product lines, Monitoring, Context, Computer architecture, Runtime, Cognition, DSPLs, Run-Time Adaptation, SPLs},
  abstract={Adaptations need to be considered at design-time (adapting complex systems to new technologies, reengineering due to new regulations etc.), but also during runtime (e.g. new emerging functional and non-functional requirement, context-specific decisions). Objective: I use SPLs as a strategy for coping with uncertainty and adapting to change, where conventionally change occurs in the requirements of the software product lines' market. Our idea is to design a variability mechanism in the domain of dynamic software product lines engineering in order to enable continuous evolution and adaptation of the software product lines at run-time. Method: I investigate dynamic change propagation of SPLs at run-time through an explorative study. A literature review and semistructured personal interviews with relevant actors in the domain of SPLs are the fundament of our research. This analysis enables us to understand how SPLs are dynamically adapted and evolved in practice. Conclusion: This study will give us an overview of the domain of DSPLs and allows us to identify the research gap regarding run-time adaptation and evolution of SPLs.}
}

@article{rayyan-727968192,
  title={Better cross company defect prediction},
  year={2013},
  pages={409-418},
  author={Peters, Fayola and Menzies, Tim and Marcus, Andrian},
  keywords={Software, data mining, Estimation, Companies, Predictive models, defect prediction, Training data, Cross company, Radio frequency, Vegetation},
  abstract={How can we find data for quality prediction? Early in the life cycle, projects may lack the data needed to build such predictors. Prior work assumed that relevant training data was found nearest to the local project. But is this the best approach? This paper introduces the Peters filter which is based on the following conjecture: When local data is scarce, more information exists in other projects. Accordingly, this filter selects training data via the structure of other projects. To assess the performance of the Peters filter, we compare it with two other approaches for quality prediction. Within-company learning and cross-company learning with the Burak filter (the state-of-the-art relevancy filter). This paper finds that: 1) within-company predictors are weak for small data-sets; 2) the Peters filter+cross-company builds better predictors than both within-company and the Burak filter+cross-company; and 3) the Peters filter builds 64% more useful predictors than both within-company and the Burak filter+cross-company approaches. Hence, we recommend the Peters filter for cross-company learning.}
}

@article{rayyan-727968193,
  title={Investigating mobile applications' requirements evolution through sentiment analysis of users' reviews},
  year={2015},
  pages={123-130},
  author={Rizk, Nancy M and Ebada, Amr and Nasr, Eman S},
  keywords={mobile applications, Synthetic aperture sonar, Asia, Chlorine, Computational linguistics, Integrated circuits, requirements evolution, sentiment analysis, users' requirements, users' reviews},
  abstract={The mobile applications industry is booming and growing rapidly. Mobile software companies and applications' stores are in a continuous need for developing new applications that meet and satisfy the users' requirements. Users' reviews posted in mobile applications' stores after installing the applications by the users can give useful information for developers; they contain good, bad, or recommended features. So the analysis of these reviews is important for the requirements engineering processes in the mobile applications industry. The use of users' feedback in finding software requirements is new in this research field. This paper is a review one, focuses on mobile applications users' reviews sentiment analysis (SA) in order to extract user requirements for building new applications or enhancing existing ones; i.e. requirements evolution. Users' reviews usually contain not only applications' features, but also users' feelings or “sentiments” about these features. We conducted an investigation of approaches used in the literature, during the period from 2009 to 2015 to answer three main research questions. Results show the significance of using SA for analyzing users' reviews and reports automated methods and tools for analyzing the reviews to features and their corresponding sentiments.}
}

@article{rayyan-727968194,
  title={Fuzzing: State of the art},
  year={2018},
  journal={IEEE Transactions on Reliability},
  issn={1558-1721},
  volume={67},
  number={3},
  pages={1199-1218},
  author={Liang, Hongliang and Pei, Xiaoxiao and Jia, Xiaodong and Shen, Wuwei and Zhang, Jian},
  keywords={Software testing, survey, Security, reliability, software testing, security, Computer bugs, Fuzzing},
  abstract={As one of the most popular software testing techniques, fuzzing can find a variety of weaknesses in a program, such as software bugs and vulnerabilities, by generating numerous test inputs. Due to its effectiveness, fuzzing is regarded as a valuable bug hunting method. In this paper, we present an overview of fuzzing that concentrates on its general process, as well as classifications, followed by detailed discussion of the key obstacles and some state-of-the-art technologies which aim to overcome or mitigate these obstacles. We further investigate and classify several widely used fuzzing tools. Our primary goal is to equip the stakeholder with a better understanding of fuzzing and the potential solutions for improving fuzzing methods in the spectrum of software testing and security. To inspire future research, we also predict some future directions with regard to fuzzing.}
}

@article{rayyan-727968195,
  title={Agile team perceptions of productivity factors},
  year={2011},
  pages={57-66},
  author={Melo, Claudia and Cruzes, Daniela S and Kon, Fabio and Conradi, Reidar},
  keywords={Software, Productivity, Programming, Interviews, Companies, team productivity, Resource management, agile methods, Personnel, empirical analysis, productivity factors},
  abstract={In this paper, we investigate agile team perceptions of factors impacting their productivity. Within this overall goal, we also investigate which productivity concept was adopted by the agile teams studied. We here conducted two case studies in the industry and analyzed data from two projects that we followed for six months. From the perspective of agile team members, the three most perceived factors impacting on their productivity were appropriate team composition and allocation, external dependencies, and staff turnover. Teams also mentioned pair programming and collocation as agile practices that impact productivity. As a secondary finding, most team members did not share the same understanding of the concept of productivity. While some known factors still impact agile team productivity, new factors emerged from the interviews as potential productivity factors impacting agile teams.}
}

@article{rayyan-727968196,
  title={Knowledge management in agile software development- a literature review},
  year={2018},
  pages={1-7},
  author={Indumini, Udeshika and Vasanthapriyan, S},
  keywords={Software, mapping study, Systematics, Collaboration, Bibliographies, Agile software development, Knowledge management, Knowledge Management, Agile Software Development, Organizations},
  abstract={Agile Software Development (ASD) is a knowledge intensive and collaborative activity. The key of acquiring, creating and co-operating the knowledge depends on Knowledge Management (KM). The field of knowledge management helps to improve the productivity of the whole agile software development process from the beginning to the end of the phase. Therefore, investigation of various aspects such as purposes, types of knowledge, technologies and research type is essential. The goal of this study is to conduct a literature review on existing researches on KM initiatives in ASD in order to identify the-state-of-the-art in the area as well as future research opportunities. A mapping study was performed by searching six electronic databases, and we considered studies published until December 2017. The initial resulting set comprised of 404 studies. From this set, a total of 10 studies were selected. for these 10, we performed snowballing and direct search of publications of researchers and research groups that accomplished these studies. Finally, we identified 12 reviewed studies addressing KM in Agile software development in order to extract relevant information on a set of research questions. Although few studies were found that addressed KM in ASD, the mapping shows an increasing attention and interest in the topic in the recent years. Reuse of knowledge of the team is a perspective that has received more attention. Moreover, as a main conclusion, the results show that KM is pointed out as an important strategy for increasing the effectiveness in ASD.}
}

@article{rayyan-727968197,
  title={Empirical evidence of code decay: A systematic mapping study},
  year={2013},
  pages={341-350},
  author={Bandi, Ajay and Williams, Byron J and Allen, Edward B},
  keywords={Software, Systematics, Data mining, Coupling, Measurement, Metrics, History, Conferences, Computer architecture, Software Evolution, Architecture Violations, Code Decay, Design Rules},
  abstract={Code decay is a gradual process that negatively impacts the quality of a software system. Developers need trusted measurement techniques to evaluate whether their systems have decayed. The research aims to find what is currently known about code decay detection techniques and metrics used to evaluate decay. We performed a systematic mapping study to determine which techniques and metrics have been empirically evaluated. A review protocol was developed and followed to identify 30 primary studies with empirical evidence of code decay. We categorized detection techniques into two broad groups: human-based and metric-based approaches. We describe the attributes of each approach and distinguish features of several subcategories of both high-level groups. A tabular overview of code decay metrics is also presented. We exclude studies that do not use time (i.e., do not use evaluation of multiple software versions) as a factor when evaluating code decay. This limitation serves to focus the review. We found that coupling metrics are the most widely used at identifying code decay. Researchers use various terms to define code decay, and we recommend additional research to operationalize the terms to provide more consistent analysis.}
}

@article{rayyan-727968198,
  title={A systematic study to improve the requirements engineering process in the domain of global software development},
  year={2020},
  journal={IEEE Access},
  issn={2169-3536},
  volume={8},
  pages={53374-53393},
  author={Akbar, Muhammad Azeem and Alsanad, Ahmed and Mahmood, Sajjad and Alsanad, Abeer Abdulaziz and Gumaei, Abdu},
  keywords={Software, Systematics, Requirements engineering, Global software development, Outsourcing, Organizations, Capability maturity model, client, vendor, empirical investigation, Object recognition, requirements engineering (RE)},
  abstract={The software organizations are outsourcing their development activities across the geographical border due to huge business gains. However, the adoption of the global software development (GSD) paradigm is not straightforward; various challenges are associated with it, particularly related to the requirements engineering (RE) process. The objective of this study is to identify the barriers to the RE process faced during GSD. To achieve this, we have conducted a systematic mapping study and questionnaire survey to identify and validate the barriers of the RE process with industry practitioners. A total of 20 barriers were identified and validated with the experts. Moreover, we have performed organization types (client and vendor), organization size (small, medium, and large) and experts' levels (junior, intermediate, and senior) based analysis to provide a clear understanding of the RE barriers in the three different context. Besides, we have also developed a theoretical framework by mapping the investigated barriers into six core knowledge areas of software process improvement. The mapping results indicated that project administration is the most significant knowledge area of investigated barriers. We believe that the findings of this study will provide a framework that assists the GSD practitioners in developing an effective plan and strategies to improve the RE process in the GSD context.}
}

@article{rayyan-727968199,
  title={A disturbing question: What is the economical impact of cloud computing? A systematic mapping},
  year={2018},
  pages={853-856},
  author={Ferraz, Felipe and Ribeiro, Francisco and Lima, Wallace and Sampaio, Carlos},
  keywords={Systematics, Security, Cloud computing, Challenges, Cloud Computing, Companies, Biological system modeling, Economics, Systematic Mapping, Economic Impacts, Opportunities},
  abstract={Cloud Computing is ranked as one of the most impacting technologies in present times. It represents a great change in how companies organize their budget, allowing then to save money otherwise spent in building and maintaining in-house data centers. If that were the only gain in the adoption of Cloud Computing technologies, it would be already a tremendous saving in costs, but Cloud solutions can provide users with furthermore services and options. With that in mind, what is the real impact of cloud computing to companies solutions and services? This paper conducts a systematic mapping in order to depict the economical and other impacts of cloud computing in companies and solution, analyzing aspects that go beyond technological questions.}
}

@article{rayyan-727968200,
  title={What do we know and how well do we know it? Current knowledge about software engineering practices},
  year={2015},
  volume={1},
  pages={1-5},
  author={Budgen, David},
  keywords={Empirical, Systematic Review, Evidence-based, Software},
  abstract={Context: The ‘prescriptions' used in software engineering for developing and maintaining systems make use of a set of ‘practice models', which have largely been derived by codifying successful experiences of expert practitioners. Aim: To review the ways in which empirical practices, and evidence-based studies in particular, have begun to provide more systematic sources of evidence about what practices work, when, and why. Method: My presentation will review the current situation regarding empirical studies in software engineering and examine some of the ways in which evidence-based studies can inform and influence practice. Results: These will be taken from a mix of secondary and tertiary studies. Conclusion: When compared with other disciplines that have become more ‘evidence-informed', the knowledge base for software engineering still needs considerable refinement. However, outcomes so far are encouraging, and indicate that in the future we can expect evidence-based research to play a larger role in informing practice, standards and teaching.}
}

@article{rayyan-727968201,
  title={Evidence-based software engineering for practitioners},
  year={2005},
  journal={IEEE Software},
  issn={1937-4194},
  volume={22},
  number={1},
  pages={58-65},
  author={Dyba, T and Kitchenham, B A and Jorgensen, M},
  keywords={evidence, Software engineering, evaluation, Project management, Production, empirical software engineering, Engineering management, Communications technology, Technology management, Laboratories, Appraisal, Appropriate technology, Medical treatment, Software},
  abstract={Software managers and practitioners often must make decisions about what technologies to employ on their projects. They might be aware of problems with their current development practices (for example, production bottlenecks or numerous defect reports from customers) and want to resolve them. Or, they might have read about a new technology and want to take advantage of its promised benefits. However, practitioners can have difficulty making informed decisions about whether to adopt a new technology because there's little objective evidence to confirm its suitability, limits, qualities, costs, and inherent risks. This can lead to poor decisions about technology adoption. Software engineers might make incorrect decisions about adopting new techniques it they don't consider scientific evidence about the techniques' efficacy. They should consider using procedures similar to ones developed for evidence-based medicine. Software companies are often under pressure to adopt immature technologies because of market and management pressures. We suggest that practitioners consider evidence-based software engineering as a mechanism to support and improve their technology adoption decisions.}
}

@article{rayyan-727968202,
  title={Table of contents},
  year={2011},
  pages={1},
  abstract={The following topics are dealt with: software engineering evaluation; software engineering assessment; software process improvement; and software architecture.}
}

@article{rayyan-727968203,
  title={Evaluating software project prediction systems},
  year={2005},
  pages={2-pp.–2},
  author={Shepperd, M},
  keywords={Software engineering, Machine learning, Software metrics, Software systems, Computer industry, Costs, Predictive models, Learning systems, Parametric statistics, Software},
  abstract={The problem of developing usable software project cost prediction systems is perennial and there are many competing approaches. Consequently, in recent years there have been exhortations to conduct empirically based evaluations in order that our understanding of project prediction might be based upon real world evidence. We now find ourselves in the interesting position of possessing this evidence in abundance. For example, a review of just three software engineering journals identified 50 separate studies and overall several hundred studies have been published. This naturally leads to the next step of needing to construct a body of knowledge, particularly when not all evidence is consistent. This process of forming a body of knowledge is generally referred to as metaanalysis. It is an essential activity if we are to have any hope of making sense of, and utilising, results from our empirical studies. However, it becomes apparent that when systematically combining results many difficulties are encountered}
}

@article{rayyan-727968204,
  title={Sensitivity of results to different data quality meta-data criteria in the sample selection of projects from the ISBSG dataset},
  year={2010},
  issn={978-1-4503-0404-7},
  author={Fernández-Diego, Marta and Martínez-Gómez, Mónica and Torralba-Martínez, José-María},
  url={https://doi.org/10.1145/1868328.1868348},
  publisher={Association for Computing Machinery},
  series={PROMISE '10},
  keywords={empirical research, datasets, data quality meta-data, effort, functional size, prediction models, software projects, Research Design},
  abstract={Background: Most prediction models, e.g. effort estimation, require preprocessing of data. Some datasets, such as ISBSG, contain data quality meta-data which can be used to filter out low quality cases from the analysis. However, an agreement has not been reached yet between researchers about these data quality selection criteria.Aims: This paper aims to analyze the influence of data quality meta-data criteria in the number of selected projects, which can have influence in the models obtained. For this, a case study has been selected to gain a more complete understanding of what might be important to focus in future research.Method: Data quality meta-data selection criteria of some works based on ISBSG dataset which propose prediction models were reviewed first. Considerable attention has been paid to two data quality meta-data variables in ISBSG dataset Release 11 which are Data Quality Rating and Unadjusted Function Point Rating. Secondly, this paper considers data from 830 projects which have been collected from the ISBSG dataset after a preliminary screening. This first screening leads mainly to a subset of projects with comparable definitions in size and effort. Then data quality meta-data criteria are applied in order to infer their influence.Results: Overall, it seems that data selection criteria, regardless data quality meta-data concerns, involve an important reduction in sample size. From 5052 projects, only 830 are really considered. Then 262 projects remain for analysis if the maximum quality rate is applied for both data quality meta-data variables. But, since the initial data preparation focuses the problem of missingness for a certain purpose, data quality criteria seem not to be the clue for the analysis results. However, some variability has been observed.Conclusions: Whilst this analysis is supported by a case study, it is hoped that it contributes to a better understanding of the subject. In fact, results found suggest that in those studies where the selection criteria of projects are not very strictly applied, these data quality criteria must be carefully taken into account.}
}

@article{rayyan-727968205,
  title={Emergent behavior in system-of-systems: A systematic mapping study},
  year={2019},
  issn={978-1-4503-7651-8},
  pages={140-149},
  author={Inocêncio, Thiago J and Gonzales, Gustavo R and Cavalcante, Everton and Horita, Flávio E A},
  url={https://doi.org/10.1145/3350768.3350779},
  publisher={Association for Computing Machinery},
  series={SBES 2019},
  keywords={Systematic mapping study, Systems-of-Systems, emergent behavior},
  abstract={A systems-of-systems (SoS) is a class of systems characterized by the collaboration among independent constituent systems, each one with its own purposes and functionalities and interacting with the others aiming to meet common global missions. An essential characteristic of any SoS is the so-called emergent behavior, which stands for the ability of an SoS of providing new functionalities resulted from the runtime collaboration among the constituent systems and that are not performed by any of them in isolation. Despite the importance of this subject, it is possible to notice that there are some lacks related to understanding emergent behavior in SoS, how it manifests, and which challenges it imposes to the development of this class of systems. This paper presents a systematic mapping study aimed to elicit and analyze the state-of-the-art on emergent behavior in SoS. By following a systematic procedure of search and selection of primary studies, a set of 37 existing studies was analyzed with respect to contributions related to emergent behavior in SoS. Obtained results show that the studies do not deeply investigate emergent behavior even though they mention this characteristic. Grounded on such a discussion, this work also presents a research agenda with directions for further work on the topic.}
}

@article{rayyan-727968206,
  title={A two-staged survey on release readiness},
  year={2017},
  issn={978-1-4503-4804-1},
  pages={374-383},
  author={Al Alam, S M Didar and Nayebi, Maleknaz and Pfahl, Dietmar and Ruhe, Guenther},
  url={https://doi.org/10.1145/3084226.3084254},
  publisher={Association for Computing Machinery},
  series={EASE'17},
  keywords={survey, literature review, empirical analysis, release readiness, Software release engineering},
  abstract={Deciding about the content and readiness when shipping a new product release can have a strong impact on the success (or failure) of the product. Having formerly analyzed the state-of-the art in this area, the objective for this paper was to better understand the process and rationale of real-world release decisions and to what extent research on release readiness is aligned with industrial needs. We designed two rounds of surveys with focus on the current (Survey-A) and the desired (Survey-B) process of how to make release readiness decisions. We received 49 and 40 valid responses for Survey-A and Survey-B, respectively.In total, we identified 12 main findings related to the process, the rationale and the tool support considered for making release readiness decisions. We found that reasons for failed releases and the factors considered for making release decisions are context specific and vary with release cycle time. Practitioners confirmed that (i) release readiness should be measured and continuously monitored during the whole release cycle, (ii) release readiness decisions are context-specific and should not be based solely on quality considerations, and iii) some of the observed reasons for failed releases such as low functionality, high cost, and immature service are not adequately studied in research where there is dominance on investigating quality and testing only. In terms of requested tool support, dashboards covering multidimensional aspects of the status of release development were articulated as key requirements.}
}

@article{rayyan-727968207,
  title={Using templates to elicit implied security requirements from functional requirements - a controlled experiment},
  year={2014},
  issn={978-1-4503-2774-9},
  author={Riaz, Maria and Slankas, John and King, Jason and Williams, Laurie},
  url={https://doi.org/10.1145/2652524.2652532},
  publisher={Association for Computing Machinery},
  series={ESEM '14},
  keywords={controlled experiment, security requirements, templates},
  abstract={Context: Security requirements for software systems can be challenging to identify and are often overlooked during the requirements engineering process. Existing functional requirements of a system can imply the need for security requirements. Systems having similar security objectives (e.g., confidentiality) often also share security requirements that can be captured in the form of reusable templates and instantiated in the context of a system to specify security requirements.Goal: We seek to improve the security requirements elicitation process by automatically suggesting appropriate security requirement templates implied by existing functional requirements.Method: We conducted a controlled experiment involving 50 graduate students enrolled in a software security course to evaluate the use of automatically-suggested templates in eliciting implied security requirements. Participants were divided into treatment (automatically-suggested templates) and control groups (no templates provided).Results: Participants using our templates identified 42% of all the implied security requirements in the oracle as compared to the control group, which identified only 16% of the implied security requirements. Template usage increased the efficiency of security requirements identified per unit of time.Conclusion: Automatically-suggested templates helped participants (security non-experts) think about security implications for the software system and consider more security requirements than they would have otherwise. We found that participants need more incentive than just a participatory grade when completing the task. Further, we recommend to ensure task completeness, participants either need a step-driven (i.e., wizard) approach or progress indicators to identify remaining work.}
}

@article{rayyan-727968208,
  title={On the need to study the impact of model driven engineering on software processes},
  year={2014},
  issn={978-1-4503-2754-1},
  pages={164-168},
  author={Hebig, Regina and Bendraou, Reda},
  url={https://doi.org/10.1145/2600821.2600846},
  publisher={Association for Computing Machinery},
  series={ICSSP 2014},
  keywords={model-driven engineering, literature survey, Software process tailoring, Software},
  abstract={There is an increasing use of model-driven engineering (MDE) in the industry. Despite the existence of research proposals for MDE-specific processes, the question arises whether and how the processes that are already used within a company can be reused, when MDE is introduced. In this position paper we report on a systematic literature review on the question how standard processes, such as SCRUM or the V-Model XT, can be combined with MDE. We come up with the observation that - although it is in some cases possible to reuse standard processes - the combination with MDE can also result in heavyweight changes to a process. Our goal is to draw attention to two arising research needs: the need to collect systematic knowledge about the influence of MDE on software processes and the need to provide guidance for the tailoring of processes based on the set of used MDE techniques.}
}

@article{rayyan-727968209,
  title={Size doesn't matter? On the value of software size features for effort estimation},
  year={2012},
  issn={978-1-4503-1241-7},
  pages={89-98},
  author={Kocaguneli, Ekrem and Menzies, Tim and Hihn, Jairus and Kang, Byeong Ho},
  url={https://doi.org/10.1145/2365324.2365336},
  publisher={Association for Computing Machinery},
  series={PROMISE '12},
  keywords={analogy-based estimation, function points, instance selection, k-NN, lines of code, popularity, Software},
  abstract={Background: Size features such as lines of code and function points are deemed essential for effort estimation. No one questions under what conditions size features are actually a "must".Aim: To question the need for size features and to propose a method that compensates their absence.Method: A baseline analogy-based estimation method (1NN) and a state-of-the-art learner (CART) are run on reduced (with no size features) and full (with all features) versions of 13 SEE data sets. 1NN is augmented with a popularity-based pre-processor to create "pop1NN". The performance of pop1NN is compared to 1NN and CART using 10-way cross validation w.r.t. MMRE, MdMRE, MAR, PRED(25), MBRE, MIBRE, and MMER.Results: Without any pre-processor, removal of size features decreases the performance of 1NN and CART. For 11 out of 13 data sets, pop1NN removes the necessity of size features. pop1NN (using reduced data) has a comparable performance to CART (using full data).Conclusion: Size features are important and their use is endorsed. However, if there are insufficient means to collect software size metrics, then the use of methods like pop1NN may compensate for size metrics with only a small loss in estimation accuracy.}
}

@article{rayyan-727968210,
  title={Understanding usability evaluation setup for VR products in industry: A review study},
  year={2020},
  journal={SIGAPP Appl. Comput. Rev.},
  issn={1559-6915},
  volume={19},
  number={4},
  pages={17-27},
  author={Karre, Sai Anirudh and Mathur, Neeraj and Reddy, Y Raghu},
  url={https://doi.org/10.1145/3381307.3381309},
  keywords={metrics, virtual reality, usability testing, industrial practices, usability evaluation},
  abstract={VR development practices have a diverse set of practices compared to traditional software development. Tasks like scene design, acoustic design, vergence manipulation, image depth, etc. are specific to VR apps and hence require evaluation processes that may be different from the traditional means. Usability Evaluation is one such process which is being executed in an unconventional way by Industrial Practitioners today. In this paper, the researchers detail a Systematic Literature Review of the Usability Evaluation Methods practised by Industrial researchers while building VR Products. The researchers found that VR Product teams follow unique methods to improve usability in their products. Further, the researchers consolidate these methods and provide insights into choosing the best to build a real-world VR Product based on the defined product constraints}
}

@article{rayyan-727968211,
  title={An empirical evaluation of OSGi dependencies best practices in the eclipse IDE},
  year={2018},
  issn={978-1-4503-5716-6},
  pages={170-180},
  author={Ochoa, Lina and Degueule, Thomas and Vinju, Jurgen},
  url={https://doi.org/10.1145/3196398.3196416},
  publisher={Association for Computing Machinery},
  series={MSR '18},
  abstract={OSGi is a module system and service framework that aims to fill Java's lack of support for modular development. Using OSGi, developers divide software into multiple bundles that declare constrained dependencies towards other bundles. However, there are various ways of declaring and managing such dependencies, and it can be confusing for developers to choose one over another. Over the course of time, experts and practitioners have defined "best practices" related to dependency management in OSGi. The underlying assumptions are that these best practices (i) are indeed relevant and (ii) help to keep OSGi systems manageable and efficient. In this paper, we investigate these assumptions by first conducting a systematic review of the best practices related to dependency management issued by the OSGi Alliance and OSGi-endorsed organizations. Using a large corpus of OSGi bundles (1,124 core plug-ins of the Eclipse IDE), we then analyze the use and impact of 6 selected best practices. Our results show that the selected best practices are not widely followed in practice. Besides, we observe that following them strictly reduces classpath size of individual bundles by up to 23% and results in up to ±13% impact on performance at bundle resolution time. In summary, this paper contributes an initial empirical validation of industry-standard OSGi best practices. Our results should influence practitioners especially, by providing evidence of the impact of these best practices in real-world systems.}
}

@article{rayyan-727968212,
  title={A reference architecture for providing tools as a service to support global software development},
  year={2014},
  issn={978-1-4503-2523-3},
  author={Chauhan, Muhammad Aufeef},
  url={https://doi.org/10.1145/2578128.2588485},
  publisher={Association for Computing Machinery},
  series={WICSA '14 companion},
  keywords={cloud computing, global software development (GSD), infrastructure as a service (IaaS), software as a service (SaaS), software engineering (SE), tools as a service (TaaS), Software},
  abstract={Global Software Development (GSD) teams encounter challenges that are associated with distribution of software development activities across multiple geographic regions. The limited support for performing collaborative development and engineering activities and lack of sufficient support for maintaining and resolving dependencies and traceability across heterogeneous tools are major challenges for GSD teams. The lack of insufficient support for cross platform tools integration also makes it hard to address the stated challenges using existing paradigms that are based upon desktop and web-based solutions. The restricted ability of the organizations to have desired alignment of tools with software engineering and development processes results in administrative and managerial overhead that incur increased development cost and poor product quality. Moreover, stakeholders involved in the projects have specific constraints regarding availability and deployments of the tools. The artifacts and data produced or consumed by the tools need to be governed according to the constraints and corresponding quality of service (QoS) parameters. In this paper, we present the research agenda to leverage cloud-computing paradigm for addressing above-mentioned issues by providing a framework to select appropriate tools as well as associated services and reference architecture of the cloud-enabled middleware platform that allows on demand provisioning of software engineering Tools as a Service (TaaS) with focus on integration of tools.}
}

@article{rayyan-727968213,
  title={Software metrics for measuring the understandability of architectural structures: A systematic mapping study},
  year={2015},
  issn={978-1-4503-3350-4},
  author={Stevanetic, Srdjan and Zdun, Uwe},
  url={https://doi.org/10.1145/2745802.2745822},
  publisher={Association for Computing Machinery},
  series={EASE '15},
  keywords={Metronidazole, Software},
  abstract={The main idea of software architecture is to concentrate on the "big picture" of a software system. In the context of object-oriented software systems higher-level architectural structures or views above the level of classes are frequently used to capture the "big picture" of the system. One of the critical aspects of these higher-level views is understandability, as one of their main purposes is to enable designers to abstract away fine-grained details. In this article we present a systematic mapping study on software metrics related to the understandability concepts of such higher-level software structures with regard to their relations to the system implementation. In our systematic mapping study, we started from 3951 studies obtained using an electronic search in the four digital libraries from ACM, IEEE, Scopus, and Springer. After applying our inclusion/exclusion criteria as well as the snowballing technique we selected 268 studies for in-depth study. From those, we selected 25 studies that contain relevant metrics. We classify the identified studies and metrics with regard to the measured artefacts, attributes, quality characteristics, and representation model used for the metrics definitions. Additionally, we present the assessment of the maturity level of the identified studies. Overall, there is a lack of maturity in the studies. We discuss possible techniques how to mitigate the identified problems. From the academic point of view we believe that our study is a good starting point for future studies aiming at improving the existing works. From a practitioner's point of view, the results of our study can be used as a catalogue and an indication of the maturity of the existing research results.}
}

@article{rayyan-727968214,
  title={Clustering dycom: An online cross-company software effort estimation study},
  year={2017},
  issn={978-1-4503-5305-2},
  pages={12-21},
  author={Minku, Leandro L and Hou, Siqing},
  url={https://doi.org/10.1145/3127005.3127007},
  publisher={Association for Computing Machinery},
  series={PROMISE},
  keywords={Software effort estimation, concept drift, cross-company learning, ensembles, online learning, Software},
  abstract={Background: Software Effort Estimation (SEE) can be formulated as an online learning problem, where new projects are completed over time and may become available for training. In this scenario, a Cross-Company (CC) SEE approach called Dycom can drastically reduce the number of Within-Company (WC) projects needed for training, saving the high cost of collecting such training projects. However, Dycom relies on splitting CC projects into different subsets in order to create its CC models. Such splitting can have a significant impact on Dycom's predictive performance. Aims: This paper investigates whether clustering methods can be used to help finding good CC splits for Dycom. Method: Dycom is extended to use clustering methods for creating the CC subsets. Three different clustering methods are investigated, namely Hierarchical Clustering, K-Means, and Expectation-Maximisation. Clustering Dycom is compared against the original Dycom with CC subsets of different sizes, based on four SEE databases. A baseline WC model is also included in the analysis. Results: Clustering Dycom with K-Means can potentially help to split the CC projects, managing to achieve similar or better predictive performance than Dycom. However, K-Means still requires the number of CC subsets to be pre-defined, and a poor choice can negatively affect predictive performance. EM enables Dycom to automatically set the number of CC subsets while still maintaining or improving predictive performance with respect to the baseline WC model. Clustering Dycom with Hierarchical Clustering did not offer significant advantage in terms of predictive performance. Conclusion: Clustering methods can be an effective way to automatically generate Dycom's CC subsets.}
}

@article{rayyan-727968215,
  title={On collecting and validating UML consistency rules: A research proposal},
  year={2014},
  issn={978-1-4503-2476-2},
  author={Torre, Damiano},
  url={https://doi.org/10.1145/2601248.2613084},
  publisher={Association for Computing Machinery},
  series={EASE '14},
  keywords={UML consistency rules, validation, UML synthesis techniques, unified modeling language (UML), Research Design},
  abstract={The main aim of my doctoral research is to create a comprehensive set of well-accepted consistency rules for UML diagrams that can be found in the literature, in reference textbooks or in the UML standard. Moreover, an important part of that aim will be the validation of the gathered UML consistency rules. This research will provide to the academic community and industrial organizations an extensive and detailed new base of knowledge about UML consistency rules which is a paramount topic in UML context. Therefore it will represent a sound starting point for all the researchers and designers involved in UML consistency.}
}

@article{rayyan-727968216,
  title={Investigating the model-driven development for systems-of-systems},
  year={2014},
  issn={978-1-4503-2778-7},
  author={Graciano Neto, Valdemar Vicente and Guessi, Milena and Oliveira, Lucas Bueno R and Oquendo, Flavio and Nakagawa, Elisa Yumi},
  url={https://doi.org/10.1145/2642803.2642825},
  publisher={Association for Computing Machinery},
  series={ECSAW '14},
  keywords={Model-Driven Development, Software Generation, System-of-Systems},
  abstract={Software-intensive systems have become increasingly large and complex and new techniques and methodologies are necessary to deal with such complexity. Model-Driven Development (MDD) has been used to deal with complex scenarios, since software models, despite details, facilitate the visualization of the whole. Moreover, MDD has been widely recognized as a way to assure quality, reducing time and effort, and making possible the automatic transformation of models to generate source code. In this direction, software-intensive Systems-of-Systems (SoS) is a class of software systems that have emerged over the iminence of large systems which have a high-level of complexity. Considering the success of MDD in other areas, we decided to investigate how MDD has been used in the context of SoS. This paper presents results of a Systematic Literature Review conducted to scrutinize and bring to light the state of the art in the field of MDD for SoS. Besides that, we discuss future research directions and perspectives, aiming at contributing to the development of SoS.}
}

@article{rayyan-727968217,
  title={Social barriers faced by newcomers placing their first contribution in open source software projects},
  year={2015},
  issn={978-1-4503-2922-4},
  pages={1379-1392},
  author={Steinmacher, Igor and Conte, Tayana and Gerosa, Marco Aurélio and Redmiles, David},
  url={https://doi.org/10.1145/2675133.2675215},
  publisher={Association for Computing Machinery},
  series={CSCW '15},
  keywords={barriers, entry, joining, new contributor, newcomers, onboarding, online communities, open collaboration, open source software, qualitative study, social barriers, socialization, Software},
  abstract={Newcomers' seamless onboarding is important for online communities that depend upon leveraging the contribution of outsiders. Previous studies investigated aspects of the joining process and motivation in open collaboration communities, but few have focused on identifying and understanding the critical barriers newcomers face when placing their first contribution, a period that frequently leads to dropout. This is important for Open Source Software (OSS) projects, which receive contributions from many one-time contributors. Focusing on OSS, our study qualitatively analyzed social barriers that hindered newcomers' first contributions. We defined a conceptual model composed of 58 barriers including 13 social barriers. The barriers were identified from a qualitative data analysis considering different sources: a systematic literature review; open question responses gathered from OSS projects' contributors; students contributing to OSS projects; and semi-structured interviews with 36 developers from 14 different projects. This paper focuses on social barriers and its contributions include gathering empirical evidence of the barriers faced by newcomers, organizing and better understanding these barriers, surveying the literature from the perspective of the barriers, and identifying new potential research streams.}
}

@article{rayyan-727968218,
  title={UML consistency rules: A systematic mapping study},
  year={2014},
  issn={978-1-4503-2476-2},
  author={Torre, Damiano and Labiche, Yvan and Genero, Marcela},
  url={https://doi.org/10.1145/2601248.2601292},
  publisher={Association for Computing Machinery},
  series={EASE '14},
  keywords={systematic mapping study, UML consistency rules, unified modeling language (UML), Unified Medical Language System},
  abstract={Context: The Unified Modeling Language (UML), with its 14 different diagram types, is the de-facto standard modeling language for object-oriented modeling and documentation. Since the various UML diagrams describe different aspects of one, and only one, software under development, they are not independent but strongly depend on each other in many ways. In other words, the UML diagrams describing a software product must be consistent. Inconsistencies between these diagrams may be a source of faults in software systems. It is therefore paramount that these inconsistencies be detected, analyzed and hopefully fixed.Objective: The aim of this article is to deliver a comprehensive summary of UML consistency rules as they are described in the literature to date to obtain an extensive and detailed overview of the current research in this area.Method: We performed a Systematic Mapping Study by following well-known guidelines. We selected 95 primary studies from a search with seven search engines performed in December 2012.Results: Different results are worth mentioning. First it appears that researchers tend to discuss very similar consistency rules, over and over again. Most rules are horizontal (98.10%) and syntactic (88.21%). The most used diagrams are the class diagram (71.58%), the sequence diagram (47.37%) and the state machine diagram (42.11%).Conclusion: The fact that many rules are duplicated in primary studies confirms the need for a well-accepted list of consistency rules. This paper is a first step in this direction. Results indicate that much more work is needed to develop consistency rules for all 14 UML diagrams, in all dimensions of consistency (e.g., semantic and syntactic on the one hand, horizontal, vertical and evolution on the other hand).}
}

@article{rayyan-727968219,
  title={Attacker-centric thinking in security: Perspectives from financial services practitioners},
  year={2020},
  issn={978-1-4503-8833-7},
  author={Moeckel, Caroline},
  url={https://doi.org/10.1145/3407023.3407082},
  publisher={Association for Computing Machinery},
  series={ARES '20},
  keywords={human-computer interaction, usable security, adversary, attacker, attacker-centric security, financial services, threat modelling, Thinking},
  abstract={In response to diverging perspectives on the usefulness of attacker-centric approaches in security, this paper examines the current role of such thinking in security, incorporating 12 in-depth interviews with senior financial services practitioners working in the areas of security, fraud and risk. The presentation of results is supported by a condensed systematic literature review on the topic and followed by the provision of a list of suggested guidelines on practical implementation strategies, enabling further theoretical reframing and extension.}
}

@article{rayyan-727968220,
  title={Classifying the nature of lecturer intervention on computer science student behaviour},
  year={2017},
  issn={978-1-4503-5301-4},
  pages={128-132},
  author={Falkner, Nickolas and Szabo, Claudia and Knutas, Antti},
  url={https://doi.org/10.1145/3141880.3141896},
  publisher={Association for Computing Machinery},
  series={Koli calling '17},
  keywords={intervention, electronic discussion forums, student behaviour},
  abstract={A key challenge for computer science educators is providing effective support and feedback to students, to ensure they are engaged with the course. Due to the significant problems of scale that need to be addressed, effective lecturer intervention is difficult, and at the same time the effect of intervention is challenging to measure accurately. In this short paper, we report on the outcomes of a systematic literature review focused on interventions in Computer Science classrooms. To provide an understanding of the types of interventions possible in a Computer Science course, we propose a taxonomy of intervention types with low mutual information and classify the 130 selected papers based on it. We present an analysis of the co-occurrence of some intervention types and the evolution of intervention type popularity over the years.}
}

@article{rayyan-727968221,
  title={Big data on cloud for government agencies: Benefits, challenges, and solutions},
  year={2018},
  issn={978-1-4503-6526-0},
  author={Rashed, Alaa Hussain and Karakaya, Ziya and Yazici, Ali},
  url={https://doi.org/10.1145/3209281.3209360},
  publisher={Association for Computing Machinery},
  series={dg.o '18},
  keywords={benefits, big data, cloud computing, challenges, big data on cloud, government agencies, solutions, Government Agencies},
  abstract={Big Data and Cloud computing are the most important technologies that give the opportunity for government agencies to gain a competitive advantage and improve their organizations. On one hand, Big Data implementation requires investing a significant amount of money in hardware, software, and workforce. On the other hand, Cloud Computing offers an unlimited, scalable and on-demand pool of resources which provide the ability to adopt Big Data technology without wasting on the financial resources of the organization and make the implementation of Big Data faster and easier. The aim of this study is to conduct a systematic literature review in order to collect data to identify the benefits and challenges of Big Data on Cloud for government agencies and to make a clear understanding of how combining Big Data and Cloud Computing help to overcome some of these challenges. The last objective of this study is to identify the solutions for related challenges of Big Data. Four research questions were designed to determine the information that is related to the objectives of this study. Data is collected using literature review method and the results are deduced from there.}
}

@article{rayyan-727968222,
  title={An assessment of tools for UML class diagram modeling: Support to adaptation and integration with other tools},
  year={2019},
  issn={978-1-4503-7282-4},
  pages={10-19},
  author={Massago, Mamoru and Colanzi, Thelma Elita},
  url={https://doi.org/10.1145/3364641.3364643},
  publisher={Association for Computing Machinery},
  series={SBQS'19},
  keywords={Diagrama de Classes, Ferramentas CASE, Revisão Sistemática},
  abstract={The software development process comprises a set of activities with the goal of developing high quality software. The software architecture design is a key activity of the software development processs as it allows the specification and visualization of products from different and structured point of views. One of the most used architectural views is the logical one, which represents the classes and their relationships. For this reason, the UML class diagram is one of the most used UML diagrams by software engineers [1]. Many companies use CASE (Computer-Aided Software Engineering) tools to carry out several software engineering activities, including modeling class diagrams. They are looking for independence and flexibility to adapt and integrate tools through the adoption of open-source tools, but this is not an easy task as there are a lot of CASE tools and most of them are incompatible. This problem arises when it is necessary to integrate tools to perform software engineering activity that needs to receive a UML class diagram as input. In this sense, the objective of this work is to assist software engineers in the selection of the tools by presenting results of a systematic review, whose goal was identifying existing free and open-source tools for UML class diagram modeling and analyzing their support to adaptation and integration with other tools. Twelve tools for UML class diagrams modeling were found and an analysis of their support to adaptation and integration with other tools was carried out. This analysis is useful in the choice and use of such kind of tool, either in industry or academic research.}
}

@article{rayyan-727968223,
  title={A literature review and comparison of three feature location techniques using ArgoUML-SPL},
  year={2019},
  issn={978-1-4503-6648-9},
  author={Cruz, Daniel and Figueiredo, Eduardo and Martinez, Jabier},
  url={https://doi.org/10.1145/3302333.3302343},
  publisher={Association for Computing Machinery},
  series={VAMOS '19},
  keywords={reverse engineering, benchmark, feature location, software product lines, variability mining},
  abstract={Over the last decades, the adoption of Software Product Line (SPL) engineering for supporting software reuse has increased. An SPL can be extracted from one single product or from a family of related software products, and feature location strategies are widely used for variability mining. Several feature location strategies have been proposed in the literature and they usually aim to map a feature to its source code implementation. In this paper, we present a systematic literature review that identifies and characterizes existing feature location strategies. We also evaluated three different strategies based on textual information retrieval in the context of the ArgoUML-SPL feature location case study. In this evaluation, we compare the strategies based on their ability to correctly identify the source code of several features from ArgoUML-SPL ground truth. We then discuss the strengths and weaknesses of each feature location strategy.}
}

@article{rayyan-727968224,
  title={Regulatory requirements compliance in e-government system development: An ontology framework},
  year={2017},
  issn={978-1-4503-4825-6},
  pages={441-449},
  author={Hasan, M Mahmudul and Aganostopoulos, Dimosthenis and Loucopoulos, Pericles and Nikolaidou, Mara},
  url={https://doi.org/10.1145/3047273.3047341},
  publisher={Association for Computing Machinery},
  series={ICEGOV '17},
  keywords={e-Government, e-Government regulation and policy, Ontology Framework, Regulatory requirements Compliance},
  abstract={Electronic government (e-Government) is increasingly gaining attention by the government and researcher to shape the public sector into digital society through enacting several e-Government system development policies and regulations. Hence, the compliance of regulatory requirement from these policies and regulations become an important accountability in e-Government project development where the concepts of regulatory requirements compliance is still scattered in the e-Government domain. This paper presents an ontology framework that describes the formal and explicit specification of the concepts of regulatory requirements compliance and their relations in e-Government system development. The ontology engineering technique 101 and Systematic Literature Review (SLR) were used in the process of developing the ontology framework of e-Government regulatory requirements compliance (eGovRRC). The e-Government system analyst can use this framework as a reference model to understand and conceptualize the interlinked set of clearly defined concepts of regulatory requirements compliance in e-Government system development projects.}
}

@article{rayyan-727968225,
  title={Global software development and collaboration: Barriers and solutions},
  year={2011},
  journal={ACM Inroads},
  issn={2153-2184},
  volume={1},
  number={3},
  pages={66-78},
  author={Noll, John and Beecham, Sarah and Richardson, Ita},
  url={https://doi.org/10.1145/1835428.1835445},
  keywords={collaboration, global software development, global software engineering, inter-cultural organizations, virtual teams, Software},
  abstract={While organisations recognise the advantages offered by global software development, many socio-technical barriers affect successful collaboration in this inter-cultural environment. In this paper, we present a review of the global software development literature where we highlight collaboration problems experienced by a cross-section of organisations in twenty-six studies. We also look at the literature to answer how organisations are overcoming these barriers in practice. We build on our previous study on global software development where we define collaboration as four practices related to agreeing, allocating, and planning goals, objectives, and tasks among distributed teams.We found that the key barriers to collaboration are geographic, temporal, cultural, and linguistic distance; the primary solutions to overcoming these barriers include site visits, synchronous communication technology, and knowledge sharing infrastructure to capture implicit knowledge and make it explicit.}
}

@article{rayyan-727968226,
  title={Evaluating identity management architectures},
  year={2012},
  issn={978-1-4503-1347-6},
  pages={11-20},
  author={Staite, Christopher and Bahsoon, Rami},
  url={https://doi.org/10.1145/2304656.2304659},
  publisher={Association for Computing Machinery},
  series={ISARCS '12},
  keywords={evaluation, security, architecture, identity management},
  abstract={Developments in the area of identity management have been subject to very little critique. Many implementations have gathered little general following, and larger scale adoption, such as OpenID, has been limited to internal systems and large identity providers.Previous evaluation has focussed on specific areas and does little to describe the trade off performed in the use of new identity management architectures. Furthermore, these evaluations have not equally considered user vs service provider perspective. This paper looks to derive a method for evaluation which encapsulates metrics from past work and areas which have not been considered. This method produces a holistic evaluation and comparison of identity management architectures.}
}

@article{rayyan-727968227,
  title={Using rule-based classifiers in systematic reviews: A semantic class association rules approach},
  year={2015},
  issn={978-1-4503-3491-4},
  author={Sellak, Hamza and Ouhbi, Brahim and Frikh, Bouchra},
  url={https://doi.org/10.1145/2837185.2837279},
  publisher={Association for Computing Machinery},
  series={iiWAS '15},
  keywords={systematic reviews, class association rules, feature selection methods, semantic association rules, text classification, Semantics},
  abstract={Systematic review is the scientific process that provides reliable answers to a particular research question by interpreting the current pertinent literature. There is a significant shift from using manual human approach to decision support tools that provides a semi-automated screening phase by reducing the required time and effort to the group of experts. Most of proposed works apply supervised Machine Learning (ML) algorithms to infer exclusion and inclusion rules by observing a human screener. Unless, these techniques holds very little promise in study identification phase, because the rate of excluding citations erroneously still unreasonable. In this paper, we contribute to this line of works by proposing an alternative approach, not yet tested in this domain based on semantic rule-based classifiers. This approach involved applying a novel Hybrid Feature Selection Method (HFSM) within a Class Association Rules (CARs) algorithm. Experiments are conducted on a corpus resulting from an actual systematic review. The obtained results show that our algorithm outperforms the existing algorithms in the literature.}
}

@article{rayyan-727968228,
  title={Techniques for developing more accessible web applications: A survey towards a process classification},
  year={2007},
  issn={978-1-59593-588-5},
  pages={162-169},
  author={Freire, Andre Pimenta and Goularte, Rudinei and de Mattos Fortes, Renata Pontin},
  url={https://doi.org/10.1145/1297144.1297177},
  publisher={Association for Computing Machinery},
  series={SIGDOC '07},
  keywords={information acessibility, information design, universal web design, universal web usability, web accessibility},
  abstract={The Web has become one of the most important communication media, since it is spread all over the world. In order to enable everyone to access this medium, Web accessibility has become an emerging topic, and many techniques have been evolved to support the development of accessible Web content. This paper presents a survey on techniques for Web accessibility and proposes a classification into the processes of ISO/IEC 12207 standard. The survey was carried out applying systematic review principles during the literature review. The results include analysis obtained from the synthesis of 53 studies, selected from an initial set of 844. Although the survey results indicate a growth in research on techniques for design and evaluation of Web applications, they also indicate that several development activities have been poorly addressed by scientific research efforts.}
}

@article{rayyan-727968229,
  title={Gathering current knowledge about quality evaluation in software product lines},
  year={2009},
  pages={91-100},
  author={Montagud, Sonia and Abrahão, Silvia},
  publisher={Carnegie Mellon University},
  series={SPLC '09},
  keywords={⛔ No DOI found, Software},
  abstract={Recently, a number of methods and techniques for assessing the quality of software product lines have been proposed. However, to the best of our knowledge, there is no study which summarizes all the existing evidence about them. This paper presents a systematic review that investigates what methods and techniques have been employed (in the last 10 years) to evaluate the quality of software product lines and how they were employed. A total of 39 research papers have been reviewed from an initial set of 1388 papers. The results show that 25% of the papers reported evaluations at the Design phase of the Domain Engineering phase. The most widely used mechanism for modeling quality attributes was extended feature models and the most evaluated artifact was the base architecture. In addition, the results of the review have identified several research gaps. Specifically, 77% of the papers employed case studies as a "proof of concept" whereas 23% of the papers did not perform any type of validation. Our results are particularly relevant in positioning new research activities and in the selection of quality evaluation methods or techniques that best fit a given purpose.}
}

@article{rayyan-727968230,
  title={Challenges of evolving sequential to parallel code: An exploratory review},
  year={2011},
  issn={978-1-4503-0848-9},
  pages={1-5},
  author={Meade, Anne and Buckley, Jim and Collins, J J},
  url={https://doi.org/10.1145/2024445.2024447},
  publisher={Association for Computing Machinery},
  series={IWPSE-EVOL '11},
  keywords={challenges, evolution, concurrency, multicore, parallel, program},
  abstract={Large legacy systems that have been in use for several decades need to evolve in order to take advantage of new technological advances. One such development is the emergence of multi-core processors and parallel platforms. However, the evolution of code written for single-core platforms into code that can take advantage of multi-core technology is challenging. The aim of this research is to explore the challenges that parallel programmers face in the evolution of existing software to exploit multicore and parallel architectures. A review of the current literature was conducted and ten frequently reported challenges were identified. It is important to raise awareness of potential challenges that practitioners may face when evolving sequential code to exploit multicore platforms in order to be better prepared for future evolution. The research community can use these results to develop a research agenda in order to design and develop solutions to address these challenges.}
}

@article{rayyan-727968231,
  title={Technical debt management in brazilian software organizations: A need, an expectation, or a fact?},
  year={2018},
  issn={978-1-4503-6565-9},
  pages={200-209},
  author={Silva, Victor and Jeronimo, Helvio and Travassos, Guilherme Horta},
  url={https://doi.org/10.1145/3275245.3275267},
  publisher={Association for Computing Machinery},
  series={SBQS},
  keywords={Survey, Literature Review, Technical Debt, Technical Debt Management, Software},
  abstract={Maintenance is often the most expensive and time-consuming of all software system lifecycle phases. Technical Debt (TD) refers to technical decisions on shortcuts and workarounds taken during the software development. It affects the internal quality of software. Therefore, software maintenance can be difficult when the TD is not perceived or managed in the projects. Despite the increasing attention of practitioners and researchers, TD studies indicate its management (TDM) is still incipient. Particularly in Brazilian Software Organizations (BSOs), there is still a lack of information on how practitioners perceive and manage the TD in software projects. This paper reports the results of two studies aiming to investigate the current knowledge level of practitioners from BSOs regarding TD and TDM, and to produce a summary of the available technologies to manage TD. To achieve these objectives, we surveyed practitioners from BSOs and undertook a quasi-Systematic Literature Review (qSLR) to gather specific TDM technologies. The survey results indicated that the general BSOs practitioners' knowledge regarding TD and TDM is still low. Few participants from the survey reported managing TD. The qSLR results provide evidence-based information about a set of TDM technologies, synthesized in evidence briefings to facilitate their use by software practitioners in the industry. Moreover, this paper offers links to a research package to aid in the replication process and support future investigations.}
}

@article{rayyan-727968232,
  title={Key concepts for a data science ethics curriculum},
  year={2018},
  issn={978-1-4503-5103-4},
  pages={952-957},
  author={Saltz, Jeffrey S and Dewar, Neil I and Heckman, Robert},
  url={https://doi.org/10.1145/3159450.3159483},
  publisher={Association for Computing Machinery},
  series={SIGCSE '18},
  keywords={big data, computing education, data science, ethics, Curriculum},
  abstract={Data science is a new field that integrates aspects of computer science, statistics and information management. As a new field, ethical issues a data scientist may encounter have received little attention to date, and ethics training within a data science curriculum has received even less attention. To address this gap, this article explores the different codes of conduct and ethics frameworks related to data science. We compare this analysis with the results of a systematic literature review focusing on ethics in data science. Our analysis identified twelve key ethics areas that should be included within a data science ethics curriculum. Our research notes that none of the existing codes or frameworks covers all of the identified themes. Data science educators and program coordinators can use our results as a way to identify key ethical concepts that can be introduced within a data science program.}
}

@article{rayyan-727968233,
  title={The brazilian challenge to accessibility and digital inclusion for people with autistic spectrum disorders},
  year={2018},
  issn={978-1-4503-6601-4},
  author={Cordeiro, Rafael F and Ferreira, Williby S and Aguiar, Yuska P C and Saraiva, Juliana A G and Tardif, Carole and Galy, Edith},
  url={https://doi.org/10.1145/3274192.3274229},
  publisher={Association for Computing Machinery},
  series={IHC 2018},
  keywords={Accessibility, Autism Spectrum Disorder, Digital Inclusion, Human Computer Interaction},
  abstract={This paper describes the outcome of a literature systematic review on methods, methodologies, and technology solutions for people with Autism Spectrum Disorder (ASD). This specific domain aroused interest in consequence of (i) the new diagnostic perspective for ASD from DSM-5; (ii) the National Policy for the Protection of the Rights of Persons with ASD (Law 12.764); and; (iii) the guidelines of the Brazilian government that highlight the importance of technology for helping and enhancing interventions with this kind of population. The final selection consisted of 20 papers showing that there is a greater study concentration where the target audience is the child with ASD, and software is the type of technology solution most used to contribute for the individual's communication and social skills development.}
}

@article{rayyan-727968234,
  title={A systematic survey of self-protecting software systems},
  year={2014},
  journal={ACM Transactions on Autonomous and Adaptive Systems},
  issn={1556-4665},
  volume={8},
  number={4},
  author={Yuan, Eric and Esfahani, Naeem and Malek, Sam},
  url={https://doi.org/10.1145/2555611},
  keywords={self-adaptive systems, adaptive security, autonomic computing, self-* properties, Self-protection, Software},
  abstract={Self-protecting software systems are a class of autonomic systems capable of detecting and mitigating security threats at runtime. They are growing in importance, as the stovepipe static methods of securing software systems have been shown to be inadequate for the challenges posed by modern software systems. Self-protection, like other self-* properties, allows the system to adapt to the changing environment through autonomic means without much human intervention, and can thereby be responsive, agile, and cost effective. While existing research has made significant progress towards autonomic and adaptive security, gaps and challenges remain. This article presents a significant extension of our preliminary study in this area. In particular, unlike our preliminary study, here we have followed a systematic literature review process, which has broadened the scope of our study and strengthened the validity of our conclusions. By proposing and applying a comprehensive taxonomy to classify and characterize the state-of-the-art research in this area, we have identified key patterns, trends and challenges in the existing approaches, which reveals a number of opportunities that will shape the focus of future research efforts.}
}

@article{rayyan-727968235,
  title={Using a functional size measurement procedure to evaluate the quality of models in MDD environments},
  year={2013},
  journal={ACM Trans. Softw. Eng. Methodol.},
  issn={1049-331X},
  volume={22},
  number={3},
  author={Marín, Beatriz and Giachetti, Giovanni and Pastor, Oscar and Vos, Tanja E J and Abran, Alain},
  url={https://doi.org/10.1145/2491509.2491520},
  keywords={Case study, functional size, defect detection, model-driven development},
  abstract={Models are key artifacts in Model-Driven Development (MDD) methods. To produce high-quality software by using MDD methods, quality assurance of models is of paramount importance. To evaluate the quality of models, defect detection is considered a suitable approach and is usually applied using reading techniques. However, these reading techniques have limitations and constraints, and new techniques are required to improve the efficiency at finding as many defects as possible. This article presents a case study that has been carried out to evaluate the use of a Functional Size Measurement (FSM) procedure in the detection of defects in models of an MDD environment. To do this, we compare the defects and the defect types found by an inspection group with the defects and the defect types found by the FSM procedure. The results indicate that the FSM is useful since it finds all the defects related to a specific defect type, it finds different defect types than an inspection group, and it finds defects related to the correctness and the consistency of the models.}
}

@article{rayyan-727968236,
  title={Cloud computing adoption assessment model (CAAM)},
  year={2011},
  issn={978-1-4503-0783-3},
  pages={34-37},
  author={Nasir, Usman and Niazi, Mahmood},
  url={https://doi.org/10.1145/2181101.2181110},
  publisher={Association for Computing Machinery},
  series={Profes '11},
  keywords={cloud computing, cloud adoption, enterprise cloud},
  abstract={Cloud Computing (Clouds) is a new trend in IT, not yet widely adopted by large-scale organisations extensive IT infrastructure and complex IT processes as they find it difficult to adopt Clouds. There are technical and socio-technical challenges that need to be addressed before adoption. There is a need for an assessment framework that would assist adoption by evaluating the organisation, its key capabilities and processes affected by Clouds. The objective of this research study is to develop a model that helps organisations in assessing and improving their readiness for Cloud Computing. Primary data in this research are user concerns and issues that are barrier in the adoption of Clouds.The primary data will be collected using Systematic Literature Review (SLR) methodology. Interviews with senior executives and IT Managers are planned to validate the findings of the SLR. The anticipated outcome of this project will be Cloud Computing Adoption Assessment Model (CAAM) that will help organisations to adopt and integrate Clouds with Enterprise IT, define new capabilities, identify their organisational readiness and provide strategies for successful adoption. The case study method will be used for evaluating CAAM in conjunction with organisations willing to adopt cloud computing. Three case studies are planned to evaluate the effectiveness of CAAM in assessing organisation's readiness for the adoption of Cloud Computing.}
}

@article{rayyan-727968237,
  title={Identifying and mitigating risks of software project management in global software development},
  year={2017},
  issn={978-1-4503-4853-9},
  pages={12-22},
  author={Chadli, Saad Yasser and Idri, Ali},
  url={https://doi.org/10.1145/3143434.3143453},
  publisher={Association for Computing Machinery},
  series={IWSM mensura '17},
  keywords={Software},
  abstract={Managing global software projects is a difficult task further complicated by the emergence of new risks inherent to the dispersion of stakeholders. Project managers of Global Software Development (GSD) projects deal with challenges related to geographical, temporal and socio-cultural distance. The aim of this paper is to identify mitigation strategies intended to counter partially or fully the effects of risk factors related to the management of GSD projects that are available in literature and update the list of risk factors proposed in a previous research. This study proposes a framework for the Software Risk Management (SRM) of GSD projects designed to help practitioners identify risk factors and alleviate their effects through a list of recommended mitigation strategies. Using a systematic literature review (SLR), 39 risk factors and 58 mitigation strategies were identified and classified using a framework inspired from Leavitt's model of organizational change. Results show that the mitigation strategies identified in this SLR target 38 out of 39 risk factors, indicating a high academic interest in resolving the challenges of managing GSD projects. Results also reveal that the list of risk factors submitted in this paper and compiled using a different set of selected studies, concurs with the list introduced in a previous research.}
}

@article{rayyan-727968238,
  title={Topic selection in industry experiments},
  year={2014},
  issn={978-1-4503-2843-2},
  pages={25-30},
  author={Misirli, Ayse Tosun and Erdogmus, Hakan and Juristo, Natalia and Dieste, Oscar},
  url={https://doi.org/10.1145/2593690.2593691},
  publisher={Association for Computing Machinery},
  series={CESI 2014},
  keywords={Industry experiments, topic elicitation, topic selection},
  abstract={This paper shares our experience with initial negotiation and topic elicitation process for conducting industry experiments in six software development organizations in Finland. The process involved interaction with company representatives in the form of both multiple group discussions and separate face-to-face meetings. Fitness criteria developed by researchers were applied to the list of generated topics to decide on a common topic. The challenges we faced include diversity of proposed topics, communication gaps, skepticism about research methods, initial disconnect between research and industry needs, and lack of prior work relationship. Lessons learned include having enough time to establish trust with partners, importance of leveraging the benefits of training and skill development that are inherent in the experimental approach, uniquely positioning the experimental approach within the landscape of other validation approaches more familiar to industrial partners, and introducing the fitness criteria early in the process.}
}

@article{rayyan-727968239,
  title={On the understanding of how to measure the benefits of behavior-driven development adoption: Preliminary literature results from a grey literature study},
  year={2020},
  issn={978-1-4503-8923-5},
  author={Couto, Thiciane and Marczak, Sabrina and Gomes, Fabio},
  url={https://doi.org/10.1145/3439961.3440000},
  publisher={Association for Computing Machinery},
  series={SBQS'20},
  keywords={Benefits, Grey Literature Review, BDD Adoption, Behavior-Driven Development, Quality Measurement},
  abstract={Behavior-Driven Development (BDD) is the integration of a ubiquitous language with Test-Driven Development and Automated Testing. From this integration, BDD supports software teams to build and deliver software. Although the perceived arguments of better results and of a more efficient development process, we still have no consolidated evidence of such benefits and how to measure them. Therefore, this long-term research aims to characterize how BDD adoption benefits can be measured. To do so, our research design includes a Multivocal Literature Review, composed of a Grey Literature to explore how industry tackles the topic and a Systematic Review to gather scientific evidences, followed of a Snowballing Review to supplement the search. Next, we will conduct empirical studies to characterize the topic from practice. This paper introduces our research and presents the results from our exploratory Grey Literature. We learned from these preliminary results that there are no models or frameworks defined to measure BDD adoption benefits but that teams indeed perceive improvements in software quality, communication, rework rates, among others. We also found that these teams also perceive that team engagement improves with the adoption of BDD and that although there is a certain cost (e.g., time and financial) involved, the investment pays off in the end. These results will inform the design of our Systematic Review and of our downstream empirical studies.}
}

@article{rayyan-727968240,
  title={Integrating ethics within machine learning courses},
  year={2019},
  journal={ACM Transactions on Computing Education},
  volume={19},
  number={4},
  author={Saltz, Jeffrey and Skirpan, Michael and Fiesler, Casey and Gorelick, Micha and Yeh, Tom and Heckman, Robert and Dewar, Neil and Beard, Nathan},
  url={https://doi.org/10.1145/3341164},
  keywords={Machine learning, education, ethics, Learning},
  abstract={This article establishes and addresses opportunities for ethics integration into Machine-learning (ML) courses. Following a survey of the history of computing ethics and the current need for ethical consideration within ML, we consider the current state of ML ethics education via an exploratory analysis of course syllabi in computing programs. The results reveal that though ethics is part of the overall educational landscape in these programs, it is not frequently a part of core technical ML courses. To help address this gap, we offer a preliminary framework, developed via a systematic literature review, of relevant ethics questions that should be addressed within an ML project. A pilot study with 85 students confirms that this framework helped them identify and articulate key ethical considerations within their ML projects. Building from this work, we also provide three example ML course modules that bring ethical thinking directly into learning core ML content. Collectively, this research demonstrates: (1) the need for ethics to be taught as integrated within ML coursework, (2) a structured set of questions useful for identifying and addressing potential issues within an ML project, and (3) novel course models that provide examples for how to practically teach ML ethics without sacrificing core course content. An additional by-product of this research is the collection and integration of recent publications in the emerging field of ML ethics education.}
}

@article{rayyan-727968241,
  title={A taxonomy for requirements engineering and software test alignment},
  year={2014},
  journal={ACM Trans. Softw. Eng. Methodol.},
  issn={1049-331X},
  volume={23},
  number={2},
  author={Unterkalmsteiner, M and Feldt, R and Gorschek, T},
  url={https://doi.org/10.1145/2523088},
  keywords={taxonomy, software process assessment, software testing, Alignment, Software},
  abstract={Requirements Engineering and Software Testing are mature areas and have seen a lot of research. Nevertheless, their interactions have been sparsely explored beyond the concept of traceability. To fill this gap, we propose a definition of requirements engineering and software test (REST) alignment, a taxonomy that characterizes the methods linking the respective areas, and a process to assess alignment. The taxonomy can support researchers to identify new opportunities for investigation, as well as practitioners to compare alignment methods and evaluate alignment, or lack thereof. We constructed the REST taxonomy by analyzing alignment methods published in literature, iteratively validating the emerging dimensions. The resulting concept of an information dyad characterizes the exchange of information required for any alignment to take place. We demonstrate use of the taxonomy by applying it on five in-depth cases and illustrate angles of analysis on a set of thirteen alignment methods. In addition, we developed an assessment framework (REST-bench), applied it in an industrial assessment, and showed that it, with a low effort, can identify opportunities to improve REST alignment. Although we expect that the taxonomy can be further refined, we believe that the information dyad is a valid and useful construct to understand alignment.}
}

@article{rayyan-727968242,
  title={Uma revisão SistemáTica sobre MéTodos de avaliação de usabilidade aplicados em software de telefones celulares},
  year={2011},
  issn={978-85-7669-257-7},
  pages={197-201},
  author={Gonçalves, Vinícius P and Neris, Vânia P A and Morandini, Marcelo and Nakagawa, Elisa Y and Ueyama, Jó},
  publisher={Brazilian Computer Society},
  series={IHC+CLIHC '11},
  keywords={dispositives móveis, métodos de avaliação, revisão sistemática, telefones celulares, usabilidade, Software},
  abstract={Many of the personal computer features were transported to the cell phones. In Brazil, the number of cell phones sold is greater than the number of landlines. However, most of the features available in cell phone software are not used. It is important that usability aspects are observed in these applications. Thus, this paper presents a systematic review of evaluation methods applied to software usability of cell phones. Three main questions were addressed in the review: Which usability evaluation methods have been applied?. What are the recurrent problems found in these evaluations? and what are the solutions?. This article presents results based on papers found in five knowledge bases. The systematic review resulted in 221 works-related which 21 were included, 93 excluded, and 107 considered repeated. The test results suggest that less than half of the evaluations consider traditional methods such as Heuristic Evaluation. The problems mentioned are related to navigation and interface elements visibility. Only 30% of the included papers indicate suggestions for solving found problems.}
}

@article{rayyan-727968243,
  title={The augmented web: Rationales, opportunities, and challenges on browser-side transcoding},
  year={2015},
  journal={ACM Transactions on the Web},
  issn={1559-1131},
  volume={9},
  number={2},
  author={Díaz, Oscar and Arellano, Cristóbal},
  url={https://doi.org/10.1145/2735633},
  keywords={adaptation, JavaScript, Personalization, transcoding},
  abstract={Today's web personalization technologies use approaches like user categorization, configuration, and customization but do not fully support individualized requirements. As a significant portion of our social and working interactions are migrating to the web, we can expect an increase in these kinds of minority requirements. Browser-side transcoding holds the promise of facilitating this aim by opening personalization to third parties through web augmentation (WA), realized in terms of extensions and userscripts. WA is to the web what augmented reality is to the physical world: to layer relevant content/layout/navigation over the existing web to improve the user experience. From this perspective, WA is not as powerful as web personalization since its scope is limited to the surface of the web. However, it permits this surface to be tuned by developers other than the sites' webmasters. This opens up the web to third parties who might come up with imaginative ways of adapting the web surface for their own purposes. Its success is backed up by millions of downloads. This work looks at this phenomenon, delving into the “what,” the “why,” and the “what for” of WA, and surveys the challenges ahead for WA to thrive. To this end, we appraise the most downloaded 45 WA extensions for Mozilla Firefox and Google Chrome as well as conduct a systematic literature review to identify what quality issues received the most attention in the literature. The aim is to raise awareness about WA as a key enabler of the personal web and point out research directions.}
}

@article{rayyan-727968244,
  title={Challenges in using open source software in product development: A review of the literature},
  year={2010},
  issn={978-1-60558-978-7},
  pages={17-22},
  author={Stol, Klaas-Jan and Ali Babar, Muhammad},
  url={https://doi.org/10.1145/1833272.1833276},
  publisher={Association for Computing Machinery},
  series={FLOSS '10},
  keywords={literature review, challenges, open source software, component-based development, Software},
  abstract={Component-Based Software Development has become a popular approach to building software intensive systems. Besides using Commercial Off-The-Shelf components, an organization may choose to use Open Source Software components. Using OSS has been reported to have many benefits, but there are also challenges involved. Understanding the potential challenges of using OSS in developing products is important for practitioners, so they become aware of them and can anticipate them and take appropriate measures to address these challenges. We have performed a thorough review of the literature to identify challenges that may arise, as reported in the literature. This paper presents and discusses these findings. Researchers can discuss potential causes and solutions of our synthesized findings as well as benefit from provided references to literature on OSS challenges as input for future research.}
}

@article{rayyan-727968245,
  title={Automatic detection of depression from text data: A systematic literacture review},
  year={2020},
  issn={978-1-4503-8873-3},
  author={Magami, Felipe and Digiampietri, Luciano Antonio},
  url={https://doi.org/10.1145/3411564.3411603},
  publisher={Association for Computing Machinery},
  series={SBSI'20},
  keywords={Sentiment analysis, Detection of depression, Social networks},
  abstract={Depression is a mental disorder that affects hundreds of millions of people worldwide, with potentially serious consequences if left without treatment. Despite that, many people still suffer from depression without a diagnosis. Recently, the amount of studies related to the automatic detection of depression has improved. The objective of this paper is to identify the methods and techniques used by studies about depression detection through text data, by conducting a systematic review.}
}

@article{rayyan-727968246,
  title={An empirical study of meta- and hyper-heuristic search for multi-objective release planning},
  year={2018},
  journal={ACM Trans. Softw. Eng. Methodol.},
  issn={1049-331X},
  volume={27},
  number={1},
  author={Zhang, Yuanyuan and Harman, Mark and Ochoa, Gabriela and Ruhe, Guenther and Brinkkemper, Sjaak},
  url={https://doi.org/10.1145/3196831},
  keywords={hyper-heuristics, meta-heuristics, Strategic release planning, Levonorgestrel},
  abstract={A variety of meta-heuristic search algorithms have been introduced for optimising software release planning. However, there has been no comprehensive empirical study of different search algorithms across multiple different real-world datasets. In this article, we present an empirical study of global, local, and hybrid meta- and hyper-heuristic search-based algorithms on 10 real-world datasets. We find that the hyper-heuristics are particularly effective. For example, the hyper-heuristic genetic algorithm significantly outperformed the other six approaches (and with high effect size) for solution quality 85% of the time, and was also faster than all others 70% of the time. Furthermore, correlation analysis reveals that it scales well as the number of requirements increases.}
}

@article{rayyan-727968247,
  title={Definindo uma abordagem para inspeção de usabilidade em modelos de projeto por meio de experimentação},
  year={2012},
  issn={978-85-7669-262-1},
  pages={165-174},
  author={Valentim, Natasha M Costa and de Oliveira, Káthia Marçal and Conte, Tayana},
  publisher={Brazilian Computer Society},
  series={IHC '12},
  keywords={usability evaluation, integration of HCI (human-computer interaction), SE (software engineering), usability of designs models},
  abstract={Inspections in the early stages of development helps to reveal problems that can be corrected at a lower cost. With this belief we decide to define a set of reading techniques specific for usability inspection in design models, called MIT (Model Inspection Technique for Usability Evaluation). To that end, we perform several experimentations that are used not only to evaluate the proposed techniques but also to better define the inspections procedures in a way that they can be really useful and ease of use. This paper presents the first cycle of definition and experimentation of MIT discussing the results and the refinement of the initial set of techniques.}
}

@article{rayyan-727968248,
  title={Software process improvement: Where is the evidence?: Initial findings from a systematic mapping study},
  year={2015},
  issn={978-1-4503-3346-7},
  pages={107-116},
  author={Kuhrmann, Marco and Konopka, Claudia and Nellemann, Peter and Diebold, Philipp and Münch, Jürgen},
  url={https://doi.org/10.1145/2785592.2785600},
  publisher={Association for Computing Machinery},
  series={ICSSP 2015},
  keywords={systematic mapping study, software process improvement, software process, Software},
  abstract={Software process improvement (SPI) is around for decades: frameworks are proposed, success factors are studied, and experiences have been reported. However, the sheer mass of concepts, approaches, and standards published over the years overwhelms practitioners as well as researchers. What is out there? Are there new emerging approaches? What are open issues? Still, we struggle to answer the question for what is the current state of SPI and related research? In this paper, we present initial results from a systematic mapping study to shed light on the field of SPI and to draw conclusions for future research directions. An analysis of 635 publications draws a big picture of SPI-related research of the past 25 years. Our study shows a high number of solution proposals, experience reports, and secondary studies, but only few theories. In particular, standard SPI models like CMMI and ISO/IEC 15504 are analyzed, enhanced, and evaluated for applicability, whereas these standards are critically discussed from the perspective of SPI in small-to-medium-sized companies, which leads to new specialized frameworks. Furthermore, we find a growing interest in success factors to aid companies in conducting SPI.}
}

@article{rayyan-727968249,
  title={Requirements for avatar in virtual environment of support learning in the literacy of deaf people in portuguese mediated by LIBRAS},
  year={2018},
  issn={978-1-4503-6601-4},
  author={Ferreira, Marta Angélica Montiel and García, Laura Sánchez},
  url={https://doi.org/10.1145/3274192.3274241},
  publisher={Association for Computing Machinery},
  series={IHC 2018},
  keywords={Avatar, Deaf, Libras, Sign language, Virtual environment, Deafness},
  abstract={This work describes a process of Systematic Review of Literature, developed to collect requirements for the development of avatar to be used in Virtual Environment of Learning applied to the literacy of deaf people. 23 articles were selected on national and international bases, from which it was possible to obtain a preliminary result of the aspects necessary for the avatar, as well as essential requirements for the virtual environment. It was also possible to verify which countries are conducting research on the subject. The earliest returns pointed requirements of the avatar itself and the environment of support for literacy. It can be verified that the use of avatar is well accepted by the deaf users. The review also served to check the key technologies used. However, the survey confirmed that a suitable avatar will not suffice for the smooth functioning of an AVA with weaknesses in the interface or interaction.}
}

@article{rayyan-727968250,
  title={UML-Driven automated software deployment},
  year={2018},
  pages={257-268},
  author={Rivera, Luis F and Villegas, Norha M and Tamura, Gabriel and Jiménez, Miguel and Müller, Hausi A},
  publisher={IBM Corp.},
  series={CASCON '18},
  keywords={⛔ No DOI found, UML, DevOps, continuous delivery, model-driven engineering, deployment, model-driven architecture, Software, Unified Medical Language System},
  abstract={Software companies face the challenge of ensuring customer satisfaction through the continuous delivery of functionalities and rapid response to quality issues. However, achieving frequent software delivery is not a trivial task. It requires agile—and continuous—design, development and deployment of existing and new software features. Over time, managing these systems becomes increasingly complex. This complexity stems, in part, from the deployment pipelines and the myriad possible configurations of the software components. Furthermore, software deployment is a time-consuming and error-prone process, which, even when automated, can lead to configuration errors and cost overruns. In this paper, we address deployment challenges that developers face during continuous delivery and DevOps. Our proposal consists of Urano, a mechanism for automating the deployment process, which uses UML, an interoperable and de facto modeling standard, as a means of specifying a software architecture and its associated deployment. Our approach is based on the model-driven architecture principles to generate executable deployment specifications from user-defined UML deployment diagrams. We extend this kind of diagrams by defining and applying a UML profile that captures the semantics and requirements of the installation, configuration, and update of software components. Thus, enabling more expressive deployment specifications and their automatic realization. To evaluate Urano, we conducted three case studies that demonstrate its potential to effectively automate software deployment processes in industry.}
}

@article{rayyan-727968251,
  title={A prescriptive software process for academic scenarios},
  year={2015},
  issn={978-1-4503-3630-7},
  pages={265-266},
  author={Marques, Maíra},
  url={https://doi.org/10.1145/2787622.2787743},
  publisher={Association for Computing Machinery},
  series={ICER '15},
  keywords={software engineering education, software process, Prescriptions, Software},
  abstract={Software engineering has been taught over the years using expositive classes, but learning this discipline requires more than just theory. Lately, universities started teaching software engineering in a theoretical-practical way. Typically, these experiences involve the development of software projects in the academia, using industry-oriented or ad hoc processes. However, most of them are not fully formalized, therefore it is not possible to repeat, measure or improve them. Moreover, it is not clear which project contexts are proper for each ad hoc process. This thesis work proposes to define and formalize a prescriptive software development process for use in undergraduate software engineering courses. We hypothesize that this process can make these experiences repeatable investing an affordable effort and will help produce positive results in student projects. This software process will be evaluated in the context of a second software engineering undergraduate course at the University of Chile, and it will address specific project contexts. This software process could be used in any university that has a similar course context. The proposed process can also be adapted to fit with new contexts.}
}

@article{rayyan-727968252,
  title={Service-oriented product lines: A systematic mapping study},
  year={2014},
  journal={SIGSOFT Softw. Eng. Notes},
  issn={0163-5948},
  volume={39},
  number={2},
  pages={1-6},
  author={Castelluccia, Daniela and Boffoli, Nicola},
  url={https://doi.org/10.1145/2579281.2579294},
  keywords={mapping study, empirical study, software product line, product line development, service-oriented architecture, service-oriented computing, variability management},
  abstract={Software product line engineering and service-oriented architectures both enable organizations to capitalize on reuse of existing software assets and capabilities and improve competitive advantage in terms of development savings, product flexibility, time-to-market. Both approaches accommodate variation of assets, including services, by changing the software being reused or composing services according a new orchestration. Therefore, variability management in Service-oriented Product Lines (SoPL) is one of the main challenges today. In order to highlight the emerging evidence-based results from the research community, we apply the well-defined method of systematic mapping in order to populate a classification scheme for the SoPL field of interest. The analysis of results throws light on the current open issues. Moreover, different facets of the scheme can be combined to answer more specific research questions. The report reveals the need for more empirical research able to provide new metrics measuring efficiency and efficacy of the proposed models, new methods and tools supporting variability management in SoPL, especially during maintenance and verification and validation. The mapping study about SoPL opens further investigations by means of a complete systematic review to select and validate the most efficient solutions to variability management in SoPL.}
}

@article{rayyan-727968253,
  title={Maximising the information gained from an experimental analysis of code inspection and static analysis for concurrent java components},
  year={2006},
  issn={1-59593-218-6},
  pages={174-183},
  author={Wojcicki, Margaret A and Strooper, Paul},
  url={https://doi.org/10.1145/1159733.1159761},
  publisher={Association for Computing Machinery},
  series={ISESE '06},
  keywords={controlled experiment, concurrent Java components, verification and validation},
  abstract={The results of empirical studies are limited to particular contexts, difficult to generalise and the studies themselves are expensive to perform. Despite these problems, empirical studies in software engineering can be made effective and they are important to both researchers and practitioners. The key to their effectiveness lies in the maximisation of the information that can be gained by examining existing studies, conducting power analyses for an accurate minimum sample size and benefiting from previous studies through replication. This approach was applied in a controlled experiment examining the combination of automated static analysis tools and code inspection in the context of verification and validation (V&V) of concurrent Java components. The combination of these V&V technologies was shown to be cost-effective despite the size of the study, which thus contributes to research in V&V technology evaluation.}
}

@article{rayyan-727968254,
  title={Why are industrial agile teams using metrics and how do they use them?},
  year={2014},
  issn={978-1-4503-2854-8},
  pages={23-29},
  author={Kupiainen, Eetu and Mäntylä, Mika V and Itkonen, Juha},
  url={https://doi.org/10.1145/2593868.2593873},
  publisher={Association for Computing Machinery},
  series={WETSoM 2014},
  keywords={Agile software development, metrics, measurement, Metronidazole},
  abstract={Agile development methods are increasing in popularity, yet there are limited studies on the reasons and use of metrics in industrial agile development. This paper presents preliminary results from a systematic literature review. Based on our study, metrics and their use are focused to the following areas: Iteration planning, Iteration tracking, Motivating and improving, Identifying process problems, Pre-release quality, Post-release quality and Changes in processes or tools. The findings are mapped against agile principles and it seems that the use of metrics supports the principles with some deviations. Surprisingly, we find little evidence of the use of code metrics. Also, we note that there is a lot of evidence on the use of planning and tracking metrics. Finally, the use of metrics to motivate and enforce process improvements as well as applicable quality metrics can be interesting future research topics.}
}

@article{rayyan-727968255,
  title={A systematic map on verifying and validating software process simulation models},
  year={2017},
  issn={978-1-4503-5270-3},
  pages={50-59},
  author={Gong, Haojie and Zhang, He and Yu, Dexian and Liu, Bohan},
  url={https://doi.org/10.1145/3084100.3084106},
  publisher={Association for Computing Machinery},
  series={ICSSP 2017},
  keywords={systematic mapping study, model verification and validation, process modeling, process simulation, Software Validation, Software},
  abstract={Verification and Validation (V&V) is a critical step in software process modelling to secure the model's quality and credibility. Software Process Simulation Models (SPSMs) that are based on descriptive process models offer the executability that is able to demonstrate the dynamic changes of software process over time. The V&V of process simulation models go beyond static process models and turn to be more complex and challenging to software modelers. This study aims to identify what aspects of process simulation models are verified and validated by using which V&V methods in what conditions in software engineering research. We conducted a systematic literature review (mapping study) on the studies of software process simulation that report of their V&V activities. We identified 72 relevant studies from a pool of 331 papers on SPSM until 2015. These studies can be mapped to ten V&V methods applied for five aspects of process models to be verified and validated, i.e., syntactic quality, semantic quality, pragmatic quality, performance, and value. A systematic map is presented to illustrate the relationships between the identified V&V methods and their supporting aspects of process models. This mapping will provide the community reference value when developing, verifying, and validating software process (simulation) models.}
}

@article{rayyan-727968256,
  title={Towards improving the organization of hybrid development approaches},
  year={2020},
  issn={978-1-4503-7512-2},
  pages={185-188},
  author={Prenner, Nils},
  url={https://doi.org/10.1145/3379177.3390304},
  publisher={Association for Computing Machinery},
  series={ICSSP '20},
  keywords={agile software development, software process, Hybrid software development, plan-based software development},
  abstract={Agile methods were proposed to address the problems of traditional or plan-based software development, e.g., late customer feedback or resistance to change. However, unlike plan-based methods, they are not designed for long-term planning or to cope with large projects. Software companies want the ability of a fast reaction to changes but also the ability of long-term planning. To profit from the strength of both approaches, software companies often use a combination of agile and plan-based methods, called hybrid development approaches. These approaches depend on the respective context of each company. Therefore, the companies have to properly arrange and connect the phases, activities, roles, and artifacts from plan-based and agile approaches individually in their hybrid development approach. This is considered as the organization of hybrid development approaches. However, the organization of hybrid approaches is often difficult for the companies. Until now, research considers only the chosen development methods without the organization of hybrid development approaches. With my work, I want to strengthen the understanding of how hybrid approaches are organized and get a detailed picture of the challenges when organizing hybrid approaches. Based on my findings, I want to develop measures to support practitioners while organizing their development approach.}
}

@article{rayyan-727968257,
  title={Quest for the golden approach: An experimental evaluation of duplicate crowdtesting reports detection},
  year={2020},
  issn={978-1-4503-7580-1},
  author={Huang, Yuekai and Wang, Junjie and Wang, Song and Liu, Zhe and Hu, Yuanzhe and Wang, Qing},
  url={https://doi.org/10.1145/3382494.3410694},
  publisher={Association for Computing Machinery},
  series={ESEM '20},
  keywords={information retrieval, machine learning, deep learning, Crowdtesting, duplicate detection},
  abstract={Background: Given the invisibility and unpredictability of distributed crowdtesting processes, there is a large number of duplicate reports, and detecting these duplicate reports is an important task to help save testing effort. Although, many approaches have been proposed to automatically detect the duplicates, the comparison among them and the practical guidelines to adopt these approaches in crowdtesting remain vague.Aims: We aim at conducting the first experimental evaluation of the commonly-used and state-of-the-art approaches for duplicate detection in crowdtesting reports, and exploring which is the golden approach.Method: We begin with a systematic review of approaches for duplicate detection, and select ten state-of-the-art approaches for our experimental evaluation. We conduct duplicate detection with each approach on 414 crowdtesting projects with 59,289 reports collected from one of the largest crowdtesting platforms.Results: Machine learning based approach, i.e., ML-REP, and deep learning based approach, i.e., DL-BiMPM, are the best two approaches for duplicate reports detection in crowdtesting, while the later one is more sensitive to the size of training data and more time-consuming for model training and prediction.Conclusions: This paper provides new insights and guidelines to select appropriate duplicate detection techniques for duplicate crowdtesting reports detection.}
}

@article{rayyan-727968258,
  title={Model-driven performance engineering of self-adaptive systems: A survey},
  year={2012},
  issn={978-1-4503-1346-9},
  pages={117-122},
  author={Becker, Matthias and Luckey, Markus and Becker, Steffen},
  url={https://doi.org/10.1145/2304696.2304716},
  publisher={Association for Computing Machinery},
  series={QoSA '12},
  keywords={model-driven performance engineering, self-*, self-adaptation, software performance},
  abstract={To meet quality-of-service requirements in changing environments, modern software systems adapt themselves. The structure, and correspondingly the behavior, of these systems undergoes continuous change. Model-driven performance engineering, however, assumes static system structures, behavior, and deployment. Hence, self-adaptive systems pose new challenges to model-driven performance engineering. There are a few surveys on self-adaptive systems, performance engineering, and the combination of both in the literature. In contrast to existing work, here we focus on model-driven performance analysis approaches. Based on a systematic literature review, we present a classification, identify open issues, and outline further research.}
}

@article{rayyan-727968259,
  title={Towards the use of machine learning algorithms to enhance the effectiveness of search strings in secondary studies},
  year={2019},
  issn={978-1-4503-7651-8},
  pages={22-26},
  author={Cairo, Leonardo and de F. Carneiro, Glauco and Monteiro, Miguel P and e Abreu, Fernando Brito},
  url={https://doi.org/10.1145/3350768.3350772},
  publisher={Association for Computing Machinery},
  series={SBES 2019},
  keywords={machine learning, natural language processing, secondary studies, Algorithms, Learning},
  abstract={Devising an appropriate Search String for a secondary study is not a trivial task and identifying suitable keywords has been reported in the literature as a difficulty faced by researchers. A poorly chosen Search String may compromise the quality of the secondary study, by missing relevant studies or leading to overwork in subsequent steps of the secondary study, in case irrelevant studies are selected. In this paper, we propose an approach for the creation and calibration of a Search String. We chose three published systematic literature reviews (SLRs) from Scopus and applied Machine Learning algorithms to create the corresponding Search Strings to be used in the SLRs. Comparison of results obtained with those published in previous SLRs, show an increase of recall of revisions by up to 12%, with no loss of recall. To motivate future studies and replications, the tool implementing the proposed approach is available in a public repository, along with the dataset used in this paper.}
}

@article{rayyan-727968260,
  title={Bayesian concepts in software testing: An initial review},
  year={2015},
  issn={978-1-4503-3813-4},
  pages={41-46},
  author={Rodriguez, Daniel and Dolado, Javier and Tuya, Javier},
  url={https://doi.org/10.1145/2804322.2804329},
  publisher={Association for Computing Machinery},
  series={A-test 2015},
  keywords={Bayesian networks, software testing, Bayesian statistics, probabilistic graphical models, Software},
  abstract={This work summarizes the main topics that have been researched in the area of software testing under the umbrella of “Bayesian approaches” since 2010. There is a growing trend on the use of the so-called Bayesian statistics and Bayesian concepts in general and software testing in particular. Following a Systematic Literature Review protocol using the main digital libraries and repositories, we selected around 40 references applying Bayesian approaches in the field of software testing since 2010. Those references summarise the current state of the art and foster better focused research. So far, the main observed use of the Bayesian concepts in the software testing field is through the application of Bayesian networks for software reliability and defect prediction (the latter is mainly based on static software metrics and Bayesian classifiers). Other areas of application are software estimation and test data generation. There are areas not fully explored beyond the basic Bayesian approaches, such as influence diagrams and dynamic networks.}
}

@article{rayyan-727968261,
  title={A taxonomy of metrics for software fault prediction},
  year={2019},
  issn={978-1-4503-5572-8},
  pages={1144-1147},
  author={Caulo, Maria},
  url={https://doi.org/10.1145/3338906.3341462},
  publisher={Association for Computing Machinery},
  series={ESEC/FSE 2019},
  keywords={taxonomy, software fault prediction, software metrics, Metronidazole, Software},
  abstract={In the field of Software Fault Prediction (SFP), researchers exploit software metrics to build predictive models using machine learning and/or statistical techniques. SFP has existed for several decades and the number of metrics used has increased dramatically. Thus, the need for a taxonomy of metrics for SFP arises firstly to standardize the lexicon used in this field so that the communication among researchers is simplified and then to organize and systematically classify the used metrics. In this doctoral symposium paper, I present my ongoing work which aims not only to build such a taxonomy as comprehensive as possible, but also to provide a global understanding of the metrics for SFP in terms of detailed information: acronym(s), extended name, univocal description, granularity of the fault prediction (e.g., method and class), category, and research papers in which they were used.}
}

@article{rayyan-727968262,
  title={A systematic mapping study on intrusion alert analysis in intrusion detection systems},
  year={2018},
  journal={ACM Computing Surveys},
  issn={0360-0300},
  volume={51},
  number={3},
  author={Ramaki, Ali Ahmadian and Rasoolzadegan, Abbas and Bafghi, Abbas Ghaemi},
  url={https://doi.org/10.1145/3184898},
  keywords={systematic review, Network security, alert correlation, intrusion alert analysis, systematic mapping study (SMS)},
  abstract={Intrusion alert analysis is an attractive and active topic in the area of intrusion detection systems. In recent decades, many research communities have been working in this field. The main objective of this article is to achieve a taxonomy of research fields in intrusion alert analysis by using a systematic mapping study of 468 high-quality papers. The results show that there are 10 different research topics in the field, which can be classified into three broad groups: pre-processing, processing, and post-processing. The processing group contains most of the research works, and the post-processing group is newer than others.}
}

@article{rayyan-727968263,
  title={Understanding the knowledge gaps of software engineers: An empirical analysis based on SWEBOK},
  year={2019},
  journal={ACM Transactions on Computing Education},
  volume={20},
  number={1},
  author={Garousi, Vahid and Giray, Gorkem and Tuzun, Eray},
  url={https://doi.org/10.1145/3360497},
  keywords={Software engineering education, empirical study, education research, knowledge gaps, opinion survey, skill gaps, Software},
  abstract={Context: Knowledge level and productivity of the software engineering (SE) workforce are the subject of regular discussions among practitioners, educators, and researchers. There have been many efforts to measure and improve the knowledge gap between SE education and industrial needs.Objective: Although the existing efforts for aligning SE education and industrial needs have provided valuable insights, there is a need for analyzing the SE topics in a more “fine-grained” manner; i.e., knowing that SE university graduates should know more about requirements engineering is important, but it is more valuable to know the exact topics of requirements engineering that are most important in the industry.Method: We achieve the above objective by assessing the knowledge gaps of software engineers by designing and executing an opinion survey on levels of knowledge learned in universities versus skills needed in industry. We designed the survey by using the SE knowledge areas (KAs) from the latest version of the Software Engineering Body of Knowledge (SWEBOK v3), which classifies the SE knowledge into 12 KAs, which are themselves broken down into 67 subareas (sub-KAs) in total. Our analysis is based on (opinion) data gathered from 129 practitioners, who are mostly based in Turkey.Results: Based on our findings, we recommend that educators should include more materials on software maintenance, software configuration management, and testing in their SE curriculum. Based on the literature as well as the current trends in industry, we provide actionable suggestions to improve SE curriculum to decrease the knowledge gap.}
}

@article{rayyan-727968264,
  title={Cloud testing framework},
  year={2013},
  issn={978-1-4503-1848-8},
  pages={252-255},
  author={Neto, C R Lima and Garcia, V C},
  url={https://doi.org/10.1145/2460999.2461037},
  publisher={Association for Computing Machinery},
  series={EASE '13},
  keywords={cloud computing, software testing, cloud testing, testing tools, domain-specific languages, DSL},
  abstract={Context: Cloud-based testing introduces a new set of challenges. While many companies are approaching cloud computing with cautious, testing appears to be next area of growing interest. Objective: This paper propose the development of a new framework focusing on cloud testing. We intend to define the activities, roles, techniques, and tools that will be used in an integrated framework. Method: A research methodology containing all the steps that will be executed during the proposal implementation was defined. Results: A schedule was suggested in order to allow the research development. Conclusion: It is concluded that is important to establish the standardization for the cloud environment in order to ensure the cloud applications quality.}
}

@article{rayyan-727968265,
  title={The state of empirical evaluation in static feature location},
  year={2018},
  journal={ACM Trans. Softw. Eng. Methodol.},
  issn={1049-331X},
  volume={28},
  number={1},
  author={Razzaq, Abdul and Wasala, Asanka and Exton, Chris and Buckley, Jim},
  url={https://doi.org/10.1145/3280988},
  keywords={bug location, concept location, Feature location, requirement traceability},
  abstract={Feature location (FL) is the task of finding the source code that implements a specific, user-observable functionality in a software system. It plays a key role in many software maintenance tasks and a wide variety of Feature Location Techniques (FLTs), which rely on source code structure or textual analysis, have been proposed by researchers. As FLTs evolve and more novel FLTs are introduced, it is important to perform comparison studies to investigate “Which are the best FLTs?” However, an initial reading of the literature suggests that performing such comparisons would be an arduous process, based on the large number of techniques to be compared, the heterogeneous nature of the empirical designs, and the lack of transparency in the literature. This article presents a systematic review of 170 FLT articles, published between the years 2000 and 2015. Results of the systematic review indicate that 95% of the articles studied are directed towards novelty, in that they propose a novel FLT. Sixty-nine percent of these novel FLTs are evaluated through standard empirical methods but, of those, only 9% use baseline technique(s) in their evaluations to allow cross comparison with other techniques. The heterogeneity of empirical evaluation is also clearly apparent: altogether, over 60 different FLT evaluation metrics are used across the 170 articles, 272 subject systems have been used, and 235 different benchmarks employed. The review also identifies numerous user input formats as contributing to the heterogeneity. Analysis of the existing research also suggests that only 27% of the FLTs presented might be reproduced from the published material. These findings suggest that comparison across the existing body of FLT evaluations is very difficult. We conclude by providing guidelines for empirical evaluation of FLTs that may ultimately help to standardise empirical research in the field, cognisant of FLTs with different goals, leveraging common practices in existing empirical evaluations and allied with rationalisations. This is seen as a step towards standardising evaluation in the field, thus facilitating comparison across FLTs.}
}

@article{rayyan-727968266,
  title={Where do we stand in requirements engineering improvement today? First results from a mapping study},
  year={2014},
  issn={978-1-4503-2774-9},
  author={Fernández, Daniel Méndez and Ognawala, Saahil and Wagner, Stefan and Daneva, Maya},
  url={https://doi.org/10.1145/2652524.2652555},
  publisher={Association for Computing Machinery},
  series={ESEM '14},
  keywords={systematic mapping study, requirements engineering, software process improvement},
  abstract={Context: Requirements engineering process improvement (REPI) approaches have gained much attention in research and practice. Goal: So far, there is no comprehensive view on the research in REPI in terms of solutions and current state of reported evidence. We aims to provide an overview on the existing solutions, their underlying principles and their research type facets, i.e. their state of empirical evidence. Method: To this end, we conducted a systematic mapping study of the REPI publication space. Results: This paper reports on the first findings regarding research type facets of the contributions as well as selected methodological principles. We found a strong focus in the existing research on solution proposals for REPI approaches that concentrate on normative assessments and benchmarks of the RE activities rather than on holistic RE improvements according to individual goals of companies. Conclusions: We conclude, so far, that there is a need to broaden the work and to investigate more problem-driven REPI which also targets the improvement of the quality of the underlying RE artefacts, which currently seem out of scope.}
}

@article{rayyan-727968267,
  title={A self-assessment instrument for assessing test automation maturity},
  year={2019},
  issn={978-1-4503-7145-2},
  pages={145-154},
  author={Wang, Yuqing and Mäntylä, Mika and Eldh, Sigrid and Markkula, Jouni and Wiklund, Kristian and Kairi, Tatu and Raulamo-Jurvanen, Päivi and Haukinen, Antti},
  url={https://doi.org/10.1145/3319008.3319020},
  publisher={Association for Computing Machinery},
  series={EASE '19},
  keywords={maturity, test automation, assessment, content validity, instrument},
  abstract={Test automation is important in the software industry but self-assessment instruments for assessing its maturity are not sufficient. The two objectives of this study are to synthesize what an organization should focus to assess its test automation; develop a self-assessment instrument (a survey) for assessing test automation maturity and scientifically evaluate it. We carried out the study in four stages. First, a literature review of 25 sources was conducted. Second, the initial instrument was developed. Third, seven experts from five companies evaluated the initial instrument. Content Validity Index and Cognitive Interview methods were used. Fourth, we revised the developed instrument. Our contributions are as follows: (a) we collected practices mapped into 15 key areas that indicate where an organization should focus to assess its test automation; (b) we developed and evaluated a self-assessment instrument for assessing test automation maturity; (c) we discuss important topics such as response bias that threatens self-assessment instruments. Our results help companies and researchers to understand and improve test automation practices and processes.}
}

@article{rayyan-727968268,
  title={Adapting a fault prediction model to allow inter languagereuse},
  year={2008},
  issn={978-1-60558-036-4},
  pages={19-24},
  author={Watanabe, Shinya and Kaiya, Haruhiko and Kaijiri, Kenji},
  url={https://doi.org/10.1145/1370788.1370794},
  publisher={Association for Computing Machinery},
  series={PROMISE '08},
  keywords={metrics, open source, error prone, inter language prediction},
  abstract={An important step in predicting error prone modules in a project is to construct the prediction model by using training data of that project, but the resulting prediction model depends on the training data. Therefore it is difficult to apply the model to other projects. The training data consists of metrics data and bug data, and these data should be prepared for each project. Metrics data can be computed by using metric tools, but it is not so easy to collect bug data. In this paper, we try to reuse the generated prediction model. By using the metrics and bug data which are computed from C++ and Java projects, we have evaluated the possibility of applying the prediction model, which is generated based on one project, to other projects, and have proposed compensation techniques for applying to other projects. We showed the evaluation result based on open source projects.}
}

@article{rayyan-727968269,
  title={Experiences from using snowballing and database searches in systematic literature studies},
  year={2015},
  issn={978-1-4503-3350-4},
  author={Badampudi, Deepika and Wohlin, Claes and Petersen, Kai},
  url={https://doi.org/10.1145/2745802.2745818},
  publisher={Association for Computing Machinery},
  series={EASE '15},
  keywords={snowballing, reliability, efficiency, BSB- backward snowballing, database search, DB- database, FSB- forward snowballing, SB- snowballing},
  abstract={Background: Systematic literature studies are commonly used in software engineering. There are two main ways of conducting the searches for these type of studies; they are snowballing and database searches. In snowballing, the reference list (backward snowballing - BSB) and citations (forward snowballing - FSB) of relevant papers are reviewed to identify new papers whereas in a database search, different databases are searched using predefined search strings to identify new papers. Objective: Snowballing has not been in use as extensively as database search. Hence it is important to evaluate its efficiency and reliability when being used as a search strategy in literature studies. Moreover, it is important to compare it to database searches. Method: In this paper, we applied snowballing in a literature study, and reflected on the outcome. We also compared database search with backward and forward snowballing. Database search and snowballing were conducted independently by different researchers. The searches of our literature study were compared with respect to the efficiency and reliability of the findings. Results: Out of the total number of papers found, snowballing identified 83% of the papers in comparison to 46% of the papers for the database search. Snowballing failed to identify a few relevant papers, which potentially could have been addressed by identifying a more comprehensive start set. Conclusion: The efficiency of snowballing is comparable to database search. It can potentially be more reliable than a database search however, the reliability is highly dependent on the creation of a suitable start set.}
}

@article{rayyan-727968270,
  title={A preliminary structure of software outsourcing vendors' readiness model},
  year={2010},
  issn={978-1-4503-0281-4},
  pages={76-79},
  author={Khan, Siffat Ullah and Niazi, Mahmood},
  url={https://doi.org/10.1145/1961258.1961277},
  publisher={Association for Computing Machinery},
  series={PROFES '10},
  keywords={outsourcing, readiness, offshore software outsourcing, vendors' readiness, Software},
  abstract={CONTEXT – Offshore software development outsourcing is a contractual business of high quality software production at offshore destinations with significant cost-saving. Vendor's readiness plays an important role in the successful outcomes of outsourcing projects.OBJECTIVE – The objective of this paper is to describe the preliminary structure of software outsourcing vendors' readiness model (SOVRM).METHOD – In order to develop SOVRM, we have performed systematic literature review (SLR) to identify critical success factors (CSFs) and critical barriers (CBs). To validate SLR findings and to find practices for the identified CSFs and CBs a questionnaire survey was conducted in the outsourcing industry. Case study approach was used for the evaluation of SOVRM.RESULTS – The SOVRM has been developed to assist software development outsourcing organisations in measuring and improving their outsourcing readiness prior to start outsourcing activities.CONCLUSIONS – SOVRM is a useful tool for offshore software development outsourcing vendors in assessing their readiness for offshore outsourcing activities. Vendors should address each CSFs and CBs in order to achieve a certain SOVRM level.}
}

@article{rayyan-727968271,
  title={How to make best use of cross-company data in software effort estimation?},
  year={2014},
  issn={978-1-4503-2756-5},
  pages={446-456},
  author={Minku, Leandro L and Yao, Xin},
  url={https://doi.org/10.1145/2568225.2568228},
  publisher={Association for Computing Machinery},
  series={ICSE 2014},
  keywords={Software effort estimation, cross-company learning, online learning, ensembles of learning machines, transfer learning, Software},
  abstract={Previous works using Cross-Company (CC) data for making Within-Company (WC) Software Effort Estimation (SEE) try to use CC data or models directly to provide predictions in the WC context. So, these data or models are only helpful when they match the WC context well. When they do not, a fair amount of WC training data, which are usually expensive to acquire, are still necessary to achieve good performance. We investigate how to make best use of CC data, so that we can reduce the amount of WC data while maintaining or improving performance in comparison to WC SEE models. This is done by proposing a new framework to learn the relationship between CC and WC projects explicitly, allowing CC models to be mapped to the WC context. Such mapped models can be useful even when the CC models themselves do not match the WC context directly. Our study shows that a new approach instantiating this framework is able not only to use substantially less WC data than a corresponding WC model, but also to achieve similar/better performance. This approach can also be used to provide insight into the behaviour of a company in comparison to others.}
}

@article{rayyan-727968272,
  title={Validating software measures using action research a method and industrial experiences},
  year={2016},
  issn={978-1-4503-3691-8},
  author={Antinyan, Vard and Staron, Miroslaw and Sandberg, Anna and Hansson, Jörgen},
  url={https://doi.org/10.1145/2915970.2916001},
  publisher={Association for Computing Machinery},
  series={EASE '16},
  keywords={validation, action research, software measure, Health Services Research, Software, Software Validation},
  abstract={Validating software measures for using them in practice is a challenging task. Usually more than one complementary validation methods are applied for rigorously validating software measures: Theoretical methods help with defining the measures with expected properties and empirical methods help with evaluating the predictive power of measures. Despite the variety of these methods there still remain cases when the validation of measures is difficult. Particularly when the response variables of interest are not accurately measurable and the practical context cannot be reduced to an experimental setup the abovementioned methods are not effective. In this paper we present a complementary empirical method for validating measures. The method relies on action research principles and is meant to be used in combination with theoretical validation methods. The industrial experiences documented in this paper show that in many practical cases the method is effective.}
}

@article{rayyan-727968273,
  title={Coupling and cohesion metrics for object-oriented software: A systematic mapping study},
  year={2018},
  issn={978-1-4503-6398-3},
  author={Tiwari, Saurabh and Rathore, Santosh Singh},
  url={https://doi.org/10.1145/3172871.3172878},
  publisher={Association for Computing Machinery},
  series={ISEC '18},
  keywords={systematic mapping, Coupling/cohesion metrics, metrics evolution, Software, Metronidazole},
  abstract={Coupling and Cohesion are two fundamental concepts that can be applied to design better modular object-oriented software. This study aims at reviewing existing research on coupling and cohesion metrics in order to identify the potential ones and needs for the future research. A systematic mapping study is presented to identify the popular coupling and cohesion metrics, and their applicability in practice. A total of 137 papers were found and classified into four different classes- evolution of coupling and cohesion metrics, research type, contribution, and context focus. Our study revealed that the significance of coupling and cohesion metrics in various software development activities has been advocated by various researchers. However, some issues such as the lack of availability of information about the contextual usages of these metrics and their multiple interpretations by different researchers need to be resolved to establish the practical use of these metrics.}
}

@article{rayyan-727968274,
  title={An empirical validation of cognitive complexity as a measure of source code understandability},
  year={2020},
  issn={978-1-4503-7580-1},
  author={Muñoz Barón, Marvin and Wyrich, Marvin and Wagner, Stefan},
  url={https://doi.org/10.1145/3382494.3410636},
  publisher={Association for Computing Machinery},
  series={ESEM '20},
  keywords={meta-analysis, software metrics, cognitive complexity, source code comprehension, source code understandability, Cognition},
  abstract={Background: Developers spend a lot of their time on understanding source code. Static code analysis tools can draw attention to code that is difficult for developers to understand. However, most of the findings are based on non-validated metrics, which can lead to confusion and code that is hard to understand not being identified.Aims: In this work, we validate a metric called Cognitive Complexity which was explicitly designed to measure code understandability and which is already widely used due to its integration in well-known static code analysis tools.Method: We conducted a systematic literature search to obtain data sets from studies which measured code understandability. This way we obtained about 24,000 understandability evaluations of 427 code snippets. We calculated the correlations of these measurements with the corresponding metric values and statistically summarized the correlation coefficients through a meta-analysis.Results: Cognitive Complexity positively correlates with comprehension time and subjective ratings of understandability. The metric showed mixed results for the correlation with the correctness of comprehension tasks and with physiological measures.Conclusions: It is the first validated and solely code-based metric which is able to reflect at least some aspects of code understandability. Moreover, due to its methodology, this work shows that code understanding is currently measured in many different ways, which we also do not know how they are related. This makes it difficult to compare the results of individual studies as well as to develop a metric that measures code understanding in all its facets.}
}

@article{rayyan-727968275,
  title={Usability and user EXperience evaluation of conversational systems: A systematic mapping study},
  year={2020},
  issn={978-1-4503-8753-8},
  pages={427-436},
  author={Guerino, Guilherme Corredato and Valentim, Natasha Malveira Costa},
  url={https://doi.org/10.1145/3422392.3422421},
  publisher={Association for Computing Machinery},
  series={SBES '20},
  keywords={Systematic Mapping Study, Usability, User eXperience, Software Quality, Conversational Systems},
  abstract={Conversational Systems (CSs) are software that uses the user's voice to perform some action. Before these systems were available to users, they must be evaluated. In this sense, Usability and User eXperience (UX) evaluations contribute to the verification of software quality, since they evaluate several aspects such as efficiency, effectiveness, immersion, and user satisfaction. Therefore, the goal of our Systematic Mapping Study (SMS) is to identify the evaluation technologies (methods, techniques, models, among others) used by researchers and professionals to evaluate Usability and UX of CSs. We selected 39 papers for data extraction and, based on these works, we identified 31 different evaluation technologies. Besides, our SMS extracted the characteristics of technologies, CSs, and empirical studies described in the papers. Our results identify a lack of evaluation technologies of CSs that unite the concepts of Usability and UX and undergo empirical evaluations. Moreover, we observed researchers tend to create their questionnaires according to the needs of the study. Overall, our SMS presents data about the researched topic, describing the gaps, and contributing to the scientific community that evaluates Usability and UX of CSs.}
}

@article{rayyan-727968276,
  title={A literature review of automatic traceability links recovery for software change impact analysis},
  year={2020},
  issn={978-1-4503-7958-8},
  pages={14-24},
  author={Aung, Thazin Win Win and Huo, Huan and Sui, Yulei},
  url={https://doi.org/10.1145/3387904.3389251},
  publisher={Association for Computing Machinery},
  series={ICPC '20},
  keywords={natural language processing, change impact analysis, traceability, Software},
  abstract={In large-scale software development projects, change impact analysis (CIA) plays an important role in controlling software design evolution. Identifying and accessing the effects of software changes using traceability links between various software artifacts is a common practice during the software development cycle. Recently, research in automated traceability-link recovery has received broad attention in the software maintenance community to reduce the manual maintenance cost of trace links by developers. In this study, we conducted a systematic literature review related to automatic traceability link recovery approaches with a focus on CIA. We identified 33 relevant studies and investigated the following aspects of CIA: traceability approaches, CIA sets, degrees of evaluation, trace direction and methods for recovering traceability link between artifacts of different types. Our review indicated that few traceability studies focused on designing and testing impact analysis sets, presumably due to the scarcity of datasets. Based on the findings, we urge further industrial case studies. Finally, we suggest developing traceability tools to support fully automatic traceability approaches, such as machine learning and deep learning.}
}

@article{rayyan-727968277,
  title={Cooperation between information system development and operations: A literature review},
  year={2014},
  issn={978-1-4503-2774-9},
  author={Erich, Floris and Amrit, Chintan and Daneva, Maya},
  url={https://doi.org/10.1145/2652524.2652598},
  publisher={Association for Computing Machinery},
  series={ESEM '14},
  keywords={automation, DevOps, cloud computing, continuous delivery, development, services, culture, measurement, operations, service oriented architecture, sharing, Information Systems},
  abstract={Software development can profit from improvements in the deployment and maintenance phases. DevOps improves these phases through a collection of principles and practices, centered around close collaboration between Development and Operations personnel. Both sides have paid little attention to issues faced by each other. Yet knowledge sharing is invaluable. Development personnel can for example make software more robust by implementing scalability and performance features desired by operations personnel.}
}

@article{rayyan-727968278,
  title={Patterns for integrating agile development processes and user centred design},
  year={2015},
  issn={978-1-4503-3847-9},
  author={Salah, Dina and Paige, Richard and Cairns, Paul},
  url={https://doi.org/10.1145/2855321.2855341},
  publisher={Association for Computing Machinery},
  series={EuroPLoP '15},
  keywords={agile, agile user centred design integration, agile user centred design integration patterns, user centred design},
  abstract={The aim of this paper is to report the patterns that emerged as a result of conducting two studies: first, a Systematic Literature Review (SLR) that investigated Agile and User Centred Design Integration (AUCDI) challenges, strategies and success factors and included a total of 71 AUCDI experience reports, lessons learned, and success and failure AUCDI case studies. Second, an interview study that investigated challenges and practices faced by industrial AUCDI attempts.The patterns that emerged are related to various aspects of the integration process, for example, design, prioritizing User Centred Design (UCD) activities, usability testing, UCD practitioners, documentation and communication between the customer and the development team.}
}

@article{rayyan-727968279,
  title={Requirements engineering and software testing in agile methodologies: A systematic mapping},
  year={2019},
  issn={978-1-4503-7651-8},
  pages={322-331},
  author={Coutinho, Jarbele C S and Andrade, Wilkerson L and Machado, Patrícia D L},
  url={https://doi.org/10.1145/3350768.3352584},
  publisher={Association for Computing Machinery},
  series={SBES 2019},
  keywords={systematic mapping study, software testing, agile, software requirements, Software},
  abstract={The insertion of agile practices in software development has increased exponentially. Both industry and academic staff constantly face challenges related to requirements specification and testing in this context. In this study, we conducted a systematic mapping of the literature to investigate the practices, strategies, techniques, tools, and challenges met in the association of Requirements Engineering with Software Testing (REST) in the agile context. By searching seven major bibliographic databases, we identified 1.099 papers related to Agile REST. Based on the systematic mapping guidelines, we selected 14 of them for a more specific analysis. In general, the main findings include the fact that weekly meetings should be held to establish frequent communication with stakeholders. Also, most projects adopt use cases as conceptual models and perform use case detailing. Test cases are an important artifact with test case design as a Software Testing practice. For the automation of test cases, fit tables have been recommended as an artifact. Finally, proper project documentation constitutes a critical basis in Agile REST.}
}

@article{rayyan-727968280,
  title={Application of ensemble techniques in predicting object-oriented software maintainability},
  year={2019},
  issn={978-1-4503-7145-2},
  pages={370-373},
  author={Alsolai, Hadeel and Roper, Marc},
  url={https://doi.org/10.1145/3319008.3319716},
  publisher={Association for Computing Machinery},
  series={EASE '19},
  keywords={prediction, software maintainability, ensemble model, individual model, Object-oriented system, Software},
  abstract={While prior object-oriented software maintainability literature acknowledges the role of machine learning techniques as valuable predictors of potential change, the most suitable technique that achieves consistently high accuracy remains undetermined. With the objective of obtaining more consistent results, an ensemble technique is investigated to advance the performance of the individual models and increase their accuracy in predicting software maintainability of the object-oriented system. This paper describes the research plan for predicting object-oriented software maintainability using ensemble techniques. First, we present a brief overview of the main research background and its different components. Second, we explain the research methodology. Third, we provide expected results. Finally, we conclude summary of the current status.}
}

@article{rayyan-727968281,
  title={A survey of fuzzy service matching approaches in the context of on-the-fly computing},
  year={2013},
  issn={978-1-4503-2122-8},
  pages={143-152},
  author={Platenius, Marie C and von Detten, Markus and Becker, Steffen and Schäfer, Wilhelm and Engels, Gregor},
  url={https://doi.org/10.1145/2465449.2465454},
  publisher={Association for Computing Machinery},
  series={CBSE '13},
  keywords={survey, fuzzy matching, gradual matching results, on-the-fly computing, service matching},
  abstract={In the last decades, development turned from monolithic software products towards more flexible software components that can be provided on world-wide markets in form of services. Customers request such services or compositions of several services. However, in many cases, discovering the best services to address a given request is a tough challenge and requires expressive, gradual matching results, considering different aspects of a service description, e.g., inputs/ouputs, protocols, or quality properties. Furthermore, in situations in which no service exactly satisfies the request, approximate matching which can deal with a certain amount of fuzziness becomes necessary. There is a wealth of service matching approaches, but it is not clear whether there is a comprehensive, fuzzy matching approach which addresses all these challenges. Although there are a few service matching surveys, none of them is able to answer this question. In this paper, we perform a systematic literature survey of 35 (out of 504) service matching approaches which consider fuzzy matching. Based on this survey, we propose a classification, discuss how different matching approaches can be combined into a comprehensive matching method, and identify future research challenges.}
}

@article{rayyan-727968282,
  title={A systematic mapping study on serious game quality},
  year={2014},
  issn={978-1-4503-2476-2},
  author={Vargas, Juan A and García-Mundo, Lilia and Genero, Marcela and Piattini, Mario},
  url={https://doi.org/10.1145/2601248.2601261},
  publisher={Association for Computing Machinery},
  series={EASE '14},
  keywords={systematic mapping study, serious games, quality, ISO 25010},
  abstract={Context: A Serious Game (SG) is a game for purposes other than entertainment [12]. SGs are currently in widespread use and their popularity has begun to steadily increase; their application areas now extend not only to education, but also to military, health and corporate [9] [12] sectors. SGs are of vital importance at present, as they can be a means to achieve relevant goals from both a personal and an institutional point of view. This may take place in fields as diverse as defense, education, scientific exploration, health care, emergency management, city planning, engineering, religion, and politics. The number of users of these systems grows each day, signifying that their impact is very high, and it is precisely for this reason that more extensive research on SG quality is needed.Objective: The aim of this study is to discover the current state of SG quality initiatives, identifying gaps that merit future investigation.Method: We conducted a systematic mapping study (SMS) on SG quality, following the guidelines proposed by Kitchenham and Charters [7]. We selected 112 papers found in six digital libraries until April of 2013.Results: Since 2007, research on SG quality proves to have grown very significantly. Research has focused mostly on addressing the effectiveness of SGs (78.57%), in addition to several entertainment characteristics that are principally related to pleasure (62.50%). The most widely-researched software artifact was the final product (97.32%), with design coming very far behind (7.14%). Less than half of all the research reviewed had been validated by means of experiments, and in most of these cases, experiments were conducted by the same researchers who had proposed the SG. The majority of experiments have not been replicated. The most common research outcome was questionnaires, closely followed by the confirmation of knowledge. Most of these outcomes evaluated the quality of a particular SG.Conclusion: Results show that SG quality has undergone a very important growth, thus making SG quality an area of opportunity for future research. Researchers are mainly concerned with demonstrating or confirming the effectiveness of SGs, but very little research has been conducted as regards the characteristics of playability that make SGs more effective. Since effectiveness and playability are evaluated in the final product there is a need to provide quality assurance methods that incorporate quality issues from the early stages of SG development. Further empirical validation is also needed, and in particular, external replications must be performed in order to corroborate and generalize the findings obtained.}
}

@article{rayyan-727968283,
  title={Testing strategies for smart cities applications: A systematic mapping study},
  year={2018},
  issn={978-1-4503-6555-0},
  pages={20-28},
  author={Costa, Alex and Teixeira, Leopoldo},
  url={https://doi.org/10.1145/3266003.3266005},
  publisher={Association for Computing Machinery},
  series={SAST '18},
  keywords={Mapping Study, Smart City, Tests},
  abstract={Context: Smart Cities are urban areas that enable the development of applications to improve city resources management, through the use of information technology such as Internet of Things and cloud computing, as well as government data availability and citizen participation. Some challenges identified in the development of solutions for this context are: scalability, modularity and security. Goal: Testing activities are critical to the verification and validation of Smart City solutions, so our goal is to develop a map of test strategies for applications developed in the context of Smart Cities. Method: For this study we defined a systematic literature review protocol to identify, select, analyze and synthesize the results of previously published empirical studies in the software engineering literature, related to testing Smart Cities applications. Results: In this systematic mapping, 13 articles were selected, that have identified test strategies used by developers for the application testing process. We have also identified some difficulties faced in the process of testing these applications through reports present in the works selected in this mapping. Conclusion: Our research synthesized evidence that we hope might serve as a data source for academic research and industrial practice. As future work we plan to apply our results highlighting a case study in real applications to validate the collected evidence.}
}

@article{rayyan-727968284,
  title={On the terms within- and cross-company in software effort estimation},
  year={2016},
  issn={978-1-4503-4772-3},
  author={Minku, Leandro L},
  url={https://doi.org/10.1145/2972958.2972968},
  publisher={Association for Computing Machinery},
  series={PROMISE 2016},
  keywords={machine learning, Software effort estimation, transfer learning, cross-company effort estimation, Software},
  abstract={Background: the terms Within-Company (WC) and Cross-Company (CC) in Software Effort Estimation (SEE) have the connotation that CC projects are considerably different from WC projects, and that WC projects are more similar to the projects being estimated. However, as WC projects can themselves be heterogeneous, this is not always the case. Therefore, the use of the terms WC and CC has been questioned as potentially misleading and possibly unhelpful. Aims: to raise awareness of the SEE community in terms of the problems presented by the terms WC and CC, and to encourage discussions on the appropriateness of these terms. Method: existing literature on CC and WC SEE is discussed to raise evidence in favour and against the use of these terms. Results: existing evidence suggests that the terms WC and CC are helpful, because distinguishing between WC and CC projects can help the predictive performance of SEE models. However, due to their connotation, they can be misleading and potentially lead to wrong conclusions in studies comparing WC and CC SEE models. Conclusions: the issue being tackled when investigating WC and CC SEE is heterogeneity, and not the different origins of the software projects per se. Given that the terms WC and CC can be misleading, researchers are encouraged to discuss and consider the problems presented by these terms in SEE papers. Labelling projects as "potentially homogeneous" and "potentially heterogeneous" may be safer than directly labelling them as WC and CC projects.}
}

@article{rayyan-727968285,
  title={Strategies for consistency checking on software product lines: A mapping study},
  year={2015},
  issn={978-1-4503-3350-4},
  author={Santos, Alcemir Rodrigues and de Oliveira, Raphael Pereira and de Almeida, Eduardo Santana},
  url={https://doi.org/10.1145/2745802.2745806},
  publisher={Association for Computing Machinery},
  series={EASE '15},
  keywords={mapping study, literature review, consistency checking, software product line engineering, Software},
  abstract={Context. Software Product Lines (SPL) has become one of the most prominents way to promote the systematic reuse of software artifacts. Like any other piece of software, with the SPL aging, it becomes necessary to manage their evolution. However, in this process, engineers might introduce divergences among the SPL artifacts. Thus, a number of initiatives address the management of such inconsistencies. Objective. In this paper, we mapped the existing approaches to inconsistency management within SPL. Method. We used the systematic mapping study methodology. Results. We classified and performed a characterization of the approaches found, which we mangaged to arrange in three main categories. Most papers selected proposed new methods as solution research. Besides, there is still a need for validation and evaluation studies. Conclusion. We identified a lack of support for a number of activities of consistency assurance. For instance, no paper addressed the tracking of findings, decisions, and actions, as well as, few papers describing either the handling or a management policy for identified inconsistencies.}
}

@article{rayyan-727968286,
  title={Using qualitative metasummary to synthesize empirical findings in literature reviews},
  year={2014},
  issn={978-1-4503-2774-9},
  author={Ribeiro, Danilo Monteiro and Cardoso, Marcos and da Silva, Fabio Q B and França, César},
  url={https://doi.org/10.1145/2652524.2652562},
  publisher={Association for Computing Machinery},
  series={ESEM '14},
  keywords={systematic review, research synthesis, qualitative metasummary, team performance},
  abstract={Context- A common problem in Systematic Reviews in software engineering is that they provide very limited syntheses. Goal- In the search for alternatives of effective methods for synthesizing empirical evidence, in this paper, we explore the use of the Qualitative Metasummary method, which is a quantitatively oriented aggregation of mixed research findings. Method - We describe the use of qualitative metasummary through an example using 15 studies addressing antecedents of performance of software development teams. Qualitative metasummary includes extraction and grouping of findings, and calculation of frequency and intensity effect sizes. Results – The instance described in this paper produced a 10-factor model that effectively summarizes the current empirical knowledge on performance of software development teams. Then, we assessed the method in terms of ease of use, usefulness and reliability of results. Conclusion – The Qualitative Metasummary method offers rich indexes of experiences and events under investigation, focusing on the effects of a variable over other, which is consistent with the central interest of systematic reviews. However, its main limitations are (i) challenging comparability/integratability between primary studies, (ii) loss of detailed contextual information, (iii) and the great deal of effort demanded to synthesize larger sets of papers.}
}

@article{rayyan-727968287,
  title={Accessibility in rich internet applications: People and research},
  year={2012},
  issn={978-85-7669-262-1},
  pages={3-12},
  author={Almeida, Leonelo Dell Anhol and Baranauskas, Maria Cecília Calani},
  publisher={Brazilian Computer Society},
  series={IHC '12},
  keywords={systematic literature review, web 2.0, accessibility, people, RIA, Internet},
  abstract={Accessibility in Rich Internet Applications (RIAs) is still far from reality for most of the Web applications currently available. Some factors that influence this scenario are the novelty of research and products for developing RIAs, and the challenging activity of identifying and involving representatives of RIAs target people. Aiming at clarifying the state-of-the-art of this research topic we conducted a Systematic Literature Review of studies addressing accessibility and awareness of others in RIAs. This paper presents our findings related to the overall contributions of the reviewed studies and analyzes the target people and the methods employed for involving them in the research lifecycle.}
}

@article{rayyan-727968288,
  title={Finding relevant research solutions for practical problems: The serp taxonomy architecture},
  year={2014},
  issn={978-1-4503-3045-9},
  pages={13-20},
  author={Petersen, Kai and Engström, Emelie},
  url={https://doi.org/10.1145/2647648.2647650},
  publisher={Association for Computing Machinery},
  series={WISE '14},
  keywords={software engineering, evidence-based software engineering, industry and research gap, Reserpine},
  abstract={Background: Experience and research indicates that there exist a communication gap between research and industry in software engineering.Objective: We propose the Software Engineering Research and Practice (SERP) taxonomy architecture to support communication between practitioners and researchers. The taxonomy architecture provides a basis for classifying research from a problem perspective which in turn supports the breaking down of complex practical challenges to researchable units. Thus such taxonomy may support the mapping of challenges in industry to research solutions in the software engineering context.Method: In this paper we present SERP and exemplifies its usage based on two literature studies in the field of software engineering. Further, we discuss how a taxonomy based on this architecture could have helped us in two past research projects that were conducted in close collaboration with industry. Finally we validate SERP by applying it to the area of software testing, developing SERP-test, and interviewing two industry practitioners and two researchers.Results: The taxonomy architecture has been applied to two problems in software testing, and has been assessed through interviews with practitioners and researchers. The interviews provided suggestions of how to improve the taxonomy architecture, which have been incorporated. With two examples, we demonstrated how the taxonomy architecture could be used to find solutions for industrial problems, and to find the problems addressed by a particular solution.Conclusion: SERP may be useful in multiple ways: (1) Given that SERP taxonomies are populated with industrial problems and scientific solutions, we could rapidly identify candidate research solutions for industrial practice. (2) Researchers could benefit from the taxonomy in the reporting of their research to ease the mapping to industrial challenges.}
}

@article{rayyan-727968289,
  title={Identifying software cost attributes of software project management in global software development: An integrative framework},
  year={2020},
  issn={978-1-4503-7733-1},
  author={El Bajta, Manal and Idri, Ali},
  url={https://doi.org/10.1145/3419604.3419780},
  publisher={Association for Computing Machinery},
  series={SITA'20},
  keywords={Systematic Review, Software Project Management, Global Software Development, Cost attributes, Software},
  abstract={The management of global and distributed software projects is a very difficult task further complicated by the emergence of new challenges inherent in stakeholder dispersion. Software cost estimation plays a central role to face challenges in the context of Global Software Development (GSD). The objective of this study is to identify software cost attributes related to GSD context to present an integrative framework encompassing these attributes. Thirty cost attributes were identified using a Systematic Literature Review (SLR) and later compiled into a framework inspired by the Software Engineering Institute (SEI) taxonomy.}
}

@article{rayyan-727968290,
  title={Constructing hybrid software process simulation models},
  year={2015},
  issn={978-1-4503-3346-7},
  pages={157-166},
  author={Gao, Chao and Zhang, He and Jiang, Shu},
  url={https://doi.org/10.1145/2785592.2785610},
  publisher={Association for Computing Machinery},
  series={ICSSP 2015},
  keywords={systematic (literature) review, Hybrid simulation, software process modeling, software process simulation, Software},
  abstract={Software process simulation (SPS) has become an active research area for managing and improving software development processes since its introduction in the last two decades. Hybrid simulation, the combination of simulation paradigms to address a problem, is becoming more popular as the problems we are presented with become more complex. However, integrating multiple simulation paradigms faces the issues of compatibility, interoperatability and synchronization when executing simulation. The objective of this research is to present the state-of-the-art of this research area, the hybrid mechanism when integrating paradigms, and more importantly provide practical support for the effective adoption of hybrid simulation in SPS context. Based on an extended systematic literature review, this paper presents the preliminary results by answering the research questions. Depending upon the way these simulation paradigms represent different aspects and levels of software process and the context in which they can be modeled by SPS, two hybrid mechanisms: Hierarchical Mechanism and Interlinked Mechanism, have been frequently employed. The detailed discussions of integration strategies and recommendations when applying hybrid simulation may offer reference value to the SPS community.}
}

@article{rayyan-727968291,
  title={Understanding software process improvement in global software development: A theoretical framework of human factors},
  year={2017},
  journal={SIGAPP Appl. Comput. Rev.},
  issn={1559-6915},
  volume={17},
  number={2},
  pages={5-15},
  author={Khan, Arif Ali and Keung, Jacky and Hussain, Shahid and Niazi, Mahmood and Tamimy, Muhammad Manzoor Ilahi},
  url={https://doi.org/10.1145/3131080.3131081},
  keywords={systematic literature review, global software development, software process improvement, human, Humanities, Humanism, Humans, Software},
  abstract={Presently, most of the software development organizations are adopting the phenomena of Global Software Development (GSD), mainly because of the significant return on investment it produces. However, GSD is a complex phenomenon and there are many challenges associated with it, especially that related to Software Process Improvement (SPI). The aim of this work is to identify humans' related success factors and barriers that could impact the SPI process in GSD organizations and proposed a theoretical framework of the factors in relation to SPI implementation. We have adopted the Systematic Literature Review (SLR) method in order to investigate the success factors and barriers. Using the SLR approach, total ten success factors and eight barriers were identified. The paper also reported the Critical Success Factors (CSFs) and Critical Barriers (CBs) for SPI implementation following the criteria of the factors having a frequency ≥ 50% as critical. Our results reveal that five out of ten factors are critical for SPI program. Moreover, total three barriers were ranked as the most critical barriers. Based on the analysis of the identified factors, we have presented a theoretical framework that has highlighted an association between the identified factors and the implementation of the SPI program in GSD environment.}
}

@article{rayyan-727968292,
  title={A machine learning approach for semi-automated search and selection in literature studies},
  year={2017},
  issn={978-1-4503-4804-1},
  pages={118-127},
  author={Ros, Rasmus and Bjarnason, Elizabeth and Runeson, Per},
  url={https://doi.org/10.1145/3084226.3084243},
  publisher={Association for Computing Machinery},
  series={EASE'17},
  keywords={Study selection, Systematic literature review, Automation, Machine learning, Reinforcement learning, Research identification},
  abstract={Background. Search and selection of primary studies in Systematic Literature Reviews (SLR) is labour intensive, and hard to replicate and update. Aims. We explore a machine learning approach to support semi-automated search and selection in SLRs to address these weaknesses. Method. We 1) train a classifier on an initial set of papers, 2) extend this set of papers by automated search and snowballing, 3) have the researcher validate the top paper, selected by the classifier, and 4) update the set of papers and iterate the process until a stopping criterion is met. Results. We demonstrate with a proof-of-concept tool that the proposed automated search and selection approach generates valid search strings and that the performance for subsets of primary studies can reduce the manual work by half. Conclusions. The approach is promising and the demonstrated advantages include cost savings and replicability. The next steps include further tool development and evaluate the approach on a complete SLR.}
}

@article{rayyan-727968293,
  title={A tool for software ecosystem models: An analysis on their implications in education},
  year={2020},
  issn={978-1-4503-8753-8},
  pages={405-414},
  author={Alencar, Igor R and Coutinho, Emanuel F and Moreira, Leonardo O and Bezerra, Carla I M},
  url={https://doi.org/10.1145/3422392.3422486},
  publisher={Association for Computing Machinery},
  series={SBES '20},
  keywords={Software Engineering, Models, Tool, Repository, Teaching, Software Ecosystems, Software},
  abstract={Software Engineering (SE) is a discipline that deals with aspects of software production. Often, only the technical aspects are taught in the classes, and the economic and social aspects involved in software development are not reinforced in the classroom. In this context, the concept of a Software Ecosystem (SECO) emerged, defined as a set of actors acting as a unit interacting with a market distributed between software and services, with relationships supported by a technological platform or a common market, carried out by exchanging information, artifacts and resources. SECO teaching is not common in SE classes. However, when adding this knowledge, it is possible to have the benefit of a more global vision of SE, with the relationships between suppliers, technological platforms and customers. Moreover, there is a lack of SECO models and examples in the literature, making its study difficult. This article presents the ARIEL tool, with the aim of mitigating this gap and supporting the teaching of SECO in SE. For this, an user experience evaluation was projected using the DECIDE framework as a reference, with undergraduate and graduate students. As a result of this work, the participants stated it was easy to use the tool and they made few mistakes, most of them felt satisfied when using the tool, and the level of mental effort to carry out the activities is low.}
}

@article{rayyan-727968294,
  title={The role of systematic reviews in identifying the state of the art in web resource estimation},
  year={2012},
  issn={978-1-4503-1509-8},
  pages={3-8},
  author={Mendes, Emilia and Azhar, Damir},
  url={https://doi.org/10.1145/2372233.2372237},
  publisher={Association for Computing Machinery},
  series={EAST '12},
  keywords={systematic review, web resource estimation, Health Resources},
  abstract={The goal of this position paper is to motivate the importance of SRs, and to present a SR of Web resource estimation. The SR results suggest that there is plenty of work to be done in the field of Web resource estimation whether it be investigating a more comprehensive approach that considers more than a single resource facet, evaluating other possible resource predictors, or trying to determine guidelines that would help simplify the process of selecting a resource estimation technique.}
}

@article{rayyan-727968295,
  title={An investigation into the development of service-oriented robotic systems},
  year={2013},
  issn={978-1-4503-1656-9},
  pages={223-228},
  author={Oliveira, Lucas Bueno R and Osório, Fernando S and Nakagawa, Elisa Yumi},
  url={https://doi.org/10.1145/2480362.2480410},
  publisher={Association for Computing Machinery},
  series={SAC '13},
  keywords={systematic review, service-oriented robotic system, Robotics},
  abstract={Robotics has emerged as one of the most prominent research areas in the last years. To cope with the great variety of robots application areas, as well as the heterogeneity of the robots, Service-Oriented Architecture (SOA) has been adopted to develop robotic systems, i.e., the software systems that manage the robots. Nevertheless, there is a lack of studies that provide an updated, fair overview of the development of Service-Oriented Robotic Systems (SORS), i.e., software systems composed by services. The main contribution of this paper is to present a detailed, analytical panorama of SORS, their implementation technologies, and software engineering guidelines that support development of such systems. For this, we have applied steps of the systematic review technique. As main results, we have observed that, in spite of relevant contributions already found in this area, it is still necessary considerable efforts to consolidate the research in SORS. Furthermore, we intend that this work makes also possible to identify important research topics for future research.}
}

@article{rayyan-727968296,
  title={ICTD systems development: Analysis of requirements elicitation approaches},
  year={2015},
  issn={978-1-4503-3163-0},
  author={Hasan, M Mahmudul},
  url={https://doi.org/10.1145/2737856.2737886},
  publisher={Association for Computing Machinery},
  series={ICTD '15},
  keywords={systematic literature review, ICTD, information and communication technologies and development, requirements elicitation approaches, system development requirements},
  abstract={Information and Communication Technologies and Development (ICTD) has created enormous potential in the expansion of socioeconomic opportunities for larger group of underprivileged and isolated population. In spite of the high expectations, there have been some noticeable disappointments in the number of ICTD projects for their poor qualities caused by not fully aware of the requirements and their elicitation process in the system development. This paper identifies the documented requirements elicitation approaches and provides an analysis of a strategic methodological process of requirements elicitation applicable in the ICTD systems development. This paper is based on a Systematic Literature Review (SLR) of documented requirements elicitation approaches in the literature. The results and analysis of this paper contribute in the requirements elicitation process by serving the ICTD project developer as the means of understanding the system development requirements, their elicitation process, and deriving appropriate methods and tools for requirements elicitation.}
}

@article{rayyan-727968297,
  title={Assessing software defection prediction performance: Why using the matthews correlation coefficient matters},
  year={2020},
  issn={978-1-4503-7731-7},
  pages={120-129},
  author={Yao, Jingxiu and Shepperd, Martin},
  url={https://doi.org/10.1145/3383219.3383232},
  publisher={Association for Computing Machinery},
  series={EASE '20},
  keywords={Software defect prediction, Classification metrics, Software engineering experimentation, Software},
  abstract={Context: There is considerable diversity in the range and design of computational experiments to assess classifiers for software defect prediction. This is particularly so, regarding the choice of classifier performance metrics. Unfortunately some widely used metrics are known to be biased, in particular F1.Objective: We want to understand the extent to which the widespread use of the F1 renders empirical results in software defect prediction unreliable.Method: We searched for defect prediction studies that report both F1 and the Matthews correlation coefficient (MCC). This enabled us to determine the proportion of results that are consistent between both metrics and the proportion that change.Results: Our systematic review identifies 8 studies comprising 4017 pairwise results. Of these results, the direction of the comparison changes in 23% of the cases when the unbiased MCC metric is employed.Conclusion: We find compelling reasons why the choice of classification performance metric matters, specifically the biased and misleading F1 metric should be deprecated.}
}

@article{rayyan-727968298,
  title={Decision support tools for SLR search string construction},
  year={2018},
  issn={978-1-4503-6518-5},
  pages={660-667},
  author={Marcos-Pablos, Samuel and García-Peñalvo, Francisco José},
  url={https://doi.org/10.1145/3284179.3284292},
  publisher={Association for Computing Machinery},
  series={TEEM'18},
  keywords={text mining, Systematic literature review, decision support tool},
  abstract={Systematic literature reviews (SLRs) have gained popularity during the last years as a form of providing state of the art about previous research. As part of the SLR tasks, devising the search strategy and particularly finding the right keywords to be included in the search string is a difficult and critical step, as it will determine what evidence will be identified in the different searched sources and thus condition the rest of the review. In order to support the search process, this paper presents an iterative methodology for search string construction along with a set of decision support tools that help in building the search string by finding appropriate key terms related to the topic of interest in order to assist the researcher in the SLR conduction.}
}

@article{rayyan-727968299,
  title={Do software process improvements lead to ISO 9126 architectural quality factor improvement},
  year={2011},
  issn={978-1-4503-0851-9},
  pages={11-17},
  author={Lavallée, Mathieu and Robillard, Pierre N},
  url={https://doi.org/10.1145/2024587.2024592},
  publisher={Association for Computing Machinery},
  series={WoSQ '11},
  keywords={systematic review, software process improvement, capability maturity model, impacts on developers, quality improvement, Software},
  abstract={This paper presents preliminary results of a systematic review performed to determine the impacts of Software Process Improvements (SPI) on developers' activities and on architectural quality. The analysis shows that most SPI research focuses on the motivations of developers like quality of work life and participation incentives, but provides little detail on the impacts of SPI on their day-to-day tasks. The impacts on product quality are limited to defect reduction, and do not consider architectural quality factors, such as changeability and stability. This study shows a very weak link between process quality, as defined by the CMMI, and architectural quality, as defined by ISO 9126. The SPI literature found by this review is mostly concerned with requirement process improvements, which are related to problem definition quality, but not to architectural quality. Future quality-oriented SPI research should therefore focus on improving design and development processes with an eye to considering architectural quality factors, or what the ISO 9126 terms "architectural capabilities".}
}

@article{rayyan-727968300,
  title={How to elicit and specify software requirements from BPMN diagrams?},
  year={2018},
  issn={978-1-4503-6559-8},
  author={Sorgatto, Doglas W and Paiva, Débora M B and Cagnin, Maria Istela},
  url={https://doi.org/10.1145/3229345.3229403},
  publisher={Association for Computing Machinery},
  series={SBSI'18},
  keywords={Systematic Review, BPMN, Requirement Elicitation, Software},
  abstract={Different techniques for eliciting requirements from business process models have arisen due to the importance of software requirements to be aligned with the business in order to achieve organizational goals. Although there are several ways to represent these models, the BPMN notation has been considered the most adequate for facilitating the communication between different types of stakeholders. This paper aims to present a systematic review with the support of the snowballing technique, to raise studies on how to elicit and specify requirements from business process models in BPMN. As main results it is pointed out that this type of elicitation is recent, well automated, uses supporting heuristics, is mainly concerned with functional requirements, and the most of primary studies specify the requirements as use cases.}
}

@article{rayyan-727968301,
  title={Teaching students critical appraisal of scientific literature using checklists},
  year={2018},
  issn={978-1-4503-6383-9},
  pages={8-17},
  author={Molléri, Jefferson Seide and bin Ali, Nauman and Petersen, Kai and Minhas, Nasir Mehmood and Chatzipetrou, Panagiota},
  url={https://doi.org/10.1145/3209087.3209099},
  publisher={Association for Computing Machinery},
  series={ECSEE'18},
  keywords={Checklist, experiment, case study, critical appraisal, student},
  abstract={Background: Teaching students to critically appraise scientific literature is an important goal for a postgraduate research methods course.Objective: To investigate the application of checklists for assessing the scientific rigor of empirical studies support students in reviewing case study research and experiments.Methods: We employed an experimental design where 76 students (in pairs) used two checklists to evaluate two papers (reporting a case study and an experiment) each. We compared the students' assessments against ratings from more senior researchers. We also collected data on students' perception of using the checklists.Results: The consistency of students' ratings and the accuracy when compared to ratings from seniors varied. A factor seemed to be that the clearer the reporting, the easier it is for students to judge the quality of studies. Students perceived checklist items related to data analysis as difficult to assess.Conclusion: As expected, this study reinforces the needs for clear reporting, as it is important that authors write to enable synthesis and quality assessment. With clearer reporting, the novices performed well in assessing the quality of the empirical work, which supports its continued use in the course as means for introducing scientific reviews.}
}

@article{rayyan-727968302,
  title={Towards a theory of software development expertise},
  year={2018},
  issn={978-1-4503-5573-5},
  pages={187-200},
  author={Baltes, Sebastian and Diehl, Stephan},
  url={https://doi.org/10.1145/3236024.3236061},
  publisher={Association for Computing Machinery},
  series={ESEC/FSE 2018},
  keywords={theory, software engineering, expertise, psychology, Software},
  abstract={Software development includes diverse tasks such as implementing new features, analyzing requirements, and fixing bugs. Being an expert in those tasks requires a certain set of skills, knowledge, and experience. Several studies investigated individual aspects of software development expertise, but what is missing is a comprehensive theory. We present a first conceptual theory of software development expertise that is grounded in data from a mixed-methods survey with 335 software developers and in literature on expertise and expert performance. Our theory currently focuses on programming, but already provides valuable insights for researchers, developers, and employers. The theory describes important properties of software development expertise and which factors foster or hinder its formation, including how developers' performance may decline over time. Moreover, our quantitative results show that developers' expertise self-assessments are context-dependent and that experience is not necessarily related to expertise.}
}

@article{rayyan-727968303,
  title={Systematic studies in software product lines: A tertiary study},
  year={2017},
  issn={978-1-4503-5221-5},
  pages={143-152},
  author={Marimuthu, C and Chandrasekaran, K},
  url={https://doi.org/10.1145/3106195.3106212},
  publisher={Association for Computing Machinery},
  series={SPLC '17},
  keywords={systematic review, tertiary study, software product line, Software},
  abstract={Software product lines are widely used in the software industries to increase the re-usability and to decrease maintenance cost. On the other hand, systematic reviews are widely used in the software engineering research community to provide the overview of the research field and practitioners guidelines. Researchers have conducted many systematic studies on the different aspects of SPLs. To the best of our knowledge, till now there is no tertiary study conducted on systematic studies of SPL related research topics. In this paper, we aim at conducting a systematic mapping study of existing systematic studies to report the overview of the findings for researchers and practitioners. We performed snowballing and automated search to find out the relevant systematic studies. As a result, we analyzed 60 relevant studies to answer 5 research questions. The main focus of this tertiary study is to highlight the research topics, type of published reviews, active researchers and publication forums. Additionally, we highlight some of the limitations of the systematic studies. The important finding of this study is that the research field is well matured as the systematic studies covered a wide range of research topics. Another important finding is that many studies provided information for practitioners as well as researchers which is a notable improvement in the systematic reviews. However, many studies failed to assess the quality of the primary studies which is the major limitation of the existing systematic studies.}
}

@article{rayyan-727968304,
  title={Factors affecting software development productivity: An empirical study},
  year={2019},
  issn={978-1-4503-7651-8},
  pages={307-316},
  author={Canedo, Edna Dias and Santos, Giovanni Almeida},
  url={https://doi.org/10.1145/3350768.3352491},
  publisher={Association for Computing Machinery},
  series={SBES 2019},
  keywords={Empirical study, Measurement, Productivity, Metrics, Influence Factors, Software, Fibrinogen},
  abstract={The competitiveness has demanded from the software industry shorter delivery times for its products resulting in optimized life cycles, generating a need to increase its performance to maintain competitiveness in the markets where they operate. This context has made productivity study so fundamental that organizations not only evaluate their performance, but also provide means to improve it. The main goal of this paper is to investigate which factors affect productivity in software development projects and in open-source projects. In this work a Systematic Literature Review (SLR) was carried out in order to answer the research questions and a survey with practitioners community about their perception in relation to the factors of the productivity of the team. This empirical study led to the discovery of interesting factors that show how the different factors do (or do not) affect productivity. It was also found out that some factors appear to allow independence and responsibility of team, while others appear to cause a better distribution of tasks. The results show how factors such as people, product, organization, investment in technology, lack of contractual relations and engagement of open-source project contributors influence productivity.}
}

@article{rayyan-727968305,
  title={Towards a model to support ¡i¿in Silico¡/i¿ studies of software evolution},
  year={2012},
  issn={978-1-4503-1056-7},
  pages={281-290},
  author={Araújo, Marco Antônio Pereira and Monteiro, Vitor Faria and Travassos, Guilherme Horta},
  url={https://doi.org/10.1145/2372251.2372303},
  publisher={Association for Computing Machinery},
  series={ESEM '12},
  keywords={software maintenance, experimental software engineering, software evolution, in silico study, object-oriented software, simulation model, Software, Computer Simulation},
  abstract={Software evolution is recognized as one of the most challenging areas in the field of Software Engineering. The observation of evolution is time-dependent, reducing opportunities for actual observations in short periods of time. Usually, maintenance cycles are proportional to the software life cycle. Therefore, the amount of research has not been enough to deal with all the issues related to the evolution of software. However, simulation through confident models represents an interesting strategy to support software decay observation in short period of time. Towards that, this paper describes a model aimed at supporting the software decay simulation through systems dynamics. The Laws of Software Evolution and ISO 9126 were used as initial knowledge to support the discovery of software characteristic (size, periodicity, complexity, effort, reliability, and maintainability) relationships. Next, evidence to strengthen the existence of such relationships was acquired through quasi-systematic literature reviews. In sequence, the model was applied to support the simulation of industrial software decay. The results suggested its feasibility and correctness, making it an interesting candidate to support future software decay studies.}
}

@article{rayyan-727968306,
  title={CASE tool support for variability management in software product lines},
  year={2017},
  journal={ACM Computing Surveys},
  issn={0360-0300},
  volume={50},
  number={1},
  author={Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
  url={https://doi.org/10.1145/3034827},
  keywords={Software engineering, computer-aided software engineering, software variability, Software},
  abstract={Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools' characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.}
}

@article{rayyan-727968307,
  title={Trends in prioritization of test cases: 2017-2019},
  year={2020},
  issn={978-1-4503-6866-7},
  pages={2005-2011},
  author={de Castro-Cabrera, M del Carmen and García-Dominguez, Antonio and Medina-Bulo, Inmaculada},
  url={https://doi.org/10.1145/3341105.3374036},
  publisher={Association for Computing Machinery},
  series={SAC '20},
  keywords={systematic literature review, ⚠️ Invalid DOI, software testing, regression testing, TCP, test case prioritization},
  abstract={A core task in software testing is the design of test suites. Large test suites may take too long to run frequently, and test case prioritization (TCP) techniques have been proposed to speed up the detection of faults. These techniques have become increasingly popular and the number of publications has grown in recent years. Surveys have covered most of the techniques, but the latest included only publications until 2016: interest is growing, and new proposals have been developed in the last three years. This paper aims to complete that survey by providing the latest developments in TCP to respond to this growing interest. Specifically we use the taxonomy proposed by Khatibsyarbin et al. on the most important publications from 2017 to the present day (2019). All in all, we found 320 papers in this period about test case prioritization. The results show that the main techniques used are search-, coverage- and similarity-based.}
}

@article{rayyan-727968308,
  title={Ontologies supporting the distributed software development: A systematic mapping study},
  year={2013},
  issn={978-1-4503-1848-8},
  pages={153-164},
  author={Borges, Alex and Soares, Sérgio and Meira, Silvio and Tomaz, Hilário and Rocha, Rodrigo and Costa, Catarina},
  url={https://doi.org/10.1145/2460999.2461022},
  publisher={Association for Computing Machinery},
  series={EASE '13},
  keywords={systematic mapping study, ontology, empirical software engineering, distributed software development, Software},
  abstract={Background: Along the last decade, there has been a significant increase in the adoption of the approaches based on Distributed Software Development (DSD). This approach has brought several competitive advantages, as well as new challenges such as communication and information sharing. In this context, the ontologies can provide benefits such as the definition, standardization and sharing of knowledge involved in the project, allowing a uniform understanding of information and facilitating the collaboration among distributed software development teams. Aim: Identifying evidence to determine which tools, models, techniques and best practices that use ontologies to support the DSD projects, and which ontologies proposed in this context. Method: This paper presents a systematic mapping study conducted in order to investigate how ontologies are being applied as a support to the DSD. The research protocol was based on Kitchenham's, and Travassos and Biolchini's guidelines. Searches were performed both in manual and automatic way in a set of digital libraries engines and leading conferences in the Software Engineering field. Results: From the initial set of 1588 studies, it was selected a total of 38 primary studies that answer the two research questions. Conclusions: This work presents evidences from each paper collected and an analysis of results reached. The results support the foundation for proposing and developing a feature based on ontologies to support the DSD, besides encouraging further researches that may promote advancements in this area and fostering the adoption of these types of resources by the global software industry.}
}

@article{rayyan-727968309,
  title={Data quality: Cinderella at the software metrics ball?},
  year={2011},
  issn={978-1-4503-0593-8},
  pages={1-4},
  author={Shepperd, Martin},
  url={https://doi.org/10.1145/1985374.1985376},
  publisher={Association for Computing Machinery},
  series={WETSoM '11},
  keywords={empirical research, data quality, software metrics, Research Design, Metronidazole, Software},
  abstract={In this keynote I explore what exactly do we mean by data quality, techniques to assess data quality and the very significant challenges that poor data quality can pose. I believe we neglect data quality at our peril since - whether we like it or not - our research results are founded upon data and our assumptions that data quality issues do not confound our results. A systematic review of the literature suggests that it is a minority practice to even explicitly discuss data quality. I therefore suggest that this topic should become a higher priority amongst empirical software engineering researchers.}
}

@article{rayyan-727968310,
  title={Second-generation systematic literature studies using snowballing},
  year={2016},
  issn={978-1-4503-3691-8},
  author={Wohlin, Claes},
  url={https://doi.org/10.1145/2915970.2916006},
  publisher={Association for Computing Machinery},
  series={EASE '16},
  keywords={snowballing, systematic literature reviews, empirical research methods},
  abstract={Systematic literature studies have become standard practice in software engineering to synthesize evidence in different areas of the discipline. As more such studies are published, there is also a need to extend previously published systematic literature studies to cover new research papers. These first extensions become second-generation systematic literature studies. It has been asserted that snowballing would be a suitable search strategy for these types of second-generation studies, since newer studies ought to refer to previous research on a topic, and in particular to systematic literature studies published in an area. This paper compares using a snowballing search strategy with a published second-generation study using a database search strategy in the area of cross-company vs. within-company effort estimation. It is concluded that the approaches are comparable when it comes to which papers they find, although the snowballing approach is more efficient in this particular case.}
}

@article{rayyan-727968311,
  title={Is there a "Golden" feature set for static warning identification? An experimental evaluation},
  year={2018},
  issn={978-1-4503-5823-1},
  author={Wang, Junjie and Wang, Song and Wang, Qing},
  url={https://doi.org/10.1145/3239235.3239523},
  publisher={Association for Computing Machinery},
  series={ESEM '18},
  keywords={static analysis, experimental evaluation, actionable warning identification},
  abstract={Background: The most important challenge regarding the use of static analysis tools (e.g., FindBugs) is that there are a large number of warnings that are not acted on by developers. Many features have been proposed to build classification models for the automatic identification of actionable warnings. Through analyzing these features and related studies, we observe several limitations that make the users lack practical guides to apply these features.Aims: This work aims at conducting a systematic experimental evaluation of all the public available features, and exploring whether there is a golden feature set for actionable warning identification.Method: We first conduct a systematic literature review to collect all public available features for warning identification. We employ 12 projects with totally 60 revisions as our subject projects. We then implement a tool to extract the values of all features for each project revision to prepare the experimental data.Results: Experimental evaluation on 116 collected features demonstrates that there is a common set of features (23 features) which take effect in warning identification for most project revisions. These features can achieve satisfied performance with far less time cost for warning identification.Conclusions: These commonly-selected features can be treated as the golden feature set for identifying actionable warnings. This finding can serve as a practical guideline for facilitating real-world warning identification.}
}

@article{rayyan-727968312,
  title={Towards continues code recommendation and implementation system: An initial framework},
  year={2020},
  issn={978-1-4503-7731-7},
  pages={439-444},
  author={Akbar, Muhammad Azeem and Huang, Zhiqiu and Yu, Zhou and Mehmood, Faisal and Hussain, Yasir and Hamza, Muhammad},
  url={https://doi.org/10.1145/3383219.3383282},
  publisher={Association for Computing Machinery},
  series={EASE '20},
  keywords={DevOps, Code recommendation system, Empirical investigation},
  abstract={In the current era, the auto and reliable recommendation system plays a significant role in human life. The code recommender systems are being used in various source code databases to recommend the most suitable source code to the user. While code recommendation, the code analysis concerning 'code quality' and 'code implementation' is important to recommend the most reliable code by considering the objective of the user. The ultimate aim of this research work is to propose a code recommendation and implementation model using the characteristics of DevOps that assist in extracting, analyzing, implementing, and updating the recommender system continuously. The current study presents an initial framework of the proposed code recommender model. The design of the model is based on the data collected through literature review and by conducting an empirical study with experts. We believe that the proposed model will assist the researchers and practitioners to recommend the most secure and suitable source code according to their requirement.}
}

@article{rayyan-727968313,
  title={Worse than spam: Issues in sampling software developers},
  year={2016},
  issn={978-1-4503-4427-2},
  author={Baltes, Sebastian and Diehl, Stephan},
  url={https://doi.org/10.1145/2961111.2962628},
  publisher={Association for Computing Machinery},
  series={ESEM '16},
  keywords={Ethics, Empirical Research, Sampling, Software Developers, Software},
  abstract={Background: Reaching out to professional software developers is a crucial part of empirical software engineering research. One important method to investigate the state of practice is survey research. As drawing a random sample of professional software developers for a survey is rarely possible, researchers rely on various sampling strategies. Objective: In this paper, we report on our experience with different sampling strategies we employed, highlight ethical issues, and motivate the need to maintain a collection of key demographics about software developers to ease the assessment of the external validity of studies. Method: Our report is based on data from two studies we conducted in the past. Results: Contacting developers over public media proved to be the most effective and efficient sampling strategy. However, we not only describe the perspective of researchers who are interested in reaching goals like a large number of participants or a high response rate, but we also shed light onto ethical implications of different sampling strategies. We present one specific ethical guideline and point to debates in other research communities to start a discussion in the software engineering research community about which sampling strategies should be considered ethical.}
}

@article{rayyan-727968314,
  title={Understanding usability defect reporting in software defect repositories},
  year={2015},
  issn={978-1-4503-3796-0},
  pages={134-137},
  author={Yusop, Nor Shahida Mohamad},
  url={https://doi.org/10.1145/2811681.2817757},
  publisher={Association for Computing Machinery},
  series={ASWEC ' 15 vol. II},
  keywords={software testing, defect reporting tools, Usability defect reporting, Software},
  abstract={Software defect management is a critical component of good software engineering practice. The information reported about a defect is a key element to ensure defects are rectified effectively. However, based on research, reporting usability defects using an existing defect tracking system (DTS) is impractical. This is due to text-centric design and lack of features to support usability attributes. In addition, not all defects can be explained textually; especially defects that involve interface redesign. Another aspect to consider is that the reporters describe usability defects based on their usability knowledge and the information available at the time the defects are found. Defects stored in a DTS in a universal format. Therefore, when reporting usability defects there are some possibilities: the data may not be relevant or irrelevant, useful or not useful, or may even be beyond the reporter's knowledge. This makes it impossible to submit a high quality defect report. To address these issues, I propose a custom defect template that can adjust defect form according to whom, when and how the defect is found. In this way, it will provide flexibility to the reporters to record data based on their expertise and knowledge.}
}

@article{rayyan-727968315,
  title={Sistematização de RevisõEs BibliográFicas em pesquisas da áRea de IHC},
  year={2012},
  issn={978-85-7669-262-1},
  pages={51-54},
  author={Munzlinger, Elizabete and Narcizo, Fabricio Batista and de Queiroz, José Eustáquio Rangel},
  publisher={Brazilian Computer Society},
  series={IHC '12},
  keywords={systematic review, human-computer interaction, scientific research},
  abstract={During the development of a scientific research in the field of Human-Computer Interaction (HCI) it is needful to identify, read, analyze and interpret relevant publications about a given theme or subject. Such a task usually results in large sets of bibliographical data. As a consequence, they must be put together in a single document, with the aim of making easy to assess the strength of the evidence of interest. Nonetheless, this is not an easy task to accomplish. Systematic reviews (SR) come to fulfill this need, by providing comprehensive, reliable, and unbiased summaries of a research on a single topic. Additionally, SR bring together large numbers of individual studies in a single report, provides the best evidence for decision making, and also help determine future research needs.}
}

@article{rayyan-727968316,
  title={Investigating the skill gap between graduating students and industry expectations},
  year={2014},
  issn={978-1-4503-2768-8},
  pages={291-300},
  author={Radermacher, Alex and Walia, Gursimran and Knudson, Dean},
  url={https://doi.org/10.1145/2591062.2591159},
  publisher={Association for Computing Machinery},
  series={ICSE companion 2014},
  keywords={Computer science education, Computer science pedagogy, Required skills, Software developer},
  abstract={Graduating computer science and software engineering students do not always possess the necessary skills, abilities, or knowledge when beginning their careers in the software industry. The lack of these skills and abilities can limit the productivity of newly hired, recent graduates, or even prevent them from gaining employment. This paper presents the results of an empirical study where twenty-three managers and hiring personnel from various software companies in the United States and Europe were interviewed. Participants were asked about areas where recent graduates frequently struggled when beginning employment at their companies and which skill deficiencies might prevent a recent graduate from being hired. The results of this study indicate that recent graduates struggle with using configuration management systems (and other software tools), effectively communicating with co-workers and customers, producing unit tests for their code, and other skills or abilities. The results also indicate that a lack of project experience and problem solving abilities are the most commonly cited issues preventing students from gaining employment. This research is intended to assist educators in identifying areas where students may not measure up the expectations of industry companies and in improving the curriculum at their universities to better prepare them for their future careers.}
}

@article{rayyan-727968317,
  title={Visual notations for software pattern languages: A mapping study},
  year={2018},
  issn={978-1-4503-6503-1},
  pages={72-81},
  author={da Silva Quirino, Glaice Kelly and Barcellos, Monalessa Perini and de Almeida Falbo, Ricardo},
  url={https://doi.org/10.1145/3266237.3266266},
  publisher={Association for Computing Machinery},
  series={SBES '18},
  keywords={mapping study, pattern language, visual notation, Software},
  abstract={Reuse has been recognized as an important practice in software engineering. The use of patterns makes it easier to reuse successful solutions, speeds up the development process, and promotes the application of good practices. Related patterns can be organized in a Pattern Language (PL), which represents the patterns and their relations, and provides guidance on how to select, reuse and integrate them. Visual notations are often used to provide a graphical representation to PLs. Aiming to investigate how PLs related to software have been visually represented, we carried out a systematic mapping. We identified and analyzed 64 PLs. As a result, we noticed a lack of consensus on the elements that should be represented in a PL and the symbols used to represent them. Moreover, most PLs have ambiguous or inexpressive visual representations.}
}

@article{rayyan-727968318,
  title={Publication bias: A detailed analysis of experiments published in ESEM},
  year={2020},
  issn={978-1-4503-7731-7},
  pages={130-139},
  author={Reyes, Rolando P and Dieste, Oscar and C., Efraín R Fonseca and Juristo, Natalia},
  url={https://doi.org/10.1145/3383219.3383233},
  publisher={Association for Computing Machinery},
  series={EASE '20},
  keywords={publication bias, survey, literature review, experimentation, exploratory research, Research bias, statistical errors, Bias (Epidemiology), Publication Bias},
  abstract={Background: Publication bias is the failure to publish the results of a study based on the direction or strength of the study findings. The existence of publication bias is firmly established in areas like medical research. Recent research suggests the existence of publication bias in Software Engineering. Aims: Finding out whether experiments published in the International Workshop on Empirical Software Engineering and Measurement (ESEM) are affected by publication bias. Method: We review experiments published in ESEM. We also survey with experimental researchers to triangulate our findings. Results: ESEM experiments do not define hypotheses and frequently perform multiple testing. One-tailed tests have a slightly higher rate of achieving statistically significant results. We could not find other practices associated with publication bias. Conclusions: Our results provide a more encouraging perspective of SE research than previous research: (1) ESEM publications do not seem to be strongly affected by biases and (2) we identify some practices that could be associated with p-hacking, but it is more likely that they are related to the conduction of exploratory research.}
}

@article{rayyan-727968319,
  title={User studies on end-user service composition: A literature review and a design framework},
  year={2019},
  journal={ACM Transactions on the Web},
  issn={1559-1131},
  volume={13},
  number={3},
  author={Zhao, Liping and Loucopoulos, Pericles and Kavakli, Evangelia and Letsholo, Keletso J},
  url={https://doi.org/10.1145/3340294},
  keywords={systematic review, empirical studies, service-oriented computing, design guideline, end-user service composition, mapshups, qualitative studies, review framework, User studies, web services},
  abstract={Context: End-user service composition (EUSC) is a service-oriented paradigm that aims to empower end users and allow them to compose their own web applications from reusable service components. User studies have been used to evaluate EUSC tools and processes. Such an approach should benefit software development, because incorporating end users' feedback into software development should make software more useful and usable. Problem: There is a gap in our understanding of what constitutes a user study and how a good user study should be designed, conducted, and reported. Goal: This article aims to address this gap. Method: The article presents a systematic review of 47 selected user studies for EUSC. Guided by a review framework, the article systematically and consistently assesses the focus, methodology and cohesion of each of these studies. Results: The article concludes that the focus of these studies is clear, but their methodology is incomplete and inadequate, their overall cohesion is poor. The findings lead to the development of a design framework and a set of questions for the design, reporting, and review of good user studies for EUSC. The detailed analysis and the insights obtained from the analysis should be applicable to the design of user studies for service-oriented systems as well and indeed for any user studies related to software artifacts.}
}

@article{rayyan-727968320,
  title={Principles of survey research part 4: Questionnaire evaluation},
  year={2002},
  journal={SIGSOFT Softw. Eng. Notes},
  issn={0163-5948},
  volume={27},
  number={3},
  pages={20-23},
  author={Kitchenham, Barbara and Pfleeger, Shari Lawrence},
  url={https://doi.org/10.1145/638574.638580},
  keywords={researcher bias, respondent motivation, survey reliability, survey validity, Questionnaires},
  abstract={This article discusses how to avoid biased questions in survey instruments, how to motivate people to complete instruments and how to evaluate instruments. In the context of survey evaluation, we discuss how to assess survey reliability i.e. how reproducible a survey's data is and survey validity i.e. how well a survey instrument measures what it sets out to measure.}
}

@article{rayyan-727968321,
  title={Jumpstart sustainability in seminars: Hands-on experiences in class},
  year={2012},
  issn={978-1-4503-1858-7},
  pages={37-44},
  author={Penzenstadler, Birgit and Bauer, Veronika},
  url={https://doi.org/10.1145/2421277.2421282},
  publisher={Association for Computing Machinery},
  series={CSERC '12},
  keywords={software engineering, education, sustainability, requirements, Hand},
  abstract={Sustainability in its different aspects is hardly addressed in software engineering education, neither as quality objective of system development nor in business process design.Consequently, students tend to be unaware of this concept and are not considering sustainability as an important aspect of systems development. This results in the development of (environmentally or socially) suboptimal solutions—although information and communication technology systems could offer great support in promoting and enabling sustainability in our society. To unlock this potential, we are sensitizing students for the issue by gradually introducing the concept in the curriculum.In this paper, we report on experiences in establishing sustainability in the software engineering curriculum of Bachelor and Master students by means of designated interactive seminars. A guideline on how to establish similar teaching activities concludes the paper.}
}

@article{rayyan-727968322,
  title={Using meta-ethnography to synthesize research on knowledge management and agile software development methodology},
  year={2018},
  issn={978-1-4503-6565-9},
  pages={230-239},
  author={Ruiz, Glauco Antonio and Napoleão, Bianca Minetto and de Souza, Erica Ferreira and Felizardo, Katia Romero and Meinerz, Giovani Volnei and da Silva, Patrick Rodrigo and Vijaykumar, Nandamudi L},
  url={https://doi.org/10.1145/3275245.3275270},
  publisher={Association for Computing Machinery},
  series={SBQS},
  keywords={Software Engineering, Knowledge Management, Agile Software Development, Software},
  abstract={Context: Software development processes are considered as knowledge intensive and therefore Knowledge Management (KM) can be applied to efficiently manage the knowledge generated. Agile practices can benefit the software organizations in terms of KM. Some studies have already presented evidence about this relationship. However, the intersection of these two areas still require further more clarification. Objective: This study aims to synthesize research on KM and Agile Software Development (ASD) using the meta-ethnography method. Method: In order to achieve the proposed goal, first, we applied the seven phases of meta-ethnography analysis method on a five articles selected from a tertiary study on KM and ASD. Second, the relations identified between the areas investigated were analysed from interviews with three agile development methodology experts. Results: A relation map that summarizes the synthesis identified between KM, agile values and scrum activities was created. Conclusion: There is a significant contribution in KM and ASD for both software engineering academics and industry.}
}

@article{rayyan-727968323,
  title={Costing secure software development: A systematic mapping study},
  year={2019},
  issn={978-1-4503-7164-3},
  author={Venson, Elaine and Guo, Xiaomeng and Yan, Zidi and Boehm, Barry},
  url={https://doi.org/10.1145/3339252.3339263},
  publisher={Association for Computing Machinery},
  series={ARES '19},
  keywords={Software security, software development effort estimation, secure software development, software cost model, Software},
  abstract={Building more secure software is a recent concern for software engineers due to increasing incidences of data breaches and other types of cyber attacks. However, software security, through the introduction of specialized practices in the software development life cycle, leads to an increase in the development cost. Although there are many studies on software cost models, few address the additional costs required to build secure software. We conducted a systematic review in the form of a mapping study to classify and analyze the literature related to the impact of security in software development costs. Our search strategy strove to achieve high completeness by the identification of a quasi-gold-standard set of papers, which we then used to establish a search string and retrieve papers from research databases automatically. The application of inclusion/exclusion criteria resulted in a final set of 54 papers, which were categorized according to the approach to software security cost analysis. Perform Security Review, Apply Threat Modeling, and Perform Security Testing were the three most frequent activities related to cost, and Common Criteria was the most applied standard. We also identified ten approaches to estimating software security costs for development projects; however, their validation remains a challenge, which could be addressed in future studies.}
}

@article{rayyan-727968324,
  title={Beyond the spreadsheet: Reflections on tool support for literature studies},
  year={2016},
  issn={978-1-4503-3691-8},
  author={Tell, Paolo and Cholewa, Jacob B and Nellemann, Peter and Kuhrmann, Marco},
  url={https://doi.org/10.1145/2915970.2916011},
  publisher={Association for Computing Machinery},
  series={EASE '16},
  keywords={tool, design exploration, literature study, prototyping},
  abstract={Background: Even though a number of tools are reported to be used by researchers undertaking systematic reviews, important shortages are still reported revealing how such solutions are unable to satisfy current needs. Method: Two research groups independently provided a design for a tool supporting systematic reviews. The resulting tools were assessed against the feature lists provided by prior research. Results: After presenting an overview of the tools and the core design decisions taken, we provide a feature analysis and a discussion regarding selected challenges deemed crucial to provide a proper tool support. Conclusions: Although the designed solutions do not yet support the entire systematic review process, their architecture has been designed to be flexible and extendable. After highlighting the difficulties of developing appropriate tools, we call for action: developing tools to support systematic reviews is a community project.}
}

@article{rayyan-727968325,
  title={Experience-based guidelines for effective and efficient data extraction in systematic reviews in software engineering},
  year={2017},
  issn={978-1-4503-4804-1},
  pages={170-179},
  author={Garousi, Vahid and Felderer, Michael},
  url={https://doi.org/10.1145/3084226.3084238},
  publisher={Association for Computing Machinery},
  series={EASE'17},
  keywords={SLR, systematic literature reviews, Systematic mapping studies, empirical software engineering, SM, data extraction, research methodology, Software},
  abstract={To systematically collect evidence and to structure a given area in software engineering (SE), Systematic Literature Reviews (SLR) and Systematic Mapping (SM) studies have become common. Data extraction is one of the main phases (activities) when conducting an SM or an SLR, whose objective is to extract required data from the primary studies and to accurately record the information researchers need to answer the questions of the SM/SLR study. Based on experience in a large number of SM/SLR studies, we and many other researchers have found the data extraction in SLRs to be time consuming and error-prone, thus raising the real need for heuristics and guidelines for effective and efficient data extraction in these studies, especially to be learnt by junior and young researchers. As a 'guideline' paper, this paper contributes a synthesized list of challenges usually faced during SLRs' data extraction phase and the corresponding solutions (guidelines). For our synthesis, we consider two data sources: (1) the pool of 16 SLR studies in which the authors have been involved in, as well as (2) a review of challenges and guidelines in the existing literature. Our experience in utilizing the presented guidelines in the near past have helped our junior colleagues to conduct data extractions more effectively and efficiently.}
}

@article{rayyan-727968326,
  title={The role of anxiety when learning to program: A systematic review of the literature},
  year={2016},
  issn={978-1-4503-4770-9},
  pages={61-70},
  author={Nolan, Keith and Bergin, Susan},
  url={https://doi.org/10.1145/2999541.2999557},
  publisher={Association for Computing Machinery},
  series={Koli calling '16},
  keywords={systematic review, computer science, learning, programming, anxiety, Anxiety},
  abstract={According to the World Health Organisation the number one global health issue for young people is their mental health. For students, mental well-being is associated with effective learning, and their ability to navigate through university/college, having the resilience to cope with the challenges and stresses of student life.In Ireland, Computer Science (CS) non-progression rates are alarming, with a large number of students failing to progress each year. Currently non-progression rates are 25% in CS, significantly higher than the national average of 16% across all other fields of study. On top of the normal stressors of transitioning or returning to university, CS students are arguably exposed to a unique set of factors that could further induce anxiety. First, they typically have no formal CS exposure or training to draw on. Second, the number of contact hours and workload are high. Third, CS courses includes programming modules. For some, learning to program is diffcult and many struggle to master the core concepts. Learning typically takes place in a lab environment where inexperienced programmers will begin to type (code") shortly after being presented with a problem rather than spending time designing a solution. Thus the lab becomes active and busy from the onset, making struggling students cripplingly perceive their peers know more. Further, novice programmers use the compiler to constantly monitor their progress and error messages can be perceived as negative feedback. Such an environment can create or compound anxiety and stress. At our institution a large number of CS students register for counselling services or leave.In this paper we present a systematic literature review on the role of anxiety when learning to program. The work is novel, valuable, and timely. The approach used is systematic, in that a structured search of electronic resources has been conducted and the results are presented and quantitatively analysed. A detailed discussion on the findings is provided and important implications for the teaching and learning of programming are described.}
}

@article{rayyan-727968327,
  title={A tertiary study: Experiences of conducting systematic literature reviews in software engineering},
  year={2013},
  issn={978-1-4503-1848-8},
  pages={177-182},
  author={Imtiaz, Salma and Bano, Muneera and Ikram, Naveed and Niazi, Mahmood},
  url={https://doi.org/10.1145/2460999.2461025},
  publisher={Association for Computing Machinery},
  series={EASE '13},
  keywords={tertiary study, systematic literature reviews, empirical software engineering, experiences, lessons learnt, Software},
  abstract={Context: The use of Systematic Literature Review (SLR) requires expertise and poses many challenges for novice researchers. The experiences of those who have used this research methodology can benefit novice researchers in effectively dealing with these challenges. Objective: The aim of this study is to record the reported experiences of conducting Systematic Literature Reviews, for the benefit of new researchers. Such a review will greatly benefit the researchers wanting to conduct SLR for the very first time. Method: We conducted a tertiary study to gather the experiences published by researchers. Studies that have used the SLR research methodology in software engineering and have implicitly or explicitly reported their experiences are included in this review. Results: Our research has revealed 116 studies relevant to the theme. The data has been extracted by two researchers working independently and conflicts resolved after discussion with third researcher. Findings from these studies highlight Search Strategy, Online Databases, Planning and Data Extraction as the most challenging phases of SLR. Lack of standard terminology in software engineering papers, poor quality of abstracts and problems with search engines are some of the most cited challenges. Conclusion: Further research and guidelines is required to facilitate novice researchers in conducting these phases properly.}
}

@article{rayyan-727968328,
  title={Realising evidence-based software engineering (REBSE-2) a report from the workshop held at ICSE 2007},
  year={2007},
  journal={SIGSOFT Softw. Eng. Notes},
  issn={0163-5948},
  volume={32},
  number={4},
  pages={36-39},
  author={Budgen, David and Brereton, Pearl},
  url={https://doi.org/10.1145/1281421.1281441},
  keywords={Software},
  abstract={Context: The REBSE international workshops are concerned with exploring the adaptation and use of the evidence-based paradigm in software engineering research and practice, through a mix of presentations and discussion.Objectives: These were to explore both experience with, and potential for, evidence-based software engineering (EBSE); to consider how this might affect empirical practices in software engineering; and to work towards creating a community of researchers to practice and promote EBSE.Method: Three sessions were dedicated to a mix of presentations and interactive discussion, while the fourth was dedicated to summarising progress and identifying both issues of concern and actions to pursue.Conclusions: While we identified a number of issues, a key need is clearly to have a central repository to both provide information and to maintain a record of activity in this area.}
}

@article{rayyan-727968329,
  title={ASE 2016: Proceedings of the 31st IEEE/ACM international conference on automated software engineering},
  year={2016},
  issn={978-1-4503-3845-5},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  keywords={Software}
}

@article{rayyan-727968330,
  title={Variability management in software product lines: A systematic review},
  year={2009},
  pages={81-90},
  author={Chen, Lianping and Ali Babar, Muhammad and Ali, Nour},
  publisher={Carnegie Mellon University},
  series={SPLC '09},
  keywords={⛔ No DOI found, systematic reviews, software product lines, variability management, Software},
  abstract={Variability Management (VM) in Software Product Line (SPL) is a key activity that usually affects the degree to which a SPL is successful. SPL community has spent huge amount of resources on developing various approaches to dealing with variability related challenges over the last decade. To provide an overview of different aspects of the proposed VM approaches, we carried out a systematic literature review of the papers reporting VM in SPL. This paper presents and discusses the findings from this systematic literature review. The results reveal the chronological backgrounds of various approaches over the history of VM research, and summarize the key issues that drove the evolution of different approaches. This study has also identified several gaps that need to be filled by future efforts in this line of research.}
}

@article{rayyan-727968331,
  title={How office layouts influence software development?},
  year={2020},
  issn={978-1-4503-8753-8},
  pages={173-182},
  author={Costa, Victor G J and França, César},
  url={https://doi.org/10.1145/3422392.3422441},
  publisher={Association for Computing Machinery},
  series={SBES '20},
  keywords={Software Engineering, Systematic Literature Review, Office Layout, Work Environment, Software},
  abstract={Background: Organizations are constantly looking for performance improvements, and office layout has been widely studied because of its hypothetical influences on the social dynamics of software engineering projects. Aim: In this article, we investigate the perceived outcomes of different workspace characteristics, from the perspective of software engineering professionals. Methods: To achieve that, we conducted a survey with software engineering practitioners, and collected data on the perceptions about their current workspaces and performance from 47 participants. We used the results of a previous systematic review to design the survey questionnaire, and focused on the four human aspects known to be influenced by the office layout. Results: Different workspace settings exhibited similar perceptions in most of the investigated factors. However, we reveal 14 items that responsible for significant differences in the performance outcomes, such as communication quality, collaboration, team learning, privacy and others. In general, open spaces were the most effective office layout to enable all these factors. Conclusions: As a conclusion, our study demonstrates that there is not a generally accepted best model for software development workspace design, as all types of setting have positive and negative aspects. Organizations that are considering investing any budget in such things as radical workspace redesign should ponder the change very carefully. Also, there is still much room for investigation in this topic.}
}

@article{rayyan-727968332,
  title={Application of search-based software engineering methodologies for test suite optimization and evolution in mission critical mobile application development},
  year={2017},
  issn={978-1-4503-5105-8},
  pages={1034-1037},
  author={Schuler, Andreas},
  url={https://doi.org/10.1145/3106237.3119876},
  publisher={Association for Computing Machinery},
  series={ESEC/FSE 2017},
  keywords={multi-objective optimization, test suite optimization, test automation, mobile application development, Software},
  abstract={The demand for high quality mobile applications is constantly rising, especially in mission critical settings. Thus, new software engineering methodologies are needed in order to ensure the desired quality of an application. The research presented proposes a quality assurance methodology for mobile applications through test automation by optimizing test suites. The desired goal is to find a minimal test suite while maintaining efficiency and reducing execution cost. Furthermore to avoid invalidating an optimized test suite as the system under test evolves, the approach further proposes to extract patterns from the applied changes to an application. The evaluation plan comprises a combination of an empirical and an industrial case study based on open source projects and an industrial project in the healthcare domain. It is expected that the presented approach supports the testing process on mobile application platforms.}
}

@article{rayyan-727968333,
  title={Plagiarism in programming assessments: A systematic review},
  year={2019},
  journal={ACM Transactions on Computing Education},
  volume={20},
  number={1},
  author={Albluwi, Ibrahim},
  url={https://doi.org/10.1145/3371156},
  keywords={academic integrity, cheating, Introductory programming, plagiarism},
  abstract={This article is a systematic review of work in the computing education literature on plagiarism. The goal of the review is to summarize the main results found in the literature and highlight areas that need further work. Despite the the large body of work on plagiarism, no systematic reviews have been published so far.The reviewed papers were categorized and analyzed using a theoretical framework from the field of Fraud Deterrence named the Fraud Triangle. According to this framework, fraudulent behavior occurs when the person is under pressure, perceives the availability of an opportunity to commit fraud, and rationalizes the fraudulent behavior in a way that makes it seem not unethical to him or her.The review found the largest amount of the reviewed papers to discuss ways for reducing the opportunity to plagiarize, as well as tools for detecting plagiarism. However, there is a clear lack of empirical work evaluating the deterrent efficacy of these strategies and tools. The reviewed papers also included mentions of a wide range of rationalizations used by computing students when justifying plagiarism, the most important of which are rationalizations that stem from confusion about what constitutes plagiarism. Finally, work on the relationship between pressure in computing courses and plagiarism was found to be very scarce and incommensurate with the significant contribution of this factor to plagiarism.}
}

@article{rayyan-727968334,
  title={The evolution of the laws of software evolution: A discussion based on a systematic literature review},
  year={2013},
  journal={ACM Computing Surveys},
  issn={0360-0300},
  volume={46},
  number={2},
  author={Herraiz, Israel and Rodriguez, Daniel and Robles, Gregorio and Gonzalez-Barahona, Jesus M},
  url={https://doi.org/10.1145/2543581.2543595},
  keywords={software evolution, Laws of software evolution, Software},
  abstract={After more than 40 years of life, software evolution should be considered as a mature field. However, despite such a long history, many research questions still remain open, and controversial studies about the validity of the laws of software evolution are common. During the first part of these 40 years, the laws themselves evolved to adapt to changes in both the research and the software industry environments. This process of adaption to new paradigms, standards, and practices stopped about 15 years ago, when the laws were revised for the last time. However, most controversial studies have been raised during this latter period. Based on a systematic and comprehensive literature review, in this article, we describe how and when the laws, and the software evolution field, evolved. We also address the current state of affairs about the validity of the laws, how they are perceived by the research community, and the developments and challenges that are likely to occur in the coming years.}
}

@article{rayyan-727968335,
  title={IoT architectural concerns: A systematic review},
  year={2017},
  issn={978-1-4503-4774-7},
  author={Gill, Asif Qumer and Behbood, Vahid and Ramadan-Jradi, Rania and Beydoun, Ghassan},
  url={https://doi.org/10.1145/3018896.3025166},
  publisher={Association for Computing Machinery},
  series={ICC '17},
  keywords={IoT, architecture, internet of things, enterprise architecture, digital-physical ecosystem},
  abstract={There is increasing interest in studying and applying Internet of Things (IoT) within the overall context of digital-physical ecosystems. Most recently, much has been published on the benefits and applications of IoT. The main question is: what are the key IoT architectural concerns, which must be addressed to effectively develop and implement an IoT architecture? There is a need to systematically review and synthesize the literature on IoT architectural challenges or concerns. Using the SLR approach and applying customised search criteria derived from the research question, 22 relevant studies were identified and reviewed in this paper. The data from these papers were extracted to identify the IoT architectural challenges and relevant solutions. These results were organised into to 9 major challenge and 7 solution categories. The results of this research will serve as a resource for practitioners and researchers for the effective adoption, and setting future research priorities and directions in this emerging area of IoT architecture.}
}

@article{rayyan-727968336,
  title={Secure V2X environment using blockchain technology},
  year={2020},
  issn={978-1-4503-7731-7},
  pages={469-474},
  author={Taiyaba, Ms. and Akbar, Muhammad Azeem and Qureshi, Bushra and Shafiq, Muhammad and Hamza, Muhammad and Riaz, Tanveer},
  url={https://doi.org/10.1145/3383219.3383287},
  publisher={Association for Computing Machinery},
  series={EASE '20},
  keywords={Systematic literature review, Blockchain, Challenges, V2X network},
  abstract={Vehicle to everything (V2X) is the new and revolutional phenomena of the recent era. In V2X environment, the data security of data and information is the most significant parameter, as the information is shared on the shared network. To secure the information in V2X environment; the blockchain technology can play a critical role as it consists of the powerful data encryption and decryption algorithms. To implement the characteristics of blockchain in V2X environment, the practitioners face various challenges. The key aim of this study is to develop a hypothetical framework that renders the impact of challenging factors on the implementation of blockchain in V2X paradigm. We have conducted systematic literature review approach and identified the 10 challenges that are critical to address for the successful implementation of blockchain technology in V2X network. We believe that the findings of this study serves as knowledge base for the researchers and practitioners to develop the effective strategies for the successful implementation of blockchain characteristics in V2X environment.}
}

@article{rayyan-727968337,
  title={RMDevOps: A road map for improvement in DevOps activities in context of software organizations},
  year={2020},
  issn={978-1-4503-7731-7},
  pages={413-418},
  author={Rafi, Saima and Yu, Wu and Akbar, Muhammad Azeem},
  url={https://doi.org/10.1145/3383219.3383278},
  publisher={Association for Computing Machinery},
  series={EASE '20},
  keywords={systematic literature review, guidelines, Readiness model, software organizations, Software},
  abstract={DevOps is a new software engineering paradigm adopted by various software organizations to develop an environment of continuous deployment and delivery within time. Numerous experts are offering their services to help organizations, how to implement DevOps activities in software organization. Though, still there are various issues for software organizations to adopt DevOps activities. To overcome such issues, there must be an approach that could assist software organizations towards better adoption of DevOps activities. The core objective of this research is to design a Readiness Model for DevOps (RMDevOps) to improve the adoption of DevOps activities in a software organization. Based on existing models in other fields of software engineering, we will develop this model. We have conducted a systematic literature review and empirical study on DevOps, for understanding the impact of the success factors of DevOps in the real world and literature. This study covers the first step of development of RMDevOps model, by identifying the success factors of DevOps and presenting the outcomes in the form of robust framework.}
}

@article{rayyan-727968338,
  title={A visual analysis approach to update systematic reviews},
  year={2014},
  issn={978-1-4503-2476-2},
  author={Felizardo, Katia Romero and Nakagawa, Elisa Yumi and MacDonell, Stephen G and Maldonado, José Carlos},
  url={https://doi.org/10.1145/2601248.2601252},
  publisher={Association for Computing Machinery},
  series={EASE '14},
  keywords={systematic review, systematic literature review, visual text mining, VTM},
  abstract={Context: In order to preserve the value of Systematic Reviews (SRs), they should be frequently updated considering new evidence that has been produced since the completion of the previous version of the reviews. However, the update of an SR is a time consuming, manual task. Thus, many SRs have not been updated as they should be and, therefore, they are currently outdated. Objective: The main contribution of this paper is to support the update of SRs. Method: We propose USR-VTM, an approach based on Visual Text Mining (VTM) techniques, to support selection of new evidence in the form of primary studies. We then present a tool, named Revis, which supports our approach. Finally, we evaluate our approach through a comparison of outcomes achieved using USR-VTM versus the traditional (manual) approach. Results: Our results show that USR-VTM increases the number of studies correctly included compared to the traditional approach. Conclusions: USR-VTM effectively supports the update of SRs.}
}

@article{rayyan-727968339,
  title={SIOT-RIMM: Towards secure IOT-Requirement implementation maturity model},
  year={2020},
  issn={978-1-4503-7731-7},
  pages={463-468},
  author={Hamza, Muhammad and Hu, Haibo and Akbar, Muhammad Azeem and Mehmood, Faisal and Hussain, Yasir and Baddour, Ali Mahmoud},
  url={https://doi.org/10.1145/3383219.3383286},
  publisher={Association for Computing Machinery},
  series={EASE '20},
  keywords={Systematic literature review, Maturity model, Empirical investigations, IoT requirements},
  abstract={It is very crucial for an organization to encapsulate the requirements in its early stage when they are intending to build a novel system such as the internet of things (IoT), particularly when it comes to capturing privacy and security requirements to gain the public confidence. The proposed research is focused to develop a secure IoT-requirement implementation maturity model (SIOT-RIMM). The proposed model will assist the software development organizations to improve and modify their requirement engineering processes in terms of security and privacy of IoT. The SIOT-RIMM model will be developed based on the existing IoT literature pertaining to security and privacy, industrial empirical study and understanding of the challenges that could negatively influence the implementation of security and privacy in IoT. To develop the maturity levels of SIOT-RIMM, we will consider the concepts of existing maturity models of other software engineering domains. In this preliminary study, 19 challenges were identified using the SLR approach that might have a negative impact on the IoT requirements engineering process. The identified challenges will contribute to the development of SIOT-RIMM maturity levels.}
}

@article{rayyan-727968340,
  title={State of the art for risk management in software acquisition},
  year={2009},
  journal={SIGSOFT Softw. Eng. Notes},
  issn={0163-5948},
  volume={34},
  number={4},
  pages={1-10},
  author={Calvo-Manzano Villalón, Jose A and Agustín, Gonzalo Cuevas and Hurtado, Gloria Gasca and San Feliu Gilabert, Tomás},
  url={https://doi.org/10.1145/1543405.1543426},
  keywords={systematic review, software engineering, process improvement, risk management, software acquisition, Risk Management, Software},
  abstract={This paper presents the state of the art for risk management in software acquisition. To determine it, a systematic review protocol for Software Engineering is used. Furthermore, the systematic re-view focuses on identifying initiatives and reports of risk manage-ment proposals for software acquisition in small settings. Results show increasing research in risk management and the need for more in-depth studies.}
}

@article{rayyan-727968341,
  title={Maintaining systematic literature reviews: Benefits and drawbacks},
  year={2018},
  issn={978-1-4503-5823-1},
  author={Nepomuceno, Vilmar and Soares, Sergio},
  url={https://doi.org/10.1145/3239235.3267432},
  publisher={Association for Computing Machinery},
  series={ESEM '18},
  keywords={systematic literature review, evidence based software engineering, traceability, maintenance},
  abstract={Background: Maintenance and traceability (versioning) are constant concerns in Software Engineering (SE), however, few works related to these topics in Systematic Literature Reviews (SLR) were found. Goal: The goal of this research is to elucidate how SLRs can be maintained and what are the benefits and drawbacks in this process. Method: This work presents a survey where experienced researchers that conducted SLRs between 2011 and 2015 answered questions about maintenance and traceability and, using software maintenance concepts, it addresses the SLRs maintenance process. From the 79 e-mails sent we reach 28 answers. Results: 19 of surveyed researchers have shown interest in keeping their SLRs up-to-date, but they have expressed concerns about the effort to be made to accomplish it. It was also observed that 20 participants would be willing to share their SLRs in common repositories, such as GitHub. Conclusions: There is a need to perform maintenance on SLRs. Thus, we are proposing a SLR maintenance process, taking into account some benefits and drawbacks identified during our study and presented through the paper.}
}

@article{rayyan-727968342,
  title={The use of grey literature review as evidence for practitioners},
  year={2019},
  journal={SIGSOFT Softw. Eng. Notes},
  issn={0163-5948},
  volume={44},
  number={3},
  pages={23},
  author={Kamei, Fernando Kenji},
  url={https://doi.org/10.1145/3356773.3356797},
  keywords={grey literature, systematic literature review, evidence- based software engineering},
  abstract={Context: In the last years, diverse research areas increased their interest in Grey Literature (GL). In Software Engineering (SE), SE practitioners became heavy consumers of GL, by way of contrast to traditional research papers. Problem: This is unfortunate, in particular, although the increase of Systematic Literature Reviews (SLR) published, researchers, claim to the lack of them of connection to the practice. Goal: Propose and evaluate a set of guidelines to help SE researchers to conduct a Grey Literature Review (GLR) that are more in line with practitioners' needs. Method: First, we are conducting a tertiary study to understand how secondary studies use GL. Second, we plan to employ qualitative research with researchers of SLRs and SE practitioners. Third, we plan to review and analyze the use of GL source according to the context in SE. Fourth, we plan to conduct a GLR. Finally, we plan to perform and evaluate our guideline. Preliminary Results: The tertiary study retrieved a total of 14,043 papers. We removed the duplicate studies and also which were not peer-reviewed articles. Currently, we are solving the conflicts of disagreement of the selection process. Conclusions: We present preliminary findings, show our proposed approach and the next steps.}
}

@article{rayyan-727968343,
  title={E-government inter-organizational integration: Types and success factors},
  year={2020},
  issn={978-1-4503-7690-7},
  pages={216-221},
  author={Putri, Mieke Eka and Sensuse, Dana Indra and Mishbah, Muhammad and Prima, Pudy},
  url={https://doi.org/10.1145/3378936.3378955},
  publisher={Association for Computing Machinery},
  series={ICSIM '20},
  keywords={systematic literature review, e-government, success factors, integration, inter-organizational, kitchenham},
  abstract={The rapid development of ICT in Indonesia encourages governments to implement e-government for supporting their services so they can improve their service delivery, strengthen accountability, and increase transparency. Unfortunately, there is a major drawback regarding the implementation of e-government. One of the drawbacks is that most organizations develop silos online services. This condition reflects a lack of e-government inter-organizational integrations. E-government integration among organizations or agencies is necessary to promote a more efficient process, accurate information, and seamless services, which are the objectives of the e-government itself. E-government integration is a complicated process as stakeholders need to consider and plan several factors to make it successful. This study identified the types of e-government inter-organizational integrations through a systematic literature review using the Kitchenham method. The study reveals the success factors which need to be concerned to implement e-government successfully. This study can be used as a reference by stakeholders to initiate an implementation of e-government inter-organizational integration.}
}

@article{rayyan-727968344,
  title={Teaching discrete structures: A systematic review of the literature},
  year={2011},
  issn={978-1-4503-0500-6},
  pages={275-280},
  author={Power, James F and Whelan, Thomas and Bergin, Susan},
  url={https://doi.org/10.1145/1953163.1953247},
  publisher={Association for Computing Machinery},
  series={SIGCSE '11},
  keywords={computing curriculum, discrete mathematics, discrete structures},
  abstract={This survey paper reviews a large sample of publications on the teaching of discrete structures and discrete mathematics in computer science curricula. The approach is systematic, in that a structured search of electronic resources has been conducted, and the results are presented and quantitatively analyzed. A number of broad themes in discrete structures education are identified relating to course content, teaching strategies and the means of evaluating the success of a course.}
}

@article{rayyan-727968345,
  title={Mapping the systematic literature studies about software ecosystems},
  year={2018},
  issn={978-1-4503-6518-5},
  pages={910-918},
  author={García-Holgado, Alicia and García-Peñalvo, Francisco J},
  url={https://doi.org/10.1145/3284179.3284330},
  publisher={Association for Computing Machinery},
  series={TEEM'18},
  keywords={systematic literature review, software engineering, systematic mapping, software ecosystems, Technological ecosystems, Software},
  abstract={There is a need to improve the definition and development of technological ecosystems in order to solve the main problems detected in previous studies. To achieve this goal, it is required to identify and analyse the solutions available in the literature in the field of software engineering applied to ecosystems. The research in software ecosystems is a relatively young research area, but there are already several works that analyse the literature associated. To conduct a new systematic literature review is necessary to ensure that there are no studies that do the same, namely, that do not answer the same research questions. The identification of the need for a review was done through a study focused on systematic literature reviews and mapping studies about software ecosystems. This work aims to describe the mapping conducted as part of that study. It provides a global state of the art of this kind of studies in the area of software ecosystems.}
}

@article{rayyan-727968346,
  title={Inventory routing problem with time windows: A systematic review of the literature},
  year={2018},
  issn={978-1-4503-6559-8},
  author={Alves, Pedro Yuri A L and Delgado, Karina Valdivia and da Silva, Valdinei Freire},
  url={https://doi.org/10.1145/3229345.3229376},
  publisher={Association for Computing Machinery},
  series={SBSI'18},
  keywords={Inventory Routing Problem, Time Windows, Vehicle Routing Problem},
  abstract={The integration of inventory management, vehicle routing, and scheduling decisions is known as Inventory Routing Problem (IRP). In this problem, the main objective is to determine: (i) the quantity of products to be delivered based on customer's demand; (ii) the delivery periods and (iii) the route of vehicles. IRPs that include time window service constraints are found in real scenarios and the attendance of this type of constraints brings benefits to both the supplier and the customer. This systematic review of the literature aims to identify trends in the area that studies Inventory Routing Problems with time windows. We identify and analyze the proposed approaches, the characteristics of the solved problems and the type of experiments performed in 9 primary studies.}
}

@article{rayyan-727968347,
  title={Traveling salesperson problem with hotel selection: A systematic review of the literature},
  year={2019},
  issn={978-1-4503-7237-4},
  author={de Sousa, Marques Moreira and Ochi, Luiz Satoru and de Lima Martins, Simone},
  url={https://doi.org/10.1145/3330204.3330243},
  publisher={Association for Computing Machinery},
  series={SBSI'19},
  keywords={Combinatorial Optimization, Hotel Selection, Traveling Salesperson Problem, Travel},
  abstract={The Traveling Salesperson Problem with Hotel Selection (TSPHS) is a novel variant of the classical Traveling Salesperson Problem. In this variant, the main objective is to minimize the number of trips and total traveled time. This problem is found in real scenarios like delivery of products by electric vehicles that need to be recharged along a tour. This systematic review of the literature aims to analyse techniques and trends to solve this logistic problem using combinatorial optimization. We have applied a search string related to TSPHS for three search engines, filtered 33 returned results and analyzed the proposed approaches to identify the following aspects: (i) characteristics of the solved problems, (ii) if statistics analyses have been performed for parameter tuning in case of heuristics approaches, (iii) characteristics of instances solved by exact methods and (iv) types of experiments. Furthermore, we have observed that the Lin-Kernighan heuristic performs well to generate initial solution for heuristics, the hotels order sequence has major impact on final solution quality and the complexity of this problem still represents for modern Information Systems a constraint to solve large instances using exact approaches.}
}

@article{rayyan-727968348,
  title={Evaluating strategies for study selection in systematic literature studies},
  year={2014},
  issn={978-1-4503-2774-9},
  author={Ali, Nauman Bin and Petersen, Kai},
  url={https://doi.org/10.1145/2652524.2652557},
  publisher={Association for Computing Machinery},
  series={ESEM '14},
  keywords={systematic review, inclusion and exclusion, study selection},
  abstract={Context: The study selection process is critical to improve the reliability of secondary studies. Goal: To evaluate the selection strategies commonly employed in secondary studies in software engineering. Method: Building on these strategies, a study selection process was formulated and evaluated in a systematic review. Results: The selection process used a more inclusive strategy than the one typically used in secondary studies, which led to additional relevant articles. Conclusions: The results indicates that a good-enough sample could be obtained by following a less inclusive but more efficient strategy, if the articles identified as relevant for the study are a representative sample of the population, and there is a homogeneity of results and quality of the articles.}
}

@article{rayyan-727968349,
  title={QoS-Aware autonomic resource management in cloud computing: A systematic review},
  year={2015},
  journal={ACM Computing Surveys},
  issn={0360-0300},
  volume={48},
  number={3},
  author={Singh, Sukhpal and Chana, Inderveer},
  url={https://doi.org/10.1145/2843889},
  keywords={cloud computing, self-management, autonomic computing, autonomic cloud computing, autonomic management, grid computing, quality of service, resource management, Resource provisioning, resource scheduling, self-configuring, self-healing, self-optimizing, self-protecting, service-level agreement},
  abstract={As computing infrastructure expands, resource management in a large, heterogeneous, and distributed environment becomes a challenging task. In a cloud environment, with uncertainty and dispersion of resources, one encounters problems of allocation of resources, which is caused by things such as heterogeneity, dynamism, and failures. Unfortunately, existing resource management techniques, frameworks, and mechanisms are insufficient to handle these environments, applications, and resource behaviors. To provide efficient performance of workloads and applications, the aforementioned characteristics should be addressed effectively. This research depicts a broad methodical literature analysis of autonomic resource management in the area of the cloud in general and QoS (Quality of Service)-aware autonomic resource management specifically. The current status of autonomic resource management in cloud computing is distributed into various categories. Methodical analysis of autonomic resource management in cloud computing and its techniques are described as developed by various industry and academic groups. Further, taxonomy of autonomic resource management in the cloud has been presented. This research work will help researchers find the important characteristics of autonomic resource management and will also help to select the most suitable technique for autonomic resource management in a specific application along with significant future research directions.}
}

@article{rayyan-727968350,
  title={Validating software metrics: A spectrum of philosophies},
  year={2013},
  journal={ACM Trans. Softw. Eng. Methodol.},
  issn={1049-331X},
  volume={21},
  number={4},
  author={Meneely, Andrew and Smith, Ben and Williams, Laurie},
  url={https://doi.org/10.1145/2377656.2377661},
  keywords={systematic literature review, Software metrics, validation criterion, Metronidazole, Software, Software Validation},
  abstract={Context. Researchers proposing a new metric have the burden of proof to demonstrate to the research community that the metric is acceptable in its intended use. This burden of proof is provided through the multi-faceted, scientific, and objective process of software metrics validation. Over the last 40 years, however, researchers have debated what constitutes a “valid” metric.Aim. The debate over what constitutes a valid metric centers on software metrics validation criteria. The objective of this article is to guide researchers in making sound contributions to the field of software engineering metrics by providing a practical summary of the metrics validation criteria found in the academic literature.Method. We conducted a systematic literature review that began with 2,288 papers and ultimately focused on 20 papers. After extracting 47 unique validation criteria from these 20 papers, we performed a comparative analysis to explore the relationships amongst the criteria.Results. Our 47 validation criteria represent a diverse view of what constitutes a valid metric. We present an analysis of the criteria's categorization, conflicts, common themes, and philosophical motivations behind the validation criteria.Conclusions. Although the 47 validation criteria are not conflict-free, the diversity of motivations and philosophies behind the validation criteria indicates that metrics validation is complex. Researchers proposing new metrics should consider the applicability of the validation criteria in terms of our categorization and analysis. Rather than arbitrarily choosing validation criteria for each metric, researchers should choose criteria that can confirm that the metric is appropriate for its intended use. We conclude that metrics validation criteria provide answers to questions that researchers have about the merits and limitations of a metric.}
}

@article{rayyan-727968351,
  title={Is there a place for qualitative studies when identifying effort predictors? A case in web effort estimation},
  year={2014},
  issn={978-1-4503-2476-2},
  author={Matos, Olavo and Conte, Tayana and Mendes, Emilia},
  url={https://doi.org/10.1145/2601248.2601281},
  publisher={Association for Computing Machinery},
  series={EASE '14},
  keywords={systematic literature review, qualitative study, web effort estimation, web effort predictors, web project management},
  abstract={Background: Effort estimation is the key for efficiently managing Web projects and achieving their success. In order to correctly estimate, it is necessary to have a broad knowledge of the factors that influence effort estimation in Web projects. Aim: In this research we aim to increase the understanding of Web effort estimation by using a set of factors identified in literature along with the knowledge from experts in effort estimation. Method: We have gathered data from two different sources: (a) our previous work, in which we applied Grounded Theory procedures to identify factors that influence Web effort estimation from the point of view of Web project estimation experts; and (b) a Systematic Literature Review (SLR) extension, in which we identified factors reported in research papers. We have used the qualitative results from these sources to make comparisons and draw conclusions on factors affecting Web effort estimation. Results: We identified a total of 90 factors that influence effort estimation in Web projects. From this set, 30 factors were identified only in the qualitative study with experts in effort estimation, not being present in the SLR extension. Conclusions: By integrating the factors found in both our qualitative study with effort estimation experts and the SLR extension, we managed to create a comprehensive list of factors influencing effort estimation. Also, this set can be a starting point in the proposal of effort estimation models. Finally, the results from our comparison can be considered an indication that it is necessary to increase the employment of qualitative research to capture evidences regarding the current state of practice in Software Engineering.}
}

@article{rayyan-727968352,
  title={Feature weighting techniques for CBR in software effort estimation studies: A review and empirical evaluation},
  year={2014},
  issn={978-1-4503-2898-2},
  pages={32-41},
  author={Sigweni, Boyce and Shepperd, Martin},
  url={https://doi.org/10.1145/2639490.2639508},
  publisher={Association for Computing Machinery},
  series={PROMISE '14},
  keywords={meta-analysis, systematic literature review, case-based reasoning, software effort estimation, feature subset selection, feature weighting, Software},
  abstract={Context: Software effort estimation is one of the most important activities in the software development process. Unfortunately, estimates are often substantially wrong. Numerous estimation methods have been proposed including Case-based Reasoning (CBR). In order to improve CBR estimation accuracy, many researchers have proposed feature weighting techniques (FWT).Objective: Our purpose is to systematically review the empirical evidence to determine whether FWT leads to improved predictions. In addition we evaluate these techniques from the perspectives of (i) approach (ii) strengths and weaknesses (iii) performance and (iv) experimental evaluation approach including the data sets used.Method: We conducted a systematic literature review of published, refereed primary studies on FWT (2000–2014).Results: We identified 19 relevant primary studies. These reported a range of different techniques. 17 out of 19 make benchmark comparisons with standard CBR and 16 out of 17 studies report improved accuracy. Using a one-sample sign test this positive impact is significant (p = 0.0003).Conclusion: The actionable conclusion from this study is that our review of all relevant empirical evidence supports the use of FWTs and we recommend that researchers and practitioners give serious consideration to their adoption.}
}

@article{rayyan-727968353,
  title={An investigation of smart parking tools, technologies, & challenges},
  year={2020},
  issn={978-1-4503-7721-8},
  pages={198-203},
  author={Zahoor, Tayyba and Azam, Farooque and Anwar, Muahmmad Waseem and Tariq, Ayesha and Javaid, Haider Ali},
  url={https://doi.org/10.1145/3436829.3436851},
  publisher={Association for Computing Machinery},
  series={ICSIE 2020},
  keywords={Systematic Literature Review, Smart Parking, Smart Parking Challenges, Smart Parking Technologies, Smart Parking Tools},
  abstract={Urbanization, exceptional increase in population and advancement in technology caused the automotive industry to grow rapidly & automobiles become essential part of daily life. Consequently, finding a parking space particularly in populous zones, is a challenging task. Researchers have proposed different solutions to assist the developments in smart parking systems. In this paper, we have investigated the key tools, techniques & challenges proposed in the recent research studies. Primarily, a Systematic Literature Review is carried out, total 35 studies are explored during time interval of (2015-2019). Subsequently, five major areas are recognized where smart parking is often functional i.e. Internet of Things (IoT) (13 studies), Cloud Computing (2 studies), Model-Driven Engineering (4 studies), Fog Computing (6 studies) and Artificial Intelligence (11 studies). Furthermore, (15) primary tools and (25) algorithms are presented. This article also portray the challenges cited by different studies. The findings of this study will definitely assist the practitioners while deciding the appropriate selections.}
}

@article{rayyan-727968354,
  title={Using the SCAS strategy to perform the initial selection of studies in systematic reviews: An experimental study},
  year={2016},
  issn={978-1-4503-3691-8},
  author={Octaviano, Fábio and Silva, Cleiton and Fabbri, Sandra},
  url={https://doi.org/10.1145/2915970.2916000},
  publisher={Association for Computing Machinery},
  series={EASE '16},
  keywords={StArt tool, systematic literature review (SLR), experiment, evidence-based software engineering (EBSE), primary study selection activity, score citation automatic selection (SCAS)},
  abstract={Context: Systematic Review (SR) is a well-defined and rigorous methodology used to find relevant evidence about a specific topic of interest. Depending on the number of identified primary studies, the selection activity can be very time-consuming and a strategy, like SCAS, to semi-automate this activity can be helpful. Objective: To present an experimental study carried out to evaluate the SCAS strategy. Method: We conducted an experiment to compare the efficiency and effectiveness between participants using SCAS and the manual approach. They received necessary training for applying SCAS using tool support. They were divided into five groups, which conducted SRs based on their research areas. Results: When applying SCAS, the average effort reduction was 22.33%, and the average percentage error was 3.95% with a minimal loss of 1.6 evidence per SR. In addition, results showed an overall precision of 65.49% on an overall recall of 90.24% when using SCAS. The overall Kappa showed that there is a substantial agreement level between the groups and SCAS. Conclusion: The experiment increased the confidence in the strategy, reinforcing that it can reduce the effort required to select primary studies without adversely affecting the overall results of SRs.}
}

@article{rayyan-727968355,
  title={Evidence briefings: Towards a medium to transfer knowledge from systematic reviews to practitioners},
  year={2016},
  issn={978-1-4503-4427-2},
  author={Cartaxo, Bruno and Pinto, Gustavo and Vieira, Elton and Soares, Sérgio},
  url={https://doi.org/10.1145/2961111.2962603},
  publisher={Association for Computing Machinery},
  series={ESEM '16},
  keywords={Software Engineering, Systematic Reviews, Evidence-Based Software Engineering, Evidence Briefings},
  abstract={Context: Integrate research evidence with practice is one of the main goals of evidence-based software engineering. However, recent studies show that the connection between systematic reviews and practitioners has not fully established.Goal: This paper presents the first steps towards a medium to transfer knowledge acquired from systematic reviews to practitioners.Method: We selected a set of systematic reviews identified by a tertiary study and extracted their findings to generate one-page Evidence Briefings to serve as mediums. A design specialist defined the briefings structure based on information design and gestalt principles. To evaluate the format and content of the briefings we conducted personal opinion surveys based on two groups: StackExchange users that posted questions in topics related to the reviews, and the authors of the selected reviews themselves. The former had a response rate of 21.9% (32 out 146) and the latter 31.8% (7 out of 22).Results: Practitioners rarely use systematic review research papers as mediums to acquire knowledge, since just 9% have told to do so. Both researchers and practitioners positively evaluated the evidence briefings, since 71% and 82% of the StackExchange users and systematic review authors, respectively, agreed or strongly agreed that the briefings' interface is clear.Conclusions: Researchers and practitioners were positive about the content and format of the evidence briefings we proposed. It is also possible to say that there is a gap between practitioners and systematic reviews due to the low percentage of practitioners that consume systematic reviews. The good reception of the evidence briefings from both sides show a possible route to reduce that gap.}
}

@article{rayyan-727968356,
  title={Gaps between industry expectations and the abilities of graduates},
  year={2013},
  issn={978-1-4503-1868-6},
  pages={525-530},
  author={Radermacher, Alex and Walia, Gursimran},
  url={https://doi.org/10.1145/2445196.2445351},
  publisher={Association for Computing Machinery},
  series={SIGCSE '13},
  keywords={systematic literature review, knowledge deficiency},
  abstract={Although computer science, information systems, and information technology educators often do an exemplary job of preparing their students for jobs in industry or for further education, there are still many areas where these students do not possess the necessary skills or knowledge based on the expectations of employers or academia. These gaps between the abilities of graduating students and those expected to have can prevent them from succeeding in their careers. This paper presents the results of a systematic literature review conducted to determine which areas graduating students most frequently fall short of the expectations of industry or macademia. The results of this review indicate that graduating students are lacking in many different areas, including technical abilities (design, testing, configuration management tools, etc.) personal skills (communication, teamwork, etc.) and professional qualities (e.g. ethics). By raising awareness of these areas, it is possible for educators to become aware of areas where students most frequently fail to meet expectations and to make curriculum changes or adjustments to address these problems}
}

@article{rayyan-727968357,
  title={A review of peer code review in higher education},
  year={2020},
  journal={ACM Transactions on Computing Education},
  volume={20},
  number={3},
  author={Indriasari, Theresia Devi and Luxton-Reilly, Andrew and Denny, Paul},
  url={https://doi.org/10.1145/3403935},
  keywords={systematic review, systematic literature review, Peer review, code review, higher education, peer code review, programming course, Peer Review},
  abstract={Peer review is the standard process within academia for maintaining publication quality, but it is also widely employed in other settings, such as education and industry, for improving work quality and for generating actionable feedback to content authors. For example, in the software industry peer review of program source code—or peer code review—is a key technique for detecting bugs and maintaining coding standards. In a programming education context, although peer code review offers potential benefits to both code reviewers and code authors, individuals are typically less experienced, which presents a number of challenges. Some of these challenges are similar to those reported in the educational literature on peer review in other academic disciplines, but reviewing code presents unique difficulties. Better understanding these challenges and the conditions under which code review can be taught and implemented successfully in computer science courses is of value to the computing education community. In this work, we conduct a systematic review of the literature on peer code review in higher education to examine instructor motivations for conducting peer code review activities, how such activities have been implemented in practice, and the primary benefits and difficulties that have been reported. We initially identified 187 potential studies and analyzed 51 empirical studies pertinent to our goals. We report the most commonly cited benefits (e.g., the development of programming-related skills) and barriers (e.g., low student engagement), and we identify a wide variety of tools that have been used to facilitate the peer code review process. While we argue that more empirical work is needed to validate currently reported results related to learning outcomes, there is also a clear need to address the challenges around student motivation, which we believe could be an important avenue for future research.}
}

@article{rayyan-727968358,
  title={Understanding the effects of lecturer intervention on computer science student behaviour},
  year={2018},
  issn={978-1-4503-5627-5},
  pages={105-124},
  author={Szabo, Claudia and Falkner, Nickolas and Knutas, Antti and Dorodchi, Mohsen},
  url={https://doi.org/10.1145/3174781.3174787},
  publisher={Association for Computing Machinery},
  series={ITiCSE-WGR '17},
  keywords={systematic literature review, intervention},
  abstract={Providing effective support and feedback to students is critical to ensure engagement and retention within Computer Science courses. Individual student learning experiences and challenges vary from student to student, and effective intervention is further hampered in a large scale context. In addition, there are a plethora of possible interventions for any given learning challenge, and it is difficult for an educator to establish which intervention is the most effective or quickest to implement. To this, we report on the outcomes of a systematic literature review focused on interventions in Computer Science classrooms. To provide an understanding of the types of interventions possible in a Computer Science course, we propose a taxonomy of intervention types with low mutual information and classify the 129 selected papers based on it. We identify the most effective interventions as presented in their respective studies and discuss gaps in the study of several intervention types. We then present an overview of two of the most popular types of interventions in the published literature: those focused on introducing technical cooperations within courses, and those focused on changing the way the course content is presented to students. To understand how interventions have evolved over time, we present the evolution of sub-classes of interventions over the years.}
}

@article{rayyan-727968359,
  title={A mapping study on method engineering: First results},
  year={2013},
  issn={978-1-4503-1848-8},
  pages={165-170},
  author={Kuhrmann, Marco and Fernández, Daniel Méndez and Tiessler, Michaela},
  url={https://doi.org/10.1145/2460999.2461023},
  publisher={Association for Computing Machinery},
  series={EASE '13},
  keywords={systematic literature review, mapping study, situational method engineering},
  abstract={Context: Software processes have become inherently complex to cope with the various situations we face in industrial project environments. In response to this problem, the research area of Method Engineering arose in the 1990s aiming at the systematization of process construction. Objective: Although the research area has gained much attention and offered a plethora of contributions so far, we still have little knowledge about the feasibility of Method Engineering. To overcome this shortcoming, necessary is a systematic investigation of the respective publication flora. Method: We conduct a systematic mapping study and investigate, inter alia, which contributions were made over time and which research type facet they address to distill a common understanding of the state-of-the-art. Results: Based on the review of 64 publications, our results show that most of those contributions only repeat and discuss formerly introduced concepts, whereas empirically sound evidence on the feasibility of Method Engineering, is still missing. Conclusion: Although the research area constitutes many contributions, yet missing are empirically sound investigations that would allow for practical application and experience extraction.}
}

@article{rayyan-727968360,
  title={Evaluating strategies for forward snowballing application to support secondary studies updates: Emergent results},
  year={2018},
  issn={978-1-4503-6503-1},
  pages={184-189},
  author={Felizardo, Katia Romero and da Silva, Anderson Y Iwazaki and de Souza, Érica Ferreira and Vijaykumar, Nandamudi L and Nakagawa, Elisa Yumi},
  url={https://doi.org/10.1145/3266237.3266240},
  publisher={Association for Computing Machinery},
  series={SBES '18},
  keywords={systematic literature review, secondary studies, forward snowballing, search evidence, update},
  abstract={Context: Secondary studies should be updated from time to time to include new evidence to preserve their value. It is recognized that one search technique to update secondary studies is forward snowballing and that the number of studies identified is dependent on the electronic databases selected. However, there is no consensus on what electronic database is most appropriate for applying forward snowballing. Objective: The main goal of this study is to evaluate the use of different electronic databases for applying forward snowballing to update secondary studies. Method: Six updates were performed using forward snowballing with support from two electronic databases, one specific (IEEE Xplore) and the other generic (Google Scholar) and three combinations were evaluated to obtain new evidence during secondary studies updating: (1) searching using Google Scholar as electronic database; (2) searching using IEEE Xplore as electronic database; and (3) searching using both, IEEE Xplore and Google Scholar as complementary electronic databases. Results: The use of a specific electronic database is not indicated for forward snowballing application to update SLRs, since many relevant studies may not be identified. However, the use of a generic database is sufficient to discover the majority of the studies. Conclusions: The emergent contribution of our work to the body of knowledge in the SLR field is to add empirical evidence regarding the use of different electronic databases to support forward snowballing application during secondary studies updates. These results should help reviewers when they decide to find evidences to update their SLRs.}
}

@article{rayyan-727968361,
  title={Towards an understanding of value creation in agile software development},
  year={2019},
  issn={978-1-4503-7237-4},
  author={Neto, Geraldo Torres G and Santos, Wylliams B and Fagundes, Roberta A A and Margaria, Tiziana},
  url={https://doi.org/10.1145/3330204.3330256},
  publisher={Association for Computing Machinery},
  series={SBSI'19},
  keywords={Systematic Literature Review, Agile Software Development, Business Value, Software},
  abstract={Recently, studies involving the creation of business value in Agile Software Development (ASD) have been growing substantially. However, the concept of value creation in ASD has not yet been clearly defined. Besides, the literature does not define practices that can create business value for ASD. Identifying these practices can change the mindset of agile teams, since surveys indicate that, from the point of view of the agile team, the creation of value is poorly understood. Thus, this study carried out a Systematic Literature Review to identify how value creation is defined in ASD, and how practices can improve this value creation. Despite the lack of studies on the subject, we identified practices and its positive and negative influence on value creation.}
}

@article{rayyan-727968362,
  title={Energy management in embedded systems: Towards a taxonomy},
  year={2012},
  issn={978-1-4673-1832-7},
  pages={41-44},
  author={Ramesh, Umesh Balaji Kothandapani and Sentilles, Severine and Crnkovic, Ivica},
  publisher={IEEE Press},
  series={GREENS '12},
  keywords={systematic review, embedded systems, energy consumption},
  abstract={Energy is an important constraint in embedded systems, and there exists a huge expertise in this domain about monitoring, managing and optimizing energy consumption in the computer systems. The aim of this paper is to present the energy management addressed in the research literature. Based on a systematic review, the paper presents a taxonomy of energy consumption and management in embedded systems.}
}

@article{rayyan-727968363,
  title={A state-of-the-art review of machine learning techniques for fraud detection research},
  year={2018},
  issn={978-1-4503-5719-7},
  pages={11-19},
  author={Omar, Sinayobye Janvier and Fred, Kiwanuka and Swaib, Kaawaase Kyanda},
  url={https://doi.org/10.1145/3195528.3195534},
  publisher={Association for Computing Machinery},
  series={SEiA '18},
  keywords={machine learning, systematic literature review, data mining, areas of fraud, fraud detection, Review Literature as Topic},
  abstract={The area of fraud detection1 has been traditionally correlated with data mining and text mining. Even before the "big data" phenomena started in 2008, text mining and data mining were used as instruments of fraud detection. However, the limited technological capabilities of the pre-big data technologies made it very difficult for researchers to run fraud detection algorithms on large amounts of data. This paper reviews the existing research done in fraud detection across different areas with the aim of investigating the machine learning techniques used and find out their strengths and weaknesses. It used the systematic quantitative literature review methodology to review the research studies in the field of fraud detection research within the last decade using machine learning techniques. Various combinations of keywords were used to identify the pertinent articles and were retrieved from ACM Digital Library, IEEE Xplorer Digital Library, Science Direct, Springer Link, etc. This search used a sample of 80 relevant articles (peer-reviewed journals articles and conference papers). The most used machine learning techniques were identified, and their strengths and weaknesses. Finally, the conclusion, limitations and future work have been shown.}
}

@article{rayyan-727968364,
  title={A critical analysis of studies that address the use of text mining for citation screening in systematic reviews},
  year={2016},
  issn={978-1-4503-3691-8},
  author={Olorisade, Babatunde K and de Quincey, Ed and Brereton, Pearl and Andras, Peter},
  url={https://doi.org/10.1145/2915970.2915982},
  publisher={Association for Computing Machinery},
  series={EASE '16},
  keywords={systematic review, automation, text mining, study selection, citation screening, study reproduction},
  abstract={Background: Since the introduction of the systematic review process to Software Engineering in 2004, researchers have investigated a number of ways to mitigate the amount of effort and time taken to filter through large volumes of literature.Aim: This study aims to provide a critical analysis of text mining techniques used to support the citation screening stage of the systematic review process.Method: We critically re-reviewed papers included in a previous systematic review which addressed the use of text mining methods to support the screening of papers for inclusion in a review. The previous review did not provide a detailed analysis of the text mining methods used. We focus on the availability in the papers of information about the text mining methods employed, including the description and explanation of the methods, parameter settings, assessment of the appropriateness of their application given the size and dimensionality of the data used, performance on training, testing and validation data sets, and further information that may support the reproducibility of the included studies.Results: Support Vector Machines (SVM), Naïve Bayes (NB) and Committee of classifiers (Ensemble) are the most used classification algorithms. In all of the studies, features were represented with Bag-of-Words (BOW) using both binary features (28%) and term frequency (66%). Five studies experimented with n-grams with n between 2 and 4, but mostly the unigram was used. χ2, information gain and tf-idf were the most commonly used feature selection techniques. Feature extraction was rarely used although LDA and topic modelling were used. Recall, precision, F and AUC were the most used metrics and cross validation was also well used. More than half of the studies used a corpus size of below 1,000 documents for their experiments while corpus size for around 80% of the studies was 3,000 or fewer documents. The major common ground we found for comparing performance assessment based on independent replication of studies was the use of the same dataset but a sound performance comparison could not be established because the studies had little else in common. In most of the studies, insufficient information was reported to enable independent replication. The studies analysed generally did not include any discussion of the statistical appropriateness of the text mining method that they applied. In the case of applications of SVM, none of the studies report the number of support vectors that they found to indicate the complexity of the prediction engine that they use, making it impossible to judge the extent to which over-fitting might account for the good performance results.Conclusions: There is yet to be concrete evidence about the effectiveness of text mining algorithms regarding their use in the automation of citation screening in systematic reviews. The studies indicate that options are still being explored, but there is a need for better reporting as well as more explicit process details and access to datasets to facilitate study replication for evidence strengthening. In general, the reader often gets the impression that text mining algorithms were applied as magic tools in the reviewed papers, relying on default settings or default optimization of available machine learning toolboxes without an in-depth understanding of the statistical validity and appropriateness of such tools for text mining purposes.}
}

@article{rayyan-727968365,
  title={Privacy: What is the research scenario in brazilian symposium IHC?},
  year={2018},
  issn={978-1-4503-6601-4},
  author={Silva, Edenildo and Torres, Bruno and Sacramento, Carolina and Capra, Eliane Pinheiro and Ferreira, Simone Bacellar Leal and Garcia, Ana Cristina Bicharra},
  url={https://doi.org/10.1145/3274192.3274226},
  publisher={Association for Computing Machinery},
  series={IHC 2018},
  keywords={systematic literature review, Privacy, HCI, GranDIHC-BR},
  abstract={The Internet improved the individual's ability to interact with others by changing the way people live and relate. However, the connected world brings many challenges, like privacy in designing system interfaces. This article aims to analyze how privacy has been treated in Brazilian Human-Computer Interaction (HCI) researches, in order to verify how the fourth challenge published in the Brazilian Great Challenges of Research in HCI (I GranDIHC-BR) report - about privacy and user's information visibility - has contributed to more researches about privacy among ten editions of Brazilian Symposium on Human Factors in Computational Systems (IHC Brazil). For this, we mapped 314 complete articles published in the last ten editions of the IHC Brazil, in a comparative approach, focusing on publications about privacy: five editions before and five editions after the I GranDIHC-BR, using systematic literature review (SLR) method. As a result, we identified few articles related to the topic, most of them about social network systems privacy and focused on understanding the user's perception of privacy. These findings demonstrate that there are still many challenges on the subject to be investigated by the HCI community.}
}

@article{rayyan-727968366,
  title={An insight into the capabilities of professionals and teams in agile software development: A systematic literature review},
  year={2018},
  issn={978-1-4503-5414-1},
  pages={10-19},
  author={Vishnubhotla, Sai Datta and Mendes, Emilia and Lundberg, Lars},
  url={https://doi.org/10.1145/3185089.3185096},
  publisher={Association for Computing Machinery},
  series={ICSCA 2018},
  keywords={Systematic literature review, agile software development, capability measurement, capability prediction, competence, individual capability, team capability, Software},
  abstract={Background: Previous studies investigated key characteristics of software engineers and factors influencing the performance of individuals, productivity of teams and project success within agile software development (ASD). They aided in the active investigation of human aspects in ASD. However, capability measurement and prediction with respect to agile workforce, owing to its importance, is an area that needs spotlight.Objective: The objective of this paper is to present the state of the art relating to capability measurement of software engineers and teams working in ASD projects.Method: We carried out a systematic literature review (SLR) focused on identifying attributes used for measuring and predicting the capabilities of individual software engineers and teams.Results: Evidence from 16 studies showed attributes that can measure capabilities of engineers and teams, and also attributes that can be used as capability predictors. Further, different instruments used to measure those attributes were presented.Conclusions: The SLR presented a wide list of attributes that were grouped into various categories. This information can be used by project managers as, for example, a checklist to consider when allocating software engineers to teams and in turn teams to a project. Further, this study indicated the necessity for an investigation into capability prediction models.}
}

@article{rayyan-727968367,
  title={Digitization of public services: A systematic literature review},
  year={2018},
  issn={978-1-4503-6565-9},
  pages={91-100},
  author={Leão, Heloise Acco Tives and Canedo, Edna Dias},
  url={https://doi.org/10.1145/3275245.3275255},
  publisher={Association for Computing Machinery},
  series={SBQS},
  keywords={Brazilian Government, Digital Governance, Digital Transformation Kit, Service Digitization, Service Pricing},
  abstract={This paper presents a systematic literature review of the digitization of services carried out by the governments of several countries. The main contribution of this work is the identification of the processes and methodologies adopted by these governments to provide their services to the citizen. These results serve as inputs to guide an analysis of the initial efforts of the Brazilian Government in the construction of a digital platform for the provision of its services directed to the citizen, seeking to analyze their needs and improving the services currently provided.}
}

@article{rayyan-727968368,
  title={A systematic review to assist in identifying teaching approaches to guide the application of an interdisciplinary software factory in IT undergraduation},
  year={2017},
  issn={978-1-4503-5326-7},
  pages={384-391},
  author={de Souza, M da C O and Oliveira, S R B and Meira, S R L},
  url={https://doi.org/10.1145/3131151.3131176},
  publisher={Association for Computing Machinery},
  series={SBES'17},
  keywords={systematic review, interdisciplinary, IT higher courses, learning process, Software factory, Software},
  abstract={The market is increasingly demanding in relation to the quality of information and communication technology professionals. The industry claims educational institutions to train future employees of technology with quality. In this context, there are software factories, whose investments in this branch of activity have been made not only by governmental entities, but also by the national and international private sector, which has also motivated the emergence of a new modality of software factory: that installed in institutions of higher education, which aims of supporting the learning process in a practical way for the students. With a focus on answering the objective of identifying, understanding and discussing evidence on teaching methodologies, in order to understand the use, benefits and difficulties in applying these practices in a software factory in the IT courses, in this work a systematic review of literature (SRL) was made in the IEEE Xplorer, Scopus, ACM Digital Library and Engineering Village databases from 2003 to 2015. Of the total of 118 preselected papers, this work identified 14 papers, in which 9 papers report some kind of experience in driving software factory in IT higher courses, these works were conducted through a systematic mapping. It was also observed that some works demonstrate greater maturity in this practice and others are starting the experiments, but all apply this environment in different ways.}
}

@article{rayyan-727968369,
  title={How practitioners perceive the relevance of software engineering research},
  year={2015},
  issn={978-1-4503-3675-8},
  pages={415-425},
  author={Lo, David and Nagappan, Nachiappan and Zimmermann, Thomas},
  url={https://doi.org/10.1145/2786805.2786809},
  publisher={Association for Computing Machinery},
  series={ESEC/FSE 2015},
  keywords={Survey, Industry, Software Engineering Research, Software},
  abstract={The number of software engineering research papers over the last few years has grown significantly. An important question here is: how relevant is software engineering research to practitioners in the field? To address this question, we conducted a survey at Microsoft where we invited 3,000 industry practitioners to rate the relevance of research ideas contained in 571 ICSE, ESEC/FSE and FSE papers that were published over a five year period. We received 17,913 ratings by 512 practitioners who labelled ideas as essential, worthwhile, unimportant, or unwise. The results from the survey suggest that practitioners are positive towards studies done by the software engineering research community: 71% of all ratings were essential or worthwhile. We found no correlation between the citation counts and the relevance scores of the papers. Through a qualitative analysis of free text responses, we identify several reasons why practitioners considered certain research ideas to be unwise. The survey approach described in this paper is lightweight: on average, a participant spent only 22.5 minutes to respond to the survey. At the same time, the results can provide useful insight to conference organizers, authors, and participating practitioners.}
}

@article{rayyan-727968370,
  title={Test case prioritization: A systematic review and mapping of the literature},
  year={2017},
  issn={978-1-4503-5326-7},
  pages={34-43},
  author={de S. Campos Junior, Heleno and Araújo, Marco Antônio P and David, José Maria N and Braga, Regina and Campos, Fernanda and Ströele, Victor},
  url={https://doi.org/10.1145/3131151.3131170},
  publisher={Association for Computing Machinery},
  series={SBES'17},
  keywords={systematic literature review, Regression testing, systematic literature mapping, software testing},
  abstract={Test case prioritization (TCP) techniques aim to reorder test cases execution according to a goal. One common goal is fault detection, in which test cases that have a higher chance of detecting a fault are executed first than the remaining test cases. The goal of this study is to investigate TCP empirical studies in order to synthesize reported effectiveness results and provide a basis for future research. We conducted a systematic literature mapping to characterize TCP empirical studies and a systematic literature review to analyze reported TCP techniques effectiveness results. Among selected studies from 1999 to 2016, we found that there is a high amount of empirical studies evaluating many TCP techniques. However, when we applied our quality assessment criteria, most of them were discarded, indicating that they might have methodological problems. Analyzed studies reported results of coverage-based TCP techniques. Furthermore, we found that some context factors regarding faults, test cases of the application being tested and coverage granularity considered by TCP techniques may significantly affect the effectiveness of their execution. These results suggest that more rigorous empirical methodology is needed when evaluating TCP techniques and also, authors need to compare effectiveness results of their proposed TCP techniques with well established techniques to generate more evidences Furthermore, our analysis of significant factors on TCP techniques effectiveness can guide researchers when planning empirical evaluations and help them choosing features to compose new techniques.}
}

@article{rayyan-727968371,
  title={Software engineering aspects of green and sustainable software: A systematic mapping study},
  year={2017},
  issn={978-1-4503-4856-0},
  pages={34-44},
  author={Marimuthu, C and Chandrasekaran, K},
  url={https://doi.org/10.1145/3021460.3021464},
  publisher={Association for Computing Machinery},
  series={ISEC '17},
  keywords={systematic mapping study, green and sustainable software engineering, green software, Software},
  abstract={Green and sustainable software engineering is an emerging research field which aims at creating, using, and disposing the energy-efficient software in an environment friendly manner with less negative impacts. The research community strongly believes that the energy efficiency and sustainability of the software can be improved by modifying the existing software engineering methods. This systematic mapping study identifies and map such methods for green and sustainable software development. Especially, this study identifies the research types, research goals, software engineering research topics, accepted validation methods and publication fora that are used in the field of green and sustainable software engineering. This study was conducted with 7 research questions and analyzed 82 relevant studies. We have used snowballing reading to find out the relevant studies that were published from 2010 to May, 2016. One of the important finding of this study is, there are less number of contributions on software design and construction. In future, sufficient research works and tools support must be provided to make this research field more matured. The main contribution of this study is to summarize the body of knowledge in the field of green and sustainable software engineering and provides a platform to conduct future research.}
}

@article{rayyan-727968372,
  title={A systematic literature review of continuous blood glucose monitoring and suggesting the quantity of insulin or artificial pancreas (AP) for diabetic type 1 patients},
  year={2019},
  issn={978-1-4503-6600-7},
  pages={539-545},
  author={Asad, Muhammad and Qamar, Usman and Khan, Aimal and Safdar, Rahmat Ullah},
  url={https://doi.org/10.1145/3318299.3318352},
  publisher={Association for Computing Machinery},
  series={ICMLC '19},
  keywords={AP, CDSS closed loop systems and PID, CGM, insulin prediction, MPC, T1DM, Glucose, Blood Glucose, Blood Glucose Self-Monitoring},
  abstract={Background: Diabetes Mellitus is one of the most common diseases, which is rapidly increasing worldwide. Early detection of Blood Glucose Level not only helps in better management of Diabetes Mellitus but also decreases the cost of treatment. In the recent past, numerous researches have been carried out to monitor blood glucose level which suggests the quantity of insulin i.e. artificial pancreas. Method: In this paper, we summarize and analyze the past work of continuous blood glucose monitoring and automatic insulin suggestion, in a systematic way. Particularly, 24 journal studies from 2015 to 2018 are identified and analyzed. The paper provided a dynamic study of insulin-glucose regulators by identifying some research questions and answering from the literature. Moreover, it provides brief of the methodology of each study and how it contributes towards this field. It also underlines the advantages of the methods used in past and how they lack in determining other aspects for achieving a completely autonomous, adaptive and individualized model. Results: A comprehensive investigation of the selected studies leads to identify four major areas i.e. Machine learning techniques (8 studies), MPC (6 studies), PID (2 studies), mixed (6) and others (2 studies).Conclusion: This study is helpful in opening a gateway for new researchers to have an overview of the past work on continuous glucose monitoring and insulin suggestion. It identifies the challenges in this particular domain in order to lay the foundation for future research. The survey discovers the most popular techniques used for blood glucose monitoring and insulin suggestion, exogenous or intravenous (Subcutaneous) or artificial pancreas. For future work, the nonlinear autoregressive neural network based model predictive controller is suggested.}
}

@article{rayyan-727968373,
  title={A gamification requirements catalog for educational software: Results from a systematic literature review and a survey with experts},
  year={2017},
  issn={978-1-4503-4486-9},
  pages={1108-1113},
  author={Peixoto, Mariana and Silva, Carla},
  url={https://doi.org/10.1145/3019612.3019752},
  publisher={Association for Computing Machinery},
  series={SAC '17},
  keywords={systematic literature review, survey, requirements engineering, educational software, gamification catalog, Software},
  abstract={Gamification is an emerging phenomenon for using in educational software in order to engage, motivate and improve the performance of students inside the learning context. However, despite its importance, the identification of significant gamification requirements for educational software is not trivial and a consensus of such requirements has not been reached. Motivated by this situation, the objective of this paper is to present a gamification requirements catalog for educational software. The requirements were identified from a systematic literature review, subsequently prioritized and validated through a survey conducted with 64 experts in the field. The results suggest that the requirements of the catalog are important to be applied in educational software.}
}

@article{rayyan-727968374,
  title={Using forward snowballing to update systematic reviews in software engineering},
  year={2016},
  issn={978-1-4503-4427-2},
  author={Felizardo, Katia Romero and Mendes, Emilia and Kalinowski, Marcos and Souza, Érica Ferreira and Vijaykumar, Nandamudi L},
  url={https://doi.org/10.1145/2961111.2962630},
  publisher={Association for Computing Machinery},
  series={ESEM '16},
  keywords={Systematic literature reviews, forward snowballing, Software},
  abstract={Background: A Systematic Literature Review (SLR) is a methodology used to aggregate relevant evidence related to one or more research questions. Whenever new evidence is published after the completion of a SLR, this SLR should be updated in order to preserve its value. However, updating SLRs involves significant effort. Objective: The goal of this paper is to investigate the application of forward snowballing to support the update of SLRs. Method: We compare outcomes of an update achieved using the forward snowballing versus a published update using the search-based approach, i.e., searching for studies in electronic databases using a search string. Results: Forward snowballing showed a higher precision and a slightly lower recall. It reduced in more than five times the number of primary studies to filter however missed one relevant study. Conclusions: Due to its high precision, we believe that the use of forward snowballing considerably reduces the effort in updating SLRs in Software Engineering; however the risk of missing relevant papers should not be underrated.}
}

@article{rayyan-727968375,
  title={An overview of researches on digital accessibility before and after the great challenges of SBC 2006-2016: A systematic literature review},
  year={2017},
  issn={978-1-4503-6377-8},
  author={Coelho, Thais and Barbosa, Glívia A R and Silva, Ismael S and Coutinho, Flávio R dos S and da Silva, Fábio R},
  url={https://doi.org/10.1145/3160504.3160537},
  publisher={Association for Computing Machinery},
  series={IHC 2017},
  keywords={Systematic Literature Review, Digital accessibility},
  abstract={Digital accessibility contributes to digital and social inclusion of people. Faced with this potential contribution, in 2006, the Brazilian Computer Society (SBC) presented the "Participative and universal access to knowledge for the Brazilian citizen" as one of the great challenges of computing in Brazil for a decade (i.e., 2006-2016). Through this challenge, SBC sought to stimulate and support research in Brazil related to the theme that includes initiatives to promote accessibility. Ten years after the launch of great challenges of SBC, there is a demand to characterize the researches in digital accessibility in Brazil in order to demonstrate the investments and evolution in this area. This work presents an overview of the investments in digital accessibility in Brazil, through the comparative characterization of the research carried out in the country ten years before and ten years after the challenge launched by SBC. The perspective of the presented characterization is relevant because it allows reflecting on how this theme has been explored in the country in the scientific scope, besides evidencing the relevance of maintaining digital accessibility as an important investment area in Brazil.}
}

@article{rayyan-727968376,
  title={Does pareto's law apply to evidence distribution in software engineering? An initial report},
  year={2014},
  issn={978-1-4503-2965-1},
  pages={9-16},
  author={Tang, Hao and Zhou, You and Huang, Xin and Rong, Guoping},
  url={https://doi.org/10.1145/2627508.2627510},
  publisher={Association for Computing Machinery},
  series={EAST 2014},
  keywords={evidence-based software engineering, systematic (literature) review, evidence distribution, Pareto's Law (80/20 Rule), Software},
  abstract={Data is the source as well as raw format of evidence. As an important research methodology in evidence-based software engineering, systematic literature reviews (SLRs) are used for identifying the evidence and critically appraising the evidence, i.e. empirical studies that report (empirical) data about specific research questions. The 80/20 Rule (or Pareto's Law) reveals a 'vital few' phenomenon widely observed in many disciplines in the last century. However, the applicability of Pareto's Law to evidence distribution in software engineering (SE) is never tested yet. The objective of this paper is to investigate the applicability of Pareto's Law to the evidence distribution on specific topic areas in software engineering (in the form of systematic reviews), which may help us better understand the possible distribution of evidence in software engineering, and further improve the effectiveness and efficiency of literature search. We performed a tertiary study of SLRs in software engineering dated between 2004 and 2012. We further tested the Pareto's Law by collecting, analyzing, and interpreting the distribution (over publication venues) of the primary studies reported in the existing SLRs. Our search identified 255 SLRs, 107 of which were included according to the selection criteria. The analysis of the extracted data from these SLRs presents a preliminary view of the evidence (study) distribution in software engineering. The nonuniform distribution of evidence is supported by the data from the existing SLRs in SE. However, the present observation reflects a weaker 'vital few' relation between study and venue than the 80/20 Rule statement. Top referenced venues are suggested when researchers search for studies in software engineering. It is also noticeable to the community that the primary studies are improperly or incompletely reported in many SLRs.}
}

@article{rayyan-727968377,
  title={A preliminary systematic mapping on software engineering for robotic systems: A software quality perspective},
  year={2020},
  issn={978-1-4503-7963-2},
  pages={647-654},
  author={dos Santos, Marcela G and Napoleão, Bianca M and Petrillo, Fabio and Ameyed, Darine and Jaafar, Fehmi},
  url={https://doi.org/10.1145/3387940.3392197},
  publisher={Association for Computing Machinery},
  series={ICSEW'20},
  keywords={software engineering, systematic mapping, software quality, Robotic systems, Robotics, Software},
  abstract={Robotic systems have been increasingly employed in everyday tasks. Considering that software plays a crucial point in robot systems, to investigate how software engineering concepts in a software quality perspective can improve robotic systems. In this work, we present a systematic mapping to identify and classify the state-of-art of software engineering for robotic systems in a quality software perspective. We selected and systematically analyzed a final set of 35 primary studies extracted from an automated search on Scopus digital library.This work presents three main contributions. Firstly, we organize a catalogue of research studies about software engineering, more specifically software quality applied in robotic systems. Next, we systematically analyze software quality areas used in robotic systems. Finally, we discuss insights into research opportunities and gaps in software engineering to robotic systems for future studies.As a result, we observed that there are studies in the robotic systems area, addressing in a combined way, software engineering approaches and software quality aspects. The less investigated software quality aspect is security. Due to this fact, we presented an overview of the state-of-art on blockchain applying in robotics systems. Blockchain brings opportunities for changing the ways that robots interact with humans. Finally, we identify research opportunities and gaps in software quality on robotic systems, presenting an overview for future studies.}
}

@article{rayyan-727968378,
  title={Effective regression test case selection: A systematic literature review},
  year={2017},
  journal={ACM Computing Surveys},
  issn={0360-0300},
  volume={50},
  number={2},
  author={Kazmi, Rafaqut and Jawawi, Dayang N A and Mohamad, Radziah and Ghani, Imran},
  url={https://doi.org/10.1145/3057269},
  keywords={Software testing, SLR, cost effectiveness, coverage, fault detection ability},
  abstract={Regression test case selection techniques attempt to increase the testing effectiveness based on the measurement capabilities, such as cost, coverage, and fault detection. This systematic literature review presents state-of-the-art research in effective regression test case selection techniques. We examined 47 empirical studies published between 2007 and 2015. The selected studies are categorized according to the selection procedure, empirical study design, and adequacy criteria with respect to their effectiveness measurement capability and methods used to measure the validity of these results.The results showed that mining and learning-based regression test case selection was reported in 39% of the studies, unit level testing was reported in 18% of the studies, and object-oriented environment (Java) was used in 26% of the studies. Structural faults, the most common target, was used in 55% of the studies. Overall, only 39% of the studies conducted followed experimental guidelines and are reproducible.There are 7 different cost measures, 13 different coverage types, and 5 fault-detection metrics reported in these studies. It is also observed that 70% of the studies being analyzed used cost as the effectiveness measure compared to 31% that used fault-detection capability and 16% that used coverage.}
}

@article{rayyan-727968379,
  title={Survey on research synthesis in software engineering},
  year={2014},
  issn={978-1-4503-2476-2},
  author={Guzmán, Liliana and Lampasona, Constanza and Seaman, Carolyn and Rombach, Dieter},
  url={https://doi.org/10.1145/2601248.2601273},
  publisher={Association for Computing Machinery},
  series={EASE '14},
  keywords={systematic reviews, research synthesis, qualitative, quantitative, Software},
  abstract={Building trustworthy knowledge in software engineering depends on the systematic synthesis of empirical evidence. In recent years the number of published syntheses has increased, but only a few showed high quality and scientific rigor. We performed an online survey of software engineering researchers to identify difficulties experienced when synthesizing evidence. The results confirm that the state of primary research and the low quality of reports are perceived as the most important difficulties. Respondents who were experienced in quantitative and qualitative synthesis methods claim a lack of support for selecting and applying synthesis methods. This indicates the need for identifying criteria for selecting synthesis methods, deriving recommendations, and developing more rigorous guidelines for applying them.}
}

@article{rayyan-727968380,
  title={Support mechanisms to conduct empirical studies in software engineering},
  year={2014},
  issn={978-1-4503-2774-9},
  author={Borges, Alex and Ferreira, Waldemar and Barreiros, Emanoel and Almeida, Adauto and Fonseca, Liliane and Teixeira, Eudis and Silva, Diogo and Alencar, Aline and Soares, Sergio},
  url={https://doi.org/10.1145/2652524.2652572},
  publisher={Association for Computing Machinery},
  series={ESEM '14},
  keywords={systematic mapping study, empirical software engineering, EASE, empirical strategies, ESEJ, ESEM, support mechanisms, Software},
  abstract={Context: Empirical studies are gaining recognition in the Software Engineering (SE) research community. In order to foster empirical research, it is essential understand the environments, guidelines, process, and other mechanisms available to support these studies in SE. Goal: Identifying the mechanisms used to support the empirical strategies adopted by the researches in the major Empirical Software Engineering (ESE) scientific venues. Method: We performed a systematic mapping study that included all full papers published at EASE, ESEM and ESEJ since their first editions. A total of 898 studies were selected. Results: We provide the full list of identified support mechanisms and the strategies that uses them. The most commonly mechanisms used to support the empirical strategies were two sets of guidelines, one to secondary studies and another to experiments. The most reported empirical strategies are experiments and case studies. Conclusions: The use of empirical methods in SE has increased over the years but many studies do not apply these methods nor use mechanisms to guide their research. Therefore, the list of support mechanisms, where and how they were applied is a major asset to the SE community. Such asset can foster empirical studies aiding the choice regarding which strategies and mechanisms to use in a research. Also, we identified new perspectives and gaps that foster the development of resources to aid empirical studies.}
}

@article{rayyan-727968381,
  title={A systematic review of service level management in the cloud},
  year={2015},
  journal={ACM Computing Surveys},
  issn={0360-0300},
  volume={48},
  number={3},
  author={Faniyi, Funmilade and Bahsoon, Rami},
  url={https://doi.org/10.1145/2843890},
  keywords={survey, Cloud computing, QoS, software architecture, autonomic, self-adaptive, self-awareness, SLA},
  abstract={Cloud computing make it possible to flexibly procure, scale, and release computational resources on demand in response to workload changes. Stakeholders in business and academia are increasingly exploring cloud deployment options for their critical applications. One open problem is that service level agreements (SLAs) in the cloud ecosystem are yet to mature to a state where critical applications can be reliably deployed in clouds. This article systematically surveys the landscape of SLA-based cloud research to understand the state of the art and identify open problems. The survey is particularly aimed at the resource allocation phase of the SLA life cycle while highlighting implications on other phases. Results indicate that (i) minimal number of SLA parameters are accounted for in most studies; (ii) heuristics, policies, and optimisation are the most commonly used techniques for resource allocation; and (iii) the monitor-analysis-plan-execute (MAPE) architecture style is predominant in autonomic cloud systems. The results contribute to the fundamentals of engineering cloud SLA and their autonomic management, motivating further research and industrial-oriented solutions.}
}

@article{rayyan-727968382,
  title={Usability evaluation of VR products in industry: A systematic literature review},
  year={2019},
  issn={978-1-4503-5933-7},
  pages={1845-1851},
  author={Karre, Sai Anirudh and Mathur, Neeraj and Reddy, Y Raghu},
  url={https://doi.org/10.1145/3297280.3297462},
  publisher={Association for Computing Machinery},
  series={SAC '19},
  keywords={metrics, virtual reality, usability testing, industrial practices, usability evaluation},
  abstract={VR development practices have a diverse set of practices compared to traditional software development. Tasks like scene design, acoustic design, vergence manipulation, image depth, etc. are specific to VR apps and hence require evaluation processes that may be different from the traditional means. Usability Evaluation is one such process which is being executed in an unconventional way by Industrial Practitioners today. In this paper, the researchers detail a Systematic Literature Review of the Usability Evaluation Methods practised by Industrial researchers while building VR Products. The researchers found that VR Product teams follow unique methods to improve usability in their products. Further, the researchers consolidate these methods and provide insights into choosing the best to build a real-world VR Product based on the defined product constraints.}
}

@article{rayyan-727968383,
  title={Architectural description of embedded systems: A systematic review},
  year={2012},
  issn={978-1-4503-1347-6},
  pages={31-40},
  author={Guessi, Milena and Nakagawa, Elisa Yumi and Oquendo, Flavio and Maldonado, José Carlos},
  url={https://doi.org/10.1145/2304656.2304661},
  publisher={Association for Computing Machinery},
  series={ISARCS '12},
  keywords={systematic review, embedded systems, reference architectures},
  abstract={Embedded systems have gained more and more attention, as variety and complexity of these systems have increased. In particular, many of these systems are also critical regarding dependability, safety, security, among others. In parallel, since software architectures and reference architectures form the backbone of any successful system, including embedded systems, an important and even essential activity is to properly describe such architectures. However, to our best knowledge, there is no detailed panorama on how software architectures and reference architectures for embedded systems could be represented. Thus, the main contribution of this paper is to present and discuss results of a systematic review, aiming at providing this wide and, at the same time, deep panorama. We found out that different approaches have been proposed and used, lacking of consensus on how to better represent architectures of embedded systems. We also identified a range of quality requirements and constraints that have been considered in the architectural description of these systems. Furthermore, these results can be considered as valuable means to identify research lines that need to be further investigated.}
}

@article{rayyan-727968384,
  title={Writing for synthesis of evidence in empirical software engineering},
  year={2014},
  issn={978-1-4503-2774-9},
  author={Wohlin, Claes},
  url={https://doi.org/10.1145/2652524.2652559},
  publisher={Association for Computing Machinery},
  series={ESEM '14},
  keywords={evidence, systematic mapping studies, systematic literature reviews, research synthesis, guidelines, research methodology, Software},
  abstract={Context: Systematic literature reviews have become common in software engineering in the last decade, but challenges remain.Goal: Given the challenges, the objective is to describe improvement areas in writing primary studies, and hence provide a good basis for researchers aiming at synthesizing research evidence in a specific area.Method: The results presented are based on a literature review with respect to synthesis of research results in software engineering with a particular focus on empirical software engineering. The literature review is complemented and exemplified with experiences from conducting systematic literature reviews and working with research methodologies in empirical software engineering.Results: The paper presents three areas where improvements are needed to become more successful in synthesizing empirical evidence. These three areas are: terminology, paper content and reviewing.Conclusion: It is concluded that it must be possible to improve the primary studies, but it requires that researchers start having synthesis in mind when writing their research papers.}
}

@article{rayyan-727968385,
  title={Undergraduate teaching assistants in computer science: A systematic literature review},
  year={2019},
  issn={978-1-4503-6185-9},
  pages={31-40},
  author={Mirza, Diba and Conrad, Phillip T and Lloyd, Christian and Matni, Ziad and Gatin, Arthur},
  url={https://doi.org/10.1145/3291279.3339422},
  publisher={Association for Computing Machinery},
  series={ICER '19},
  keywords={cs1, cs2, undergraduate teaching assistants},
  abstract={We present a systematic literature review of the prior work on Undergraduate Teaching Assistants (UTAs) in Computer Science with two goals: (1) to create a taxonomy of practices that relate to the design and implementation of UTA programs, (2) to identify the benefits of using UTAs as claimed by the literature and characterize the level of evidence for those claims. We analyze 336 excerpts from 40 papers related to these goals. We use content analysis on excerpts describing practices to extract high-level themes that include recruiting, UTA and program coordinator duties, training, evaluation and organization of UTA programs. We perform a more fine-grained analysis within each theme to identify specific questions about UTA programs and the answers provided by the literature. Using a similar technique, we report on the claimed benefits of UTA programs to students, UTAs, instructors and institutions. Our analysis follows well-defined protocols involving multiple reviewers and we report on the inter-rater reliability. The results provided in this paper lay the groundwork for developing evidence-based best practices in UTA programs and inform practice and policy related to the use of UTAs at tertiary institutions. As such, it is relevant to educators establishing a new UTA program, expanding an existing program, or continuously improving an established program, as well as those designing research studies of such programs.}
}

@article{rayyan-727968386,
  title={A systematic review of automated feature engineering solutions in machine learning problems},
  year={2020},
  issn={978-1-4503-8873-3},
  author={Prado, Fernando F and Digiampietri, Luciano A},
  url={https://doi.org/10.1145/3411564.3411610},
  publisher={Association for Computing Machinery},
  series={SBSI'20},
  keywords={Auto Learning, Feature Engineering, Feature Generation, Feature Selection},
  abstract={During the last decades, machine learning has played an important role in building data-driven experiments. Based on information extracted from a variety of sources, new patterns can be identified, predictions can be made more easily and decisions made faster and more effective. The specialized application of machine learning solutions requires specific knowledge in areas such as math, computation, and statistics, as well as being extremely costly in time and having a high chance of incurring any kind of human error during the process. Automated Machine Learning Techniques (AutoML) seek to automate parts of the process of building machine learning applications, allowing non-experts to perform this process. An important part of this kind of problem is the feature engineering part which creates a transformation in the data, making it more representative for the final model. This paper presents a systematic review of automated feature engineering solutions in machine learning problems. With the main objective of identifying and analyzing the existing methods and techniques for performing the automated feature engineering step within the framework of machine learning problems.}
}

@article{rayyan-727968387,
  title={Integrating evidence from systematic reviews with software engineering practice through evidence briefings},
  year={2016},
  issn={978-1-4503-3691-8},
  author={Cartaxo, Bruno},
  url={https://doi.org/10.1145/2915970.2915973},
  publisher={Association for Computing Machinery},
  series={EASE '16},
  keywords={systematic reviews, evidence-based software engineering, evidence briefings, knowledge transfer, stackexchange, Software},
  abstract={This paper proposes the questions and method to conduct a research intending to promote the first steps towards a better integration between evidence from systematic reviews and software engineering practice. First, we are planning to conduct a tertiary study to identify systematic reviews in software engineering. Second, we intend to investigate how those reviews relate to practitioners' issues reported in a leading software engineering Question & Answer platform. Third, we plan to generate and evaluate Evidence Briefings from those reviews in order to establish a medium to transfer knowledge acquired from systematic reviews to practice. This paper also presents some preliminary results from pilot studies we have been conducting.}
}

@article{rayyan-727968388,
  title={Support mechanisms to conduct empirical studies in software engineering: A systematic mapping study},
  year={2015},
  issn={978-1-4503-3350-4},
  author={Borges, Alex and Ferreira, Waldemar and Barreiros, Emanoel and Almeida, Adauto and Fonseca, Liliane and Teixeira, Eudis and Silva, Diogo and Alencar, Aline and Soares, Sergio},
  url={https://doi.org/10.1145/2745802.2745823},
  publisher={Association for Computing Machinery},
  series={EASE '15},
  keywords={systematic mapping study, empirical software engineering, empirical strategies, support mechanisms, Software},
  abstract={Context: Empirical studies are gaining recognition in the Software Engineering (SE) research community, allowing improved quality of research and accelerating the adoption of new technologies in the software market. However, empirical studies in this area are still limited. In order to foster empirical research in SE, it is essential to understand the resources available to aid these studies. Goal: Identify support mechanisms (methodology, tool, guideline, process, etc.) used to conduct empirical studies in the Empirical Software Engineering (ESE) community. Method: We performed a systematic mapping study that included all full papers published at EASE, ESEM and ESEJ since their first editions. Were selected 891 studies between 1996 and 2013. Results: A total of 375 support mechanisms were identified. We provide the full list of mechanisms and the strategies that uses them. Despite this, we identified a high number of studies that do not cite any mechanism to support their empirical strategies: 433 studies (48%). Experiment is the strategy that has more resources to support their activities. And guideline was the most used type of mechanism. Moreover we observed that the most mechanisms used as reference to empirical studies are not specific to SE area. And some mechanisms were used only in specific activities of empirical research, such as statistical and qualitative data analysis. Experiment and case studies are the strategies most applied. Conclusions: The use of empirical methods in SE has increased over the years. Despite this, many studies did not apply these methods and do not cite any resource to guide their research. Therefore, the list of support mechanisms, where and how they were applied is a major asset to the SE community. Such asset can encourage empirical studies aiding the choice regarding which strategies and mechanisms to use in a research, as well as pointing out examples where they were used, mainly to novice researchers. We also identified new perspectives and gaps that foster other research for the improvement of empirical research in this area.}
}

@article{rayyan-727968389,
  title={Pragmatic assessment of research intensive areas in cloud: A systematic review},
  year={2013},
  journal={SIGSOFT Softw. Eng. Notes},
  issn={0163-5948},
  volume={38},
  number={3},
  pages={1-6},
  author={Hasteer, Nitasha and Bansal, Abhay and Murthy, B K},
  url={https://doi.org/10.1145/2464526.2464533},
  keywords={systematic review, cloud computing, cloud services, cloud deployment, cloud migration, cloud security, networking},
  abstract={Cloud computing is a name given to a set of systems for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. Cloud computing is aimed at making an organization more agile and cost effective. Due to the rapid evolution of Cloud Computing in the recent past, it is relevant to investigate the key areas of research of this technology. In this paper, we present a systematic review of research intensive areas in the field of cloud computing. Research papers in the period from 2009 to 2012 were gathered. A total of 36 research papers were reviewed systematically and categorized into four broad categories based on the issues addressed by them. We identified that the majority of the research papers focused on Cloud Security. By systematically analyzing the work accomplished so far, the gaps and yet to be explored areas in this field are brought to light.}
}

@article{rayyan-727968390,
  title={Cross- vs. within-Company cost estimation studies revisited: An extended systematic review},
  year={2014},
  issn={978-1-4503-2476-2},
  author={Mendes, Emilia and Kalinowski, Marcos and Martins, Daves and Ferrucci, Filomena and Sarro, Federica},
  url={https://doi.org/10.1145/2601248.2601284},
  publisher={Association for Computing Machinery},
  series={EASE '14},
  keywords={systematic review, cost estimation models, cross-company data, estimation accuracy, within-company data},
  abstract={[Objective] The objective of this paper is to extend a previously conducted systematic literature review (SLR) that investigated under what circumstances individual organizations would be able to rely on cross-company based estimation models. [Method] We applied the same methodology used in the SLR we are extending herein (covering the period 2006-2013) based on primary studies that compared predictions from cross-company models with predictions from within-company models constructed from analysis of project data. [Results] We identified 11 additional papers; however two of these did not present independent results and one had inconclusive findings. Two of the remaining eight papers presented both, trials where cross-company predictions were not significantly different from within-company predictions and others where they were significantly different. Four found that cross-company models gave prediction accuracy significantly different from within-company models (one of them in favor of cross-company models), while two found no significant difference. The main pattern when examining the study related factors was that studies where cross-company predictions were significantly different from within-company predictions employed larger within-company data sets. [Conclusions] Overall, half of the analyzed evidence indicated that cross-company estimation models are not significantly worse than within-company estimation models. Moreover, there is some evidence that sample size does not imply in higher estimation accuracy, and that samples for building estimation models should be carefully selected/filtered based on quality control and project similarity aspects. The results need to be combined with the findings from the SLR we are extending to allow further investigating this topic.}
}

@article{rayyan-727968391,
  title={Report of the 4th international symposium on empirical software engineering and measurement ESEM 2010},
  year={2011},
  journal={SIGSOFT Softw. Eng. Notes},
  issn={0163-5948},
  volume={36},
  number={2},
  pages={28-34},
  author={Lutteri, Emiliano and Russo, Barbara and Succi, Giancarlo},
  url={https://doi.org/10.1145/1943371.1943393},
  keywords={software engineering, empirical software engineering, software measurement, ESEM, Software},
  abstract={This report summarizes the research works, in particular the full and short papers, presented at the 4th International Symposium on Empirical Software Engineering and Measurement (ESEM 2010), held the 16th and 17th of September in Bolzano-Bozen, Italy. The program provided thirty full papers, twenty six short papers and three invited talks held by Bertrand Meyer, Steve Fraser and Carlo Ghezzi.}
}

@article{rayyan-727968392,
  title={Contextualizing research evidence through knowledge translation in software engineering},
  year={2019},
  issn={978-1-4503-7145-2},
  pages={306-311},
  author={Badampudi, Deepika and Wohlin, Claes and Gorschek, Tony},
  url={https://doi.org/10.1145/3319008.3319358},
  publisher={Association for Computing Machinery},
  series={EASE '19},
  keywords={Decision-making, Bayesian synthesis, Knowledge translation, Software},
  abstract={Usage of software engineering research in industrial practice is a well-known challenge. Synthesis of knowledge from multiple research studies is needed to provide evidence-based decision-support for industry. The objective of this paper is to present a vision of how a knowledge translation framework may look like in software engineering research, in particular how to translate research evidence into practice by combining contextualized expert opinions with research evidence. We adopted the framework of knowledge translation from health care research, adapted and combined it with a Bayesian synthesis method. The framework provided in this paper includes a description of each step of knowledge translation in software engineering. Knowledge translation using Bayesian synthesis intends to provide a systematic approach towards contextualized, collaborative and consensus-driven application of research results. In conclusion, this paper contributes towards the application of knowledge translation in software engineering through the presented framework.}
}

@article{rayyan-727968393,
  title={A systematic review of the use of bloom's taxonomy in computer science education},
  year={2018},
  issn={978-1-4503-5103-4},
  pages={441-446},
  author={Masapanta-Carrión, Susana and Velázquez-Iturbide, J Ángel},
  url={https://doi.org/10.1145/3159450.3159491},
  publisher={Association for Computing Machinery},
  series={SIGCSE '18},
  keywords={Bloom's taxonomy, computer science education, difficulties},
  abstract={Bloom's taxonomy is a model that allows characterizing students' learning achievements. It is frequently used in computer science education (CSE), but its use is not straightforward. We present a systematic review conducted to know actual use of the taxonomy in CSE. We found that it was mostly used in programming education and to assess students' performance. A more relevant contribution is a classification of authors' difficulties. In particular, the most often reported difficulty is determining the level of the taxonomy where an assessment task can be classified. In addition, we present authors' hypotheses about possible causes of the difficulties and the solutions they adopted.}
}

@article{rayyan-727968394,
  title={Features of second screen applications: A systematic review},
  year={2016},
  issn={978-1-4503-4512-5},
  pages={83-86},
  author={do Nascimento, Francisca Joamila Brito and de Souza, Cidcley Teixeira},
  url={https://doi.org/10.1145/2976796.2988169},
  publisher={Association for Computing Machinery},
  series={Webmedia '16},
  keywords={systematic review, applications, features, mobile devices, digital tv, second screen},
  abstract={In this paper, we present a systematic literature review, whose object of study are the available features in second screen applications. The second screen is the ability to interact with the TV programming by using mobile devices. The research was conducted in order to find out what the most common features in second screen apps. The selected articles refer to 16 different features, which were grouped into 5 categories. Interactivity was the category with more features mentioned in the researched works. Therefore, we concluded the features that provide interactivity between users and TV programming are the most common on the second screen.}
}

@article{rayyan-727968395,
  title={Exploring and improving industry-academia communication in software engineering},
  year={2020},
  issn={978-1-4503-7731-7},
  pages={379-382},
  author={Rico, Sergio},
  url={https://doi.org/10.1145/3383219.3386125},
  publisher={Association for Computing Machinery},
  series={EASE '20},
  keywords={design-science, Industry academia communication, industry-academia collaboration, rapid reviews, Software},
  abstract={Background: Despite the mutual benefit of the industry-academia partnership, the level of joint work is still low. For this reason, the interest in connecting research and practice has increased recently in the academic community. Objective: This research aims to design and apply approaches to improve the knowledge exchange between academic researchers and software engineering practitioners. Methodology: This work can be seen from a design science perspective. Following the design-science paradigm, the knowledge regarding a phenomenon is obtained through the design and evaluation of solutions that apply in a specific context. Consequently, this research work proposes and evaluates approaches to bridge the communication gap. Results: Two approaches have been explored and partially evaluated. The SERP-taxonomy architecture that can be used to describe and link research results and industry challenges, and rapid reviews to foster communication between industry and academia. Conclusion: This thesis will provide empirical evidence of the application of collaborative approaches to improve industry-academia communication and get closer research and practice.}
}

@article{rayyan-727968396,
  title={Research in concurrent software testing: A systematic review},
  year={2011},
  issn={978-1-4503-0809-0},
  pages={1-5},
  author={Souza, Simone R S and Brito, Maria A S and Silva, Rodolfo A and Souza, Paulo S L and Zaluska, Ed},
  url={https://doi.org/10.1145/2002962.2002964},
  publisher={Association for Computing Machinery},
  series={PADTAD '11},
  keywords={systematic review, software testing, testing tools, bug classification, concurrent program, concurrent program testing, Software},
  abstract={The current increased demand for distributed applications in domains such as web services and cloud computing has significantly increased interest in concurrent programming. This demand in turn has resulted in new testing methodologies for such systems, which take account of the challenges necessary to test these applications. This paper presents a systematic review of the published research related to concurrent testing approaches, bug classification and testing tools. A systematic review is a process of collection, assessment and interpretation of the published papers related to a specific search question, designed to provide a background for further research. The results include information about the research relationships and research teams that are working in the different areas of concurrent programs testing.}
}

@article{rayyan-727968397,
  title={Organizational social structures for software engineering},
  year={2013},
  journal={ACM Computing Surveys},
  issn={0360-0300},
  volume={46},
  number={1},
  author={Tamburri, Damian A and Lago, Patricia and van Vliet, Hans},
  url={https://doi.org/10.1145/2522968.2522971},
  keywords={knowledge management, software organizations, communities, cultural implications, governance, information trust, organizational decision-making, Organizational social structures, social adaptivity, social context, social networks, social structures, software practice, user perspective, users, Software},
  abstract={Software engineering evolved from a rigid process to a dynamic interplay of people (e.g., stakeholders or developers). Organizational and social literature call this interplay an Organizational Social Structure (OSS). Software practitioners still lack a systematic way to select, analyze, and support OSSs best fitting their problems (e.g., software development). We provide the state-of-the-art in OSSs, and discuss mechanisms to support OSS-related decisions in software engineering (e.g., choosing the OSS best fitting development scenarios). Our data supports two conclusions. First, software engineering focused on building software using project teams alone, yet these are one of thirteen OSS flavors from literature. Second, an emerging OSS should be further explored for software development: social networks. This article represents a first glimpse at OSS-aware software engineering, that is, to engineer software using OSSs best fit for the problem.}
}

@article{rayyan-727968398,
  title={An ecological perspective towards the evolution of quantitative studies in software engineering},
  year={2013},
  issn={978-1-4503-1848-8},
  pages={216-219},
  author={de Mello, Rafael M and Travassos, Guilherme H},
  url={https://doi.org/10.1145/2460999.2461031},
  publisher={Association for Computing Machinery},
  series={EASE '13},
  keywords={meta-analysis, threats to validity, evidence based software engineering, ecology, experimental chains, food chains, population sampling, quantitative survey, quasi-experiments, Software},
  abstract={Context: Two of the most common external threats to validity in quantitative studies in software engineering (SE) are concerned with defining the population by convenience and nonrandom sampling assignment. Although these limitations can be reduced by increasing the number of replications and aggregating their results, the acquired evidence rarely can be generalized to the field.Objective: To investigate the state of practice of meta-analysis in SE and its limitations, intending to propose an alternative perspective to understand the relationships among experimentation, production, threats to validity and evidence. To propose and evaluate means to strengthen quantitative studies in software engineering and making them less risky due to population and sampling issues.Method: To use the underlying idea from the Theory of Food Chains to alternatively understand the impact of external threats to validity in the SE experimental cycle (experimental chains). Next, to accomplish an initial technical literature survey to observe basic features of secondary studies aggregating primary studies results. Third, to organize a set of experimental chain's concepts and make initial discussions regarding the observed secondary studies concerned with this metaphor.Results: By applying the experimental chains concepts it was initially observed that, although important and necessary, most of the current effort in the conduction of quantitative studies in SE does not produce (mainly due to population/sampling constraints) results strong enough to positively impact the engineering of software. It promotes an imbalance between research and practice. However, more investigation is necessary to support this claim.Conclusion: We argue that research energy has been lost in SE studies due to population/sampling constraints. Therefore, we believe more investigation must be undertaken to understand how better organizing, enlarging, setting up and sampling SE quantitative studies' population by using, for instance, alternative technologies such as social networks or other crowdsourcing technologies.}
}

@article{rayyan-727968399,
  title={Effort and cost in software engineering: A comparison of two industrial data sets},
  year={2017},
  issn={978-1-4503-4804-1},
  pages={51-60},
  author={Huijgens, Hennie and van Deursen, Arie and Minku, Leandro L and Lokan, Chris},
  url={https://doi.org/10.1145/3084226.3084249},
  publisher={Association for Computing Machinery},
  series={EASE'17},
  keywords={Benchmarking, ISBSG, Cost Prediction, EBSPM, Evidence-Based Software Portfolio Management, Software Economics, Software},
  abstract={Context The research literature on software development projects usually assumes that effort is a good proxy for cost. Practice, however, suggests that there are circumstances in which costs and effort should be distinguished. Objectives: We determine similarities and differences between size, effort, cost, duration, and number of defects of software projects. Method: We compare two established repositories (ISBSG and EBSPM) comprising almost 700 projects from industry. Results: We demonstrate a (log)-linear relation between cost on the one hand, and size, duration and number of defects on the other. This justifies conducting linear regression for cost. We establish that ISBSG is substantially different from EBSPM, in terms of cost (cheaper) and duration (faster), and the relation between cost and effort. We show that while in ISBSG effort is the most important cost factor, this is not the case in other repositories, such as EBSPM in which size is the dominant factor. Conclusion: Practitioners and researchers alike should be cautious when drawing conclusions from a single repository.}
}

@article{rayyan-727968400,
  title={The role of rapid reviews in supporting decision-making in software engineering practice},
  year={2018},
  issn={978-1-4503-6403-4},
  pages={24-34},
  author={Cartaxo, Bruno and Pinto, Gustavo and Soares, Sergio},
  url={https://doi.org/10.1145/3210459.3210462},
  publisher={Association for Computing Machinery},
  series={EASE'18},
  keywords={Evidence Based Software Engineering, Rapid Reviews, Systematic Reviews, Support Decision-Making, Decision Making, Software},
  abstract={Context: Recent work on Evidence Based Software Engineering (EBSE) suggests that systematic reviews lack connection with Software Engineering (SE) practice. In Evidence Based Medicine there is a growing initiative to address this kind of problem, in particular through what has been named as Rapid Reviews (RRs). They are adaptations of regular systematic reviews made to fit practitioners constraints.Goal: Evaluate the perceptions from SE practitioners on the use of Rapid Reviews to support decision-making in SE practice.Method: We conducted an Action Research to evaluate RRs insertion in a real-world software development project.Results: Our results show that practitioners are rater positive about Rapid Reviews. They reported to have learned new concepts, reduced time and cost of decision-making, improved their understanding about the problem their facing, among other benefits. Additionally, two months after the introduction of the Rapid Review, in a follow up visit, we perceived that the practitioners have indeed adopted the evidence provided.Conclusions: Based on the positive results we obtained with this study, and the experiences reported in medicine, we believe RRs could play an important role towards knowledge transfer and decision-making support in SE practice.}
}

@article{rayyan-727968401,
  title={Empirical evaluation of cloud-based testing techniques: A systematic review},
  year={2012},
  journal={SIGSOFT Softw. Eng. Notes},
  issn={0163-5948},
  volume={37},
  number={3},
  pages={1-9},
  author={Priyanka and Chana, Inderveer and Rana, Ajay},
  url={https://doi.org/10.1145/2180921.2180938},
  keywords={software testing, cloud testing, performance testing, cloud-based testing, metamorphic testing, privacy-aware testing, security testing, symbolic execution, testing cloud services},
  abstract={Software Testing is a challenging activity for many software engineering projects, especially for large scale systems. The amount of tests cases can range from a few hundred to several thousands, requiring significant computing resources and lengthy execution times. Cloud computing offers the potential to address both of these issues: it offers resources such as virtualized hardware, effectively unlimited storage, and software services that can aid in reducing the execution time of large test suites in a cost-effective manner. In this paper we report on a systematic review of cloud based testing techniques published in major software engineering journals and conferences conducted by other researchers. Research papers were gathered from various scholarly databases using provided search engines within a given period of time. A total of 82 research papers are analyzed in this systematic review and we classified it into four categories according to issues addressed by them. We identified majority of the research papers focused on Cloud based Testing and Issues (38 papers) and 23 papers focused on Cloud based Testing Frameworks. By looking at the areas focused by existing researchers, gaps and untouched areas of cloud based testing can be discovered}
}

@article{rayyan-727968402,
  title={Towards a hypothetical framework of humans related success factors for process improvement in global software development: Systematic review},
  year={2017},
  issn={978-1-4503-4486-9},
  pages={180-186},
  author={Khan, Arif Ali and Keung, Jacky and Niazi, Mahmood and Hussain, Shahid},
  url={https://doi.org/10.1145/3019612.3019685},
  publisher={Association for Computing Machinery},
  series={SAC '17},
  keywords={systematic literature review, global software development, software process improvement, human, Humanities, Humanism, Humans, Software},
  abstract={Presently, the majority of the software development organizations are adopting the phenomena of Global Software Development (GSD), mainly because of the significant return on investment it produces. However, GSD is a complex phenomenon and there are many challenges associated with it, especially that related to Software Process Improvement (SPI). The aim of this work is to identify humans' related factors that can positively impact the SPI process in GSD organizations and proposed a hypothetical framework of the identified success factors in relation to SPI implementation. We have adopted the Systematic Literature Review (SLR) method in order to identify the success factors. Using the SLR approach, total ten success factors were identified. The paper also reported the Critical Success Factors (CSFs) for SPI implementation following the criteria of the factors having a frequency ≥ 50% as critical. Our results reveal that five out of ten factors are critical for SPI program. Based on the analysis of the identified success factors, we have presented a hypothetical framework that has highlighted an association between the identified success factors and the implementation of the SPI program in GSD environment.}
}

@article{rayyan-727968403,
  title={Social metrics included in prediction models on software engineering: A mapping study},
  year={2014},
  issn={978-1-4503-2898-2},
  pages={72-81},
  author={Wiese, Igor Scaliante and Côgo, Filipe Roseiro and Ré, Reginaldo and Steinmacher, Igor and Gerosa, Marco Aurélio},
  url={https://doi.org/10.1145/2639490.2639505},
  publisher={Association for Computing Machinery},
  series={PROMISE '14},
  keywords={mapping study, social network analysis, prediction models, social metrics, Metronidazole, Software},
  abstract={Context: Previous work that used prediction models on Software Engineering included few social metrics as predictors, even though many researchers argue that Software Engineering is a social activity. Even when social metrics were considered, they were classified as part of other dimensions, such as process, history, or change. Moreover, few papers report the individual effects of social metrics. Thus, it is not clear yet which social metrics are used in prediction models and what are the results of their use in different contexts. Objective: To identify, characterize, and classify social metrics included in prediction models reported in the literature. Method: We conducted a mapping study (MS) using a snowballing citation analysis. We built an initial seed list adapting strings of two previous systematic reviews on software prediction models. After that, we conducted backward and forward citation analysis using the initial seed list. Finally, we visited the profile of each distinct author identified in the previous steps and contacted each author that published more than 2 papers to ask for additional candidate studies. Results: We identified 48 primary studies and 51 social metrics. We organized the metrics into nine categories, which were divided into three groups - communication, project, and commit-related. We also mapped the applications of each group of metrics, indicating their positive or negative effects. Conclusions: This mapping may support researchers and practitioners to build their prediction models considering more social metrics.}
}

@article{rayyan-727968404,
  title={On vulnerability and security log analysis: A systematic literature review on recent trends},
  year={2020},
  issn={978-1-4503-8025-6},
  pages={175-180},
  author={Svacina, Jan and Raffety, Jackson and Woodahl, Connor and Stone, Brooklynn and Cerny, Tomas and Bures, Miroslav and Shin, Dongwan and Frajtak, Karel and Tisnovsky, Pavel},
  url={https://doi.org/10.1145/3400286.3418261},
  publisher={Association for Computing Machinery},
  series={RACS '20},
  keywords={Machine Learning, Log Analysis, Log Mining, Anomaly Detection, Intrusion Detection},
  abstract={Log analysis is a technique of deriving knowledge from log files containing records of events in a computer system. A common application of log analysis is to derive critical information about a system's security issues and intrusions, which subsequently leads to being able to identify and potentially stop intruders attacking the system. However, many systems produce a high volume of log data with high frequency, posing serious challenges in analysis. This paper contributes with a systematic literature review and discusses current trends, advancements, and future directions in log security analysis within the past decade. We summarized current research strategies with respect to technology approaches from 34 current publications. We identified limitations that poses challenges to future research and opened discussion on issues towards logging mechanism in the software systems. Findings of this study are relevant for software systems as well as software parts of the Internet of Things (IoT) systems.}
}

@article{rayyan-727968405,
  title={Realising evidence-based software engineering a report from the workshop held at ICSE 2005},
  year={2005},
  journal={SIGSOFT Softw. Eng. Notes},
  issn={0163-5948},
  volume={30},
  number={5},
  pages={1-5},
  author={Budgen, David and Kitchenham, Barbara},
  url={https://doi.org/10.1145/1095430.1095435},
  keywords={evidence, Software},
  abstract={Context: The workshop was held to explore the potential for adapting the ideas of evidence-based practices as used in medicine and other disciplines for use in software engineering.Objectives: To devise ways of developing suitable evidence-based practices and procedures, especially the use of structured literature reviews, and introducing these into software engineering research and practice.Method: Three sessions were dedicated to a mix of presentations based on position papers and interactive discussion, while the fourth focused upon the key issues as decided by the participants.Results: An initial scoping of the major issues, identification of useful parallels, and some plans for future development of an evidence-based software engineering community.Conclusions: While there are substantial challenges to introducing evidence-based practices, there are useful experiences to be drawn from a variety of other domains.}
}

@article{rayyan-727968406,
  title={How do secondary studies in software engineering report automated searches? A preliminary analysis},
  year={2018},
  issn={978-1-4503-6403-4},
  pages={145-150},
  author={Singh, Paramvir and Galster, Matthias and Singh, Karanpreet},
  url={https://doi.org/10.1145/3210459.3210474},
  publisher={Association for Computing Machinery},
  series={EASE'18},
  keywords={Systematic literature reviews, tertiary study, reliability, mapping studies, secondary studies, reproducibility, search process, Software},
  abstract={Context: Systematic literature reviews and mapping studies usually rely on automated searches of digital libraries to identify primary studies. Defining proper search strings, executing semantically similar searches on different libraries, and reporting limitations of searches increase the reliability of secondary studies. Objective: We aim to survey the current state of using automated searches in secondary software engineering studies. In particular, we aim at analyzing how automated searches are reported and at understanding the reproducibility of secondary studies. Method: We perform a preliminary tertiary study that covers 50 recently published representative secondary studies from different software engineering venues and subfields. Results: We found that most secondary studies complement an automated search with a manual search and use four or more digital libraries. Also, we found that the quality of reporting search strings is rather poor. Finally, we found that most secondary studies do not acknowledge limitations of automated searches and implications of limitations on study findings. Conclusions: Our findings highlight implications for researchers (e.g., to properly report the search process) and for reviewers (e.g., to execute search strings reported in papers). Also, our findings indicate that secondary studies are difficult to replicate.}
}

@article{rayyan-727968407,
  title={A critical appraisal of systematic reviews in software engineering from the perspective of the research questions asked in the reviews},
  year={2010},
  issn={978-1-4503-0039-1},
  author={da Silva, Fabio Q B and Santos, André L M and Soares, Sérgio C B and França, A César C and Monteiro, Cleviton V F},
  url={https://doi.org/10.1145/1852786.1852830},
  publisher={Association for Computing Machinery},
  series={ESEM '10},
  keywords={systematic reviews, software engineering, mapping studies, Software},
  abstract={After a seminal article introducing-evidence based software engineering in 2004, systematic reviews (SR) have been increasingly used as a method for conducting secondary studies in software engineering. Our goal is to critically appraise the use of SR in software engineering with respect to the research questions asked and the ways the questions were used in the reviews. We analyzed 53 literature reviews that had been collected in two published tertiary studies. We found that over 65% of the research questions asked in the reviews were exploratory and only 15% investigated causality questions. We concluded that there is a need for a consistent use of terminology to classify secondary studies and that reports of literature reviews should follow reporting guidelines to support assessment and comparison.}
}

@article{rayyan-727968408,
  title={Supporting knowledge transfer from secondary studies to software engineering practice},
  year={2018},
  journal={SIGSOFT Softw. Eng. Notes},
  issn={0163-5948},
  volume={43},
  number={1},
  pages={1-6},
  author={Cartaxo, Bruno},
  url={https://doi.org/10.1145/3178315.3178325},
  keywords={Software},
  abstract={Researchers have been arguing that there is a lack of connection between Secondary Studies (SSs) and Software Engi-neering (SE) practice. The medical eld has faced the same prob-lem, and recently introduced the concept of brie ngs/summaries, and Rapid Reviews as alternatives to transfer knowledge to prac-tice.Goal: The overarching goal of this research is to investigate, pro-pose, and evaluate strategies to support researchers to transfer knowledge from SSs to SE practice.Method: First, we investigated how SSs in SE cover practition-ers» issues reported in StackExchange, a leading Question & An-swer platform. Second, we generated Evidence Brie ngs based on those SSs in order to propose a medium to transfer knowledge to practice. Third, we are planning to conduct an action research, with close collaboration with practitioners, in order explore and evaluate the applicability of Rapid Reviews in SE practice. Preliminary Results: Among 424 practitioners» issues on Stack Exchange, that were considered as related to a set o selected SSs, the SSs could successfully cover 14.1% (60) of them. Based on a qualitative techniques, we identi ed 45 recurrent issues spread in many SE topics. Additionally, both practitioners and researchers positively evaluated the content and format of 12 Evidence Brief-ings that we created based on SSs.Conclusions: Our results until now corroborate with claims that SSs lack connection with practice. On the other side, the good reception of the Evidence Brie ngs shows a possible route toward an e ective knowledge transfer from SSs to practice.}
}

@article{rayyan-727968409,
  title={Systematic review in software engineering: Where we are and where we should be going},
  year={2012},
  issn={978-1-4503-1509-8},
  pages={1-2},
  author={Kitchenham, Barbara A},
  url={https://doi.org/10.1145/2372233.2372235},
  publisher={Association for Computing Machinery},
  series={EAST '12},
  keywords={research methods, systematic literature review process, Software},
  abstract={In 2004 Kitchenham et al. first proposed the idea of evidence-based software engineering (EBSE). EBSE requires a systematic and unbiased method of aggregating empirical studies and has encouraged software engineering researches to undertake systematic literature reviews (SLRs) of Software Engineering topics and research questions. As software engineers began to use the SLR technology, they also began to comment on the SLR process itself. Brereton et al (2007) was one of the first papers that commented on issues connected with performing SLRs and many such papers have followed since covering topics such as: The use of SLRs in education; Experiences of novices using SLRs; The adoption of mapping and scoping studies; The repeatability of SLRs; Improving the search and selection processes; Quality assessment of primary studies; Improving aggregation processes.It therefore seems appropriate to identify the current status of such studies in software engineering, and identify whether there is evidence for revising and/or extending the guidelines for performing systematic literature reviews (Kitchenham and Charters, 2007). This keynote will report the current results of an ongoing systematic literature review that aims: A1: To identify and categorise papers investigating the SLR process and the claims relating to that process; A2: To identify the extent to which the claims of repeatability, lack of bias, and openness are supported; A3: To identify any areas where current guidelines need to be amended or extended to reflect current knowledge of applying SLRs in the context of software engineering.}
}

@article{rayyan-727968410,
  title={Introductory programming: A systematic literature review},
  year={2018},
  issn={978-1-4503-6223-8},
  pages={55-106},
  author={Luxton-Reilly, Andrew and Simon and Albluwi, Ibrahim and Becker, Brett A and Giannakos, Michail and Kumar, Amruth N and Ott, Linda and Paterson, James and Scott, Michael James and Sheard, Judy and Szabo, Claudia},
  url={https://doi.org/10.1145/3293881.3295779},
  publisher={Association for Computing Machinery},
  series={ITiCSE 2018 companion},
  keywords={systematic review, overview, systematic literature review, SLR, literature review, introductory programming, review, CS1, ITiCSE working group, novice programming},
  abstract={As computing becomes a mainstream discipline embedded in the school curriculum and acts as an enabler for an increasing range of academic disciplines in higher education, the literature on introductory programming is growing. Although there have been several reviews that focus on specific aspects of introductory programming, there has been no broad overview of the literature exploring recent trends across the breadth of introductory programming. This paper is the report of an ITiCSE working group that conducted a systematic review in order to gain an overview of the introductory programming literature. Partitioning the literature into papers addressing the student, teaching, the curriculum, and assessment, we explore trends, highlight advances in knowledge over the past 15 years, and indicate possible directions for future research.}
}

@article{rayyan-727968411,
  title={The adoption of capture-recapture in software engineering: A systematic literature review},
  year={2015},
  issn={978-1-4503-3350-4},
  author={Liu, Gaoxuan and Rong, Guoping and Zhang, He and Shan, Qi},
  url={https://doi.org/10.1145/2745802.2745816},
  publisher={Association for Computing Machinery},
  series={EASE '15},
  keywords={systematic literature review, capture-recapture method, defect estimation, software inspection, Software},
  abstract={Context: Capture-recapture method has long been adopted in software engineering as a relatively objective way for defect estimation. While many relevant studies have been carried out to evaluate various capture-recapture models and estimators, there still lacks common understanding on the adoption status of the method in software engineering. It is necessary to systematically collect empirical evidence of Capture-recapture adoption hence form necessary understanding on the method.Objective: This study aims to synthesize relevant primary studies on the adoption of capture-recapture method in software engineering, and try to identify possible gaps between the state-of-practice and the state-of-art so as to provide clues for future research.Method: By following the guidelines of Kitchenham, we conducted a Systematic Literature Review(SLR) on studies of the adoption of capture-recapture method in software engineering.Results: From 5 common digital libraries, we retrieved 506 published articles, among them 44 were identified as relevant primary studies. We identified 18 capture-recapture estimators under 4 basic models. Types of the currently existing studies as well as the relevant influencing factors to adoption of the capture-recapture method are also discussed.Conclusion: Results show that there are no conclusive decisions on the best capture-recapture models and estimators. Besides, the number of inspectors and their capability to detect defects as well as the difficulty to detect defects are most critical influencing factors. In addition, lacking of industrial application may be the major issue of current adoption status of capture-recapture method in software engineering.}
}

@article{rayyan-727968412,
  title={A systematic literature review on global software development life cycle},
  year={2015},
  journal={SIGSOFT Softw. Eng. Notes},
  issn={0163-5948},
  volume={40},
  number={2},
  pages={1-14},
  author={Jain, Ritu and Suman, Ugrasen},
  url={https://doi.org/10.1145/2735399.2735408},
  keywords={systematic literature review, software engineering, challenges, global software development, tools, process, distributed software development, software development life cycle, best practices, problems, Software},
  abstract={Global software development (GSD) has now become a prominent software development paradigm. Software companies are increasingly adopting GSD approaches in order to produce high quality software. GSD's popularity has attracted the researchers to investigate this field, but most of the research work related to global software development cycle is scattered. Therefore, there is a need to integrate and compile all research work related to GSD life cycle to provide a consolidated understanding for software practitioners as well as researchers. In this paper, we report our findings through systematic literature review that aimed at identifying the challenges faced by the globally distributed teams during various phases of software development. We have also discussed suggested best practices, and tools that can be helpful in alleviating these challenges.}
}

@article{rayyan-727968413,
  title={A systematic literature review of improved knowledge management in agile software development},
  year={2019},
  issn={978-1-4503-6642-7},
  pages={102-105},
  author={Al Hafidz, Mochamad Umar and Sensuse, Dana Indra},
  url={https://doi.org/10.1145/3305160.3305192},
  publisher={Association for Computing Machinery},
  series={ICSIM 2019},
  keywords={Systematic literature review, ⚠️ Invalid DOI, agile software development, knowledge management, improvement, Software},
  abstract={Agile Software Development (ASD) is an adaptive software development approach that easily adapts to changing software requirements. It offers an advantage in time management but has disadvantages such as lack of software documentation and knowledge management. This research aims to understand more about research development in the knowledge management improvisation in Agile Software Development by collecting various themes of improved area and method used. To achieve this goal, 226 articles written in 2009-2018 are screened by using Kitchenham method to produce 15 best articles. This systematic literature review (SLR) results in a summary of improvements in knowledge management. The summary includes various approaches of several themes such as documentation, tools or technology, and others. The areas that need improvement are tools for supporting communication and documentation. The suggested improvement that has been proposed by researcher focuses mostly on artifact documentation, decision making, effort estimation and tools. In these studies, research question can be identified, analyzed, and answered.}
}

@article{rayyan-727968414,
  title={A systematic literature review on interaction flow modeling language (IFML)},
  year={2018},
  issn={978-1-4503-5431-8},
  pages={134-138},
  author={Hamdani, Maryum and Butt, Wasi Haider and Anwar, Muhammad Waseem and Azam, Farooque},
  url={https://doi.org/10.1145/3180374.3181333},
  publisher={Association for Computing Machinery},
  series={ICMSS 2018},
  keywords={SLR, IFML, Interaction flow modeling language, MDA},
  abstract={Design of front-end interfaces in software applications is a complex process. In this context, Interaction Flow Modeling Language (IFML) is an emerging standard introduced in 2013 by Object Management Group (OMG). In this article, a Systematic Literature Review (SLR) is performed to examine the applications of IFML. Particularly, 22 research studies (2014-2017) are identified and analyzed. Consequently, four major areas are recognized where IFML is frequently applied i.e. mobile applications (9 studies), web applications (8 studies), others (4 studies) and desktop applications (1 study). Furthermore, 9 leading IFML tools are presented i.e. Modeling (3) and model transformation (6). It has been concluded that IFML certainly simplifies the design and implementation of front-end interfaces. However, the existing IFML tools are not mature enough to be utilized for complex and large software applications.}
}

@article{rayyan-727968415,
  title={A systematic literature review of traceability approaches between software architecture and source code},
  year={2014},
  issn={978-1-4503-2476-2},
  author={Javed, Muhammad Atif and Zdun, Uwe},
  url={https://doi.org/10.1145/2601248.2601278},
  publisher={Association for Computing Machinery},
  series={EASE '14},
  keywords={systematic literature review, software architecture, traceability, source code, Software},
  abstract={The links between the software architecture and the source code of a software system should be based on solid traceability mechanisms in order to effectively perform quality control and maintenance of the software system. There are several primary studies on traceability between software architecture and source code but so far no systematic literature review (SLR) has been undertaken. This study presents an SLR which has been carried out to discover the existing traceability approaches and tools between software architecture and source code, as well as the empirical evidence for these approaches, their benefits and liabilities, their relations to software architecture understanding, and issues, barriers, and challenges of the approaches. In our SLR the ACM Guide to Computing Literature has been electronically searched to accumulate the biggest share of relevant scientific bibliographic citations from the major publishers in computing. The search strategy identified 742 citations, out of which 11 have been included in our study, dated from 1999 to July, 2013, after applying our inclusion and exclusion criteria. Our SLR resulted in the identification of the current state-of-the-art of traceability approaches and tools between software architecture and source code, as well as gaps and pointers for further research. Moreover, the classification scheme developed in this paper can serve as a guide for researchers and practitioners to find a specific approach or set of approaches that is of interest to them.}
}

@article{rayyan-727968416,
  title={A systematic literature review of technical debt prioritization},
  year={2020},
  issn={978-1-4503-7960-1},
  pages={1-10},
  author={Alfayez, Reem and Alwehaibi, Wesam and Winn, Robert and Venson, Elaine and Boehm, Barry},
  url={https://doi.org/10.1145/3387906.3388630},
  publisher={Association for Computing Machinery},
  series={TechDebt '20},
  keywords={software, software maintenance, technical debt, prioritization, software management},
  abstract={Repaying all technical debt (TD) present in a system may be unfeasible, as there is typically a shortage in the resources allocated for TD repayment. Therefore, TD prioritization is essential to best allocate such resources to determine which TD items are to be repaid first and which items are to be delayed until later releases. This study conducts a systematic literature review (SLR) to identify and analyze the currently researched TD prioritization approaches. The employed search strategy strove to achieve high completeness through the identification of a quasi-gold standard set, which was used to establish a search string to automatically retrieve papers from select research databases. The application of selection criteria, along with forward and backward snowballing, identified 24 TD prioritization approaches. The analysis of the identified approaches revealed a scarcity of approaches that account for cost, value, and resources constraint and a lack of industry evaluation. Furthermore, this SLR unveils potential gaps in the current TD prioritization research, which future research may explore.}
}

@article{rayyan-727968417,
  title={Outcomes of a community workshop to identify and rank barriers to the systematic literature review process},
  year={2014},
  issn={978-1-4503-2476-2},
  author={Hassler, Edgar and Carver, Jeffrey C and Kraft, Nicholas A and Hale, David},
  url={https://doi.org/10.1145/2601248.2601274},
  publisher={Association for Computing Machinery},
  series={EASE '14},
  keywords={systematic literature review, empirical software engineering, workshop},
  abstract={Systematic Literature Reviews (SLRs) are an important tool used by software engineering researchers to summarize the state of knowledge about a particular topic. Currently, SLR authors must perform the difficult, time-consuming task in largely manual fashion. To identify barriers faced by SLR authors, we conducted an interactive community workshop prior to ESEM'13. Workshop participants generated a total of 100 ideas that, through group discussions, formed 37 composite barriers to the SLR process. Further analysis reveals the barriers relate to latent themes regarding the SLR process, primary studies, the practitioner community, and tooling. This paper describes the barriers identified during the workshop along with a ranking of those barriers that is based on votes by workshop attendees. The paper concludes by describing the impact of these barriers on three important constituencies: SLR Methodology Researchers, SLR Authors and SLR consumers.}
}

@article{rayyan-727968418,
  title={A systematic literature review for software portability measurement: Preliminary results},
  year={2020},
  issn={978-1-4503-7665-5},
  pages={152-157},
  author={Ghandorh, Hamza and Noorwali, Abdulfattah and Nassif, Ali Bou and Capretz, Luiz Fernando and Eagleson, Roy},
  url={https://doi.org/10.1145/3384544.3384569},
  publisher={Association for Computing Machinery},
  series={ICSCA 2020},
  keywords={Software measurement, Empirical study, Software quality, Software portability, Software},
  abstract={Software developers agree that software portability is a desirable attribute for their software quality. Software portability is mostly acquired by ad-hoc techniques when trying to port existing products. There is a lack of unified measuring approach of software portability in most computing platforms. This paper presents preliminary results of a systematic literature review, conducted to collect evidence on measuring software portability. The evidence was gathered from selected studies and based on a set of meaningful and focused questions. 49 studies of these were selected for data extraction performed against the research questions. We provide an overview of usedproposed measurement metrics of software portability. Our results suggested that there are scattered efforts to understand measurement of software portability, and no census has been achieved.}
}

@article{rayyan-727968419,
  title={A systematic literature review of UML-Based domain-specific modeling languages for self-adaptive systems},
  year={2018},
  issn={978-1-4503-5715-9},
  pages={87-93},
  author={da Silva, João Pablo S and Ecar, Miguel and Pimenta, Marcelo S and Guedes, Gilleanes T A and Franz, Luiz Paulo and Marchezan, Luciano},
  url={https://doi.org/10.1145/3194133.3194136},
  publisher={Association for Computing Machinery},
  series={SEAMS '18},
  keywords={systematic literature review (SLR), unified modeling language (UML), domain-specific modeling language (DSML), self-adaptive systems (SaS), snowballing technique},
  abstract={Self-adaptive Systems (SaSs) operate under uncertainty conditions and have intrinsic properties that make their modeling a non-trivial activity. This complexity can be minimized by using Domain-Specific Modeling Languages (DSMLs), which may be created by extending Unified Modeling Language (UML). In face of this, we propose investigating how the UML has been customized to create DSMLs that provide proper support for SaSs modeling. To achieve this, we performed a Systematic Literature Review (SRL) by retrieving studies with snowballing technique, selecting studies according to inclusion and exclusion criteria, and extracting and analyzing data to answer our research questions. As the outcome, we retrieved 786 studies and selected 16 primary studies published between 2005 and 2017. The results reveal that the class diagram has been customized through the profile-based mechanism to provide proper support to analysis and design of context-awareness and self-adaptiveness properties.}
}

@article{rayyan-727968420,
  title={A systematic literature review to support the selection of user acceptance testing techniques},
  year={2018},
  issn={978-1-4503-5663-3},
  pages={418-419},
  author={Santos, Ernani César Dos and Vilain, Patrícia and Longo, Douglas Hiura},
  url={https://doi.org/10.1145/3183440.3195036},
  publisher={Association for Computing Machinery},
  series={ICSE '18},
  keywords={techniques, classification, features, user acceptance testing},
  abstract={User Acceptance Testing (UAT) aims to determine whether or not a software satisfies users acceptance criteria. Although some studies have used acceptance tests as software requirements, no previous study has collected information about available UAT techniques and established a comparison of them, to support an organization in the selection of one over another. This work presents a Systematic Literature Review on UAT to find out available techniques and compare their main features. We selected 80 studies and found out 21 UAT techniques. As result, we created a comparative table summarizing these techniques and their features.}
}

@article{rayyan-727968421,
  title={A systematic review on cloud testing},
  year={2019},
  journal={ACM Computing Surveys},
  issn={0360-0300},
  volume={52},
  number={5},
  author={Bertolino, Antonia and Angelis, Guglielmo De and Gallego, Micael and García, Boni and Gortázar, Francisco and Lonetti, Francesca and Marchetti, Eda},
  url={https://doi.org/10.1145/3331447},
  keywords={systematic literature review, Cloud computing, testing},
  abstract={A systematic literature review is presented that surveyed the topic of cloud testing over the period 2012–2017. Cloud testing can refer either to testing cloud-based systems (testing of the cloud) or to leveraging the cloud for testing purposes (testing in the cloud): both approaches (and their combination into testing of the cloud in the cloud) have drawn research interest. An extensive paper search was conducted by both automated query of popular digital libraries and snowballing, which resulted in the final selection of 147 primary studies. Along the survey, a framework has been incrementally derived that classifies cloud testing research among six main areas and their topics. The article includes a detailed analysis of the selected primary studies to identify trends and gaps, as well as an extensive report of the state-of-the-art as it emerges by answering the identified Research Questions. We find that cloud testing is an active research field, although not all topics have received enough attention and conclude by presenting the most relevant open research challenges for each area of the classification framework.}
}

@article{rayyan-727968422,
  title={A model-based approach to systematic review of research literature},
  year={2017},
  issn={978-1-4503-4856-0},
  pages={15-25},
  author={Barat, Souvik and Clark, Tony and Barn, Balbir and Kulkarni, Vinay},
  url={https://doi.org/10.1145/3021460.3021462},
  publisher={Association for Computing Machinery},
  series={ISEC '17},
  keywords={Systematic Literature Review, Systematic Mapping Study, Literature Review, Meta Modeling, Model Based Literature Review},
  abstract={A systematic approach to develop a literature review is attractive because it aims to achieve a repeatable, unbiased and evidence-based outcome. However the existing form of systematic review such as Systematic Literature Review (SLR) and Systematic Mapping Study (SMS) are known to be an effort, time, and intellectual intensive endeavour. To address these issues, this paper proposes a model-based approach to Systematic Review (SR) production. The approach uses a domain-specific language expressed as a meta-model to represent research literature, a meta-model to specify SR constructs in a uniform manner, and an associated development process all of which can benefit from computer-based support. The meta-models and process are validated using real-life case study. We claim that the use of meta-modeling and model synthesis lead to a reduction in time, effort and the current dependence on human expertise.}
}

@article{rayyan-727968423,
  title={A systematic literature review on factors impacting agile adaptation in global software development},
  year={2019},
  issn={978-1-4503-7195-7},
  pages={158-163},
  author={Altaf, Areebah and Fatima, Urooj and Butt, Wasi Haider and Anwar, Muhammad Waseem and Hamdani, Maryum},
  url={https://doi.org/10.1145/3348445.3348463},
  publisher={Association for Computing Machinery},
  series={ICCCM 2019},
  keywords={XP, Agile practices, global software development, scrum, Software},
  abstract={Agile practices are considered as a major attraction for global software development (GSD) projects owing to its flexible nature. Beside the major benefits it offers to GSD, there are few challenges that hinders its implementation across the global software industry. This study contributes in constructing a systematic literature review for exploring the major factors impacting the agile adaptation at global level. We have identified and analyzed 28 research studies (2015-2019). These selected studies have revealed Scrum and Extreme Programming (XP) as the most popular agile practices that are adapted irrespective of the software type and organizational structure. Furthermore 5 tool categories are also presented i.e. modeling, requirement elicitation, data tracking tools etc. that are commonly used while practicing agile. The major findings of this study conclude that these agile methodologies are heavily adapted due to their iterative model and quick code delivery but basic challenges like poor customer involvement and lack of documentation are badly affecting its growth at global level.}
}

@article{rayyan-727968424,
  title={A systematic review of web resource estimation},
  year={2012},
  issn={978-1-4503-1241-7},
  pages={49-58},
  author={Azhar, Damir and Mendes, Emilia and Riddle, Patricia},
  url={https://doi.org/10.1145/2365324.2365332},
  publisher={Association for Computing Machinery},
  series={PROMISE '12},
  keywords={systematic review, web resource estimation, Health Resources},
  abstract={Background: Web development plays an important role in today's industry, so an in depth view into Web resource estimation would be valuable. However a systematic review (SR) on Web resource estimation in its entirety has not been done.Aim: The aim of this paper is to present a SR of Web resource estimation in order to define the current state of the art, and to identify any research gaps that may be present.Method: Research questions that would address the current state of the art in Web resource estimation were first identified. A comprehensive literature search was then executed resulting in the retrieval of 84 empirical studies that investigated any aspect of Web resource estimation. Data extraction and synthesis was performed on these studies with these research questions in mind.Results: We have found that there are no guidelines with regards to what resource estimation technique should be used in a particular estimation scenario, how it should be implemented, and how its effectiveness should be evaluated. Accuracy results vary widely and are dependent on numerous factors. Research has focused on development effort/cost estimation, neglecting other facets of resource estimation like quality and maintenance. Size measures have been used in all but one study as a resource predictor.Conclusions: Our results suggest that there is plenty of work to be done in the field of Web resource estimation whether it be investigating a more comprehensive approach that considers more than a single resource facet, evaluating other possible resource predictors, or trying to determine guidelines that would help simplify the process of selecting a resource estimation technique.}
}

@article{rayyan-727968425,
  title={A systematic review of system-of-systems architecture research},
  year={2013},
  issn={978-1-4503-2126-6},
  pages={13-22},
  author={Klein, John and van Vliet, Hans},
  url={https://doi.org/10.1145/2465478.2465490},
  publisher={Association for Computing Machinery},
  series={QoSA '13},
  keywords={systematic review, architecture, system of systems},
  abstract={Context: A system of systems is an assemblage of components which individually may be regarded as systems, and which possesses the additional properties that the constituent systems are operationally independent, and are managerially independent. Much has been published about the field of systems of systems by researchers and practitioners, often with the assertion that the system-of-systems design context necessitates the use of architecture approaches that are somewhat different from system-level architecture. However, no systematic review has been conducted to provide an extensive overview of system of systems architecture research.Objective: This paper presents such a systematic review. The objective of this review is to classify and provide a thematic analysis of the reported results in system of systems architecture.Method: The primary studies for the systematic review were identified using a predefined search strategy followed by an extensive manual selection process.Results: We found the primary studies published in a large number of venues, mostly domain-oriented, with no obvious center of a research community of practice. The field seems to be maturing more slowly than other software technologies: Most reported results described individuals or teams working in apparent isolation to develop solutions to particular system-of-systems architecture problems, with no techniques gaining widespread adoption.Conclusions: A comprehensive research agenda for this field should be developed, and further studies should be performed to determine whether the information system-related problems of system of systems architecture are covered by existing software architecture knowledge, and if not, to develop general methods for system-of-systems architecture.}
}

@article{rayyan-727968426,
  title={Criteria for software process tailoring: A systematic review},
  year={2013},
  issn={978-1-4503-2062-7},
  pages={171-180},
  author={Kalus, Georg and Kuhrmann, Marco},
  url={https://doi.org/10.1145/2486046.2486078},
  publisher={Association for Computing Machinery},
  series={ICSSP 2013},
  keywords={Systematic Literature Review, Software Process, Tailoring, Software},
  abstract={Independently from which software process was selected for a company or a project, the selected software process usually cannot be applied without any customization. Although the need to tailor a software process to specific project requirements seems to be widely accepted and unquestioned, the way of doing the tailoring remains unclear and is, therefore, often left to the expertise of process engineers or project managers. What are the criteria to be applied in the tailoring? What are dependencies between different criteria and how should certain criteria influence the software process? In this paper we investigate concrete tailoring criteria for the tailoring of software processes. To this end, we present a collection of 49 tailoring criteria as the outcomes of a systematic literature review. We further analyze the impact of the discovered tailoring criteria by relating them to a set of 20 exemplary tailoring actions, which affect the project-specific software process. Our outcomes show that the factors influencing the tailoring are well understood, however, the consequences of the criteria remain abstract and need to be interpreted on a project-per-project basis.}
}

@article{rayyan-727968427,
  title={'Follow the moon' development: Writing a systematic literature review on global software engineering education},
  year={2015},
  issn={978-1-4503-4020-5},
  pages={1-4},
  author={Clear, Tony},
  url={https://doi.org/10.1145/2828959.2835019},
  publisher={Association for Computing Machinery},
  series={Koli calling '15},
  keywords={systematic literature review, evidence-based software engineering, global software development, research methodology, capstone, global software engineering education, international collaboration, open ended group project, teaching and learning, Software},
  abstract={This presentation reflects on method and practice in Computer Science Education Research, through introducing the process of conducting a Systematic Literature Review. While Systematic Literature Reviews are an established research method within the Software Engineering discipline, they are a relatively unfamiliar research approach within Computer Science Education. Yet research disciplines can be strengthened by borrowing and adapting methods from other fields. I reflect on the rationale and underlying philosophy behind Systematic Reviews, and the implications for conducting a rigorous study and the quality of the resulting outputs. This chronicle of the journey of an ITiCSE working group, outlines the process we adopted and reflects on the methodological and logistical challenges we had to overcome in producing a review titled Challenges and Recommendations for the Design and Conduct of Global Software Engineering Courses. I conclude by discussing how systematic literature reviews can be adapted to an undergraduate teaching setting.}
}

@article{rayyan-727968428,
  title={Protocol for a systematic literature review of research on the wikipedia},
  year={2009},
  issn={978-1-60558-829-2},
  author={Okoli, Chitu and Schabram, Kira},
  url={https://doi.org/10.1145/1643823.1643912},
  publisher={Association for Computing Machinery},
  series={MEDES '09},
  keywords={literature review, open source, open content, Wikipedia},
  abstract={Context: Wikipedia has become one of the ten-most visited sites on the Web, and the world's leading source of Web reference information. Its rapid success has attracted over 1,000 scholarly studies that treat Wikipedia as a major topic or data source. Objectives: This article presents a protocol for conducting a systematic mapping (a broad-based literature review) of research on Wikipedia. It identifies what research has been conducted; what research questions have been asked, which have been answered; and what theories and methodologies have been employed to study Wikipedia. Methods: This protocol follows the rigorous methodology of evidence-based software engineering to conduct a systematic mapping study. Results and conclusions: This protocol reports a study in progress.}
}

@article{rayyan-727968429,
  title={Challenges and recommendations for the design and conduct of global software engineering courses: A systematic review},
  year={2015},
  issn={978-1-4503-4146-2},
  pages={1-39},
  author={Clear, Tony and Beecham, Sarah and Barr, John and Daniels, Mats and McDermott, Roger and Oudshoorn, Michael and Savickaite, Airina and Noll, John},
  url={https://doi.org/10.1145/2858796.2858797},
  publisher={Association for Computing Machinery},
  series={ITICSE-WGR '15},
  keywords={systematic literature review, global software development, global software engineering, capstone, international collaboration, open ended group project, teaching and learning, Software},
  abstract={Context: Global Software Engineering (GSE) has become the predominant form of software development for global companies and has given rise to a demand for students trained in GSE. In response, universities are developing courses and curricula around GSE and researchers have begun to disseminate studies of these new approaches. Problem: GSE differs from most other computer science fields, however, in that practice is inseparable from theory. As a result, educators looking to create GSE courses face a daunting task: integrating global practice into the local classroom. Aim: This study aims to ameliorate the very difficult task of teaching GSE by delineating the challenges and providing some recommendations for overcoming them. Method: To meet our aims we pose two research questions ("When teaching GSE to students in Higher Education, what are the (a) challenges, and (b) recommendations for addressing them") and then conduct a systematic literature review (SLR) to determine the answers to these questions. Our SLR follows a carefully designed and validated protocol.Results: We found 82 papers that addressed our research questions. Our findings indicate that in addition to the challenges posed by GSE in general, particular problems arise in educational situations. The majority of these challenges fall into the "global distance" category, though teamwork challenges and people issues (such as trust) also commonly arise. Organizational differences between institutions, differing skill sets between students in different locations, and varying cultural work norms, for example, all operate within educational settings in quite different ways than in professional development teams. Integrating cultural training, conducting teamwork exercises to build trust, and instructor monitoring of team communication are all examples of techniques that have been used successfully by educators according to our review Conclusion: Despite the severity of the challenges in GSE education, many institutions have successfully developed courses and curricula targeting GSE. Indeed, for each of the challenges we have identified in the literature there are numerous recommendations for overcoming them. Instructors can use the recommendations given in this study as a starting point to running successful GSE courses.}
}

@article{rayyan-727968430,
  title={Software visualization today: Systematic literature review},
  year={2016},
  issn={978-1-4503-4367-1},
  pages={262-271},
  author={Mattila, Anna-Liisa and Ihantola, Petri and Kilamo, Terhi and Luoto, Antti and Nurminen, Mikko and Väätäjä, Heli},
  url={https://doi.org/10.1145/2994310.2994327},
  publisher={Association for Computing Machinery},
  series={AcademicMindtrek '16},
  keywords={systematic literature review, human-centered computing, software visualization, Software},
  abstract={Software visualization means visualizing various aspects and artifacts related to software. By this definition a wide range of different software engineering aspects from program comprehension to understanding software process and usage are covered. This paper presents the results of systematic literature review spanning six years of software visualization literature. The main result shows that the most studied topics in the past six years are related to software structure, behavior and evolution. Software process and usage are addressed only in few studies. In the future studying the adoption of software visualization tools in industry context would be beneficial.}
}

@article{rayyan-727968431,
  title={Systematic review of software behavioral model consistency checking},
  year={2017},
  journal={ACM Computing Surveys},
  issn={0360-0300},
  volume={50},
  number={2},
  author={ul Muram, Faiz and Tran, Huy and Zdun, Uwe},
  url={https://doi.org/10.1145/3037755},
  keywords={systematic literature review, consistency checking, consistency types, Software behavioral model, Software},
  abstract={In software development, models are often used to represent multiple views of the same system. Such models need to be properly related to each other in order to provide a consistent description of the developed system. Models may contain contradictory system specifications, for instance, when they evolve independently. Therefore, it is very crucial to ensure that models conform to each other. In this context, we focus on consistency checking of behavior models. Several techniques and approaches have been proposed in the existing literature to support behavioral model consistency checking. This article presents a Systematic Literature Review (SLR) that was carried out to obtain an overview of the various consistency concepts, problems, and solutions proposed regarding behavior models. In our study, the identification and selection of the primary studies was based on a well-planned search strategy. The search process identified a total of 1770 studies, out of which 96 have been thoroughly analyzed according to our predefined SLR protocol. The SLR aims to highlight the state-of-the-art of software behavior model consistency checking and identify potential gaps for future research. Based on research topics in selected studies, we have identified seven main categories: targeted software models, types of consistency checking, consistency checking techniques, inconsistency handling, type of study and evaluation, automation support, and practical impact. The findings of the systematic review also reveal suggestions for future research, such as improving the quality of study design and conducting evaluations, and application of research outcomes in industrial settings. For this purpose, appropriate strategy for inconsistency handling, better tool support for consistency checking and/or development tool integration should be considered in future studies.}
}

@article{rayyan-727968432,
  title={How are hybrid development approaches organized? A systematic literature review},
  year={2020},
  issn={978-1-4503-7512-2},
  pages={145-154},
  author={Prenner, Nils and Unger-Windeler, Carolin and Schneider, Kurt},
  url={https://doi.org/10.1145/3379177.3388907},
  publisher={Association for Computing Machinery},
  series={ICSSP '20},
  keywords={systematic literature review, agile software development, software process, Hybrid software development},
  abstract={Agile software development methods promise shorter time-to-market and higher product quality, but lack the ability of long-term planning or coping with large projects. However, software companies often also want the ability of long-term planning, promised by traditional or plan-based methods. To benefit from the strengths of both approaches, software companies often use a combination of agile and plan-based methods, known as hybrid development approaches. These approaches strongly depend on the individual context and are customized. Therefore, companies have to organize their hybrid development approach individually. However, practitioners often have difficulties with the organization of hybrid approaches. The organization considers how the phases, activities, roles, and artifacts are arranged and connected. Research lacks the necessary detailed insight into how hybrid development approaches are organized to support practitioners. To gain better understanding of the organization of hybrid approaches, we conducted a systematic literature review to gather descriptions of hybrid approaches. We analyzed the found papers thoroughly and could identify three general patterns of how hybrid approaches are organized. We found that all these patterns are still based on Royce's waterfall model and use the standard software engineering activities. Our findings shall help to lead further research and help practitioners to better organize their individual development approach.}
}

@article{rayyan-727968433,
  title={Quality assessment of systematic reviews in software engineering: A tertiary study},
  year={2015},
  issn={978-1-4503-3350-4},
  author={Zhou, You and Zhang, He and Huang, Xin and Yang, Song and Babar, Muhammad Ali and Tang, Hao},
  url={https://doi.org/10.1145/2745802.2745815},
  publisher={Association for Computing Machinery},
  series={EASE '15},
  keywords={quality assessment, software engineering, systematic (literature) review, Software},
  abstract={Context: The quality of an Systematic Literature Review (SLR) is as good as the quality of the reviewed papers. Hence, it is vital to rigorously assess the papers included in an SLR. There has been no tertiary study aimed at reporting the state of the practice of quality assessment used in SLRs in Software Engineering (SE).Objective: We aimed to study the practices of quality assessment of the papers included in SLRs in SE.Method: We conducted a tertiary study of the SLRs that have performed quality assessment of the reviewed papers.Results: We identified and analyzed different aspects of the quality assessment of the papers included in 127 SLRs.Conclusion: Researchers use a variety of strategies for quality assessment of the papers reviewed, but report little about the justification for the used criteria. The focus is creditability but not relevance aspect of the papers. Appropriate guidelines are required for devising quality assessment strategies.}
}

@article{rayyan-727968434,
  title={Software process simulation modeling: Preliminary results from an updated systematic review},
  year={2014},
  issn={978-1-4503-2754-1},
  pages={50-54},
  author={Gao, Chao and Jiang, Shu and Rong, Guoping},
  url={https://doi.org/10.1145/2600821.2600844},
  publisher={Association for Computing Machinery},
  series={ICSSP 2014},
  keywords={systematic literature review, Software process simulation modeling, QGS, Software},
  abstract={Software Process Simulation Modeling (SPSM) has raised research interest since 1980s. However, it is observed that SPSM studies published in the ICSSP community may have dropped in recent years. The objective of this research is to update the recent status of this area. We conducted a Systematic Literature Review (SLR) using the QGS-based search strategy. The review identified 74 primary studies in the past five years (2008-2012). This paper presents the preliminary results from this updated SLR by answering the first four research questions. Based on the findings from this updated review, it can be concluded that in terms of the number of SPSM studies found in the overall software engineering community, there is no significant change (drop) compared to the former review stage (1998-2007).}
}

@article{rayyan-727968435,
  title={Towards process improvement in DevOps: A systematic literature review},
  year={2020},
  issn={978-1-4503-7731-7},
  pages={427-433},
  author={Badshah, Sher and Khan, Arif Ali and Khan, Bilal},
  url={https://doi.org/10.1145/3383219.3383280},
  publisher={Association for Computing Machinery},
  series={EASE '20},
  keywords={systematic review, DevOps, process improvement, Continuous software engineering, maturity models},
  abstract={In recent years, the software release cost has been reduced dramatically due to the alteration from traditional shrink-wrapped software to software as a service. Organizations that can deliver their services continuously and with a high frequency have a higher ability to compete in the market. As a response to this, a substantial number of software companies acquired DevOps to establish a culture of effective communication and collaboration between development and operation teams and in order to enhance the production release frequency as well as to maintain the product quality. However, the DevOps environment requires a platform that aid in evaluating the performance of existing processes and provide improvement recommendations. On top of that, organizations can only achieve the perceived benefits of DevOps if their processes are mature and continuously measured. The objective of this research is to investigate the process improvement contributions made by researchers in the DevOps field. For this purpose, we performed a systematic literature review that resulted in several maturity models and best practices. Our ultimate aim is to develop a DevOps maturity model that can appraise and improve the processes in the DevOps environment.}
}

@article{rayyan-727968436,
  title={Effort estimation in agile software development: A systematic literature review},
  year={2014},
  issn={978-1-4503-2898-2},
  pages={82-91},
  author={Usman, Muhammad and Mendes, Emilia and Weidt, Francila and Britto, Ricardo},
  url={https://doi.org/10.1145/2639490.2639503},
  publisher={Association for Computing Machinery},
  series={PROMISE '14},
  keywords={systematic literature review, agile software development, effort estimation, Software},
  abstract={Context: Ever since the emergence of agile methodologies in 2001, many software companies have shifted to Agile Software Development (ASD), and since then many studies have been conducted to investigate effort estimation within such context; however to date there is no single study that presents a detailed overview of the state of the art in effort estimation for ASD. Objectives: The aim of this study is to provide a detailed overview of the state of the art in the area of effort estimation in ASD. Method: To report the state of the art, we conducted a systematic literature review in accordance with the guidelines proposed in the evidence-based software engineering literature. Results: A total of 25 primary studies were selected; the main findings are: i) Subjective estimation techniques (e.g. expert judgment, planning poker, use case points estimation method) are the most frequently applied in an agile context; ii) Use case points and story points are the most frequently used size metrics respectively; iii) MMRE (Mean Magnitude of Relative Error) and MRE (Magnitude of Relative Error) are the most frequently used accuracy metrics; iv) team skills, prior experience and task size are cited as the three important cost drivers for effort estimation in ASD; and v) Extreme Programming (XP) and SCRUM are the only two agile methods that are identified in the primary studies. Conclusion: Subjective estimation techniques, e.g. expert judgment-based techniques, planning poker or the use case points method, are the one used the most in agile effort estimation studies. As for the size metrics, the ones that were used the most in the primary studies were story points and use case points. Several research gaps were identified, relating to the agile methods, size metrics and cost drivers, thus suggesting numerous possible avenues for future work.}
}

@article{rayyan-727968437,
  title={Omission of quality software development practices: A systematic literature review},
  year={2018},
  journal={ACM Computing Surveys},
  issn={0360-0300},
  volume={51},
  number={2},
  author={Ghanbari, Hadi and Vartiainen, Tero and Siponen, Mikko},
  url={https://doi.org/10.1145/3177746},
  keywords={systematic literature review, Behavioral software engineering, technical debt, Software},
  abstract={Software deficiencies are minimized by utilizing recommended software development and quality assurance practices. However, these recommended practices (i.e., quality practices) become ineffective if software professionals purposefully ignore them. Conducting a systematic literature review (n = 4,838), we discovered that only a small number of previous studies, within software engineering and information systems literature, have investigated the omission of quality practices. These studies explain the omission of quality practices mainly as a result of organizational decisions and trade-offs made under resource constraints or market pressure. However, our study indicates that different aspects of this phenomenon deserve further research. In particular, future research must investigate the conditions triggering the omission of quality practices and the processes through which this phenomenon occurs. Especially, since software development is a human-centric phenomenon, the psychological and behavioral aspects of this process deserve in-depth empirical investigation. In addition, futures research must clarify the social, organizational, and economical consequences of ignoring quality practices. Gaining in-depth theoretically sound and empirically grounded understandings about different aspects of this phenomenon enables research and practice to suggest interventions to overcome this issue.}
}

@article{rayyan-727968438,
  title={Continuous development and testing of access and usage control: A systematic literature review},
  year={2020},
  issn={978-1-4503-7762-1},
  pages={51-59},
  author={Daoudagh, Said and Lonetti, Francesca and Marchetti, Eda},
  url={https://doi.org/10.1145/3393822.3432330},
  publisher={Association for Computing Machinery},
  series={ESSE 2020},
  keywords={Systematic Literature Review, Testing, DevOps, Access Control, XACML},
  abstract={Context: Development and testing of access/usage control systems is a growing research area. With new trends in software development such as DevOps, the development of access/usage control also has to evolve. Objective: The main aim of this paper is to provide an overview of research proposals in the area of continuous development and testing of access and usage control systems. Method: The paper uses a Systematic Literature Review as a research method to define the research questions and answer them following a systematic approach. With the specified search string, 210 studies were retrieved. After applying the inclusion and exclusion criteria in two phases, a final set of 20 primary studies was selected for this review. Results: Results show that primary studies are mostly published in security venues followed by software engineering venues. Furthermore, most of the studies are based on the standard XACML access control language. In addition, a significant portion of the proposals for development and testing is automated with test assessment and generation the most targeted areas. Some general guidelines for leveraging continuous developing and testing of the usage and access control systems inside the DevOps process are also provided.}
}

@article{rayyan-727968439,
  title={Strategies for use case modeling: A systematic literature review},
  year={2019},
  issn={978-1-4503-7651-8},
  pages={254-263},
  author={Bispo, Cristiana and Fernandes, Sergio and Magalhães, Ana Patrícia},
  url={https://doi.org/10.1145/3350768.3351795},
  publisher={Association for Computing Machinery},
  series={SBES 2019},
  keywords={Systematic Review, Requirement, Software Modeling, Strategy for Use Case Modeling, Use Case},
  abstract={A major challenge in teaching use-case modeling (UCM) is to mitigate the difficulties of students that prevent them from producing use-case models with quality. The strategies for UCM are scattered in the literature in several areas, and may not be known to the students, who therefore fail to receive the benefits that would mitigate their difficulties. This paper aims to present a systematic literature review (SLR) to identify, gather and analyze strategies for UCM. During the SLR, two thousand two hundred sixty-six studies published between 2008 and 2018 were returned from 6 bases (ACM, IEEE, Scopus, Science Direct, SpringerLink and Engineering Village), which resulted in the selection of 39 primary studies. These were classified, following the coding procedures of Grounded Theory, into 13 categories of different strategies for UCM. The results can help teachers in the adoption of the most appropriate UCM strategies for their students. Besides, they provide a quick reference for teachers and researchers interested in conducting additional studies on teaching strategies for UCM.}
}

@article{rayyan-727968440,
  title={SESRA: A web-based automated tool to support the systematic literature review process},
  year={2015},
  issn={978-1-4503-3350-4},
  author={Molléri, Jefferson Seide and Benitti, Fabiane Barreto Vavassori},
  url={https://doi.org/10.1145/2745802.2745825},
  publisher={Association for Computing Machinery},
  series={EASE '15},
  keywords={systematic literature review, automated tool, SLR},
  abstract={Systematic Literature Review (SLR) is a key tool for evidence-based practice as it combines results from multiple studies of a specific topic of research. Due its characteristics, it is a time consuming, hard process that requires a properly documented protocol for scientific acknowledgment. In this context, this paper presents the SESRA – a web-based automated tool to support all phases of the SLR process, contributing to its productivity and reliability. We also present a use case on the software engineering field, applying specific knowledge to set the process in the discipline. Further, we discuss how to use the tool to establish a more formal and controlled process and to reduce effort on its repetitive activities. The outcomes and feedback obtained in early use have shown that SESRA could support the SLR process, automating some of its key activities.}
}

@article{rayyan-727968441,
  title={Agile project management challenges and mapping solutions: A systematic literature review},
  year={2020},
  issn={978-1-4503-7690-7},
  pages={123-129},
  author={Raharjo, Teguh and Purwandari, Betty},
  url={https://doi.org/10.1145/3378936.3378949},
  publisher={Association for Computing Machinery},
  series={ICSIM '20},
  keywords={Systematic Literature Review, Agile, Agile Approach, Agile Project Management},
  abstract={The Project Management Institute reported that the Agile approach is widely being used for project management practices. This approach has a significant impact on business growth and project performance. However, its implementation is challenging. Therefore, a systematic literature review (SLR) is used to reveal the challenges faced in Agile project execution. The Project Management Body of Knowledge (PMBOK) knowledge areas were adopted to classify the challenges. A total of 23 papers from 400 were identified as the result of SLR extraction. The challenges from related studies were categorized into the PMBOK knowledge areas. A mapping from the challenges to the solutions was performed using the PMBOK Guide, Prince2 Agile, Agile Practice Guide, and other related references. This study provides a list of Agile challenges and their mapped solutions. The biggest challenge arises from stakeholder management, which includes challenges related to Agile adaption, Agile transition, and Agile transformation. Other challenges include project resource management, project integration management, project scope management, and project schedule management. For academicians, this study provides a new understanding of Agile challenges and their suitable solutions from the perspective of project management. For practitioners, the findings provide potential lessons learned and recommendations to deal with the challenges.}
}

@article{rayyan-727968442,
  title={Ethnographic research in software engineering: A critical review and checklist},
  year={2019},
  issn={978-1-4503-5572-8},
  pages={659-670},
  author={Zhang, He and Huang, Xin and Zhou, Xin and Huang, Huang and Babar, Muhammad Ali},
  url={https://doi.org/10.1145/3338906.3338976},
  publisher={Association for Computing Machinery},
  series={ESEC/FSE 2019},
  keywords={empirical software engineering, systematic (literature) review, Ethnography, qualitative research, Software},
  abstract={Software Engineering (SE) community has recently been investing significant amount of effort in qualitative research to study the human and social aspects of SE processes, practices, and technologies. Ethnography is one of the major qualitative research methods, which is based on constructivist paradigm that is different from the hypothetic-deductive research model usually used in SE. Hence, the adoption of ethnographic research method in SE can present significant challenges in terms of sufficient understanding of the methodological requirements and the logistics of its applications. It is important to systematically identify and understand various aspects of adopting ethnography in SE and provide effective guidance. We carried out an empirical inquiry by integrating a systematic literature review and a confirmatory survey. By reviewing the ethnographic studies reported in 111 identified papers and 26 doctoral theses and analyzing the authors' responses of 29 of those papers, we revealed several unique insights. These identified insights were then transformed into a preliminary checklist that helps improve the state-of-the-practice of using ethnography in SE. This study also identifies the areas where methodological improvements of ethnography are needed in SE.}
}

@article{rayyan-727968443,
  title={Usability of requirements techniques: A systematic literature review},
  year={2016},
  issn={978-1-4503-3739-7},
  pages={1270-1275},
  author={Bombonatti, Denise and Gralha, Catarina and Moreira, Ana and Araújo, João and Goulão, Miguel},
  url={https://doi.org/10.1145/2851613.2851758},
  publisher={Association for Computing Machinery},
  series={SAC '16},
  keywords={systematic literature review, usability, requirements engineering approaches},
  abstract={The usability of requirements engineering (RE) techniques has been recognised as a key factor for their successful adoption by industry. RE techniques must be accessible to stakeholders with different backgrounds, so they can be empowered to effectively and efficiently contribute to building successful systems. When selecting an appropriate requirements engineering technique for a given context, one should consider the usability supported by each of the candidate techniques. The first step towards achieving this goal is to gather the best evidence available on the usability of RE approaches by performing a systematic literature review, to answer one research question: How is the usability of requirements engineering techniques and tools addressed? We systematically review articles published in the Requirements Engineering Journal, one of the main sources for mature work in RE, to motivate a research roadmap to make RE approaches more accessible to stakeholders with different backgrounds.}
}

@article{rayyan-727968444,
  title={E-government usability evaluation: Insights from a systematic literature review},
  year={2019},
  issn={978-1-4503-6642-7},
  pages={249-253},
  author={Lyzara, Ria and Purwandari, Betty and Zulfikar, Muhammad Fadhil and Santoso, Harry Budi and Solichah, Iis},
  url={https://doi.org/10.1145/3305160.3305178},
  publisher={Association for Computing Machinery},
  series={ICSIM 2019},
  keywords={Systematic Literature Review (SLR), ⚠️ Invalid DOI, Usability Evaluation, E-government},
  abstract={E-Government aims to deliver benefits to government and citizens by improving transparency, efficiency, trust, and citizen participation. However, e-government initiatives face several barriers. One of them is poor usability. To advance quality of usability, literatures indicate that usability evaluation is a key success factor. There are many approaches to conduct usability evaluation. Each of it has advantages and challenges. On the other hand, there are several aspects that must be considered related to usability evaluation in e-government context. It includes large stakeholders and their diversity, extra needs for ethical practices, as well as high privacy. Therefore, it is crucial to investigate the right usability evaluation method in e-government. In order to address this issue, a study using Systematic Literature Review (SLR) was conducted to identify the suitable usability evaluation methods. There are 519 literatures that have been selected in the initial stage. It was then followed by an extraction process, which produced 22 selected references. Each method was grouped into usability testing, inspection, and inquiry. These results can guide academics and practitioners to carry out usability evaluation in e-government.}
}

@article{rayyan-727968445,
  title={A systematic review on the use of definition of done on agile software development projects},
  year={2017},
  issn={978-1-4503-4804-1},
  pages={364-373},
  author={Silva, Ana and Araújo, Thalles and Nunes, João and Perkusich, Mirko and Dilorenzo, Ednaldo and Almeida, Hyggo and Perkusich, Angelo},
  url={https://doi.org/10.1145/3084226.3084262},
  publisher={Association for Computing Machinery},
  series={EASE'17},
  keywords={Systematic Literature Review, Agile Software Development, Definition of Done, Software},
  abstract={Background: Definition of Done (DoD) is a Scrum practice that consists of a simple list of criteria that adds verifiable or demonstrable value to the product. It is one of the most popular agile practices and assures a balance between short-term delivery of features and long-term product quality, but little is known of its actual use in Agile teams.Objective: To identify possible gaps in the literature and define a starting point to define DoD for practitioners through the identification and synthesis of the DoD criteria used in agile projects as presented in the scientific literature.Method: We applied a Systematic Literature Review of studies published up to (and including) 2016 through database search and backward and forward snowballing.Results: In total, we evaluated 2326 papers, of which 8 included DoD criteria used in agile projects. We identified that some studies presented up to 4 levels of DoD, which include story, sprint, release or project. We identified 62 done criteria, which are related to software verification and validation, deploy, code inspection, test process quality, regulatory compliance, software architecture design, process management, configuration management and non-functional requirements.Conclusion: The main implication for research is a need for more and better empirical studies documenting and evaluating the use of the DoD in agile software development. For the industry, the review provides a map of how DoD is currently being used in the industry and can be used as a starting point to define or compare with their own DoD definition.}
}

@article{rayyan-727968446,
  title={On the application of genetic algorithms for test case prioritization: A systematic literature review},
  year={2012},
  issn={978-1-4503-1509-8},
  pages={9-14},
  author={Catal, Cagatay},
  url={https://doi.org/10.1145/2372233.2372238},
  publisher={Association for Computing Machinery},
  series={EAST '12},
  keywords={systematic literature review, software engineering, software testing, regression testing, genetic algorithms, test case prioritization, evolutionary computation, Algorithms},
  abstract={We conducted a Systematic Literature Review (SLR) to investigate the effectiveness of genetic algorithms for test case prioritization. The search string retrieved 120 test case prioritization papers, but after we read them in full, we identified that genetic algorithm was used in seven primary studies. One paper does not provide any experimental data. Based on the results of these six papers, we conclude that there is evidence that genetic algorithm-based techniques are effective for test case prioritization and the field is still open for further research.}
}

@article{rayyan-727968447,
  title={A review of meta-ethnographies in software engineering},
  year={2019},
  issn={978-1-4503-7145-2},
  pages={68-77},
  author={Fu, Changlan and Zhang, He and Huang, Xin and Zhou, Xin and Li, Zhi},
  url={https://doi.org/10.1145/3319008.3319015},
  publisher={Association for Computing Machinery},
  series={EASE '19},
  keywords={systematic (literature) review, meta-ethnography, qualitative research synthesis, Software},
  abstract={Context: Data synthesis is one of the most significant tasks in Systematic Literature Review (SLR). Software Engineering (SE) researchers have adopted a variety of methods of synthesizing data that originated in other disciplines. One of the qualitative data synthesis methods is meta-ethnography, which is being used in SE SLRs. Objective: We aim at studying the adoption of meta-ethnography in SE SLRs in order to understand how this method has been used in SE. Method: We conducted a tertiary study of the use of meta-ethnography by reviewing sixteen SLRs. We carried out an empirical inquiry by integrating SLR and confirmatory email survey. Results: There is a general lack of knowledge, or even awareness, of different aspects of meta-ethnography and/or how to apply it. Conclusion: There is a need of investment in gaining in-depth knowledge and skills of correctly applying meta-ethnography in order to increase the quality and reliability of the findings generated from SE SLRs. Our study reveals that meta-ethnography is a suitable method to SE research. We discuss challenges and propose recommendations of adopting meta-ethnography in SE. Our effort also offers a preliminary checklist of the systematic considerations for doing meta-ethnography in SE and improving the quality of meta-ethnographic research in SE.}
}

@article{rayyan-727968448,
  title={A systematic review of business and information technology alignment},
  year={2013},
  journal={ACM Transactions on Management Information Systems},
  issn={2158-656X},
  volume={4},
  number={1},
  author={Ullah, Azmat and Lai, Richard},
  url={https://doi.org/10.1145/2445560.2445564},
  keywords={Systematic review, alignment measurement, alignment phases, business, business environment modeling, business issues, IT alignment, IT issues, IT support, literature},
  abstract={Business organizations have become heavily dependent on information technology (IT) services. The process of alignment is defined as the mutual synchronization of business goals and IT services. However, achieving mature alignment between business and IT is difficult due to the rapid changes in the business and IT environments. This article provides a systematic review of studies on the alignment of business and IT. The research articles reviewed are based on topics of alignment, the definition of alignment, history, alignment challenges, phases of alignment, alignment measurement approaches, the importance of alignment in business industries, how software engineering helps in better alignment, and the role of the business environment in aligning business with IT. It aims to present a thorough understanding of business-IT alignment and to provide a list of future research directions regarding alignment. To perform the systematic review, we used the guidelines developed by Kitchenham for reviewing the available research papers relevant to our topic.}
}

@article{rayyan-727968449,
  title={A systematic review for smart city data analytics},
  year={2018},
  journal={ACM Computing Surveys},
  issn={0360-0300},
  volume={51},
  number={5},
  author={Moustaka, Vaia and Vakali, Athena and Anthopoulos, Leonidas G},
  url={https://doi.org/10.1145/3239566},
  keywords={systematic review, Data mining, Internet of Things, taxonomy, crowd-sensing, crowd-sourcing, data harvesting, open data, smart cities, smart dimensions, smart services},
  abstract={Smart cities (SCs) are becoming highly sophisticated ecosystems at which innovative solutions and smart services are being deployed. These ecosystems consider SCs as data production and sharing engines, setting new challenges for building effective SC architectures and novel services. The aim of this article is to “connect the pieces” among Data Science and SC domains, with a systematic literature review which identifies the core topics, services, and methods applied in SC data monitoring. The survey focuses on data harvesting and data mining processes over repeated SC data cycles. A survey protocol is followed to reach both quantitative and semantically important entities. The review results generate useful taxonomies for data scientists in the SC context, which offers clear guidelines for corresponding future works. In particular, a taxonomy is proposed for each of the main SC data entities, namely, the “D Taxonomy” for the data production, the “M Taxonomy” for data analytics methods, and the “S Taxonomy” for smart services. Each of these taxonomies clearly places entities in a classification which is beneficial for multiple stakeholders and for multiple domains in urban smartness targeting. Such indicative scenarios are outlined and conclusions are quite promising for systemizing.}
}

@article{rayyan-727968450,
  title={A systematic literature review on combining ontology with bayesian network to support logical and probabilistic reasoning},
  year={2017},
  issn={978-1-4503-5488-2},
  pages={1-12},
  author={Setiawan, Foni Agus and Budiardjo, Eko K and Basaruddin, T and Aminah, Siti},
  url={https://doi.org/10.1145/3178212.3178223},
  publisher={Association for Computing Machinery},
  series={ICSEB 2017},
  keywords={Ontology, Bayesian network, Logical reasoning, Ontology-based application, Probabilistic reasoning},
  abstract={Reasoning in ontology is currently limited to logical reasoning. It is because ontology does not have a standard for probabilistic reasoning. Various approaches have been made by researchers to add the ability for ontological reasoner to do probabilistic reasoning. The approach is done by combining ontology with Bayesian network that does have probabilistic reasoning abilities. This study mapped out various approaches performed in combining ontologies with Bayesian networks to realize logical and probabilistic reasoning simultaneously. We use a systematic literature review method to identify the primary studies on combining ontology with Bayesian network following a predefined review protocol. We searched from four indexing services (SCOPUS, IEEE Xplore, ACM Digital Library, and SpringerLink) and got the result of 74 papers accepted for the review. We extracted properties from these studies and found 8 motivations of the studies, 5 contexts, 4 factors involved, and 5 techniques used in combining ontology with Bayesian network. The aim and the context are clearly stated in most of the studies, while most of the authors did not completely discuss the threats in their papers. As for the method and findings, most of the studies describe and discuss it moderately.}
}

@article{rayyan-727968451,
  title={Towards a behavioral software engineering},
  year={2014},
  issn={978-1-4503-2860-9},
  pages={48-55},
  author={Lenberg, Per and Feldt, Robert and Wallgren, Lars-Göran},
  url={https://doi.org/10.1145/2593702.2593711},
  publisher={Association for Computing Machinery},
  series={CHASE 2014},
  keywords={Psychology, Behavioral Software Engineering, Software},
  abstract={Throughout the history of Software Engineering (SE) it has been repeatedly found that the humans involved, i.e. the engineers and developers in addition to other stakeholders, are a key factor in determining project outcomes and success. However, the amount of research that focuses on human aspects has been limited compared to research with technology or process focus. With increasing maturity of the field, interest in agile methods and a growing dissatisfaction with the continued challenges of developing high-quality software on time, the amount of SE research putting human aspect in primary focus has increased. In this paper we argue that a synthesized view of the emerging human-focused SE research is needed and can add value through giving focus, direction and help identify gaps. Taking cues from the addition of Behavioral Economics as an important part of the area of Economics we propose the term Behavioral Software Engineering (BSE) as an umbrella concept for research that focus on behavioral and social aspects in the work activities of software engineers. We propose that a model based on three units of analysis can give structure and point to concepts that are important for BSE. To add detail to this model we are conducting a systematic review to map out what is currently known. To exemplify the model and the area we here present the results from a subset of the identified concepts.}
}

@article{rayyan-727968452,
  title={Investigating the applicability of the evidence-based paradigm to software engineering},
  year={2006},
  issn={1-59593-409-X},
  pages={7-14},
  author={Budgen, David and Charters, Stuart and Turner, Mark and Brereton, Pearl and Kitchenham, Barbara and Linkman, Stephen},
  url={https://doi.org/10.1145/1137661.1137665},
  publisher={Association for Computing Machinery},
  series={WISER '06},
  keywords={systematic literature review, evidence-based, structured abstracts, Software},
  abstract={Context: The success of the evidence-based paradigm in other domains, especially medicine, has raised the question of how this might be employed in software engineering.Objectives: To report the research we are doing to evaluate problems associated with adopting the evidence-based paradigm in software engineering and identifying strategies to address these problems.Method: Currently the experimental paradigms used in a selected set of domains are being examined along with the experimental protocols that they employ. Our aim is to identify those domains that have generally similar characteristics to software engineering and to study the strategies that they employ to overcome the lack of rigorous empirical protocols. We are also undertaking a series of systematic literature reviews to identify the factors that may limit their applicability in the software engineering domain.Conclusions: We have identified two domains that experience problems with experimental protocols that are similar to those occurring for software engineering, and will investigate these further to assess whether the approaches used to aggregate evidence in these domains can be adapted for use in software engineering. Our experiences from performing systematic literature reviews are positive, but reveal infrastructure problems caused by poor indexing of the literature.}
}

@article{rayyan-727968453,
  title={An overview of published agile studies: A systematic literature review},
  year={2010},
  issn={978-1-4503-0026-1},
  author={Hasnain, Eisha},
  url={https://doi.org/10.1145/1890810.1890813},
  publisher={Association for Computing Machinery},
  series={NSEC '10},
  keywords={agile methods, academics, empirical and experience, practitioners},
  abstract={In recent years, agile methods have become more popular in the software industry. Agile methods are a new approach compared to plan-driven approaches. This paper aims to provide a basis for the improvement of agile methods research through a systematic review of previous work. By doing this systematic literature review, I will help Software Engineer managers, Software Engineers, and researchers to determine the current state of research and knowledge in agile methods. My systematic approach to analyzing published studies enables us to find gaps in the existing body of work. I have reviewed 183 papers published in the 4 IEEE Agile conferences from 2003-2007. I found that fewer academics than practitioners publish their findings in these IEEE agile conferences; few empirical studies are presented at the conferences, and most papers at these conferences address agile methods only at a general level.}
}

@article{rayyan-727968454,
  title={Applicability of the semiotic inspection method: A systematic literature review},
  year={2011},
  issn={978-85-7669-257-7},
  pages={177-186},
  author={De S. Reis, Soraia and Prates, Raquel O},
  publisher={Brazilian Computer Society},
  series={IHC+CLIHC '11},
  keywords={systematic literature review, applicability, semiotic engineering, semiotic inspection method},
  abstract={In 2006 the Semiotic Inspection Method (SIM) was proposed and the authors raised the hypothesis that it was a technology and domain independent evaluation method. The aim of this study was to investigate whether published studies describing the use of SIM support this claim. In order to identify the papers to be analyzed a systematic literature review was conducted. Analysis of the papers indicated that SIM has been applied without adaptations to different domains and in each of them it has been able to identify issues specific to the domain. Hence, our findings support the claim that the method is independent of both domain and technology.}
}

@article{rayyan-727968455,
  title={Synthesizing evidence in software engineering research},
  year={2010},
  issn={978-1-4503-0039-1},
  author={Cruzes, Daniela S and Dyb, Tore},
  url={https://doi.org/10.1145/1852786.1852788},
  publisher={Association for Computing Machinery},
  series={ESEM '10},
  keywords={systematic reviews, evidence-based software engineering, research synthesis, qualitative, quantitative, mixed methods, Software},
  abstract={Synthesizing the evidence from a set of studies that spans many countries and years, and that incorporates a wide variety of research methods and theoretical perspectives, is probably the single most challenging task of performing a systematic review. In this paper, we perform a tertiary review to assess the types and methods of research synthesis in systematic reviews in software engineering. Almost half of the 31 studies included in our review did not contain any synthesis; of the ones that did, two thirds performed a narrative or a thematic synthesis. The results show that, despite the focus on systematic reviews, there is, currently, limited attention to research synthesis in software engineering. This needs to change and a repertoire of synthesis methods needs to be an integral part of systematic reviews to increase their significance and utility for research and practice.}
}

@article{rayyan-727968456,
  title={The use of systematic reviews in evidence based software engineering: A systematic mapping study},
  year={2014},
  issn={978-1-4503-2774-9},
  author={Santos, Ronnie E S and de Magalhães, Cleyton V C and da Silva, Fabio Q B},
  url={https://doi.org/10.1145/2652524.2652553},
  publisher={Association for Computing Machinery},
  series={ESEM '14},
  keywords={systematic literature review, software engineering, mapping study, evidence-based software engineering, Software},
  abstract={Context. A decade ago, Kitchenham, Dybå and Jørgensen argued that software engineering could benefit from an evidence-based research approach similar that that used in medicine, introducing the basis for Evidence Based Software Engineering (EBSE). Objective. Our main goal is to understand the evolution of the use of systematic reviews as the main research method in EBSE, as proposed by Kitchenham et al., by investigating primary and tertiary studies that explore any aspect, theory, or concept around the use of systematic reviews in software engineering. Method. A systematic mapping study protocol was used to find and selected studies about EBSE and systematic reviews in SE, published between 2004 and 2013. Results. We selected 52 unique papers classified as non-empirical studies (12), empirical studies (31), and tertiary studies (9). Conclusion. SLR has become an important component of software engineering research with nearly 200 unique reviews catalogued by the tertiary studies. Most important limitations are related to the industrial relevance and application of the results of reviews and the poor use of synthesis method to aggregate evidence}
}

@article{rayyan-727968457,
  title={Environment modeling in model-based testing: Concepts, prospects and research challenges: A systematic literature review},
  year={2015},
  issn={978-1-4503-3350-4},
  author={Siavashi, Faezeh and Truscan, Dragos},
  url={https://doi.org/10.1145/2745802.2745830},
  publisher={Association for Computing Machinery},
  series={EASE '15},
  keywords={systematic literature review, software testing, environment model, model-based testing},
  abstract={In this paper, we describe a systematic literature review (SLR) on the use of environment models in model-based testing (MBT). By applying selection criteria, we narrowed down the identified studies from two hundred ninety seven papers to sixty one papers which are used in this analysis. The results show that environment models are especially useful in testing systems with high complexity and nondeterministic behaviors in terms of facilitating automatic test generation. However, building environment models is not a trivial task due to the lack of a systematic methodology and of supporting tools for automation.}
}

@article{rayyan-727968458,
  title={Landscaping performance research at the ICPE and its predecessors: A systematic literature review},
  year={2015},
  issn={978-1-4503-3248-4},
  pages={91-96},
  author={Danciu, Alexandru and Kroß, Johannes and Brunnert, Andreas and Willnecker, Felix and Vögele, Christian and Kapadia, Anand and Krcmar, Helmut},
  url={https://doi.org/10.1145/2668930.2688039},
  publisher={Association for Computing Machinery},
  series={ICPE '15},
  keywords={systematic literature review, icpe, performance engineering, performance research, sipew, wosp, Intracranial Hypertension},
  abstract={This paper conducts a systematic literature review of papers published in the proceedings of the International Conference on Performance Engineering (ICPE) and its predecessors. It provides an overview of prevailing topics within the community over time. We look at research and contribution facets that have been used to address these topics. Trends are outlined in terms of evaluation methods to validate contributions. The results are complemented with a geographical and organizational dimension. The paper concludes with a look at the top ten contributing countries and organizations for this purpose.}
}

@article{rayyan-727968459,
  title={Awareness supporting technologies used in collaborative systems: A systematic literature review},
  year={2017},
  issn={978-1-4503-4335-0},
  pages={808-820},
  author={Lopez, Gustavo and Guerrero, Luis A},
  url={https://doi.org/10.1145/2998181.2998281},
  publisher={Association for Computing Machinery},
  series={CSCW '17},
  keywords={systematic literature review, awareness, cscw, notification mechanisms},
  abstract={Since the establishment of Computer Supported Collaborative Work as a research area, computer advances have change the paradigm of how technology is applied to improve the performance in collaborative scenarios. Notifications are an important part of this improvement. Technological systems have been applied in order to provide collaborators with the sufficient awareness to keep a task going. In this paper we present the protocol and results of a Systematic Literature Review that delves in the application of new technologies to provide awareness in collaborative systems. Moreover, we classify the collaborative systems found in literature using two traditional taxonomies for CSCW in order to understand which notification mechanisms are used to support which systems. Our review covers the last 10 years and classifies system prototypes based on the context in which they are applied, the notification and information gathering mechanism used, and the assessment performed. With over 400 papers reviewed, 83 that met the review requirements were included. The review results show that traditional interfaces and mobile devices are still the most common notification mechanisms. However, ubiquitous devices and non-traditional interfaces have also been used.}
}

@article{rayyan-727968460,
  title={Frameworks for collective intelligence: A systematic literature review},
  year={2020},
  journal={ACM Computing Surveys},
  issn={0360-0300},
  volume={53},
  number={1},
  author={Suran, Shweta and Pattanaik, Vishwajeet and Draheim, Dirk},
  url={https://doi.org/10.1145/3368986},
  keywords={systematic literature review, Web 2.0, crowdsourcing, Collective intelligence, human computer interaction, wisdom of crowds, Intelligence},
  abstract={Over the last few years, Collective Intelligence (CI) platforms have become a vital resource for learning, problem solving, decision-making, and predictions. This rising interest in the topic has to led to the development of several models and frameworks available in published literature. Unfortunately, most of these models are built around domain-specific requirements, i.e., they are often based on the intuitions of their domain experts and developers. This has created a gap in our knowledge in the theoretical foundations of CI systems and models, in general. In this article, we attempt to fill this gap by conducting a systematic review of CI models and frameworks, identified from a collection of 9,418 scholarly articles published since 2000. Eventually, we contribute by aggregating the available knowledge from 12 CI models into one novel framework and present a generic model that describes CI systems irrespective of their domains. We add to the previously available CI models by providing a more granular view of how different components of CI systems interact. We evaluate the proposed model by examining it with respect to six popular, ongoing CI initiatives available on the Web.}
}

@article{rayyan-727968461,
  title={Energy-efficient networking solutions in cloud-based environments: A systematic literature review},
  year={2015},
  journal={ACM Computing Surveys},
  issn={0360-0300},
  volume={47},
  number={4},
  author={Moghaddam, Fahimeh Alizadeh and Lago, Patricia and Grosso, Paola},
  url={https://doi.org/10.1145/2764464},
  keywords={systematic literature review, cloud, Energy efficiency, networking},
  abstract={The energy consumed by data centers hosting cloud services is increasing enormously. This brings the need to reduce energy consumption of different components in data centers. In this work, we focus on energy efficiency of the networking component. However, how different networking solutions impact energy consumption is still an open question. We investigate the state of the art in energy-efficient networking solutions in cloud-based environments. We follow a systematic literature review method to select primary studies. We create a metamodel based on the codes extracted from our primary studies using the Coding analytical method. Our findings show three abstraction levels of the proposed networking solutions to achieve energy efficiency in cloud-based environments: Strategy, Solution, and Technology. We study the historical trends in the investigated solutions and conclude that the emerging and most widely adopted one is the Decision framework.}
}

@article{rayyan-727968462,
  title={A systematic review of the use of requirements engineering techniques in model-driven development},
  year={2010},
  issn={3-642-16128-6},
  pages={213-227},
  author={Loniewski, Grzegorz and Insfran, Emilio and Abrahão, Silvia},
  publisher={Springer-Verlag},
  series={MODELS'10},
  keywords={systematic review, requirements engineering, model-driven development},
  abstract={Model-Driven Development (MDD) emphasizes the use of models at a higher abstraction level in the software development process and argues in favor of automation via model execution, transformation, and code generation. However, one current challenge is how to manage requirements during this process whilst simultaneously stressing the benefits of automation. This paper presents a systematic review of the current use of requirements engineering techniques in MDD processes and their actual automation level. 72 papers from the last decade have been reviewed from an initial set of 884 papers. The results show that although MDD techniques are used to a great extent in platform-independent models, platform-specific models, and at code level, at the requirements level most MDD approaches use only partially defined requirements models or even natural language. We additionally identify several research gaps such as a need for more efforts to explicitly deal with requirements traceability and the provision of better tool support.}
}

@article{rayyan-727968463,
  title={Strength of evidence in systematic reviews in software engineering},
  year={2008},
  issn={978-1-59593-971-5},
  pages={178-187},
  author={Dyb, Tore and Dings, Torgeir},
  url={https://doi.org/10.1145/1414004.1414034},
  publisher={Association for Computing Machinery},
  series={ESEM '08},
  keywords={systematic review, quality assessment, strength of evidence, Software},
  abstract={Systematic reviews are only as good as the evidence they are based on. It is important, therefore, that users of systematic reviews know how much confidence they can place in the conclusions and recommendations arising from such reviews. In this paper we present an overview of some of the most influential systems for assessing the quality of individual primary studies and for grading the overall strength of a body of evidence. We also present an example of the use of such systems based on a systematic review of empirical studies of agile software development. Our findings suggest that the systems used in other disciplines for grading the strength of evidence for and reporting of systematic reviews, especially those that take account of qualitative and observational studies are of particular relevance for software engineering.}
}

@article{rayyan-727968464,
  title={Software paradigms, assessment types and non-functional requirements in model-based integration testing: A systematic literature review},
  year={2014},
  issn={978-1-4503-2476-2},
  author={Häser, Florian and Felderer, Michael and Breu, Ruth},
  url={https://doi.org/10.1145/2601248.2601257},
  publisher={Association for Computing Machinery},
  series={EASE '14},
  keywords={systematic literature review, non-functional requirements, assessment types, model-based integration testing, Software},
  abstract={Context: In modern systems, like cyber-physical systems, where software and physical services are interacting, safety, security or performance play an important role. In order to guarantee the correct interoperability of such systems, with respect to functional and non-functional requirements, integration testing is an effective measure to achieve this. Model-based testing moreover not only enables early definition and validation, but also test automation. This makes it a good choice to overcome urgent challenges of integration testing. Objective: Many publications on model-based integration testing (MBIT) approaches can be found. Nevertheless, a study giving a systematic overview on the underlying software paradigms, measures for guiding the integration testing process as well as non-functional requirements they are suitable for, is missing. The aim of this paper is to find and synthesize the relevant primary studies to gain a comprehensive understanding of the current state of model-based integration testing. Method: For synthesizing the relevant studies, we conducted a systematic literature review (SLR) according to the guidelines of Kitchenham. Results: The systematic search and selection retrieved 83 relevant studies from which data has been extracted. Our review identified three assessment criteria for guiding the testing process, namely static metrics, dynamic metrics and stochastic &random. In addition it shows that just a small fraction considers non-functional requirements. Most approaches are for component-oriented systems. Conclusion: Results from the SLR show that there are two major research gaps. First, there is an accumulated need for approaches in the MBIT field that support non-functional requirements, as they are gaining importance. Second, means for steering the integration testing process, especially together with automation, need to evolve.}
}

@article{rayyan-727968465,
  title={A systematic review on mining techniques for crosscutting concerns},
  year={2013},
  issn={978-1-4503-1656-9},
  pages={1080-1087},
  author={Durelli, Rafael S and Santibáñez, Daniel S M and Anquetil, Nicolas and Delamaro, Márcio E and de Camargo, Valter Vieira},
  url={https://doi.org/10.1145/2480362.2480567},
  publisher={Association for Computing Machinery},
  series={SAC '13},
  keywords={systematic review, aspect mining, concern mining, cross-cutting concerns},
  abstract={¡u¿Background:¡/u¿ The several maintenance tasks a system is submitted during its life usually cause its architecture deviates from the original conceivable design, ending up with scattered and tangled concerns across the software. The research area named concern mining attempts to identify such scattered and tangled concerns to support maintenance and reverse-engineering. ¡u¿Objectives:¡/u¿ The aim of this paper is threefold: (i) identifying techniques employed in this research area, (ii) extending a taxonomy available on the literature and (iii) recommending an initial combination of some techniques. ¡u¿Results:¡/u¿ We selected 62 papers by their mining technique. Among these papers, we identified 18 mining techniques for crosscutting concern. Based on these techniques, we have extended a taxonomy available in the literature, which can be used to position each new technique, and to compare it with the existing ones along relevant dimensions. As consequence, we present some combinations of these techniques taking into account high values of precision and recall that could improve the identification of both Persistence and Observer concerns. The combination that we recommend may serve as a roadmap to potential users of mining techniques for crosscutting concerns.}
}

@article{rayyan-727968466,
  title={Taxonomy of performance testing tools: A systematic literature review},
  year={2020},
  issn={978-1-4503-6866-7},
  pages={1997-2004},
  author={Costa, Victor and Girardon, Gustavo and Bernardino, Maicon and Machado, Rodrigo and Legramante, Guilherme and Neto, Anibal and Basso, Fábio Paulo and de Macedo Rodrigues, Elder},
  url={https://doi.org/10.1145/3341105.3374006},
  publisher={Association for Computing Machinery},
  series={SAC '20},
  keywords={systematic review, taxonomy, performance testing tools},
  abstract={Background: The knowledge and application of tools to automate performance testing is essential to ensure software reliability and therefore its quality. Aims: To identify and characterize existing performance testing tools reported in the literature. Method: A protocol was formulated and executed according to the guidelines for performing systematic literature reviews in Software Engineering. Results: The performance testing tools were classified according to their relevance in the literature, highlighting the most commonly used tools, their supported input approaches, workload approaches, monitored metrics and logging strategies. From the analysis of these results a taxonomy on performance testing tools was proposed using a Feature Model. Conclusion: With the results of this study, it was possible to quantify and qualify research related to existing performance testing technologies in the literature, and also to characterize them for decision-making purposes. Thus, contributing for professionals, researchers and academic students looking for these assets through certain features from performance testing strategies.}
}

@article{rayyan-727968467,
  title={An investigation into inner source software development: Preliminary findings from a systematic literature review},
  year={2018},
  issn={978-1-4503-5936-8},
  author={Edison, Henry and Carroll, Noel and Conboy, Kieran and Morgan, Lorraine},
  url={https://doi.org/10.1145/3233391.3233529},
  publisher={Association for Computing Machinery},
  series={OpenSym '18},
  keywords={systematic literature review, open source, inner source, inner source software development, Software},
  abstract={Given the value and effectiveness of open source software development to date, practitioners are keen to replicate these practices inside their respective corporations. This application of open source practices inside the confines of a corporate entity has been coined inner source software development. However, while organisations have found ways to directly benefit from revenue streams as a result of leveraging open source practices internally, the current research on inner source is scattered among different areas. Thus gaining clarity on the state-of-the-art in inner source research is challenging. In particular, there is no systematic literature review of known research to date on inner source. We address this challenge by presenting a systematic literature review that identifies, critically evaluates and integrates the findings of 29 primary studies on inner source. Case study approach is the common research approach undertaken in the area. We also identified 8 frameworks/methods, models and tools proposed in the literature to support inner source, as well as a set of benefits and challenges associated with inner source. We envision future work to perform deeper analysis and synthesis on the empirical research on inner source software development.}
}

@article{rayyan-727968468,
  title={Natural language processing in business process identification and modeling: A systematic literature review},
  year={2018},
  issn={978-1-4503-6559-8},
  author={de Almeida Bordignon, Ana Cláudia and Thom, Lucinéia Heloisa and Silva, Thanner Soares and Dani, Vinicius Stein and Fantinato, Marcelo and Ferreira, Renato Cesar Borges},
  url={https://doi.org/10.1145/3229345.3229373},
  publisher={Association for Computing Machinery},
  series={SBSI'18},
  keywords={Systematic Literature Review, Natural Language Processing, Business Process Management, Process Discovery, Process Analysis},
  abstract={Business Process Management (BPM) has been receiving increasing attention in recent years. Many organizations have been adapting their business to a process-centered view since they started noticing its potential to reduce costs, improve productivity and achieve higher levels of quality. However, implementing BPM in organizations requires time, making the automation of process identification and discovery highly desirable. To achieve this expectation, the application of Natural Language Processing (NLP) techniques and tools has emerged to generate process models from unstructured text. In this paper, we provide the results of a systematic literature review conducted in preparation and processing of natural language text aiming the extraction of business processes and process quality assurance. The study presents techniques applied to the BPM life-cycle phases of process identification, process discovery and process analysis as well as tools to support process discovery. This review covered papers from 2009 up to 2016 and identifies 518 articles of which 33 were selected as relevant to our work. The results of the present study may be valuable to support research in extraction of business process models from natural language text.}
}

@article{rayyan-727968469,
  title={Software process simulation over the past decade: Trends discovery from a systematic review},
  year={2008},
  issn={978-1-59593-971-5},
  pages={345-347},
  author={Zhang, He and Kitchenham, Barbara and Pfahl, Dietmar},
  url={https://doi.org/10.1145/1414004.1414077},
  publisher={Association for Computing Machinery},
  series={ESEM '08},
  keywords={systematic literature review, software process simulation, Software},
  abstract={Software Process Simulation (SPS) research has increased since 1998 when the first ProSim Workshop was held. This paper aims to reveal how SPS has evolved during the past 10 years based on the preliminary results from the systematic literature review of SPS publications from 1998 to 2007. Trends over the period showed that interest in continuous modelling was decreasing and interest in micro-processes was increasing. Hybrid models were based primarily on system dynamics and discrete event simulation and were all implemented by vertical integration.}
}

@article{rayyan-727968470,
  title={A systematic review of theory use in studies investigating the motivations of software engineers},
  year={2009},
  journal={ACM Trans. Softw. Eng. Methodol.},
  issn={1049-331X},
  volume={18},
  number={3},
  author={Hall, Tracy and Baddoo, Nathan and Beecham, Sarah and Robinson, Hugh and Sharp, Helen},
  url={https://doi.org/10.1145/1525880.1525883},
  keywords={software engineering, Motivation, Software},
  abstract={Motivated software engineers make a critical contribution to delivering successful software systems. Understanding the motivations of software engineers and the impact of motivation on software engineering outcomes could significantly affect the industry's ability to deliver good quality software systems. Understanding the motivations of people generally in relation to their work is underpinned by eight classic motivation theories from the social sciences. We would expect these classic motivation theories to play an important role in developing a rigorous understanding of the specific motivations of software engineers. In this article we investigate how this theoretical basis has been exploited in previous studies of software engineering. We analyzed 92 studies of motivation in software engineering that were published in the literature between 1980 and 2006. Our main findings are that many studies of software engineers' motivations are not explicitly underpinned by reference to the classic motivation theories. Furthermore, the findings presented in these studies are often not explicitly interpreted in terms of those theories, despite the fact that in many cases there is a relationship between those findings and the theories. Our conclusion is that although there has been a great deal of previous work looking at motivation in software engineering, the lack of reference to classic theories of motivation means that the current body of work in the area is weakened and our understanding of motivation in software engineering is not as rigorous as it may at first appear. This weakness in the current state of knowledge highlights important areas for future researchers to contribute towards developing a rigorous and usable body of knowledge in motivating software engineers.}
}

@article{rayyan-727968471,
  title={Collaborative learning as educational strategy for deaf children: A systematic literature review},
  year={2017},
  issn={978-1-4503-5229-1},
  author={Aristizábal, Leandro Flórez and Cano, Sandra and Collazos, César A and Solano, Andrés and Slegers, Karin},
  url={https://doi.org/10.1145/3123818.3123830},
  publisher={Association for Computing Machinery},
  series={Interacción '17},
  keywords={systematic review, education, cooperative learning, deaf children, Only Child, Child, Deafness},
  abstract={The education of people with disabilities requires special attention and the use of teaching and learning strategies that can be adapted to every particular disability. This study focuses on the education of deaf children as part of a larger project that aims to mix teaching strategies like Logogenia and Fitzgerald Key with interactive storytelling and collaborative learning to support literacy teaching to these children. Since deaf people learn using the visual channel as main input, we believe that technology could play a key role in the development of such environments where user interfaces should be specifically designed to attract children's attention. We conducted a systematic literature review in order to find what researchers have done to apply Collaborative or Cooperative Learning in the education of deaf children and also what kind of emerging technologies are used to enhance collaborative environments. A total of 229 studies were found in 7 different databases. The results of this study show that Collaborative Learning has been used along with different kinds of technology in the education of deaf people with positive outcomes like improving skills in sign language, literacy and communication.}
}

@article{rayyan-727968472,
  title={The state of the art on secure software engineering: A systematic mapping study},
  year={2020},
  issn={978-1-4503-7731-7},
  pages={487-492},
  author={Khan, Rafiq A and Khan, Siffat U and Ilyas, Muhammad and Idris, Mohd Y},
  url={https://doi.org/10.1145/3383219.3383290},
  publisher={Association for Computing Machinery},
  series={EASE '20},
  keywords={Software Engineering, Systematic mapping study, Software Development Life Cycle, Software Security, Software},
  abstract={Secure Software Development (SSD) is becoming a major challenge, due to the increasing complexity, openness and extensibility of Information and Communication Technologies (ICTs). These make the overall security requirements analysis very difficult. Many techniques have been theoretically developed, however, there is a lack of empirical evidence of its application in building secure software system. A Systematic Mapping Study (SMS) has been conducted in this paper to examine the existence of software security frameworks, models and methods. In total, we selected 116 primary studies. After examining the selected studies, we identified 37 Secure Software Engineering (SSE) paradigms/frameworks/models. The results show that the most frequently used SSE frameworks/models are "Microsoft Software Development Life Cycle (MS-SDL)", "Misuse case modeling", "Abuse case modeling", "Knowledge Acquisition for Automated Specification", "System Security Engineering-Capability Maturity Model (SSE-CMM)" and "Secure Tropos Methodology". This work will help organizations in the development of software to better understand existing security initiatives used in the development of secure software. It can also provide researchers with a basis for designing and developing new methods of software security and identifying new axis of research.}
}

@article{rayyan-727968473,
  title={User involvement in software development and system success: A systematic literature review},
  year={2013},
  issn={978-1-4503-1848-8},
  pages={125-130},
  author={Bano, Muneera and Zowghi, Didar},
  url={https://doi.org/10.1145/2460999.2461017},
  publisher={Association for Computing Machinery},
  series={EASE '13},
  keywords={software development, system success, user involvement, Software},
  abstract={Context: In the last four decades involving users in the software development process is claimed to have a positive impact on the success of that software. However, previous reviews on this topic have produced conflicting results. Objectives: Our aim is to present a review on user involvement in software development process and investigate its relationship to software system success. Methods: For our exploration, we performed a Systematic Literature Review using the guidelines provided in the Evidence Based Software Engineering literature. Results: 87 relevant empirical studies were selected and reviewed that investigate various perspectives and concepts of user involvement in software development process during the period of 1980–2012. Among 87 studies reviewed, 59 report that user involvement positively contributes to system success, 7 suggest a negative contribution and 21 are uncertain. Conclusions: Our results show an overall positive impact of user involvement on system success. It also suggests that the relationship between user involvement and system success is neither direct nor simple, and it depends on many different factors and conditions surrounding systems development processes.}
}

@article{rayyan-727968474,
  title={Challenges and recommendations in DevOps education: A systematic literature review},
  year={2020},
  issn={978-1-4503-8753-8},
  pages={648-657},
  author={Fernandes, Marcelo and Ferino, Samuel and Kulesza, Uirá and Aranha, Eduardo},
  url={https://doi.org/10.1145/3422392.3422496},
  publisher={Association for Computing Machinery},
  series={SBES '20},
  keywords={DevOps, challenges, education, courses, recommendations},
  abstract={Over the last years, DevOps has gained more importance and attention from the software industry, given its role in enabling continuous software delivery. As a new area, DevOps has brought significant challenges for the academy, both in terms of research topics and teaching strategies. In this paper, we present a systematic literature review that aims to identify challenges and recommendations for teaching DevOps. Our findings show a total of 73 challenges and 85 recommendations organized into different seven categories from a total of 18 papers selected. We also discuss how existing recommendations address the challenges found in the study, thus contributing to the preparation and execution of DevOps courses. Finally, we investigate if challenges and recommendations are specific for teaching DevOps.}
}

@article{rayyan-727968475,
  title={Realising evidence-based software engineering},
  year={2005},
  issn={1-59593-121-X},
  pages={1-3},
  author={Kitchenham, Barbara and Budgen, David and Brereton, Pearl and Linkman, Step Hen},
  url={https://doi.org/10.1145/1083174.1083175},
  publisher={Association for Computing Machinery},
  series={REBSE '05},
  keywords={evidence, Software},
  abstract={This paper provides an introduction to the papers for the Workshop on Realising Evidence-Based Software Engineering.}
}

@article{rayyan-727968476,
  title={Realising evidence-based software engineering},
  year={2005},
  journal={SIGSOFT Softw. Eng. Notes},
  issn={0163-5948},
  volume={30},
  number={4},
  pages={1-3},
  author={Kitchenham, Barbara and Budgen, David and Brereton, Pearl and Linkman, Step Hen},
  url={https://doi.org/10.1145/1082983.1083175},
  keywords={evidence, Software},
  abstract={This paper provides an introduction to the papers for the Workshop on Realising Evidence-Based Software Engineering.}
}

@article{rayyan-727968477,
  title={Flipped classroom in software engineering: A systematic mapping study},
  year={2020},
  issn={978-1-4503-8753-8},
  pages={720-729},
  author={Veras, Nécio L and Rocha, Lincoln S and Viana, Windson},
  url={https://doi.org/10.1145/3422392.3422490},
  publisher={Association for Computing Machinery},
  series={SBES '20},
  keywords={software engineering education, active learning, flipped classroom, Software},
  abstract={Context. Software Engineering (SE) teaching is evolving continually, with new methods being developed and evaluated. In this sense, it is important to gain more knowledge of how such methods are actually implemented. Objective. The aim of this study is to systematically examine the literature on the use of the flipped classroom method in SE teaching. Method. To achieve the study objective, we conducted a Systematic Mapping Study (SMS) starting with 769 studies. After the filtering process, we extracted data from 26 primaries studies, which meet the study selection criteria. Results. We found papers from 2008 to 2020, most of them published in SE conferences. In fifteen papers, the content is delivered to the students before class, nine of them using a specific system developed to this task. We found that the in-class activities follow three main strategies: (1) project-based learning (38.3%); (2) problem-based learning and self-direct learning (50.0%); and (3) team-based learning (7.7%). Reviewed studies reported challenges in implementing FC in ES course such as overworked and time-constrained professors and difficulty in sustaining student motivation. Also, we found studies reporting improvements in student learning and motivation Conclusion. Based on our findings, we conclude the use of an active method has proved to be useful for in-class practical activities, especially related to the software development field. We also observed that adaptive educational content delivery has not been explored in software engineering studies with flipped classes.}
}

@article{rayyan-727968478,
  title={How has the health of software ecosystems been evaluated? A systematic review},
  year={2017},
  issn={978-1-4503-5326-7},
  pages={14-23},
  author={da Silva Amorim, Simone and Neto, Félix Simas S and McGregor, John D and de Almeida, Eduardo Santana and von Flach G. Chavez, Christina},
  url={https://doi.org/10.1145/3131151.3131174},
  publisher={Association for Computing Machinery},
  series={SBES'17},
  keywords={Systematic Literature Review, Software Evaluation, Software Ecosystems Health, Software},
  abstract={The health of the software ecosystems concerns to the growing and continuity to exist remaining variable and productive over time. Research on this area is becoming more important. Even today, no studies have been available summarizing the research on evaluation approaches for the health of software ecosystems. The objective of this study is to structure and analyze the available literature on this field identifying the state-of-the-art of the research. We conducted a systematic literature review to obtain an overview of the existing studies in this area. 23 studies were selected as primary studies by applying inclusion, exclusion and quality criteria. The findings show that the research area is quite immature. There are few approaches and tools to support the evaluation work. In these studies, only 3 reported a complete evaluation of the health of ecosystems, 5 studies were considered as initial proposals, and the others evaluated the health partially.}
}

@article{rayyan-727968479,
  title={An analysis of the empirical software engineering over the last 10 editions of brazilian software engineering symposium},
  year={2017},
  issn={978-1-4503-5326-7},
  pages={44-53},
  author={Monteiro, Davi and Gadelha, Rômulo and Alencar, Thayse and Neves, Bruno and Yeltsin, Italo and Gomes, Thiago and Cortés, Mariela},
  url={https://doi.org/10.1145/3131151.3131158},
  publisher={Association for Computing Machinery},
  series={SBES'17},
  keywords={Quality assessment, Empirical Software Engineering, Empirical Evaluation, Research Protocol, Software},
  abstract={Empirical evaluations developed in the software engineering area have been widely applied as a formalism to validate and ensure the credibility of the works proposed by the researchers. Even though the adoption of empirical evaluation techniques has gained popularity in recent years, its application has been questioned both qualitatively and quantitatively. This study aims at analyzing how empirical software engineering research has evolved in the Brazilian Symposium on Software Engineering (SBES) community. We performed a controlled quasi-experiment, using published papers over the last 10 years in SBES. Our experiment was divided into two phases: classification by type and quality assessment of the main empirical types. In the first phase, the sample was 201 papers; in the second one, the sample decreased to 126 papers. The results have shown failures and gaps in the application of empirical methods when assessing the quality of the Software Engineering works. We believe that we can contribute to improve how the studies were conducted and consequently help to produce more reliable results, reducing or eliminating biases: an important qualitative factor in scientific work. In addition, due to the lack of assessment supporting tools, we developed a theoretical protocol to support the assessment process and proposed improvements for papers that obtained below-expected rates.}
}

@article{rayyan-727968480,
  title={A systematic review of cloud modeling languages},
  year={2018},
  journal={ACM Computing Surveys},
  issn={0360-0300},
  volume={51},
  number={1},
  author={Bergmayr, Alexander and Breitenbücher, Uwe and Ferry, Nicolas and Rossini, Alessandro and Solberg, Arnor and Wimmer, Manuel and Kappel, Gerti and Leymann, Frank},
  url={https://doi.org/10.1145/3150227},
  keywords={Cloud computing, domain-specific languages, modeling},
  abstract={Modern cloud computing environments support a relatively high degree of automation in service provisioning, which allows cloud service customers (CSCs) to dynamically acquire services required for deploying cloud applications. Cloud modeling languages (CMLs) have been proposed to address the diversity of features provided by cloud computing environments and support different application scenarios, such as migrating existing applications to the cloud, developing new cloud applications, or optimizing them. There is, however, still much debate in the research community on what a CML is, and what aspects of a cloud application and its target cloud computing environment should be modeled by a CML. Furthermore, the distinction between CMLs on a fine-grain level exposing their modeling concepts is rarely made. In this article, we investigate the diverse features currently provided by existing CMLs. We classify and compare them according to a common framework with the goal to support CSCs in selecting the CML that fits the needs of their application scenario and setting. As a result, not only features of existing CMLs are pointed out for which extensive support is already provided but also in which existing CMLs are deficient, thereby suggesting a research agenda.}
}

@article{rayyan-727968481,
  title={Capturing cost avoidance through reuse: Systematic literature review and industrial evaluation},
  year={2016},
  issn={978-1-4503-3691-8},
  author={Irshad, Mohsin and Torkar, Richard and Petersen, Kai and Afzal, Wasif},
  url={https://doi.org/10.1145/2915970.2915989},
  publisher={Association for Computing Machinery},
  series={EASE '16},
  keywords={cost avoidance, cost savings, software reuse},
  abstract={Background: Cost avoidance through reuse shows the benefits gained by the software organisations when reusing an artefact. Cost avoidance captures benefits that are not captured by cost savings e.g. spending that would have increased in the absence of the cost avoidance activity. This type of benefit can be combined with quality aspects of the product e.g. costs avoided because of defect prevention. Cost avoidance is a key driver for software reuse. Objectives: The main objectives of this study are: (1) To assess the status of capturing cost avoidance through reuse in the academia; (2) Based on the first objective, propose improvements in capturing of reuse cost avoidance, integrate these into an instrument, and evaluate the instrument in the software industry. Method: The study starts with a systematic literature review (SLR) on capturing of cost avoidance through reuse. Later, a solution is proposed and evaluated in the industry to address the shortcomings identified during the systematic literature review. Results: The results of a systematic literature review describe three previous studies on reuse cost avoidance and show that no solution, to capture reuse cost avoidance, was validated in industry. Afterwards, an instrument and a data collection form are proposed that can be used to capture the cost avoided by reusing any type of reuse artefact. The instrument and data collection form (describing guidelines) were demonstrated to a focus group, as part of static evaluation. Based on the feedback, the instrument was updated and evaluated in industry at 6 development sites, in 3 different countries, covering 24 projects in total. Conclusion: The proposed solution performed well in industrial evaluation. With this solution, practitioners were able to do calculations for reuse costs avoidance and use the results as decision support for identifying potential artefacts to reuse.}
}

@article{rayyan-727968482,
  title={A systematic review of design diversity-based solutions for fault-tolerant SOAs},
  year={2013},
  issn={978-1-4503-1848-8},
  pages={107-118},
  author={Nascimento, Amanda S and Rubira, Cecília M F and Burrows, Rachel and Castor, Fernando},
  url={https://doi.org/10.1145/2460999.2461015},
  publisher={Association for Computing Machinery},
  series={EASE '13},
  keywords={SLR, SOA, composite services, fault tolerance},
  abstract={Context: Over recent years, software developers have been evaluating the benefits of both Service-Oriented Architecture and software fault tolerance techniques based on design diversity by creating fault-tolerant composite services that leverage functionally equivalent services, or variant services. Three major design issues need to be considered while building software fault-tolerant architectures based on design diversity namely, selection and execution of variants and selection of an adjudication algorithm to determine the correct or adjudicated result from the variants. Each design issue, in turn, can be realized by a set of alternative design solutions, which present different degrees of quality requirements (e.g. memory consumption and reliability). Objective: To investigate whether existing approaches for fault-tolerant composite services support the above mentioned design issues and to provide a detailed classification of the analysed approaches. Method: A systematic literature review of diversity-based approaches for fault-tolerant composite services, which compose our primary studies. Results: We found 17 primary studies providing direct evidence about the research question. Our findings reveal that the primary studies support a wide variety of design decisions. For example, (i) variant services may be chosen at different points during the software lifecycle; (ii) both parallel and sequential execution schemes have been addressed; and (iii) a variety of adjudication mechanisms were found amongst the target papers. Conclusion: We build up a broad picture of what design issues have been addressed by existing diversity-based approaches for fault-tolerant composite services. Finally, practical issues and difficulties are summarized and directions for future work are suggested.}
}

@article{rayyan-727968483,
  title={Empirical evaluations of regression test selection techniques: A systematic review},
  year={2008},
  issn={978-1-59593-971-5},
  pages={22-31},
  author={Engström, Emelie and Skoglund, Mats and Runeson, Per},
  url={https://doi.org/10.1145/1414004.1414011},
  publisher={Association for Computing Machinery},
  series={ESEM '08},
  keywords={systematic review, regression testing, test selection},
  abstract={Regression testing is the verification that previously functioning software remains after a change. In this paper we report on a systematic review of empirical evaluations of regression test selection techniques, published in major software engineering journals and conferences. Out of 2,923 papers analyzed in this systematic review, we identified 28 papers reporting on empirical comparative evaluations of regression test selection techniques. They report on 38 unique studies (23 experiments and 15 case studies), and in total 32 different techniques for regression test selection are evaluated. Our study concludes that no clear picture of the evaluated techniques can be provided based on existing empirical evidence, except for a small group of related techniques. Instead, we identified a need for more and better empirical studies were concepts are evaluated rather than small variations. It is also necessary to carefully consider the context in which studies are undertaken.}
}

@article{rayyan-727968484,
  title={Process simulation for software engineering education},
  year={2015},
  issn={978-1-4503-3346-7},
  pages={147-156},
  author={Jiang, Shu and Zhang, He and Gao, Chao and Shao, Dong and Rong, Guoping},
  url={https://doi.org/10.1145/2785592.2785606},
  publisher={Association for Computing Machinery},
  series={ICSSP 2015},
  keywords={software engineering education, process simulation, computer game, non-game simulation, Software},
  abstract={Training and learning are one important purpose of Software Process Simulation (SPS). Some previous reviews showed a noticeable number of studies that combine SPS and Soft- ware Engineering Education (SEE). The objective of this research is to present the latest state-of-the-art of this area, and more importantly provide practical support for the effective adoption of SPS in educational context. We conducted an extended Systematic Literature Review (SLR) based on our previous reviews. The review identified 42 primary studies from 1992 to 2013. This paper presents the preliminary results by answering the research questions. The overall findings confirmed the positive impact of SPS on education. The detailed discussions and recommendations may offer reference value to the community.}
}

@article{rayyan-727968485,
  title={Data quality in empirical software engineering: A targeted review},
  year={2013},
  issn={978-1-4503-1848-8},
  pages={171-176},
  author={Bosu, Michael Franklin and MacDonell, Stephen G},
  url={https://doi.org/10.1145/2460999.2461024},
  publisher={Association for Computing Machinery},
  series={EASE '13},
  keywords={literature review, empirical software engineering, data quality, data sets, Research Design, Software},
  abstract={Context: The utility of prediction models in empirical software engineering (ESE) is heavily reliant on the quality of the data used in building those models. Several data quality challenges such as noise, incompleteness, outliers and duplicate data points may be relevant in this regard. Objective: We investigate the reporting of three potentially influential elements of data quality in ESE studies: data collection, data pre-processing, and the identification of data quality issues. This enables us to establish how researchers view the topic of data quality and the mechanisms that are being used to address it. Greater awareness of data quality should inform both the sound conduct of ESE research and the robust practice of ESE data collection and processing. Method: We performed a targeted literature review of empirical software engineering studies covering the period January 2007 to September 2012. A total of 221 relevant studies met our inclusion criteria and were characterized in terms of their consideration and treatment of data quality. Results: We obtained useful insights as to how the ESE community considers these three elements of data quality. Only 23 of these 221 studies reported on all three elements of data quality considered in this paper. Conclusion: The reporting of data collection procedures is not documented consistently in ESE studies. It will be useful if data collection challenges are reported in order to improve our understanding of why there are problems with software engineering data sets and the models developed from them. More generally, data quality should be given far greater attention by the community. The improvement of data sets through enhanced data collection, pre-processing and quality assessment should lead to more reliable prediction models, thus improving the practice of software engineering.}
}

